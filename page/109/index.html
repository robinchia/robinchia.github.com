
<!DOCTYPE HTML>
<html>
<head>
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<title>The 109 Page | It so life</title>
<meta name="author" content="RobinChia">

<meta name="description" content="It so life">


<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">


<meta property="og:site_name" content="It so life"/>

<!--[if IE]><style>.testIE.IE{display:inline;}</style><![endif]-->
<!--[if lte IE 7]><link rel="stylesheet" href="/css/ie7.css" type="text/css"><![endif]-->
<!--[if (lt IE 9)&(gt IE 7)]><style>.testIE.IE8{display:inline;}</style><![endif]-->
<!--[if gt IE 8]><style>.testIE.IE9{display:inline;}</style><![endif]-->

<link href="/favicon.png" rel="icon">
<link rel="alternate" href="/atom.xml" title="It so life Feed" type="application/atom+xml">

<link rel="stylesheet" href="http://cdn.staticfile.org/twitter-bootstrap/3.1.0/css/bootstrap.min.css" type="text/css">
<link rel="stylesheet" href="http://cdn.bootcss.com/font-awesome/4.0.3/css/font-awesome.min.css" type="text/css">
<link rel="stylesheet" href="http://cdn.bootcss.com/prettify/r298/prettify.min.css" type="text/css">
<link rel="stylesheet" href="http://cdn.bootcss.com/fancybox/2.1.5/jquery.fancybox.min.css" type="text/css">
<link rel="stylesheet" href="/css/style.css" type="text/css">
<!--[if lt IE 9]>
   <style>article,aside,dialog,footer,header,section,footer,nav,figure,menu{display:block}</style>
   <script src="http://cdn.staticfile.org/html5shiv/r29/html5.min.js"></script>
   <script src="http://cdn.staticfile.org/respond.js/1.4.2/respond.min.js"></script>
<link href="http://cdn.staticfile.org/respond.js/1.4.2/respond-proxy.html" id="respond-proxy" rel="respond-proxy" />
<link href="/js/respond.proxy.gif" id="respond-redirect" rel="respond-redirect" />
<script src="/js/respond.proxy.js"></script>
   <script src="http://cdn.bootcss.com/selectivizr/1.0.2/selectivizr-min.js"></script>
<![endif]-->
<script type="text/javascript">
function loadjs(c,d){var a=document.createElement("script");a.async=!0;a.type="text/javascript";a.src=c;a.charset=d||"gbk";(document.getElementsByTagName("HEAD")[0]||document.getElementsByTagName("BODY")[0]).appendChild(a)};
var _protocol = (("https:" == document.location.protocol) ? " https://" : " http://");
var _js2load = [];
</script>

</head>
<body>
      <header id="header" class="container"><nav id="main-nav" class="navbar navbar-default navbar-fixed-top " role="navigation">
  <div class="container">
    <div class="navbar-header">
    <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".navbar-collapse">
      <span class="sr-only">Toggle navigation</span>
      <span class="icon-bar"></span>
      <span class="icon-bar"></span>
      <span class="icon-bar"></span>
    </button>
    <a class="navbar-brand" href="/">It so life</a>
    </div>
    <div  class="collapse navbar-collapse">
      <ul  class="nav navbar-nav">
  
        <li><a href="/" title="Home">Home</a></li>      
        <li><a href="/about/" title="About">About</a></li>      
        <li><a href="/archives/" title="Archives">Archives</a></li>      
      <li class='dropdown'>
        <a class='dropdown-toggle' data-toggle='dropdown' href='#'>website<b class='caret'></b></a>
        <ul class='dropdown-menu pure-menu-selected'>
    
          <li><a href="//groups.google.com/forum/#!forum/pongba" title="TopLanguage">TopLanguage</a></li>    
          <li><a href="//itpub.net/" title="ITPub">ITPub</a></li>    
          <li><a href="//blog.jobbole.com/" title="Bole">Bole</a></li>    
          <li><a href="//nosql-database.org/" title="nosql">nosql</a></li>    
          <li><a href="//gitimmersion.googol.im/" title="Git">Git</a></li>    
        </ul>
      </li>
    
      </ul>
      <ul class="nav navbar-nav navbar-right">
      
        <li><a href="/atom.xml">RSS</a></li>
      
      
        <li><a href="https://twitter.com/robinchia">twitter</a></li>
      
      
      
      
        <li><a href="https://github.com/robinchia">github</a></li>
      
      </ul>
    </div>
  </div>
</nav>
<div class="clearfix"></div>
</header>
  <div id='content' class="container">
     <div class="page-header-wrapper">
      <!--[if lt IE 9]><div class="alert alert-warning alert-dismissable"><button type="button" class="close" data-dismiss="alert" aria-hidden="true">&times;</button><strong>提示:</strong>您的浏览器版本太低了,建议升级到 <strong><a href="http://windows.microsoft.com/zh-cn/internet-explorer/download-ie" title="IE9">IE9</a></strong> 以上,本站使用<a href="https://www.google.com/intl/zh-CN/chrome/">Chrome浏览器</a>可以获得最好的显示效果.</div><![endif]-->
      <div class="page-header"><h1 align="center"><big>It so life</big> </h1>
        <h5 align="center"><big>love as life</big></h5>
      </div>
     </div>
     <div class="row">
       <div id="main-col" class="alignleft col-sx-12 col-sm-8 col-md-9 col-lg-9">
      <section id='header_widget'></section>
          <div id="wrapper"><article>
<div id="post" class="post well">
  <div class="post-content">
    <header class="well-sm">
      <i class="fa icon fa-5x pull-left"></i>
      <h1 class="title"><a href="/2014/02/02/2014-02-02-hadoop--Hadoop121编译Eclipse插件/">Hadoop 1.2.1编译Eclipse插件</a></h1>
      
        <span>Posted on<time datetime="2014-02-02T01:54:36.000Z"> <a href="/2014/02/02/2014-02-02-hadoop--Hadoop121编译Eclipse插件/">feb. 2 2014</a></time></span>      
    </header>
    
    
    <div class="entry">
      
        <h1 id="hadoop-1-2-1-eclipse-">Hadoop 1.2.1编译Eclipse插件</h1>
<h2 id="-src-contrib-eclipse-plugin-build-xml">/src/contrib/eclipse-plugin/build.xml</h2>
<h3 id="1-ivy-download-">1）取消ivy-download：</h3>
<p><a href="http://static.oschina.net/uploads/space/2013/0520/194613_IBLw_256028.png" target="_blank"><img src="" alt=""></a></p>
<h3 id="2-plugin-jar-">2）添加将要打包到plugin中的第三方jar包列表：</h3>
<p>01</p>
<!-- Override jar target to specify manifest -->


<p>02</p>
<p>&lt;</p>
<p>target</p>
<p>name</p>
<p>=</p>
<p>&quot;jar&quot;</p>
<p>depends</p>
<p>=</p>
<p>&quot;compile&quot;</p>
<p>unless</p>
<p>=</p>
<p>&quot;skip.contrib&quot;</p>
<p>&gt;
03</p>
<p>&lt;</p>
<p>mkdir</p>
<p>dir</p>
<p>=</p>
<p>&quot;${build.dir}/lib&quot;</p>
<p>/&gt; </p>
<p>04</p>
<p>05</p>
<!-- 自定义的修改内容：begin -->


<p>06</p>
<!--
07

<copy file="${hadoop.root}/build/hadoop-core-${version}.jar"

08



tofile="${build.dir}/lib/hadoop-core.jar" verbose="true"/>
09

<copy file="${hadoop.root}/build/ivy/lib/Hadoop/common/commons-cli-${commons-cli.version}.jar" 

10



todir="${build.dir}/lib" verbose="true"/> 
11

-->


<p>12</p>
<p>&lt;</p>
<p>copy</p>
<p>file</p>
<p>=</p>
<p>&quot;${hadoop.root}/hadoop-core-${version}.jar&quot;</p>
<p>tofile</p>
<p>=</p>
<p>&quot;${build.dir}/lib/hadoop-core.jar&quot;</p>
<p>verbose</p>
<p>=</p>
<p>&quot;true&quot;</p>
<p>/&gt;
13</p>
<p>&lt;</p>
<p>copy</p>
<p>file</p>
<p>=</p>
<p>&quot;${hadoop.root}/lib/commons-cli-1.2.jar&quot;</p>
<p>todir</p>
<p>=</p>
<p>&quot;${build.dir}/lib&quot;</p>
<p>verbose</p>
<p>=</p>
<p>&quot;true&quot;</p>
<p>/&gt; </p>
<p>14</p>
<p>&lt;</p>
<p>copy</p>
<p>file</p>
<p>=</p>
<p>&quot;${hadoop.root}/lib/commons-configuration-1.6.jar&quot;</p>
<p>todir</p>
<p>=</p>
<p>&quot;${build.dir}/lib&quot;</p>
<p>verbose</p>
<p>=</p>
<p>&quot;true&quot;</p>
<p>/&gt; 
15</p>
<p>&lt;</p>
<p>copy</p>
<p>file</p>
<p>=</p>
<p>&quot;${hadoop.root}/lib/commons-httpclient-3.0.1.jar&quot;</p>
<p>todir</p>
<p>=</p>
<p>&quot;${build.dir}/lib&quot;</p>
<p>verbose</p>
<p>=</p>
<p>&quot;true&quot;</p>
<p>/&gt; </p>
<p>16</p>
<p>&lt;</p>
<p>copy</p>
<p>file</p>
<p>=</p>
<p>&quot;${hadoop.root}/lib/commons-lang-2.4.jar&quot;</p>
<p>todir</p>
<p>=</p>
<p>&quot;${build.dir}/lib&quot;</p>
<p>verbose</p>
<p>=</p>
<p>&quot;true&quot;</p>
<p>/&gt;<br>17</p>
<p>&lt;</p>
<p>copy</p>
<p>file</p>
<p>=</p>
<p>&quot;${hadoop.root}/lib/jackson-core-asl-1.8.8.jar&quot;</p>
<p>todir</p>
<p>=</p>
<p>&quot;${build.dir}/lib&quot;</p>
<p>verbose</p>
<p>=</p>
<p>&quot;true&quot;</p>
<p>/&gt;</p>
<p>18</p>
<p>&lt;</p>
<p>copy</p>
<p>file</p>
<p>=</p>
<p>&quot;${hadoop.root}/lib/jackson-mapper-asl-1.8.8.jar&quot;</p>
<p>todir</p>
<p>=</p>
<p>&quot;${build.dir}/lib&quot;</p>
<p>verbose</p>
<p>=</p>
<p>&quot;true&quot;</p>
<p>/&gt;<br>19</p>
<!-- 自定义的修改内容：end -->


<p>20</p>
<p>&lt;</p>
<p>jar
21</p>
<p>jarfile</p>
<p>=</p>
<p>&quot;${build.dir}/hadoop-${name}-${version}.jar&quot;</p>
<p>22</p>
<p>manifest</p>
<p>=</p>
<p>&quot;${root}/META-INF/MANIFEST.MF&quot;</p>
<blockquote>
<p>23</p>
</blockquote>
<p>&lt;</p>
<p>fileset</p>
<p>dir</p>
<p>=</p>
<p>&quot;${build.dir}&quot;</p>
<p>includes</p>
<p>=</p>
<p>&quot;classes/ lib/&quot;</p>
<p>/&gt; </p>
<p>24</p>
<p>&lt;</p>
<p>fileset</p>
<p>dir</p>
<p>=</p>
<p>&quot;${root}&quot;</p>
<p>includes</p>
<p>=</p>
<p>&quot;resources/ plugin.xml&quot;</p>
<p>/&gt; 
25</p>
<p>&lt;/</p>
<p>jar</p>
<blockquote>
</blockquote>
<p>26</p>
<p>&lt;/</p>
<p>target</p>
<blockquote>
<p>&lt;</p>
</blockquote>
<p>span</p>
<p>style</p>
<p>=</p>
<p>&quot;font-size:10pt;line-height:1.5;font-family:&#39;sans serif&#39;, tahoma, verdana, helvetica;&quot;</p>
<blockquote>
<p> &lt;/</p>
</blockquote>
<p>span</p>
<p>&gt;</p>
<p><a href="http://static.oschina.net/uploads/space/2013/0520/194752_bBaX_256028.png" target="_blank"><img src="" alt=""></a> </p>
<h2 id="3-hadoop-src-contrib-build-contrib-xml-">3.%hadoop%/src/contrib/build-contrib.xml ：</h2>
<h3 id="1-hadoop-version-eclipse-eclipse-home-">1）添加hadoop的version和eclipse的eclipse.home属性：</h3>
<p>1</p>
<p>&lt;?</p>
<p>xml</p>
<p>version</p>
<p>=</p>
<p>&quot;1.0&quot;</p>
<p>?&gt;</p>
<p>2</p>
<!-- Imported by contrib//*/build.xml files to share generic targets. -->
3

&lt;

project


name

=

&quot;hadoopbuildcontrib&quot;


xmlns:ivy

=

&quot;antlib:org.apache.ivy.ant&quot;

&gt;

4

&lt;

property


name

=

&quot;name&quot;


value

=

&quot;${ant.project.name}&quot;

/&gt;
5

&lt;

property


name

=

&quot;root&quot;


value

=

&quot;${basedir}&quot;

/&gt;

6

&lt;

property


name

=

&quot;hadoop.root&quot;


location

=

&quot;${root}/../../../&quot;

/&gt;
7

<!-- hadoop版本、eclipse安装路径 -->

<p>8</p>
<p>&lt;</p>
<p>property</p>
<p>name</p>
<p>=</p>
<p>&quot;version&quot;</p>
<p>value</p>
<p>=</p>
<p>&quot;1.1.2&quot;</p>
<p>/&gt;
9</p>
<p>&lt;</p>
<p>property</p>
<p>name</p>
<p>=</p>
<p>&quot;eclipse.home&quot;</p>
<p>location</p>
<p>=</p>
<p>&quot;%eclipse%&quot;</p>
<p>/&gt;</p>
<p><a href="http://static.oschina.net/uploads/space/2013/0520/194838_mpUD_256028.png" target="_blank"><img src="" alt=""></a>  </p>
<h3 id="2-ivy-download-">2）取消ivy-download：</h3>
<p><a href="http://static.oschina.net/uploads/space/2013/0520/194859_Cc5J_256028.png" target="_blank"><img src="" alt=""></a> 
来源： <a href="[http://my.oschina.net/vigiles/blog/132238](http://my.oschina.net/vigiles/blog/132238)">[http://my.oschina.net/vigiles/blog/132238](http://my.oschina.net/vigiles/blog/132238)</a></p>
<p>配置/${hadoop-home}/src/contrib/eclipse-plugins/build.xml</p>
<p>找到</p>
<path id=”classpath”>

<p>然后添加</p>
<fileset dir=”${hadoop.root}/”>
      <include name=”/*.jar”/>
</fileset>

<p><strong>这一步很重要，如果不添加的话会出现找不到相应的程序包，错误如下（给出部分）</strong> ：</p>
<p>[javac]      Counters.Group group = counters.getGroup(groupName);
    [javac]              ^
    [javac] /home/summerdg/hadoop_src/hadoop-1.2.1/src/contrib/eclipse-plugin/src/java/org/apache/hadoop/eclipse/server/HadoopJob.java:305: 错误: 程序包Counters不存在
    [javac]      for (Counters.Counter counter : group) {
    [javac]                    ^
    [javac] /home/summerdg/hadoop_src/hadoop-1.2.1/src/contrib/eclipse-plugin/src/java/org/apache/hadoop/eclipse/dfs/DFSFile.java:74: 错误: 找不到符号
    [javac]      FileStatus fs = getDFS().getFileStatus(path);
    [javac]      ^
    [javac]  符号:  类 FileStatus
    [javac]  位置: 类 DFSFile
    [javac] 注: 某些输入文件使用或覆盖了已过时的 API。
    [javac] 注: 有关详细信息, 请使用 -Xlint:deprecation 重新编译。
    [javac] 注: 某些输入文件使用了未经检查或不安全的操作。
    [javac] 注: 有关详细信息, 请使用 -Xlint:unchecked 重新编译。
    [javac] 100 个错误</p>
<p>当然，出现这个错误还有个原因，就是你已经设置了这句，但你依然出现这个错误，那就是你eclipse版本的问题了。</p>
<p>找到
来源： <a href="[http://www.kankanews.com/ICkengine/archives/63441.shtml](http://www.kankanews.com/ICkengine/archives/63441.shtml)">[http://www.kankanews.com/ICkengine/archives/63441.shtml](http://www.kankanews.com/ICkengine/archives/63441.shtml)</a> </p>
<h2 id="4-hadoop_home-build-xml-">4.编辑%HADOOP_HOME%/build.xml：</h2>
<h3 id="1-hadoop-">1）修改hadoop版本号：</h3>
<p><a href="http://static.oschina.net/uploads/space/2013/0520/194950_g5cw_256028.png" target="_blank"><img src="" alt=""></a></p>
<h3 id="2-ivy-download-">2）取消ivy-download：</h3>
<p><a href="http://static.oschina.net/uploads/space/2013/0520/195011_l8BK_256028.png" target="_blank"><img src="" alt=""></a> </p>
<h2 id="5-hadoop-src-contrib-eclipse-plugin-meta-inf-manifest-mf-">5.修改%hadoop%/src/contrib/eclipse-plugin/META-INF/MANIFEST.MF：</h2>
<p>修改${HADOOP_HOME}/src/contrib/eclipse-plugin/META-INF/MANIFEST.MF的Bundle-ClassPath：
1</p>
<p>Bundle-ClassPath: classes/,</p>
<p>2</p>
<p>lib/hadoop-core.jar,
3</p>
<p>lib/commons-cli-1.2.jar,</p>
<p>4</p>
<p>lib/commons-configuration-1.6.jar,
5</p>
<p>lib/commons-httpclient-3.0.1.jar,</p>
<p>6</p>
<p>lib/commons-lang-2.4.jar,
7</p>
<p>lib/jackson-core-asl-1.8.8.jar,</p>
<p>8</p>
<p>lib/jackson-mapper-asl-1.8.8.jar</p>
<p><a href="http://static.oschina.net/uploads/space/2013/0520/195106_mqwV_256028.png" target="_blank"><img src="" alt=""></a> </p>
<h2 id="-build-contrib-eclipse-plugin-ant-jar-">进入到 /build/contrib/eclipse-plugin/执行 ant jar ：</h2>
<p>01</p>
<p>hep@hep-ubuntu:~$ </p>
<p>cd</p>
<p>~/hadoop/src/contrib/eclipse-plugin</p>
<p>02</p>
<p>hep@hep-ubuntu:~/hadoop/src/contrib/eclipse-plugin$ ant jar
03</p>
<p>Buildfile: /home/hep/hadoop/src/contrib/eclipse-plugin/build.xml</p>
<p>04</p>
<p>05</p>
<p>check-contrib:</p>
<p>06</p>
<p>07</p>
<p>init:</p>
<p>08</p>
<p>[</p>
<p>echo</p>
<p>] contrib: eclipse-plugin
09</p>
<p>10</p>
<p>init-contrib:
11</p>
<p>12</p>
<p>ivy-probe-antlib:
13</p>
<p>14</p>
<p>ivy-init-antlib:
15</p>
<p>16</p>
<p>ivy-init:
17</p>
<p>[ivy:configure] :: Ivy 2.1.0 - 20090925235825 :: <a href="http://ant.apache.org/ivy/" target="_blank">http://ant.apache.org/ivy/</a> ::</p>
<p>18</p>
<p>[ivy:configure] :: loading settings :: </p>
<p>file</p>
<p>= /home/hep/hadoop/ivy/ivysettings.xml
19</p>
<p>20</p>
<p>ivy-resolve-common:
21</p>
<p>[ivy:resolve] :: resolving dependencies :: org.apache.hadoop</p>
<p>/#eclipse-plugin;working@hep-ubuntu</p>
<p>22</p>
<p>[ivy:resolve]   confs: [common]
23</p>
<p>[ivy:resolve]   found commons-logging</p>
<p>/#commons-logging;1.0.4 in maven2</p>
<p>24</p>
<p>[ivy:resolve]   found log4j</p>
<p>/#log4j;1.2.15 in maven2
25</p>
<p>[ivy:resolve] :: resolution report :: resolve 171ms :: artifacts dl 4ms</p>
<p>26</p>
<hr>
<p>27</p>
<p>|                  |            modules            ||   artifacts   |</p>
<p>28</p>
<p>|       conf       | number| search|dwnlded|evicted|| number|dwnlded|
29</p>
<hr>
<p>30</p>
<p>|      common      |   2   |   0   |   0   |   0   ||   2   |   0   |
31</p>
<hr>
<p>32</p>
<p>33</p>
<p>ivy-retrieve-common:</p>
<p>34</p>
<p>[ivy:retrieve] :: retrieving :: org.apache.hadoop</p>
<p>/#eclipse-plugin [sync]
35</p>
<p>[ivy:retrieve]  confs: [common]</p>
<p>36</p>
<p>[ivy:retrieve]  0 artifacts copied, 2 already retrieved (0kB/6ms)
37</p>
<p>[ivy:cachepath] DEPRECATED: </p>
<p>&#39;ivy.conf.file&#39;</p>
<p>is deprecated, use </p>
<p>&#39;ivy.settings.file&#39;</p>
<p>instead</p>
<p>38</p>
<p>[ivy:cachepath] :: loading settings :: </p>
<p>file</p>
<p>= /home/hep/hadoop/ivy/ivysettings.xml
39</p>
<p>40</p>
<p>compile:
41</p>
<p>[</p>
<p>echo</p>
<p>] contrib: eclipse-plugin</p>
<p>42</p>
<p>[javac] /home/hep/hadoop/src/contrib/eclipse-plugin/build.xml:61: warning: </p>
<p>&#39;includeantruntime&#39;</p>
<p>was not </p>
<p>set</p>
<p>, defaulting to build.sysclasspath=last; </p>
<p>set</p>
<p>to </p>
<p>false</p>
<p>for</p>
<p>repeatable builds
43</p>
<p>44</p>
<p>jar:
45</p>
<p>[</p>
<p>mkdir</p>
<p>] Created </p>
<p>dir</p>
<p>: /home/hep/hadoop/build/contrib/eclipse-plugin/lib</p>
<p>46</p>
<p>[copy] Copying 1 </p>
<p>file</p>
<p>to /home/hep/hadoop/build/contrib/eclipse-plugin/lib
47</p>
<p>[copy] Copying /home/hep/hadoop/hadoop-core-1.1.2.jar to /home/hep/hadoop/build/contrib/eclipse-plugin/lib/hadoop-core.jar</p>
<p>48</p>
<p>[copy] Copying 1 </p>
<p>file</p>
<p>to /home/hep/hadoop/build/contrib/eclipse-plugin/lib
49</p>
<p>[copy] Copying /home/hep/hadoop/lib/commons-cli-1.2.jar to /home/hep/hadoop/build/contrib/eclipse-plugin/lib/commons-cli-1.2.jar</p>
<p>50</p>
<p>[copy] Copying 1 </p>
<p>file</p>
<p>to /home/hep/hadoop/build/contrib/eclipse-plugin/lib
51</p>
<p>[copy] Copying /home/hep/hadoop/lib/commons-configuration-1.6.jar to /home/hep/hadoop/build/contrib/eclipse-plugin/lib/commons-configuration-1.6.jar</p>
<p>52</p>
<p>[copy] Copying 1 </p>
<p>file</p>
<p>to /home/hep/hadoop/build/contrib/eclipse-plugin/lib
53</p>
<p>[copy] Copying /home/hep/hadoop/lib/commons-httpclient-3.0.1.jar to /home/hep/hadoop/build/contrib/eclipse-plugin/lib/commons-httpclient-3.0.1.jar</p>
<p>54</p>
<p>[copy] Copying 1 </p>
<p>file</p>
<p>to /home/hep/hadoop/build/contrib/eclipse-plugin/lib
55</p>
<p>[copy] Copying /home/hep/hadoop/lib/commons-lang-2.4.jar to /home/hep/hadoop/build/contrib/eclipse-plugin/lib/commons-lang-2.4.jar</p>
<p>56</p>
<p>[copy] Copying 1 </p>
<p>file</p>
<p>to /home/hep/hadoop/build/contrib/eclipse-plugin/lib
57</p>
<p>[copy] Copying /home/hep/hadoop/lib/jackson-core-asl-1.8.8.jar to /home/hep/hadoop/build/contrib/eclipse-plugin/lib/jackson-core-asl-1.8.8.jar</p>
<p>58</p>
<p>[copy] Copying 1 </p>
<p>file</p>
<p>to /home/hep/hadoop/build/contrib/eclipse-plugin/lib
59</p>
<p>[copy] Copying /home/hep/hadoop/lib/jackson-mapper-asl-1.8.8.jar to /home/hep/hadoop/build/contrib/eclipse-plugin/lib/jackson-mapper-asl-1.8.8.jar</p>
<p>60</p>
<p>[jar] Building jar: /home/hep/hadoop/build/contrib/eclipse-plugin/hadoop-eclipse-plugin-1.1.2.jar
61</p>
<p>62</p>
<p>BUILD SUCCESSFUL
63</p>
<p>Total </p>
<p>time</p>
<p>: 3 seconds</p>
<p>生成的插件jar就在本目录中。</p>
<p>把编译好的插件放入 $eclipse_home/dropins/hadoop/plugins/ (mkdir -p 创建)</p>
<p>启动eclipse 新建mapreduce project， 配置好map/reduce location, 就可以看到hdfs中的文件了</p>

      
    </div>
    
    
      
    
    
        <footer id="post-meta">
        <span class="categories">Posted in<span class="breadcrumb fa fa-folder"><li><a href="/categories/hadoop/">hadoop</a></li></span></span> | <span class="tags">Tagged <a href="/tags/hadoop/" class="label label-primary">hadoop</a></span> | <span class="time">recent updated:<time title="2014-03-07 09:54:36"datetime="2014-03-07 09:54:36"> mar. 7 2014</time></span> | <span class="comment-link">
<a href="http://itsolife.com/2014/02/02/2014-02-02-hadoop--Hadoop121编译Eclipse插件/#comments" class="ds-thread-count comment-link" data-thread-key="2014-02-02-hadoop--Hadoop121编译Eclipse插件" data-count-type="comments">暂无评论</a></span>
        </footer>
    
    <div class="clearfix"></div>
  </div>
</div>
</article>



  <article>
<div id="post" class="post well">
  <div class="post-content">
    <header class="well-sm">
      <i class="fa icon fa-5x pull-left"></i>
      <h1 class="title"><a href="/2014/02/02/2014-02-02-hadoop--CentOS的Hadoop集群配置/">CentOS的Hadoop集群配置</a></h1>
      
        <span>Posted on<time datetime="2014-02-02T01:54:36.000Z"> <a href="/2014/02/02/2014-02-02-hadoop--CentOS的Hadoop集群配置/">feb. 2 2014</a></time></span>      
    </header>
    
    
    <div class="entry">
      
        <h1 id="centos-hadoop-">CentOS的Hadoop集群配置</h1>
<h3 id="-centos-hadoop-http-blog-csdn-net-inte_sleeper-article-details-6569985-"><a href="http://blog.csdn.net/inte_sleeper/article/details/6569985" target="_blank">CentOS的Hadoop集群配置（一）</a></h3>
<h3 id="-"> </h3>
<p>参考资料：</p>
<p><a href="http://www.michael-noll.com/tutorials/running-hadoop-on-ubuntu-linux-single-node-cluster/" target="_blank"><a href="http://www.michael-noll.com/tutorials/running-hadoop-on-ubuntu-linux-single-node-cluster/">http://www.michael-noll.com/tutorials/running-hadoop-on-ubuntu-linux-single-node-cluster/</a></a></p>
<p><a href="http://www.michael-noll.com/tutorials/running-hadoop-on-ubuntu-linux-multi-node-cluster/" target="_blank"><a href="http://www.michael-noll.com/tutorials/running-hadoop-on-ubuntu-linux-multi-node-cluster/">http://www.michael-noll.com/tutorials/running-hadoop-on-ubuntu-linux-multi-node-cluster/</a></a></p>
<p><a href="http://www.michael-noll.com/tutorials/writing-an-hadoop-mapreduce-program-in-python/" target="_blank"><a href="http://www.michael-noll.com/tutorials/writing-an-hadoop-mapreduce-program-in-python/">http://www.michael-noll.com/tutorials/writing-an-hadoop-mapreduce-program-in-python/</a></a></p>
<p><a href="http://hadoop.apache.org/common/docs/current/cluster_setup.html" target="_blank"><a href="http://hadoop.apache.org/common/docs/current/cluster_setup.html">http://hadoop.apache.org/common/docs/current/cluster_setup.html</a></a></p>
<p>以下集群配置内容，以两台机器为例。其中一台是 master ，另一台是 slave1 。</p>
<p>master 上运行 name node, data node, task tracker, job tracker ， secondary name node ；</p>
<p>slave1 上运行 data node, task tracker 。</p>
<p>前面加 /* 表示对两台机器采取相同的操作</p>
<ol>
<li>安装 JDK /*</li>
</ol>
<p>yum install java-1.6.0-openjdk-devel</p>
<ol>
<li>设置环境变量 /*</li>
</ol>
<p>编辑 /etc/profile 文件，设置 JAVA_HOME 环境变量以及类路径：</p>
<p>export JAVA_HOME=&quot;/usr/lib/jvm/java-1.6.0-openjdk-1.6.0.0.x86_64&quot;</p>
<p>export PATH=$PATH:$JAVA_HOME/bin</p>
<p>export CLASSPATH=.:$JAVA_HOME/lib/tools.jar:$JAVA_HOME/lib/dt.jar</p>
<ol>
<li>添加 hosts 的映射 /*</li>
</ol>
<p>编辑 /etc/hosts 文件，<strong>注意</strong> <strong>host name **</strong>不要有下划线，见下步骤 9**</p>
<p>192.168.225.16 master</p>
<p>192.168.225.66 slave1</p>
<ol>
<li>配置 SSH /*</li>
</ol>
<p>cd /root &amp; mkdir .ssh</p>
<p>chmod 700 .ssh &amp; cd .ssh</p>
<p>创建密码为空的 RSA 密钥对：</p>
<p>ssh-keygen -t rsa -P &quot;&quot;</p>
<p>在提示的对称密钥名称中输入 id_rsa</p>
<p>将公钥添加至 authorized_keys 中：</p>
<p>cat id_rsa.pub &gt;&gt; authorized_keys</p>
<p>chmod 644 authorized_keys <strong>/#</strong> <strong>重要</strong></p>
<p>编辑 sshd 配置文件 /etc/ssh/sshd_config ，把 /#AuthorizedKeysFile  .ssh/authorized_keys 前面的注释取消掉。</p>
<p>重启 sshd 服务：</p>
<p>service sshd restart</p>
<p>测试 SSH 连接。连接时会提示是否连接，按回车后会将此公钥加入至 knows_hosts 中：</p>
<p>ssh localhost</p>
<ol>
<li>配置 master 和 slave1 的 ssh 互通</li>
</ol>
<p>在 slave1 中重复步骤 4 ，然后把 slave1 中的 .ssh/authorized_keys 复制至 master 的 .ssh/authorized_keys中。注意复制过去之后，要看最后的类似 root@localhost 的字符串，修改成 root@slave1 。同样将 master的 key 也复制至 slave1 ，并将最后的串修改成 root@master 。</p>
<p>或者使用如下命令：</p>
<p>ssh-copy-id -i ~/.ssh/id_rsa.pub root@slave1</p>
<p>测试 SSH 连接：</p>
<p>在 master 上运行：</p>
<p>ssh slave1</p>
<p>在 slave1 上运行：</p>
<p>ssh master</p>
<ol>
<li>安装 Hadoop</li>
</ol>
<p>下载 hadoop 安装包：</p>
<p>wget <a href="http://mirror.bjtu.edu.cn/apache/hadoop/common/hadoop-0.20.203.0/hadoop-0.20.203.0rc1.tar.gz" target="_blank"><a href="http://mirror.bjtu.edu.cn/apache/hadoop/common/hadoop-0.20.203.0/hadoop-0.20.203.0rc1.tar.gz">http://mirror.bjtu.edu.cn/apache/hadoop/common/hadoop-0.20.203.0/hadoop-0.20.203.0rc1.tar.gz</a></a></p>
<p>复制安装包至 slave1 ：</p>
<p>scp hadoop-0.20.203.0rc1.tar.gz root@slave1:/root/</p>
<p>解压：</p>
<p>tar xzvf hadoop-0.20.203.0rc1.tar.gz</p>
<p>mkdir /usr/local/hadoop</p>
<p>mv hadoop-0.20.203.0//* /usr/local/hadoop</p>
<pre><code>     修改 .bashrc 文件（位于用户目录下，即 ~/.bashrc ，对于 root ，即为 /root/.bashrc ）

     添加环境变量：

     export HADOOP_HOME=/usr/local/hadoop

export PATH=$PATH:$HADOOP_HOME/bin
</code></pre><ol>
<li>配置 Hadoop 环境变量 /*</li>
</ol>
<p><strong>以下所有 hadoop **</strong>目录下的文件，均以相对路径 hadoop <strong>**开始</strong></p>
<p>修改 hadoop/conf/hadoop-env.sh 文件，将里面的 JAVA_HOME 改成步骤 2 中设置的值。</p>
<ol>
<li>创建 Hadoop 本地临时文件夹 /*</li>
</ol>
<p>mkdir /root/hadoop_tmp （<strong>注意这一步，千万不要放在</strong> <strong>/tmp **</strong>目录下面！！因为 <strong><strong>/tmp </strong></strong>默认分配的空间是很小的，往 <strong><strong>hdfs </strong></strong>里放几个大文件就会导致空间满了，就会报错）**</p>
<p>修改权限：</p>
<p>chown -R hadoop:hadoop /root/hadoop_tmp</p>
<p>更松地，也可以这样：</p>
<p>chmod –R 777 /root/hadoop_tmp</p>
<ol>
<li>配置 Hadoop</li>
</ol>
<p>修改 master 的 hadoop/conf/core-site.xml ，在 <configuration> 节中添加如下内容：</p>
<p>注意： <strong>fs.default.name **</strong>的值不能带下划线**</p>
<property>

    <name>hadoop.tmp.dir</name>

    <value>/root/hadoop<em>tmp/hadoop</em>${user.name}</value>

</property>

<property>

    <name>fs.default.name</name>

    <value>hdfs://localhost:54310</value> 

</property>

<property>

    <name>io.sort.mb</name>

    <value>1024</value> 

</property>

<pre><code>     其中 io.sort.mb 值，指定了排序使用的内存，大的内存可以加快 job 的处理速度。



     修改 hadoop/conf/mapred-site.xml ，在 &lt;configuration&gt; 节中添加如下内容：
</code></pre><property>

    <name>mapred.job.tracker</name>

    <value>localhost:54311</value>

</property>

<property>

    <name>mapred.map.child.java.opts</name>

    <value>-Xmx4096m</value>

</property>

<property>

    <name>mapred.reduce.child.java.opts</name>

    <value>-Xmx4096m</value>

</property>

<pre><code>     其中 mapred.map.child.java.opts, mapred.reduce.child.java.opts 分别指定 map/reduce 任务使用的最大堆内存。较小的内存可能导致程序抛出 OutOfMemoryException 。
</code></pre><p>修改 conf/hdfs -site.xml ，在 <configuration> 节中添加如下内容：</p>
<property>

    <name>dfs.replication</name>

    <value>2</value>

</property>



<p>同样，修改 slave1 的 /usr/local/hadoop/conf/core-site.xml ，在 <configuration> 节中添加如下内容：</p>
<property>

    <name>hadoop.tmp.dir</name>

    <value>/root/hadoop<em>tmp/hadoop</em>${user.name}</value>

</property>

<property>

    <name>fs.default.name</name>

    <value>hdfs://localhost:54310</value> 

</property>

<property>

    <name>io.sort.mb</name>

    <value>1024</value> 

</property>



<pre><code>     修改 conf/mapred-site.xml ，在 &lt;configuration&gt; 节中添加如下内容：
</code></pre><property>

    <name>mapred.job.tracker</name>

    <value>localhost:54311</value>

</property>

<property>

    <name>mapred.map.child.java.opts</name>

    <value>-Xmx4096m</value>

</property>

<property>

    <name>mapred.reduce.child.java.opts</name>

    <value>-Xmx4096m</value>

</property>



<pre><code>     修改 conf/hdfs -site.xml ，在 &lt;configuration&gt; 节中添加如下内容：
</code></pre><property>

    <name>dfs.replication</name>

    <value>2</value>

    </property>



<ol>
<li>修改 hadoop/bin/hadoop 文件</li>
</ol>
<p>把 221 行修改成如下。因为对于 root 用户， -jvm 参数是有问题的，所以需要加一个判断 ( 或者以非 root 用户运行这个脚本也没问题 )</p>
<p>HADOOP_OPTS=&quot;$HADOOP_OPTS -jvm server $HADOOP_DATANODE_OPTS&quot;  à</p>
<pre><code>/#for root, -jvm option is invalid.

CUR_USER=`whoami`

if [ &quot;$CUR_USER&quot; = &quot;root&quot; ]; then

    HADOOP_OPTS=&quot;$HADOOP_OPTS -server $HADOOP_DATANODE_OPTS&quot;

else

    HADOOP_OPTS=&quot;$HADOOP_OPTS -jvm server $HADOOP_DATANODE_OPTS&quot;

fi 
</code></pre><p>unset $CUR_USER</p>
<p>至此， master 和 slave1 都已经完成了 single_node 的搭建，可以分别在两台机器上测试单节点。</p>
<p>启动节点：</p>
<p>hadoop/bin/start-all.sh</p>
<p>运行 jps 命令，应能看到类似如下的输出：</p>
<p>937 DataNode</p>
<p>9232 Jps</p>
<p>8811 NameNode</p>
<p>12033 JobTracker</p>
<p>12041 TaskTracker
来源： <a href="[http://blog.csdn.net/inte_sleeper/article/details/6569985](http://blog.csdn.net/inte_sleeper/article/details/6569985)">[http://blog.csdn.net/inte_sleeper/article/details/6569985](http://blog.csdn.net/inte_sleeper/article/details/6569985)</a> </p>
<p><a href="http://blog.csdn.net/inte_sleeper/article/details/6569990" target="_blank">CentOS的Hadoop集群配置（二）</a>
下面的教程把它们合并至 multi-node cluster 。</p>
<ol>
<li>合并 single-node 至 multi-node cluster</li>
</ol>
<p>修改 master 的 hadoop/conf/core-site.xml ：</p>
<property>

    <name>hadoop.tmp.dir</name>

    <value>/root/hadoop<em>tmp/hadoop</em>${user.name}</value>

</property>

<property>

    <name>fs.default.name</name>

    <value>hdfs://<strong>master</strong> :54310</value>

</property>

<property>

    <name>io.sort.mb</name>

    <value>1024</value> 

</property>



<pre><code>     修改 conf/mapred-site.xml ，在 &lt;configuration&gt; 节中添加如下内容：
</code></pre><property>

    <name>mapred.job.tracker</name>

    <value><strong>master</strong> :54311</value>

</property>

<property>

    <name>mapred.map.child.java.opts</name>

    <value>-Xmx4096m</value>

</property>

<property>

    <name>mapred.reduce.child.java.opts</name>

    <value>-Xmx4096m</value>

</property>

<pre><code>     修改 conf/hdfs -site.xml ，在 &lt;configuration&gt; 节中添加如下内容：
</code></pre><property>

    <name>dfs.replication</name>

    <value>2</value>

</property>

<p>把这三个文件复制至 slave1 相应的目录 hadoop/conf 中 <strong>( **</strong>即 master <strong><strong>和 slave1 </strong></strong>的内容完全一致 )**</p>
<pre><code>     修改所有节点的 hadoop/conf/masters ，把文件内容改成： master

     修改所有节点的 hadoop/conf/slaves ，把文件内容改成：
</code></pre><p>master</p>
<pre><code>slave1



     分别删除 master 和 slave1 的 dfs/data 文件：

     rm –rf /root/hadoop_tmp/hadoop_root/dfs/data
</code></pre><p>重新格式化 namenode ：</p>
<pre><code>     hadoop/bin/hadoop namenode -format



     测试，在 master 上运行：

     hadoop/bin/start-all.sh

     在 master 上运行 jps 命令
</code></pre><p>此时输出应类似于：</p>
<pre><code>     11648 TaskTracker
</code></pre><p>11166 NameNode</p>
<p>11433 SecondaryNameNode</p>
<p>12552 Jps</p>
<p>11282 DataNode</p>
<p>11525 JobTracker</p>
<p>在 slave1 上运行 jps</p>
<p>此时输出应包含 ( 即至少有 DataNode, 否则即为出错 ) ：</p>
<p>3950 Jps</p>
<p>3121 TaskTracker</p>
<p>3044 DataNode</p>
<ol>
<li>测试一个 JOB</li>
</ol>
<p>首先升级 python( 可选，如果 JOB 是 python 写的 ) ：</p>
<p>cd /etc/yum.repos.d/</p>
<p>wget <a href="http://mirrors.geekymedia.com/centos/geekymedia.repo" target="_blank">http://mirrors.geekymedia.com/centos/geekymedia.repo</a></p>
<p>yum makecache</p>
<p>yum -y install python26</p>
<p><strong>升级 python **</strong>的教程，见另外一篇文档。如果已经通过以上方法安装了 python2.6 <strong>**，那需要先卸载：</strong></p>
<p>yum remove python26 python26-devel</p>
<pre><code>     CentOS 的 yum 依赖于 python2.4 ，而 /usr/bin 中 python 程序即为 python2.4 。我们需要把它修改成python2.6 。



     cd /usr/bin/

     编辑 yum 文件，把第一行的
</code></pre><p>/#!/usr/bin/python   à   /#!/usr/bin/python2.4  </p>
<p>保存文件。</p>
<pre><code>     删除旧版本的 python 可执行文件（这个文件跟该目录下 python2.4 其实是一样的，所以可以直接删除）

     rm -f python

     让 python 指向 python2.6 的可执行程序。

     ln -s python26 python  
</code></pre><ol>
<li>Word count python 版本</li>
</ol>
<p><strong>Map.py</strong></p>
<p>/#! /usr/bin/python</p>
<p>import sys;</p>
<p>for line in sys.stdin:</p>
<p>  line =  line.strip();</p>
<p>  words = line.split();</p>
<p>  for word in words:</p>
<pre><code>  print &#39;%s/t%s&#39; % (word,1);
</code></pre><p><strong>Reduce.py</strong></p>
<p>/#!/usr/bin/python</p>
<p>import sys;</p>
<p>wc = {};</p>
<p>for line in sys.stdin:</p>
<p>  line = line.strip();</p>
<p>  word,count = line.split(&#39;/t&#39;,1);</p>
<p>  try:</p>
<pre><code>  count = int(count);
</code></pre><p>  except Error:</p>
<pre><code>  pass;
</code></pre><p>  if wc.has_key(word):</p>
<pre><code>  wc[word] += count;
</code></pre><p>  else: wc[word] = count;</p>
<p>for key in wc.keys():</p>
<p>  print &#39;%s/t%s&#39; % (key, wc[key]);</p>
<p>本机测试：</p>
<p>echo &quot;foo foo bar bar foo abc&quot; | map.py</p>
<p>echo &quot;foo foo bar bar foo abc&quot; | map.py | sort | reduce.py</p>
<p>在 hadoop 中测试：</p>
<p>hadoop jar /usr/local/hadoop/contrib/streaming/hadoop-streaming-0.20.203.0.jar -file mapper.py -mapper mapper.py -file reducer.py -reducer reducer.py -input wc//* -output wc-out</p>
<p>Job 成功后，会在 HDFS 中生成 wc-out 目录。</p>
<p>查看结果：</p>
<p>hadoop fs –ls wc-out</p>
<p>hadoop fs –cat wc-out/part-00000</p>
<ol>
<li>集群增加新节点</li>
</ol>
<p>a.       执行步骤 1 ， 2.</p>
<p>b.       修改 hosts 文件，将集群中的 hosts 加入本身 /etc/hosts 中。并修改集群中其他节点的 hosts ，将新节点加入。</p>
<p>c.       master 的 conf/slaves 文件中，添加新节点。</p>
<p>d.       启动 datanode 和 task tracker 。</p>
<p>hadoop-daemon.sh start datanode</p>
<p>hadoop-daemon.sh start tasktracker</p>
<ol>
<li>Trouble-shooting</li>
</ol>
<p>hadoop 的日志在 hadoop/logs 中。</p>
<p>其中， logs 根目录包含的是 namenode, datanode, jobtracker, tasktracker 等的日志。分别以 hadoop-{username}-namenode/datanode/jobtracker/tasktracker-hostname.log 命名。</p>
<p>userlogs 目录里包含了具体的 job 日志，每个 job 有一个单独的目录，以 job<em>YYYYmmddHHmm_xxxx 命名。里面包含数个 attempt</em>{jobname}<em>m_xxxxx 或 attempt</em>{jobname}_r_xxxx 等数个目录。其中目录名中的m 表示 map 任务的日志， r 表示 reduce 任务的日志。因此，出错时，可以有针对性地查看特定的日志。</p>
<p>常见错误：</p>
<ol>
<li>出现类似：</li>
</ol>
<p><em>ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: java.io.IOException: Incompatible namespaceIDs …</em></p>
<p>的异常，是因为先格式化了 namenode ，后来又修改了配置导致。将 dfs/data 文件夹内容删除，再重新格式化 namenode 即可。</p>
<ol>
<li>出现类似：</li>
</ol>
<p><em>INFO org.apache.hadoop.ipc.Client: Retrying connect to server:…</em></p>
<p>的异常，首先确认 name node 是否启动。如果已经启动，有可能是 master 或 slave1 中的配置出错，集群配置参考步骤 11 。也有可能是防火墙问题，需添加以下例外：</p>
<p>50010 端口用于数据传输， 50020 用于 RPC 调用， 50030 是 WEB 版的 JOB 状态监控， 54311 是job tracker ， 54310 是与 master 通信的端口。</p>
<p>完整的端口列表见：</p>
<p><a href="http://www.cloudera.com/blog/2009/08/hadoop-default-ports-quick-reference/" target="_blank"><a href="http://www.cloudera.com/blog/2009/08/hadoop-default-ports-quick-reference/">http://www.cloudera.com/blog/2009/08/hadoop-default-ports-quick-reference/</a></a></p>
<p>iptables -A RH-Firewall-1-INPUT -p tcp -m tcp --dport 50010 -j ACCEPT</p>
<p>iptables -A RH-Firewall-1-INPUT -p tcp -m tcp --dport 50020 -j ACCEPT</p>
<p>iptables -A RH-Firewall-1-INPUT -p tcp -m tcp --dport 50030 -j ACCEPT</p>
<p>iptables -A RH-Firewall-1-INPUT -p tcp -m tcp --dport 50060 -j ACCEPT</p>
<p>iptables -A RH-Firewall-1-INPUT -p tcp -m tcp --dport 54310 -j ACCEPT</p>
<p>iptables -A RH-Firewall-1-INPUT -p tcp -m tcp --dport 54311 -j ACCEPT</p>
<p>iptables -A OUTPUT -p tcp -m tcp --dport 50010 -j ACCEPT</p>
<p>iptables -A OUTPUT -p tcp -m tcp --dport 50020 -j ACCEPT</p>
<p>iptables -A OUTPUT -p tcp -m tcp --sport 50010 -j ACCEPT</p>
<p>iptables -A OUTPUT -p tcp -m tcp --sport 50020 -j ACCEPT</p>
<p>iptables -A OUTPUT -p tcp -m tcp --dport 50030 -j ACCEPT</p>
<p>iptables -A OUTPUT -p tcp -m tcp --sport 50030 -j ACCEPT</p>
<p>iptables -A OUTPUT -p tcp -m tcp --dport 50060 -j ACCEPT</p>
<p>iptables -A OUTPUT -p tcp -m tcp --sport 50060 -j ACCEPT</p>
<p>iptables -A OUTPUT -p tcp -m tcp --dport 54310 -j ACCEPT</p>
<p>iptables -A OUTPUT -p tcp -m tcp --sport 54310 -j ACCEPT</p>
<p>iptables -A OUTPUT -p tcp -m tcp --dport 54311 -j ACCEPT</p>
<p>iptables -A OUTPUT -p tcp -m tcp --sport 54311 -j ACCEPT</p>
<p>保存规则：</p>
<p>/etc/init.d/iptables save</p>
<p>重启 iptables 服务：</p>
<p>service iptables restart</p>
<p>如果还是出现问题 2 的错误，那可能需要手工修改 /etc/sysconfig/iptables 的规则。手动添加这些规则。若有 ”reject-with icmp-host-prohibited” 的规则，需将规则加到它的前面。注意修改配置文件的时候，不需要带 iptables 命令。直接为类似于：</p>
<p>-A OUTPUT -p tcp -m tcp --sport 54311 -j ACCEPT</p>
<p>或关闭防火墙 <strong>( **</strong>建议，因为端口太多，要加的例外很多 )**</p>
<p>service iptables stop</p>
<ol>
<li><p>在 /etc/hosts 文件中，确保一个 host 只对应一个 IP ，否则会出错（如同时将 slave1 指向 127.0.0.1 和192.168.225.66 ），<strong>可能导致数据无法从一个节点复制至另一节点。</strong></p>
</li>
<li><p>出现类似：</p>
</li>
</ol>
<p><em>FATAL org.apache.hadoop.mapred.TaskTracker: Error running child : java.lang.OutOfMemoryError: Java heap space…</em></p>
<p>的异常，是因为堆内存不够。有以下几个地方可以考虑配置：</p>
<p>a.       conf/hadoop-env.sh 中， export HADOOP_HEAPSIZE=1000 这一行，默认为注释掉，堆大小为1000M ，可以取消注释，将这个值调大一些（对于 16G 的内存，可以调至 8G ）。</p>
<p>b.       conf/mapred-site.xml 中，添加 mapred.map.child.java.opts 属性，手动指定 JAVA 堆的参数值为 -Xmx2048m 或更大。这个值调整 map 任务的堆大小。即：</p>
<property>

    <name>mapred.map.child.java.opts </name>

    <value>-Xmx2048m</value>

</property>

<p>c.       conf/mapred-site.xml 中，添加 mapred.reduce.child.java.opts 属性，手动指定 JAVA 堆的参数值为 -Xmx2048m 或更大。这个值调整 reduce 任务的堆大小。即：</p>
<property>

    <name>mapred.reduce.child.java.opts </name>

    <value>-Xmx2048m</value>

</property>

<pre><code>               注意调整这些值之后，要重启 name node 。
</code></pre><p><em>5.       </em>出现类似： <em>java.io.IOException: File /user/root/pv_product_110124 could only be replicated to 0 nodes, instead of 1…</em></p>
<p>的异常，首先确保 hadoop 临时文件夹中有足够的空间，空间不够会导致这个错误。</p>
<p>如果空间没问题，那就尝试把临时文件夹中 dfs/data 目录删除，然后重新格式化 name node ：</p>
<p>hadoop namenode -format</p>
<p>注意：此命令会删除 hdfs 上的文件</p>
<p><em>6.       </em>出现类似： <em>java.io.IOException: Broken pipe…</em></p>
<p>的异常，检查你的程序吧，没准输出了不该输出的信息，如调试信息等。
来源： <a href="[http://blog.csdn.net/inte_sleeper/article/details/6569990](http://blog.csdn.net/inte_sleeper/article/details/6569990)">[http://blog.csdn.net/inte_sleeper/article/details/6569990](http://blog.csdn.net/inte_sleeper/article/details/6569990)</a> </p>

      
    </div>
    
    
      
    
    
        <footer id="post-meta">
        <span class="categories">Posted in<span class="breadcrumb fa fa-folder"><li><a href="/categories/hadoop/">hadoop</a></li></span></span> | <span class="tags">Tagged <a href="/tags/hadoop/" class="label label-primary">hadoop</a></span> | <span class="time">recent updated:<time title="2014-03-07 09:54:36"datetime="2014-03-07 09:54:36"> mar. 7 2014</time></span> | <span class="comment-link">
<a href="http://itsolife.com/2014/02/02/2014-02-02-hadoop--CentOS的Hadoop集群配置/#comments" class="ds-thread-count comment-link" data-thread-key="2014-02-02-hadoop--CentOS的Hadoop集群配置" data-count-type="comments">暂无评论</a></span>
        </footer>
    
    <div class="clearfix"></div>
  </div>
</div>
</article>



  <article>
<div id="post" class="post well">
  <div class="post-content">
    <header class="well-sm">
      <i class="fa icon fa-5x pull-left"></i>
      <h1 class="title"><a href="/2014/02/02/2014-02-02-hadoop--hdfs_shell/">hdfs_shell</a></h1>
      
        <span>Posted on<time datetime="2014-02-02T01:54:36.000Z"> <a href="/2014/02/02/2014-02-02-hadoop--hdfs_shell/">feb. 2 2014</a></time></span>      
    </header>
    
    
    <div class="entry">
      
        <h1 id="hdfs_shell">hdfs_shell</h1>
<p>HDFS File System Shell Guide
Table of contents
1
Overview............................................................................................................................3
1.1 1.2 1.3 1.4 1.5 1.6 1.7 1.8 1.9
cat ................................................................................................................................. 3 chgrp ............................................................................................................................. 3 chmod ........................................................................................................................... 3 chown ........................................................................................................................... 4 copyFromLocal..............................................................................................................4 copyToLocal..................................................................................................................4 count ............................................................................................................................. 4 cp .................................................................................................................................. 4 du................................................................................................................................... 5 dus ...............................................................................................................................5 expunge .......................................................................................................................5 get ................................................................................................................................5 getmerge ......................................................................................................................6 ls...................................................................................................................................6 lsr................................................................................................................................. 6 mkdir ...........................................................................................................................7 moveFromLocal ..........................................................................................................7 moveToLocal............................................................................................................... 7 mv ............................................................................................................................... 7 put ............................................................................................................................... 8 rm ................................................................................................................................ 8 rmr ...............................................................................................................................8 setrep ...........................................................................................................................9
Copyright © 2008 The Apache Software Foundation. All rights reserved.
1.10 1.11 1.12 1.13 1.14 1.15 1.16 1.17 1.18 1.19 1.20 1.21 1.22 1.23
HDFS File System Shell Guide
1.24 1.25 1.26 1.27 1.28
stat ...............................................................................................................................9 tail ............................................................................................................................... 9 test .............................................................................................................................10 text ............................................................................................................................ 10 touchz ........................................................................................................................10
Page 2
Copyright © 2008 The Apache Software Foundation. All rights reserved.
HDFS File System Shell Guide</p>
<ol>
<li>Overview
The FileSystem (FS) shell is invoked by bin/hadoop fs <args>. All FS shell commands take path URIs as arguments. The URI format is scheme://autority/path. For HDFS the scheme is hdfs, and for the local filesystem the scheme is file. The scheme and authority are optional. If not specified, the default scheme specified in the configuration is used. An HDFS file or directory such as /parent/child can be specified as hdfs://namenodehost/parent/child or simply as /parent/child (given that your configuration is set to point to hdfs://namenodehost). Most of the commands in FS shell behave like corresponding Unix commands. Differences are described with each of the commands. Error information is sent to stderr and the output is sent to stdout.
1.1. cat
Usage: hadoop fs -cat URI [URI …] Copies source paths to stdout. Example: • hadoop fs -cat hdfs://nn1.example.com/file1 hdfs://nn2.example.com/file2 • hadoop fs -cat file:///file3 /user/hadoop/file4 Exit Code: Returns 0 on success and -1 on error.
1.2. chgrp
Usage: hadoop fs -chgrp [-R] GROUP URI [URI …] Change group association of files. With -R, make the change recursively through the directory structure. The user must be the owner of files, or else a super-user. Additional information is in the HDFS Admin Guide: Permissions.
1.3. chmod
Usage: hadoop fs -chmod [-R] <MODE[,MODE]... | OCTALMODE> URI [URI …] Change the permissions of files. With -R, make the change recursively through the directory structure. The user must be the owner of the file, or else a super-user. Additional information is in the HDFS Admin Guide: Permissions.
Page 3
Copyright © 2008 The Apache Software Foundation. All rights reserved.
HDFS File System Shell Guide
1.4. chown
Usage: hadoop fs -chown [-R] [OWNER][:[GROUP]] URI [URI ] Change the owner of files. With -R, make the change recursively through the directory structure. The user must be a super-user. Additional information is in the HDFS Admin Guide: Permissions.
1.5. copyFromLocal
Usage: hadoop fs -copyFromLocal <localsrc> URI Similar to put command, except that the source is restricted to a local file reference.
1.6. copyToLocal
Usage: hadoop fs -copyToLocal [-ignorecrc] [-crc] URI <localdst> Similar to get command, except that the destination is restricted to a local file reference.
1.7. count
Usage: hadoop fs -count [-q] <paths> Count the number of directories, files and bytes under the paths that match the specified file pattern. The output columns are: DIR_COUNT, FILE_COUNT, CONTENT_SIZE FILE_NAME. The output columns with -q are: QUOTA, REMAINING_QUATA, SPACE_QUOTA, REMAINING_SPACE_QUOTA, DIR_COUNT, FILE_COUNT, CONTENT_SIZE, FILE_NAME. Example: • hadoop fs -count hdfs://nn1.example.com/file1 hdfs://nn2.example.com/file2 • hadoop fs -count -q hdfs://nn1.example.com/file1 Exit Code: Returns 0 on success and -1 on error.
1.8. cp
Page 4
Copyright © 2008 The Apache Software Foundation. All rights reserved.
HDFS File System Shell Guide
Usage: hadoop fs -cp URI [URI …] <dest> Copy files from source to destination. This command allows multiple sources as well in which case the destination must be a directory. Example: • hadoop fs -cp /user/hadoop/file1 /user/hadoop/file2 • hadoop fs -cp /user/hadoop/file1 /user/hadoop/file2 /user/hadoop/dir Exit Code: Returns 0 on success and -1 on error.
1.9. du
Usage: hadoop fs -du URI [URI …] Displays aggregate length of files contained in the directory or the length of a file in case its just a file. Example: hadoop fs -du /user/hadoop/dir1 /user/hadoop/file1 hdfs://nn.example.com/user/hadoop/dir1 Exit Code: Returns 0 on success and -1 on error.
1.10. dus
Usage: hadoop fs -dus <args> Displays a summary of file lengths.
1.11. expunge
Usage: hadoop fs -expunge Empty the Trash. Refer to HDFS Architecture for more information on Trash feature.
1.12. get
Usage: hadoop fs -get [-ignorecrc] [-crc] <src> <localdst> Copy files to the local file system. Files that fail the CRC check may be copied with the -ignorecrc option. Files and CRCs may be copied using the -crc option.
Page 5
Copyright © 2008 The Apache Software Foundation. All rights reserved.
HDFS File System Shell Guide
Example: • hadoop fs -get /user/hadoop/file localfile • hadoop fs -get hdfs://nn.example.com/user/hadoop/file localfile Exit Code: Returns 0 on success and -1 on error.
1.13. getmerge
Usage: hadoop fs -getmerge <src> <localdst> [addnl] Takes a source directory and a destination file as input and concatenates files in src into the destination local file. Optionally addnl can be set to enable adding a newline character at the end of each file.
1.14. ls
Usage: hadoop fs -ls <args> For a file returns stat on the file with the following format: permissions number_of_replicas userid groupid filesize modification_date modification_time filename For a directory it returns list of its direct children as in unix.A directory is listed as: permissions userid groupid modification_date modification_time dirname Example: hadoop fs -ls /user/hadoop/file1 Exit Code: Returns 0 on success and -1 on error.
1.15. lsr
Usage: hadoop fs -lsr <args> Recursive version of ls. Similar to Unix ls -R.
Page 6
Copyright © 2008 The Apache Software Foundation. All rights reserved.
HDFS File System Shell Guide
1.16. mkdir
Usage: hadoop fs -mkdir <paths> Takes path uri&#39;s as argument and creates directories. The behavior is much like unix mkdir -p creating parent directories along the path. Example: • hadoop fs -mkdir /user/hadoop/dir1 /user/hadoop/dir2 • hadoop fs -mkdir hdfs://nn1.example.com/user/hadoop/dir hdfs://nn2.example.com/user/hadoop/dir Exit Code: Returns 0 on success and -1 on error.
1.17. moveFromLocal
Usage: dfs -moveFromLocal <localsrc> <dst> Similar to put command, except that the source localsrc is deleted after it&#39;s copied.
1.18. moveToLocal
Usage: hadoop fs -moveToLocal [-crc] <src> <dst> Displays a &quot;Not implemented yet&quot; message.
1.19. mv
Usage: hadoop fs -mv URI [URI …] <dest> Moves files from source to destination. This command allows multiple sources as well in which case the destination needs to be a directory. Moving files across filesystems is not permitted. Example: • hadoop fs -mv /user/hadoop/file1 /user/hadoop/file2 • hadoop fs -mv hdfs://nn.example.com/file1 hdfs://nn.example.com/file2 hdfs://nn.example.com/file3 hdfs://nn.example.com/dir1 Exit Code:
Page 7
Copyright © 2008 The Apache Software Foundation. All rights reserved.
HDFS File System Shell Guide
Returns 0 on success and -1 on error.
1.20. put
Usage: hadoop fs -put <localsrc> ... <dst> Copy single src, or multiple srcs from local file system to the destination filesystem. Also reads input from stdin and writes to destination filesystem. • hadoop fs -put localfile /user/hadoop/hadoopfile • hadoop fs -put localfile1 localfile2 /user/hadoop/hadoopdir • hadoop fs -put localfile hdfs://nn.example.com/hadoop/hadoopfile • hadoop fs -put - hdfs://nn.example.com/hadoop/hadoopfile Reads the input from stdin. Exit Code: Returns 0 on success and -1 on error.
1.21. rm
Usage: hadoop fs -rm [-skipTrash] URI [URI …] Delete files specified as args. Only deletes non empty directory and files. If the -skipTrash option is specified, the trash, if enabled, will be bypassed and the specified file(s) deleted immediately. This can be useful when it is necessary to delete files from an over-quota directory. Refer to rmr for recursive deletes. Example: • hadoop fs -rm hdfs://nn.example.com/file /user/hadoop/emptydir Exit Code: Returns 0 on success and -1 on error.
1.22. rmr
Usage: hadoop fs -rmr [-skipTrash] URI [URI …] Recursive version of delete. If the -skipTrash option is specified, the trash, if enabled, will be bypassed and the specified file(s) deleted immediately. This can be useful when it is necessary to delete files from an over-quota directory.
Page 8
Copyright © 2008 The Apache Software Foundation. All rights reserved.
HDFS File System Shell Guide
Example: • hadoop fs -rmr /user/hadoop/dir • hadoop fs -rmr hdfs://nn.example.com/user/hadoop/dir Exit Code: Returns 0 on success and -1 on error.
1.23. setrep
Usage: hadoop fs -setrep [-R] <path> Changes the replication factor of a file. -R option is for recursively increasing the replication factor of files within a directory. Example: • hadoop fs -setrep -w 3 -R /user/hadoop/dir1 Exit Code: Returns 0 on success and -1 on error.
1.24. stat
Usage: hadoop fs -stat URI [URI …] Returns the stat information on the path. Example: • hadoop fs -stat path Exit Code: Returns 0 on success and -1 on error.
1.25. tail
Usage: hadoop fs -tail [-f] URI Displays last kilobyte of the file to stdout. -f option can be used as in Unix. Example: • hadoop fs -tail pathname Exit Code:
Page 9
Copyright © 2008 The Apache Software Foundation. All rights reserved.
HDFS File System Shell Guide
Returns 0 on success and -1 on error.
1.26. test
Usage: hadoop fs -test -[ezd] URI Options: -e check to see if the file exists. Return 0 if true. -z check to see if the file is zero length. Return 0 if true. -d check to see if the path is directory. Return 0 if true. Example: • hadoop fs -test -e filename
1.27. text
Usage: hadoop fs -text <src> Takes a source file and outputs the file in text format. The allowed formats are zip and TextRecordInputStream.
1.28. touchz
Usage: hadoop fs -touchz URI [URI …] Create a file of zero length. Example: • hadoop -touchz pathname Exit Code: Returns 0 on success and -1 on error.
Page 10
Copyright © 2008 The Apache Software Foundation. All rights reserved.</li>
</ol>

      
    </div>
    
    
      
    
    
        <footer id="post-meta">
        <span class="categories">Posted in<span class="breadcrumb fa fa-folder"><li><a href="/categories/hadoop/">hadoop</a></li></span></span> | <span class="tags">Tagged <a href="/tags/hadoop/" class="label label-primary">hadoop</a></span> | <span class="time">recent updated:<time title="2014-03-07 09:54:36"datetime="2014-03-07 09:54:36"> mar. 7 2014</time></span> | <span class="comment-link">
<a href="http://itsolife.com/2014/02/02/2014-02-02-hadoop--hdfs_shell/#comments" class="ds-thread-count comment-link" data-thread-key="2014-02-02-hadoop--hdfs_shell" data-count-type="comments">暂无评论</a></span>
        </footer>
    
    <div class="clearfix"></div>
  </div>
</div>
</article>



  <article>
<div id="post" class="post well">
  <div class="post-content">
    <header class="well-sm">
      <i class="fa icon fa-5x pull-left"></i>
      <h1 class="title"><a href="/2014/02/02/2014-02-02-linux-ubuntu--Ubuntu1210软件更新源列表_Linux教程_Linux公社-Linux系统门户网站/">Ubuntu 12.10 软件更新源列表_Linux教程_Linux公社</a></h1>
      
        <span>Posted on<time datetime="2014-02-02T01:54:36.000Z"> <a href="/2014/02/02/2014-02-02-linux-ubuntu--Ubuntu1210软件更新源列表_Linux教程_Linux公社-Linux系统门户网站/">feb. 2 2014</a></time></span>      
    </header>
    
    
    <div class="entry">
      
        <h1 id="ubuntu-12-10-_linux-_linux-linux-">Ubuntu 12.10 软件更新源列表_Linux教程_Linux公社-Linux系统门户网站</h1>
<h3 id="-">分享到</h3>
<ul>
<li><a href="http://www.linuxidc.com/Linux/2012-10/73114.htm#" target="_blank">一键分享</a></li>
<li><a href="http://www.linuxidc.com/Linux/2012-10/73114.htm#" target="_blank">QQ空间</a></li>
<li><a href="http://www.linuxidc.com/Linux/2012-10/73114.htm#" target="_blank">新浪微博</a></li>
<li><a href="http://www.linuxidc.com/Linux/2012-10/73114.htm#" target="_blank">百度搜藏</a></li>
<li><a href="http://www.linuxidc.com/Linux/2012-10/73114.htm#" target="_blank">人人网</a></li>
<li><a href="http://www.linuxidc.com/Linux/2012-10/73114.htm#" target="_blank">腾讯微博</a></li>
<li><a href="http://www.linuxidc.com/Linux/2012-10/73114.htm#" target="_blank">百度相册</a></li>
<li><a href="http://www.linuxidc.com/Linux/2012-10/73114.htm#" target="_blank">开心网</a></li>
<li><a href="http://www.linuxidc.com/Linux/2012-10/73114.htm#" target="_blank">腾讯朋友</a></li>
<li><a href="http://www.linuxidc.com/Linux/2012-10/73114.htm#" target="_blank">百度贴吧</a></li>
<li><a href="http://www.linuxidc.com/Linux/2012-10/73114.htm#" target="_blank">豆瓣网</a></li>
<li><a href="http://www.linuxidc.com/Linux/2012-10/73114.htm#" target="_blank">搜狐微博</a></li>
<li><a href="http://www.linuxidc.com/Linux/2012-10/73114.htm#" target="_blank">百度新首页</a></li>
<li><a href="http://www.linuxidc.com/Linux/2012-10/73114.htm#" target="_blank">QQ</a></li>
<li><a href="http://www.linuxidc.com/Linux/2012-10/73114.htm#" target="_blank">和讯微博</a></li>
<li><a href="http://www.linuxidc.com/Linux/2012-10/73114.htm#" target="_blank">更多...</a></li>
</ul>
<p><a href="http://www.linuxidc.com/Linux/2012-10/73114.htm#" target="_blank">百度分享</a></p>
<p><a href="&quot;點擊以繁體中文方式浏覽&quot;">繁體</a></p>
<p>你好，游客 <a href="http://www.linuxidc.com/Linux/2012-10/73114.htm#" target="_blank">登录</a> <a href="http://www.linuxidc.com/memberreg.aspx" target="_blank">注册</a> <a href="http://www.linuxidc.com/membernewsadd.aspx" target="_blank">发布</a><a href="http://www.linuxidc.com/search.aspx" target="_blank">搜索</a>
<a href="http://www.linuxidc.com/" target="_blank"><img src="" alt="Linux公社"></a>
<a href="http://www.linuxidc.com/topicnews.aspx?tid=11" title="Android专题" target="_blank"><img src="" alt="Android专题"></a></p>
<p><a href="http://www.linuxidc.com/index.htm" target="_blank">首页</a><a href="http://www.linuxidc.com/it/" target="_blank">Linux新闻</a><a href="http://www.linuxidc.com/Linuxit/" target="_blank">Linux教程</a><a href="http://www.linuxidc.com/MySql/" target="_blank">数据库技术</a><a href="http://www.linuxidc.com/RedLinux/" target="_blank">Linux编程</a><a href="http://www.linuxidc.com/Apache/" target="_blank">服务器应用</a><a href="http://www.linuxidc.com/Unix/" target="_blank">Linux安全</a><a href="http://www.linuxidc.com/download/" target="_blank">Linux下载</a><a href="http://www.linuxidc.com/Linuxrz/" target="_blank">Linux认证</a><a href="http://www.linuxidc.com/theme/" target="_blank">Linux主题</a><a href="http://www.linuxidc.com/Linuxwallpaper/" target="_blank">Linux壁纸</a><a href="http://www.linuxidc.com/linuxsoft/" target="_blank">Linux软件</a><a href="http://www.linuxidc.com/digi/" target="_blank">数码</a><a href="http://www.linuxidc.com/mobile/" target="_blank">手机</a><a href="http://www.linuxidc.com/diannao/" target="_blank">电脑</a>
<a href="http://www.linuxidc.com/index.htm" target="_blank">首页</a> → <a href="http://www.linuxidc.com/Linuxit/" target="_blank">Linux教程</a></p>
<p><a href="http://www.yutianedu.com/list.asp?Unid=22239" target="_blank"><img src="" alt=""></a> <a href="http://www.boxue.com.cn/" target="_blank"><img src="" alt=""></a>
背景：<img src="" alt="#EDF0F5"> <img src="" alt="#FAFBE6"> <img src="" alt="#FFF2E2"> <img src="" alt="#FDE6E0"> <img src="" alt="#F3FFE1"> <img src="" alt="#DAFAF3"> <img src="" alt="#EAEAEF"> <img src="" alt="默认"> 阅读新闻</p>
<h1 id="ubuntu-12-10-">Ubuntu 12.10 软件更新源列表</h1>
<p> [日期：2012-10-28] 来源：Linux公社  作者：Linux [字体：<a href="">大</a> <a href="">中</a> <a href="">小</a>]</p>
<p><a href="http://www.upemb.com/page/uealinuxidc12531.php" title="尚观Linux" target="_blank"><img src="" alt=""></a>
<a href="http://www.linuxidc.com/topicnews.aspx?tid=2" title="Ubuntu" target="_blank">Ubuntu</a> 12.10也正式发布了， 安装好后第一件事就是更换源，Ubuntu网易的更新源速度很不错。</p>
<p>Ubuntu 12.10正式版发布下载  <a href="http://www.linuxidc.com/Linux/2012-10/72581.htm" target="_blank"><a href="http://www.linuxidc.com/Linux/2012-10/72581.htm">http://www.linuxidc.com/Linux/2012-10/72581.htm</a></a></p>
<p>废话少说， 上源：</p>
<p>首先，备份一下Ubuntu 12.10 原来的源地址列表文件</p>
<p>sudo cp /etc/apt/sources.list /etc/apt/sources.list.old</p>
<p>然后进行修改
sudo gedit /etc/apt/sources.list</p>
<p>可以在里面添加资源地址，我是直接覆盖掉原来的。</p>
<p>下面是网上找到的一些较好的源，有大型网站的，也有教育网的，可以根据自己的情况添加两三个即可。</p>
<p>/#网易的源（163源，无论是不是教育网，速度都很快）
deb <a href="http://mirrors.163.com/ubuntu/" target="_blank">http://mirrors.163.com/ubuntu/</a> quantal main universe restricted multiverse
deb-src <a href="http://mirrors.163.com/ubuntu/" target="_blank">http://mirrors.163.com/ubuntu/</a> quantal main universe restricted multiverse
deb <a href="http://mirrors.163.com/ubuntu/" target="_blank">http://mirrors.163.com/ubuntu/</a> quantal-security universe main multiverse restricted
deb-src <a href="http://mirrors.163.com/ubuntu/" target="_blank">http://mirrors.163.com/ubuntu/</a> quantal-security universe main multiverse restricted
deb <a href="http://mirrors.163.com/ubuntu/" target="_blank">http://mirrors.163.com/ubuntu/</a> quantal-updates universe main multiverse restricted
deb <a href="http://mirrors.163.com/ubuntu/" target="_blank">http://mirrors.163.com/ubuntu/</a> quantal-proposed universe main multiverse restricted
deb-src <a href="http://mirrors.163.com/ubuntu/" target="_blank">http://mirrors.163.com/ubuntu/</a> quantal-proposed universe main multiverse restricted
deb <a href="http://mirrors.163.com/ubuntu/" target="_blank">http://mirrors.163.com/ubuntu/</a> quantal-backports universe main multiverse restricted
deb-src <a href="http://mirrors.163.com/ubuntu/" target="_blank">http://mirrors.163.com/ubuntu/</a> quantal-backports universe main multiverse restricted
deb-src <a href="http://mirrors.163.com/ubuntu/" target="_blank">http://mirrors.163.com/ubuntu/</a> quantal-updates universe main multiverse restricted</p>
<p>/#搜狐的源（sohu 源今天还没有更新，不过应该快了）
deb <a href="http://mirrors.sohu.com/ubuntu/" target="_blank">http://mirrors.sohu.com/ubuntu/</a> quantal main restricted
deb-src <a href="http://mirrors.sohu.com/ubuntu/" target="_blank">http://mirrors.sohu.com/ubuntu/</a> quantal main restricted
deb <a href="http://mirrors.sohu.com/ubuntu/" target="_blank">http://mirrors.sohu.com/ubuntu/</a> quantal-updates main restricted
deb-src <a href="http://mirrors.sohu.com/ubuntu/" target="_blank">http://mirrors.sohu.com/ubuntu/</a> quantal-updates main restricted
deb <a href="http://mirrors.sohu.com/ubuntu/" target="_blank">http://mirrors.sohu.com/ubuntu/</a> quantal universe
deb-src <a href="http://mirrors.sohu.com/ubuntu/" target="_blank">http://mirrors.sohu.com/ubuntu/</a> quantal universe
deb <a href="http://mirrors.sohu.com/ubuntu/" target="_blank">http://mirrors.sohu.com/ubuntu/</a> quantal-updates universe
deb-src <a href="http://mirrors.sohu.com/ubuntu/" target="_blank">http://mirrors.sohu.com/ubuntu/</a> quantal-updates universe
deb <a href="http://mirrors.sohu.com/ubuntu/" target="_blank">http://mirrors.sohu.com/ubuntu/</a> quantal multiverse
deb-src <a href="http://mirrors.sohu.com/ubuntu/" target="_blank">http://mirrors.sohu.com/ubuntu/</a> quantal multiverse
deb <a href="http://mirrors.sohu.com/ubuntu/" target="_blank">http://mirrors.sohu.com/ubuntu/</a> quantal-updates multiverse
deb-src <a href="http://mirrors.sohu.com/ubuntu/" target="_blank">http://mirrors.sohu.com/ubuntu/</a> quantal-updates multiverse
deb <a href="http://mirrors.sohu.com/ubuntu/" target="_blank">http://mirrors.sohu.com/ubuntu/</a> quantal-backports main restricted universe multiverse
deb-src <a href="http://mirrors.sohu.com/ubuntu/" target="_blank">http://mirrors.sohu.com/ubuntu/</a> quantal-backports main restricted universe multiverse
deb <a href="http://mirrors.sohu.com/ubuntu/" target="_blank">http://mirrors.sohu.com/ubuntu/</a> quantal-security main restricted
deb-src <a href="http://mirrors.sohu.com/ubuntu/" target="_blank">http://mirrors.sohu.com/ubuntu/</a> quantal-security main restricted
deb <a href="http://mirrors.sohu.com/ubuntu/" target="_blank">http://mirrors.sohu.com/ubuntu/</a> quantal-security universe
deb-src <a href="http://mirrors.sohu.com/ubuntu/" target="_blank">http://mirrors.sohu.com/ubuntu/</a> quantal-security universe
deb <a href="http://mirrors.sohu.com/ubuntu/" target="_blank">http://mirrors.sohu.com/ubuntu/</a> quantal-security multiverse
deb-src <a href="http://mirrors.sohu.com/ubuntu/" target="_blank">http://mirrors.sohu.com/ubuntu/</a> quantal-security multiverse
deb <a href="http://extras.ubuntu.com/ubuntu" target="_blank">http://extras.ubuntu.com/ubuntu</a> quantal main
deb-src <a href="http://extras.ubuntu.com/ubuntu" target="_blank">http://extras.ubuntu.com/ubuntu</a> quantal main</p>
<p>/#台湾源（台湾的ubuntu 更新源还是很给力的）
deb <a href="http://tw.archive.ubuntu.com/ubuntu/" target="_blank">http://tw.archive.ubuntu.com/ubuntu/</a> quantal main universe restricted multiverse
deb-src <a href="http://tw.archive.ubuntu.com/ubuntu/" target="_blank">http://tw.archive.ubuntu.com/ubuntu/</a> quantal main universe restricted multiverse
deb <a href="http://tw.archive.ubuntu.com/ubuntu/" target="_blank">http://tw.archive.ubuntu.com/ubuntu/</a> quantal-security universe main multiverse restricted
deb-src <a href="http://tw.archive.ubuntu.com/ubuntu/" target="_blank">http://tw.archive.ubuntu.com/ubuntu/</a> quantal-security universe main multiverse restricted
deb <a href="http://tw.archive.ubuntu.com/ubuntu/" target="_blank">http://tw.archive.ubuntu.com/ubuntu/</a> quantal-updates universe main multiverse restricted
deb-src <a href="http://tw.archive.ubuntu.com/ubuntu/" target="_blank">http://tw.archive.ubuntu.com/ubuntu/</a> quantal-updates universe main multiverse restricted</p>
<p>/#骨头源，骨头源是bones7456架设的一个Ubuntu源 ，提供ubuntu,deepin
deb <a href="http://ubuntu.srt.cn/ubuntu/" target="_blank">http://ubuntu.srt.cn/ubuntu/</a> quantal main universe restricted multiverse
deb-src <a href="http://ubuntu.srt.cn/ubuntu/" target="_blank">http://ubuntu.srt.cn/ubuntu/</a> quantal main universe restricted multiverse
deb <a href="http://ubuntu.srt.cn/ubuntu/" target="_blank">http://ubuntu.srt.cn/ubuntu/</a> quantal-security universe main multiverse restricted
deb-src <a href="http://ubuntu.srt.cn/ubuntu/" target="_blank">http://ubuntu.srt.cn/ubuntu/</a> quantal-security universe main multiverse restricted
deb <a href="http://ubuntu.srt.cn/ubuntu/" target="_blank">http://ubuntu.srt.cn/ubuntu/</a> quantal-updates universe main multiverse restricted
deb <a href="http://ubuntu.srt.cn/ubuntu/" target="_blank">http://ubuntu.srt.cn/ubuntu/</a> quantal-proposed universe main multiverse restricted
deb-src <a href="http://ubuntu.srt.cn/ubuntu/" target="_blank">http://ubuntu.srt.cn/ubuntu/</a> quantal-proposed universe main multiverse restricted
deb <a href="http://ubuntu.srt.cn/ubuntu/" target="_blank">http://ubuntu.srt.cn/ubuntu/</a> quantal-backports universe main multiverse restricted
deb-src <a href="http://ubuntu.srt.cn/ubuntu/" target="_blank">http://ubuntu.srt.cn/ubuntu/</a> quantal-backports universe main multiverse restricted
deb-src <a href="http://ubuntu.srt.cn/ubuntu/" target="_blank">http://ubuntu.srt.cn/ubuntu/</a> quantal-updates universe main multiverse restricted</p>
<p>/#ubuntu.cn99.com源（推荐）:
deb <a href="http://ubuntu.cn99.com/ubuntu/" target="_blank">http://ubuntu.cn99.com/ubuntu/</a> quantal main restricted universe multiverse
deb <a href="http://ubuntu.cn99.com/ubuntu/" target="_blank">http://ubuntu.cn99.com/ubuntu/</a> quantal-updates main restricted universe multiverse
deb <a href="http://ubuntu.cn99.com/ubuntu/" target="_blank">http://ubuntu.cn99.com/ubuntu/</a> quantal-security main restricted universe multiverse
deb <a href="http://ubuntu.cn99.com/ubuntu/" target="_blank">http://ubuntu.cn99.com/ubuntu/</a> quantal-backports main restricted universe multiverse
deb <a href="http://ubuntu.cn99.com/ubuntu-cn/" target="_blank">http://ubuntu.cn99.com/ubuntu-cn/</a> quantal main restricted universe multiverse</p>
<p>/#教育网源
/#电子科技大学
deb <a href="http://ubuntu.uestc.edu.cn/ubuntu/" target="_blank">http://ubuntu.uestc.edu.cn/ubuntu/</a> quantal main restricted universe multiverse
deb <a href="http://ubuntu.uestc.edu.cn/ubuntu/" target="_blank">http://ubuntu.uestc.edu.cn/ubuntu/</a> quantal-backports main restricted universe multiverse
deb <a href="http://ubuntu.uestc.edu.cn/ubuntu/" target="_blank">http://ubuntu.uestc.edu.cn/ubuntu/</a> quantal-proposed main restricted universe multiverse
deb <a href="http://ubuntu.uestc.edu.cn/ubuntu/" target="_blank">http://ubuntu.uestc.edu.cn/ubuntu/</a> quantal-security main restricted universe multiverse
deb <a href="http://ubuntu.uestc.edu.cn/ubuntu/" target="_blank">http://ubuntu.uestc.edu.cn/ubuntu/</a> quantal-updates main restricted universe multiverse
deb-src <a href="http://ubuntu.uestc.edu.cn/ubuntu/" target="_blank">http://ubuntu.uestc.edu.cn/ubuntu/</a> quantal main restricted universe multiverse
deb-src <a href="http://ubuntu.uestc.edu.cn/ubuntu/" target="_blank">http://ubuntu.uestc.edu.cn/ubuntu/</a> quantal-backports main restricted universe multiverse
deb-src <a href="http://ubuntu.uestc.edu.cn/ubuntu/" target="_blank">http://ubuntu.uestc.edu.cn/ubuntu/</a> quantal-proposed main restricted universe multiverse
deb-src <a href="http://ubuntu.uestc.edu.cn/ubuntu/" target="_blank">http://ubuntu.uestc.edu.cn/ubuntu/</a> quantal-security main restricted universe multiverse
deb-src <a href="http://ubuntu.uestc.edu.cn/ubuntu/" target="_blank">http://ubuntu.uestc.edu.cn/ubuntu/</a> quantal-updates main restricted universe multiverse</p>
<p>/#中国科技大学
deb <a href="http://debian.ustc.edu.cn/ubuntu/" target="_blank">http://debian.ustc.edu.cn/ubuntu/</a> quantal main restricted universe multiverse
deb <a href="http://debian.ustc.edu.cn/ubuntu/" target="_blank">http://debian.ustc.edu.cn/ubuntu/</a> quantal-backports restricted universe multiverse
deb <a href="http://debian.ustc.edu.cn/ubuntu/" target="_blank">http://debian.ustc.edu.cn/ubuntu/</a> quantal-proposed main restricted universe multiverse
deb <a href="http://debian.ustc.edu.cn/ubuntu/" target="_blank">http://debian.ustc.edu.cn/ubuntu/</a> quantal-security main restricted universe multiverse
deb <a href="http://debian.ustc.edu.cn/ubuntu/" target="_blank">http://debian.ustc.edu.cn/ubuntu/</a> quantal-updates main restricted universe multiverse
deb-src <a href="http://debian.ustc.edu.cn/ubuntu/" target="_blank">http://debian.ustc.edu.cn/ubuntu/</a> quantal main restricted universe multiverse
deb-src <a href="http://debian.ustc.edu.cn/ubuntu/" target="_blank">http://debian.ustc.edu.cn/ubuntu/</a> quantal-backports main restricted universe multiverse
deb-src <a href="http://debian.ustc.edu.cn/ubuntu/" target="_blank">http://debian.ustc.edu.cn/ubuntu/</a> quantal-proposed main restricted universe multiverse
deb-src <a href="http://debian.ustc.edu.cn/ubuntu/" target="_blank">http://debian.ustc.edu.cn/ubuntu/</a> quantal-security main restricted universe multiverse
deb-src <a href="http://debian.ustc.edu.cn/ubuntu/" target="_blank">http://debian.ustc.edu.cn/ubuntu/</a> quantal-updates main restricted universe multiverse
/#北京理工大学
deb <a href="http://mirror.bjtu.edu.cn/ubuntu/" target="_blank">http://mirror.bjtu.edu.cn/ubuntu/</a> quantal main multiverse restricted universe
deb <a href="http://mirror.bjtu.edu.cn/ubuntu/" target="_blank">http://mirror.bjtu.edu.cn/ubuntu/</a> quantal-backports main multiverse restricted universe
deb <a href="http://mirror.bjtu.edu.cn/ubuntu/" target="_blank">http://mirror.bjtu.edu.cn/ubuntu/</a> quantal-proposed main multiverse restricted universe
deb <a href="http://mirror.bjtu.edu.cn/ubuntu/" target="_blank">http://mirror.bjtu.edu.cn/ubuntu/</a> quantal-security main multiverse restricted universe
deb <a href="http://mirror.bjtu.edu.cn/ubuntu/" target="_blank">http://mirror.bjtu.edu.cn/ubuntu/</a> quantal-updates main multiverse restricted universe
deb-src <a href="http://mirror.bjtu.edu.cn/ubuntu/" target="_blank">http://mirror.bjtu.edu.cn/ubuntu/</a> quantal main multiverse restricted universe
deb-src <a href="http://mirror.bjtu.edu.cn/ubuntu/" target="_blank">http://mirror.bjtu.edu.cn/ubuntu/</a> quantal-backports main multiverse restricted universe
deb-src <a href="http://mirror.bjtu.edu.cn/ubuntu/" target="_blank">http://mirror.bjtu.edu.cn/ubuntu/</a> quantal-proposed main multiverse restricted universe
deb-src <a href="http://mirror.bjtu.edu.cn/ubuntu/" target="_blank">http://mirror.bjtu.edu.cn/ubuntu/</a> quantal-security main multiverse restricted universe
deb-src <a href="http://mirror.bjtu.edu.cn/ubuntu/" target="_blank">http://mirror.bjtu.edu.cn/ubuntu/</a> quantal-updates main multiverse restricted universe</p>
<p>/#兰州大学
deb ftp://mirror.lzu.edu.cn/ubuntu/ quantal main multiverse restricted universe
deb ftp://mirror.lzu.edu.cn/ubuntu/ quantal-backports main multiverse restricted universe
deb ftp://mirror.lzu.edu.cn/ubuntu/ quantal-proposed main multiverse restricted universe
deb ftp://mirror.lzu.edu.cn/ubuntu/ quantal-security main multiverse restricted universe
deb ftp://mirror.lzu.edu.cn/ubuntu/ quantal-updates main multiverse restricted universe
deb ftp://mirror.lzu.edu.cn/ubuntu-cn/ quantal main multiverse restricted universe</p>
<p>/#上海交通大学（上海交大源，教育网的速度不用说了）
deb <a href="http://ftp.sjtu.edu.cn/ubuntu/" target="_blank">http://ftp.sjtu.edu.cn/ubuntu/</a> quantal main multiverse restricted universe
deb <a href="http://ftp.sjtu.edu.cn/ubuntu/" target="_blank">http://ftp.sjtu.edu.cn/ubuntu/</a> quantal-backports main multiverse restricted universe
deb <a href="http://ftp.sjtu.edu.cn/ubuntu/" target="_blank">http://ftp.sjtu.edu.cn/ubuntu/</a> quantal-proposed main multiverse restricted universe
deb <a href="http://ftp.sjtu.edu.cn/ubuntu/" target="_blank">http://ftp.sjtu.edu.cn/ubuntu/</a> quantal-security main multiverse restricted universe
deb <a href="http://ftp.sjtu.edu.cn/ubuntu/" target="_blank">http://ftp.sjtu.edu.cn/ubuntu/</a> quantal-updates main multiverse restricted universe
deb <a href="http://ftp.sjtu.edu.cn/ubuntu-cn/" target="_blank">http://ftp.sjtu.edu.cn/ubuntu-cn/</a> quantal main multiverse restricted universe
deb-src <a href="http://ftp.sjtu.edu.cn/ubuntu/" target="_blank">http://ftp.sjtu.edu.cn/ubuntu/</a> quantal main multiverse restricted universe
deb-src <a href="http://ftp.sjtu.edu.cn/ubuntu/" target="_blank">http://ftp.sjtu.edu.cn/ubuntu/</a> quantal-backports main multiverse restricted universe
deb-src <a href="http://ftp.sjtu.edu.cn/ubuntu/" target="_blank">http://ftp.sjtu.edu.cn/ubuntu/</a> quantal-proposed main multiverse restricted universe
deb-src <a href="http://ftp.sjtu.edu.cn/ubuntu/" target="_blank">http://ftp.sjtu.edu.cn/ubuntu/</a> quantal-security main multiverse restricted universe
deb-src <a href="http://ftp.sjtu.edu.cn/ubuntu/" target="_blank">http://ftp.sjtu.edu.cn/ubuntu/</a> quantal-updates main multiverse restricted universe</p>
<p>添加好后保存，再输入 sudo apt-get update 就可以更新了，等着慢慢下载东西吧。</p>
<ul>
<li>1</li>
<li><a href="http://www.linuxidc.com/Linux/2012-10/73114.htm###" target="_blank">顶一下</a>
<a href="http://www.linuxidc.com/Linux/2012-10/73114.htm#" title="分享到新浪微博" target="_blank"></a> <a href="http://www.linuxidc.com/Linux/2012-10/73114.htm#" title="分享到百度搜藏"></a> <a href="http://www.linuxidc.com/Linux/2012-10/73114.htm#" title="分享到QQ空间" target="_blank"></a> <a href="http://www.linuxidc.com/Linux/2012-10/73114.htm#" title="分享到腾讯微博"></a> <a href="http://www.linuxidc.com/Linux/2012-10/73114.htm#" title="分享到人人网" target="_blank"></a> <a href="http://www.linuxidc.com/Linux/2012-10/73114.htm#" title="分享到搜狐微博"></a> <a href="http://www.linuxidc.com/Linux/2012-10/73114.htm#" title="分享到有道云笔记" target="_blank"></a> <a href="http://www.linuxidc.com/Linux/2012-10/73114.htm#" title="累计分享2次">2</a>
<a href="http://www.linuxidc.com/Linux/2012-10/73111.htm" target="_blank">Ubuntu Grub2启动上一次正确启动的内核</a></li>
</ul>
<p><a href="http://www.linuxidc.com/Linux/2012-10/73115.htm" target="_blank">Linux PAM make err : undefine yywrap()问题</a></p>
<p>相关资讯       <a href="http://www.linuxidc.com/search.aspx?where=nkey&amp;keyword=3408" target="_blank">Ubuntu源</a>  <a href="http://www.linuxidc.com/search.aspx?where=nkey&amp;keyword=3835" target="_blank">ubuntu更新源</a>  <a href="http://www.linuxidc.com/search.aspx?where=nkey&amp;keyword=14209" target="_blank">Ubuntu 12.10更新源</a>  <a href="http://www.linuxidc.com/search.aspx?where=nkey&amp;keyword=14210" target="_blank">Ubuntu教育网更新源</a> </p>
<ul>
<li><a href="http://www.linuxidc.com/Linux/2012-07/64562.htm" target="_blank">Ubuntu 12.04 Server 用户更新源</a>  (07月07日)</li>
<li><a href="http://www.linuxidc.com/Linux/2010-12/30938.htm" target="_blank">Ubuntu 10.10更新源</a>  (12/29/2010 20:10:37)</li>
<li><p><a href="http://www.linuxidc.com/Linux/2009-09/21827.htm" target="_blank">架设Ubuntu源时的两个脚本</a>  (09/22/2009 05:48:40)</p>
</li>
<li><p><a href="http://www.linuxidc.com/Linux/2011-11/47624.htm" target="_blank">Ubuntu 11.10 教育网源</a>  (11/20/2011 04:08:48)</p>
</li>
<li><a href="http://www.linuxidc.com/Linux/2010-01/24170.htm" target="_blank">Ubuntu 9.10更新源的添加和更新</a>  (01/23/2010 12:09:55)</li>
<li><p><a href="http://www.linuxidc.com/Linux/2009-09/21812.htm" title="利用Nginx反向代理功能架设Ubuntu升级源" target="_blank">利用Nginx反向代理功能架设Ubuntu</a>  (09/20/2009 14:38:44)
图片资讯      </p>
</li>
<li><p><a href="http://www.linuxidc.com/Linux/2010-12/30938.htm" target="_blank"><img src="" alt="Ubuntu 10.10更新源">Ubuntu 10.10更新源</a></p>
</li>
<li><a href="http://www.linuxidc.com/Linux/2010-01/24170.htm" title="Ubuntu 9.10更新源的添加和更新" target="_blank"><img src="" alt="Ubuntu 9.10更新源的添加和更新">Ubuntu 9.10更新源的</a></li>
</ul>
<p>本文评论 　　<a href="http://www.linuxidc.com/remark.aspx?id=73114" target="_blank">查看全部评论</a> (0)</p>
<p>表情： <img src="" alt="表情"> 姓名：  匿名 字数
点评：
同意评论声明 　　　发表评论声明</p>
<ul>
<li>尊重网上道德，遵守中华人民共和国的各项有关法律法规</li>
<li>承担一切因您的行为而直接或间接导致的民事或刑事法律责任</li>
<li>本站管理人员有权保留或删除其管辖留言中的任意内容</li>
<li>本站有权在网站内转载或引用您的评论</li>
<li><p>参与本评论即表明您已经阅读并接受上述条款
Digg排行</p>
</li>
<li><p><a href="http://www.linuxidc.com/Linux/2012-05/59564.htm" target="_blank">10Ubuntu 12.04安装QQ2012</a></p>
</li>
<li><a href="http://www.linuxidc.com/Linux/2011-11/47705.htm" title="Linux下除了某个文件外的其他文件全部删除" target="_blank">6Linux下除了某个文件外的其他文</a></li>
<li><a href="http://www.linuxidc.com/Linux/2010-04/25829.htm" title="在VMware虚拟机上安装Ubuntu 10.04" target="_blank">4在VMware虚拟机上安装Ubuntu 10.</a></li>
<li><a href="http://www.linuxidc.com/Linux/2012-10/71796.htm" title="如何从CentOS 6.0升级到CentOS 6.2" target="_blank">3如何从CentOS 6.0升级到CentOS 6</a></li>
<li><a href="http://www.linuxidc.com/Linux/2012-06/63424.htm" target="_blank">3Win7用VMware安装Fedora 16</a></li>
<li><a href="http://www.linuxidc.com/Linux/2012-05/60806.htm" target="_blank">3Ubuntu 12.04 root用户登录设置</a></li>
<li><a href="http://www.linuxidc.com/Linux/2011-12/48609.htm" title="虚拟机下安装BackTrack5 (BT5)教程及BT5汉化" target="_blank">3虚拟机下安装BackTrack5 (BT5)教</a></li>
<li><a href="http://www.linuxidc.com/Linux/2012-10/71874.htm" title="新安装 Ubuntu 12.10 需要做的 10 件事" target="_blank">2新安装 Ubuntu 12.10 需要做的</a></li>
<li><a href="http://www.linuxidc.com/Linux/2012-09/71478.htm" title="Ubuntu使用conky美化监测系统状态" target="_blank">2Ubuntu使用conky美化监测系统状</a></li>
<li><a href="http://www.linuxidc.com/Linux/2012-09/71477.htm" title="Ubuntu在顶部面板显示系统负载、流量，磁盘I/O" target="_blank">2Ubuntu在顶部面板显示系统负载、</a></li>
</ul>
<p><a href="http://www.linuxidc.com/Linux/2012-07/66157.htm" title="Oracle" target="_blank"><img src="" alt=""></a> <a href="http://www.linuxidc.net/" target="_blank"><img src="" alt="LinuxIDC"></a> <a href="http://www.linuxidc.com/search.aspx?Where=Nkey&amp;Keyword=Ubuntu+11.10" target="_blank"><img src="" alt="Ubuntu"></a> <a href="http://www.linuxidc.com/topicnews.aspx?tid=2" target="_blank"><img src="" alt=""></a>
最新资讯</p>
<ul>
<li><a href="http://www.linuxidc.com/Linux/2012-10/73115.htm" target="_blank">Linux PAM make err : undefine yywrap()问题</a></li>
<li><a href="">Ubuntu 12.10 软件更新源列表</a></li>
<li><a href="http://www.linuxidc.com/Linux/2012-10/73113.htm" target="_blank">C/# 使用定时任务 之 Timer类</a></li>
<li><a href="http://www.linuxidc.com/Linux/2012-10/73112.htm" target="_blank">C/# 使用SQLite数据库</a></li>
<li><a href="http://www.linuxidc.com/Linux/2012-10/73111.htm" target="_blank">Ubuntu Grub2启动上一次正确启动的内核</a></li>
<li><a href="http://www.linuxidc.com/Linux/2012-10/73110.htm" target="_blank">Mozilla 向微软送蛋糕 祝贺其 IE10 发布</a></li>
<li><a href="http://www.linuxidc.com/Linux/2012-10/73109.htm" target="_blank">Wine 1.5.16 发布</a></li>
<li><a href="http://www.linuxidc.com/Linux/2012-10/73108.htm" target="_blank">FreeNAS 8.3.0 正式版发布</a></li>
<li><a href="http://www.linuxidc.com/Linux/2012-10/73107.htm" target="_blank">Debian 7.0 “Wheezy” Beta3 发布</a></li>
<li><a href="http://www.linuxidc.com/Linux/2012-10/73106.htm" target="_blank">Hadoop安装配置入门手册</a></li>
<li><a href="http://www.linuxidc.com/Linux/2012-10/73105.htm" target="_blank">配置Hive使用嵌入式derby或客服模式derby方法</a></li>
<li><a href="http://www.linuxidc.com/Linux/2012-10/73104.htm" title="Canonical发布Ubuntu Nexus 7 Desktop Installer" target="_blank">Canonical发布Ubuntu Nexus 7 Desktop</a></li>
</ul>
<p>本周热门</p>
<ul>
<li><a href="http://www.linuxidc.com/Linux/2010-04/25829.htm" target="_blank">在VMware虚拟机上安装Ubuntu 10.04</a></li>
<li><a href="http://www.linuxidc.com/Linux/2012-05/59564.htm" target="_blank">Ubuntu 12.04安装QQ2012</a></li>
<li><a href="http://www.linuxidc.com/Linux/2010-03/24993.htm" target="_blank">Fedora 12 LiveCD安装记录[图文]</a></li>
<li><a href="http://www.linuxidc.com/Linux/2010-09/28435.htm" title="Ubuntu中用VirtualBox虚拟机安装Windows XP完整图解" target="_blank">Ubuntu中用VirtualBox虚拟机安装Windows XP完整</a></li>
<li><a href="http://www.linuxidc.com/Linux/2012-05/59663.htm" target="_blank">Ubuntu 12.04和Windows 7双系统安装图解</a></li>
<li><a href="http://www.linuxidc.com/Linux/2011-12/48609.htm" target="_blank">虚拟机下安装BackTrack5 (BT5)教程及BT5汉化</a></li>
<li><a href="http://www.linuxidc.com/Linux/2012-04/59433.htm" target="_blank">Windows XP硬盘安装Ubuntu 12.04双系统图文详解</a></li>
<li><a href="http://www.linuxidc.com/Linux/2010-04/25573.htm" target="_blank">Virtualbox虚拟机安装Ubuntu图文教程</a></li>
<li><a href="http://www.linuxidc.com/Linux/2006-11/1006.htm" target="_blank">红旗Linux5.0安装教程</a></li>
<li><a href="http://www.linuxidc.com/Linux/2011-10/46327.htm" target="_blank">Windows XP硬盘安装Ubuntu 11.10双系统全程图解</a></li>
<li><a href="http://www.linuxidc.com/Linux/2012-05/60321.htm" target="_blank">Linux卷管理详解--VG LV PV</a></li>
<li><a href="http://www.linuxidc.com/Linux/2012-06/63424.htm" target="_blank">Win7用VMware安装Fedora 16</a></li>
</ul>
<p><a href="http://www.linuxidc.com/aboutus.htm" target="_blank">Linux公社简介</a> - <a href="http://www.linuxidc.com/adsense.htm" target="_blank">广告服务</a> - <a href="http://www.linuxidc.com/sitemap.aspx" target="_blank">网站地图</a> - <a href="http://www.linuxidc.com/help.htm" target="_blank">帮助信息</a> - <a href="http://www.linuxidc.com/contactus.htm" target="_blank">联系我们</a>
本站（LinuxIDC）所刊载文章不代表同意其说法或描述，仅为提供更多信息，也不构成任何建议。
主编：漏网的鱼 (QQ:3165270) 联系邮箱:<img src="" alt=""> (如有版权及广告合作请联系)
本站带宽由[<a href="http://www.6688.cc/" target="_blank">6688.CC</a>]友情提供
关注Linux，关注LinuxIDC.com，请向您的QQ好友宣传LinuxIDC.com，多谢支持！
Copyright © 2006-2011　<a href="http://www.linuxidc.com/" target="_blank">Linux公社</a>　All rights reserved 浙ICP备06018118号</p>
<p>成功接收数据</p>

      
    </div>
    
    
      
    
    
        <footer id="post-meta">
        <span class="categories">Posted in<span class="breadcrumb fa fa-folder"><li><a href="/categories/linux/">linux</a></li></span><span class="breadcrumb"><li><a href="/categories/linux/">linux</a></li><li><a href="/categories/linux/ubuntu/">ubuntu</a></li></span></span> | <span class="tags">Tagged <a href="/tags/linux/" class="label label-primary">linux</a><a href="/tags/ubuntu/" class="label label-success">ubuntu</a></span> | <span class="time">recent updated:<time title="2014-03-07 09:54:36"datetime="2014-03-07 09:54:36"> mar. 7 2014</time></span> | <span class="comment-link">
<a href="http://itsolife.com/2014/02/02/2014-02-02-linux-ubuntu--Ubuntu1210软件更新源列表_Linux教程_Linux公社-Linux系统门户网站/#comments" class="ds-thread-count comment-link" data-thread-key="2014-02-02-linux-ubuntu--Ubuntu1210软件更新源列表_Linux教程_Linux公社-Linux系统门户网站" data-count-type="comments">暂无评论</a></span>
        </footer>
    
    <div class="clearfix"></div>
  </div>
</div>
</article>



  <article>
<div id="post" class="post well">
  <div class="post-content">
    <header class="well-sm">
      <i class="fa icon fa-5x pull-left"></i>
      <h1 class="title"><a href="/2014/02/02/2014-02-02-hadoop--mapred_tutorial/">mapred_tutorial</a></h1>
      
        <span>Posted on<time datetime="2014-02-02T01:54:36.000Z"> <a href="/2014/02/02/2014-02-02-hadoop--mapred_tutorial/">feb. 2 2014</a></time></span>      
    </header>
    
    
    <div class="entry">
      
        <h1 id="mapred_tutorial">mapred_tutorial</h1>
<p>Map/Reduce Tutorial
Table of contents
1 2 3 4 5
Purpose...............................................................................................................................2 Pre-requisites......................................................................................................................2 Overview............................................................................................................................2 Inputs and Outputs............................................................................................................. 3 Example: WordCount v1.0................................................................................................ 3
5.1 5.2 5.3
Source Code...................................................................................................................3 Usage............................................................................................................................. 6 Walk-through.................................................................................................................7 Payload.......................................................................................................................... 9 Job Configuration........................................................................................................ 13 Task Execution &amp; Environment.................................................................................. 14 Job Submission and Monitoring..................................................................................21 Job Input...................................................................................................................... 22 Job Output................................................................................................................... 23 Other Useful Features..................................................................................................25 Source Code.................................................................................................................31 Sample Runs................................................................................................................37 Highlights.................................................................................................................... 39
6
Map/Reduce - User Interfaces............................................................................................9
6.1 6.2 6.3 6.4 6.5 6.6 6.7
7
Example: WordCount v2.0.............................................................................................. 30
7.1 7.2 7.3
Copyright © 2008 The Apache Software Foundation. All rights reserved.
Map/Reduce Tutorial</p>
<ol>
<li>Purpose
This document comprehensively describes all user-facing facets of the Hadoop Map/Reduce framework and serves as a tutorial.</li>
<li>Pre-requisites
Ensure that Hadoop is installed, configured and is running. More details: • Hadoop Quick Start for first-time users. • Hadoop Cluster Setup for large, distributed clusters.</li>
<li>Overview
Hadoop Map/Reduce is a software framework for easily writing applications which process vast amounts of data (multi-terabyte data-sets) in-parallel on large clusters (thousands of nodes) of commodity hardware in a reliable, fault-tolerant manner. A Map/Reduce job usually splits the input data-set into independent chunks which are processed by the map tasks in a completely parallel manner. The framework sorts the outputs of the maps, which are then input to the reduce tasks. Typically both the input and the output of the job are stored in a file-system. The framework takes care of scheduling tasks, monitoring them and re-executes the failed tasks. Typically the compute nodes and the storage nodes are the same, that is, the Map/Reduce framework and the Hadoop Distributed File System (see HDFS Architecture ) are running on the same set of nodes. This configuration allows the framework to effectively schedule tasks on the nodes where data is already present, resulting in very high aggregate bandwidth across the cluster. The Map/Reduce framework consists of a single master JobTracker and one slave TaskTracker per cluster-node. The master is responsible for scheduling the jobs&#39; component tasks on the slaves, monitoring them and re-executing the failed tasks. The slaves execute the tasks as directed by the master. Minimally, applications specify the input/output locations and supply map and reduce functions via implementations of appropriate interfaces and/or abstract-classes. These, and other job parameters, comprise the job configuration. The Hadoop job client then submits the job (jar/executable etc.) and configuration to the JobTracker which then assumes the responsibility of distributing the software/configuration to the slaves, scheduling tasks and monitoring them, providing status and diagnostic information to the job-client.
Page 2
Copyright © 2008 The Apache Software Foundation. All rights reserved.
Map/Reduce Tutorial
Although the Hadoop framework is implemented in JavaTM, Map/Reduce applications need not be written in Java. • Hadoop Streaming is a utility which allows users to create and run jobs with any executables (e.g. shell utilities) as the mapper and/or the reducer. • Hadoop Pipes is a SWIG- compatible C++ API to implement Map/Reduce applications (non JNITM based).</li>
<li>Inputs and Outputs
The Map/Reduce framework operates exclusively on <key, value> pairs, that is, the framework views the input to the job as a set of <key, value> pairs and produces a set of <key, value> pairs as the output of the job, conceivably of different types. The key and value classes have to be serializable by the framework and hence need to implement the Writable interface. Additionally, the key classes have to implement the WritableComparable interface to facilitate sorting by the framework. Input and Output types of a Map/Reduce job: (input) <k1, v1> -&gt; map -&gt; <k2, v2> -&gt; combine -&gt; <k2, v2> -&gt; reduce -&gt; <k3, v3> (output)</li>
<li>Example: WordCount v1.0
Before we jump into the details, lets walk through an example Map/Reduce application to get a flavour for how they work. WordCount is a simple application that counts the number of occurences of each word in a given input set. This works with a local-standalone, pseudo-distributed or fully-distributed Hadoop installation(see Hadoop Quick Start).
5.1. Source Code
WordCount.java 1. 2. 3. 4. import java.io.IOException; import java.util./*; package org.myorg;
Page 3
Copyright © 2008 The Apache Software Foundation. All rights reserved.
Map/Reduce Tutorial</li>
<li><ol>
<li><ol>
<li><ol>
<li><ol>
<li><ol>
<li><ol>
<li><ol>
<li><ol>
<li><ol>
<li>public static class Map extends MapReduceBase implements Mapper<LongWritable, Text, Text, IntWritable> { private final static IntWritable one = new IntWritable(1); private Text word = new Text(); public class WordCount { import org.apache.hadoop.fs.Path; import org.apache.hadoop.conf./<em>; import org.apache.hadoop.io./</em>; import org.apache.hadoop.mapred./<em>; import org.apache.hadoop.util./</em>;</li>
</ol>
</li>
</ol>
</li>
</ol>
</li>
</ol>
</li>
</ol>
</li>
</ol>
</li>
</ol>
</li>
</ol>
</li>
</ol>
</li>
<li><ol>
<li><ol>
<li>18.
public void map(LongWritable key, Text value, OutputCollector<Text, IntWritable> output, Reporter reporter) throws IOException { String line = value.toString(); StringTokenizer tokenizer = new StringTokenizer(line); while (tokenizer.hasMoreTokens()) { word.set(tokenizer.nextToken());</li>
</ol>
</li>
</ol>
</li>
<li><ol>
<li><ol>
<li><ol>
<li><ol>
<li><ol>
<li>25.
output.collect(word, one); } }
Page 4
Copyright © 2008 The Apache Software Foundation. All rights reserved.
Map/Reduce Tutorial</li>
</ol>
</li>
</ol>
</li>
</ol>
</li>
</ol>
</li>
</ol>
</li>
<li><ol>
<li>28.
}
public static class Reduce extends MapReduceBase implements Reducer<Text, IntWritable, Text, IntWritable> { public void reduce(Text key, Iterator<IntWritable> values, OutputCollector<Text, IntWritable> output, Reporter reporter) throws IOException { int sum = 0; while (values.hasNext()) { sum += values.next().get(); } output.collect(key, new IntWritable(sum)); } }
29.</li>
</ol>
</li>
<li><ol>
<li><ol>
<li><ol>
<li><ol>
<li><ol>
<li><ol>
<li><ol>
<li><ol>
<li><ol>
<li><ol>
<li><ol>
<li>42.
public static void main(String[] args) throws Exception { JobConf conf = new JobConf(WordCount.class); conf.setJobName(&quot;wordcount&quot;);
conf.setOutputKeyClass(Text.class); 43. conf.setOutputValueClass(IntWritable.class); 44. 45. conf.setMapperClass(Map.class);
Page 5
Copyright © 2008 The Apache Software Foundation. All rights reserved.
Map/Reduce Tutorial</li>
</ol>
</li>
</ol>
</li>
</ol>
</li>
</ol>
</li>
</ol>
</li>
</ol>
</li>
</ol>
</li>
</ol>
</li>
</ol>
</li>
</ol>
</li>
</ol>
</li>
<li>conf.setCombinerClass(Reduce.class); 47. conf.setReducerClass(Reduce.class); 48. 49. conf.setInputFormat(TextInputFormat.class); 50. conf.setOutputFormat(TextOutputFormat.class); 51. 52. FileInputFormat.setInputPaths(conf, new Path(args[0])); 53. FileOutputFormat.setOutputPath(conf, new Path(args[1])); 54. 55. 57. 58. 59. } JobClient.runJob(conf); }
5.2. Usage
Assuming HADOOP_HOME is the root of the installation and HADOOP_VERSION is the Hadoop version installed, compile WordCount.java and create a jar: $ mkdir wordcount_classes $ javac -classpath ${HADOOP_HOME}/hadoop-${HADOOP_VERSION}-core.jar -d wordcount_classes WordCount.java $ jar -cvf /usr/joe/wordcount.jar -C wordcount_classes/ . Assuming that: • /usr/joe/wordcount/input - input directory in HDFS
Page 6
Copyright © 2008 The Apache Software Foundation. All rights reserved.
Map/Reduce Tutorial
•
/usr/joe/wordcount/output - output directory in HDFS
Sample text-files as input: $ bin/hadoop dfs -ls /usr/joe/wordcount/input/ /usr/joe/wordcount/input/file01 /usr/joe/wordcount/input/file02 $ bin/hadoop dfs -cat /usr/joe/wordcount/input/file01 Hello World Bye World $ bin/hadoop dfs -cat /usr/joe/wordcount/input/file02 Hello Hadoop Goodbye Hadoop Run the application: $ bin/hadoop jar /usr/joe/wordcount.jar org.myorg.WordCount /usr/joe/wordcount/input /usr/joe/wordcount/output Output: $ bin/hadoop dfs -cat /usr/joe/wordcount/output/part-00000 Bye 1 Goodbye 1 Hadoop 2 Hello 2 World 2 Applications can specify a comma separated list of paths which would be present in the current working directory of the task using the option -files. The -libjars option allows applications to add jars to the classpaths of the maps and reduces. The -archives allows them to pass archives as arguments that are unzipped/unjarred and a link with name of the jar/zip are created in the current working directory of tasks. More details about the command line options are available at Hadoop Command Guide. Running wordcount example with -libjars and -files: hadoop jar hadoop-examples.jar wordcount -files cachefile.txt -libjars mylib.jar input output
5.3. Walk-through
The WordCount application is quite straight-forward. The Mapper implementation (lines 14-26), via the map method (lines 18-25), processes one line at a time, as provided by the specified TextInputFormat (line 49). It then splits the line into tokens separated by whitespaces, via the StringTokenizer, and emits a
Page 7
Copyright © 2008 The Apache Software Foundation. All rights reserved.
Map/Reduce Tutorial
key-value pair of &lt; <word>, 1&gt;. For the given sample input the first map emits: &lt; Hello, 1&gt; &lt; World, 1&gt; &lt; Bye, 1&gt; &lt; World, 1&gt; The second map emits: &lt; Hello, 1&gt; &lt; Hadoop, 1&gt; &lt; Goodbye, 1&gt; &lt; Hadoop, 1&gt; We&#39;ll learn more about the number of maps spawned for a given job, and how to control them in a fine-grained manner, a bit later in the tutorial. WordCount also specifies a combiner (line 46). Hence, the output of each map is passed through the local combiner (which is same as the Reducer as per the job configuration) for local aggregation, after being sorted on the keys. The output of the first map: &lt; Bye, 1&gt; &lt; Hello, 1&gt; &lt; World, 2&gt; The output of the second map: &lt; Goodbye, 1&gt; &lt; Hadoop, 2&gt; &lt; Hello, 1&gt; The Reducer implementation (lines 28-36), via the reduce method (lines 29-35) just sums up the values, which are the occurence counts for each key (i.e. words in this example). Thus the output of the job is: &lt; Bye, 1&gt; &lt; Goodbye, 1&gt; &lt; Hadoop, 2&gt; &lt; Hello, 2&gt; &lt; World, 2&gt; The run method specifies various facets of the job, such as the input/output paths (passed via the command line), key/value types, input/output formats etc., in the JobConf. It then calls the JobClient.runJob (line 55) to submit the and monitor its progress.
Page 8
Copyright © 2008 The Apache Software Foundation. All rights reserved.
Map/Reduce Tutorial
We&#39;ll learn more about JobConf, JobClient, Tool and other interfaces and classes a bit later in the tutorial.</li>
<li>Map/Reduce - User Interfaces
This section provides a reasonable amount of detail on every user-facing aspect of the Map/Reduce framwork. This should help users implement, configure and tune their jobs in a fine-grained manner. However, please note that the javadoc for each class/interface remains the most comprehensive documentation available; this is only meant to be a tutorial. Let us first take the Mapper and Reducer interfaces. Applications typically implement them to provide the map and reduce methods. We will then discuss other core interfaces including JobConf, JobClient, Partitioner, OutputCollector, Reporter, InputFormat, OutputFormat, OutputCommitter and others. Finally, we will wrap up by discussing some useful features of the framework such as the DistributedCache, IsolationRunner etc.
6.1. Payload
Applications typically implement the Mapper and Reducer interfaces to provide the map and reduce methods. These form the core of the job. 6.1.1. Mapper Mapper maps input key/value pairs to a set of intermediate key/value pairs. Maps are the individual tasks that transform input records into intermediate records. The transformed intermediate records do not need to be of the same type as the input records. A given input pair may map to zero or many output pairs. The Hadoop Map/Reduce framework spawns one map task for each InputSplit generated by the InputFormat for the job. Overall, Mapper implementations are passed the JobConf for the job via the JobConfigurable.configure(JobConf) method and override it to initialize themselves. The framework then calls map(WritableComparable, Writable, OutputCollector, Reporter) for each key/value pair in the InputSplit for that task. Applications can then override the Closeable.close() method to perform any required cleanup. Output pairs do not need to be of the same types as input pairs. A given input pair may map
Page 9
Copyright © 2008 The Apache Software Foundation. All rights reserved.
Map/Reduce Tutorial
to zero or many output pairs. Output pairs are collected with calls to OutputCollector.collect(WritableComparable,Writable). Applications can use the Reporter to report progress, set application-level status messages and update Counters, or just indicate that they are alive. All intermediate values associated with a given output key are subsequently grouped by the framework, and passed to the Reducer(s) to determine the final output. Users can control the grouping by specifying a Comparator via JobConf.setOutputKeyComparatorClass(Class). The Mapper outputs are sorted and then partitioned per Reducer. The total number of partitions is the same as the number of reduce tasks for the job. Users can control which keys (and hence records) go to which Reducer by implementing a custom Partitioner. Users can optionally specify a combiner, via JobConf.setCombinerClass(Class), to perform local aggregation of the intermediate outputs, which helps to cut down the amount of data transferred from the Mapper to the Reducer. The intermediate, sorted outputs are always stored in a simple (key-len, key, value-len, value) format. Applications can control if, and how, the intermediate outputs are to be compressed and the CompressionCodec to be used via the JobConf.
6.1.1.1. How Many Maps?
The number of maps is usually driven by the total size of the inputs, that is, the total number of blocks of the input files. The right level of parallelism for maps seems to be around 10-100 maps per-node, although it has been set up to 300 maps for very cpu-light map tasks. Task setup takes awhile, so it is best if the maps take at least a minute to execute. Thus, if you expect 10TB of input data and have a blocksize of 128MB, you&#39;ll end up with 82,000 maps, unless setNumMapTasks(int) (which only provides a hint to the framework) is used to set it even higher. 6.1.2. Reducer Reducer reduces a set of intermediate values which share a key to a smaller set of values. The number of reduces for the job is set by the user via JobConf.setNumReduceTasks(int). Overall, Reducer implementations are passed the JobConf for the job via the JobConfigurable.configure(JobConf) method and can override it to initialize themselves. The
Page 10
Copyright © 2008 The Apache Software Foundation. All rights reserved.
Map/Reduce Tutorial
framework then calls reduce(WritableComparable, Iterator, OutputCollector, Reporter) method for each <key, (list of values)> pair in the grouped inputs. Applications can then override the Closeable.close() method to perform any required cleanup. Reducer has 3 primary phases: shuffle, sort and reduce.
6.1.2.1. Shuffle
Input to the Reducer is the sorted output of the mappers. In this phase the framework fetches the relevant partition of the output of all the mappers, via HTTP.
6.1.2.2. Sort
The framework groups Reducer inputs by keys (since different mappers may have output the same key) in this stage. The shuffle and sort phases occur simultaneously; while map-outputs are being fetched they are merged.
Secondary Sort
If equivalence rules for grouping the intermediate keys are required to be different from those for grouping keys before reduction, then one may specify a Comparator via JobConf.setOutputValueGroupingComparator(Class). Since JobConf.setOutputKeyComparatorClass(Class) can be used to control how intermediate keys are grouped, these can be used in conjunction to simulate secondary sort on values.
6.1.2.3. Reduce
In this phase the reduce(WritableComparable, Iterator, OutputCollector, Reporter) method is called for each <key, (list of values)> pair in the grouped inputs. The output of the reduce task is typically written to the FileSystem via OutputCollector.collect(WritableComparable, Writable). Applications can use the Reporter to report progress, set application-level status messages and update Counters, or just indicate that they are alive. The output of the Reducer is not sorted.
6.1.2.4. How Many Reduces?
The right number of reduces seems to be 0.95 or 1.75 multiplied by (<no. of nodes> /<em> mapred.tasktracker.reduce.tasks.maximum).
Page 11
Copyright © 2008 The Apache Software Foundation. All rights reserved.
Map/Reduce Tutorial
With 0.95 all of the reduces can launch immediately and start transfering map outputs as the maps finish. With 1.75 the faster nodes will finish their first round of reduces and launch a second wave of reduces doing a much better job of load balancing. Increasing the number of reduces increases the framework overhead, but increases load balancing and lowers the cost of failures. The scaling factors above are slightly less than whole numbers to reserve a few reduce slots in the framework for speculative-tasks and failed tasks.
6.1.2.5. Reducer NONE
It is legal to set the number of reduce-tasks to zero if no reduction is desired. In this case the outputs of the map-tasks go directly to the FileSystem, into the output path set by setOutputPath(Path). The framework does not sort the map-outputs before writing them out to the FileSystem. 6.1.3. Partitioner Partitioner partitions the key space. Partitioner controls the partitioning of the keys of the intermediate map-outputs. The key (or a subset of the key) is used to derive the partition, typically by a hash function. The total number of partitions is the same as the number of reduce tasks for the job. Hence this controls which of the m reduce tasks the intermediate key (and hence the record) is sent to for reduction. HashPartitioner is the default Partitioner. 6.1.4. Reporter Reporter is a facility for Map/Reduce applications to report progress, set application-level status messages and update Counters. Mapper and Reducer implementations can use the Reporter to report progress or just indicate that they are alive. In scenarios where the application takes a significant amount of time to process individual key/value pairs, this is crucial since the framework might assume that the task has timed-out and kill that task. Another way to avoid this is to set the configuration parameter mapred.task.timeout to a high-enough value (or even set it to zero for no time-outs). Applications can also update Counters using the Reporter.
Page 12
Copyright © 2008 The Apache Software Foundation. All rights reserved.
Map/Reduce Tutorial
6.1.5. OutputCollector OutputCollector is a generalization of the facility provided by the Map/Reduce framework to collect data output by the Mapper or the Reducer (either the intermediate outputs or the output of the job). Hadoop Map/Reduce comes bundled with a library of generally useful mappers, reducers, and partitioners.
6.2. Job Configuration
JobConf represents a Map/Reduce job configuration. JobConf is the primary interface for a user to describe a Map/Reduce job to the Hadoop framework for execution. The framework tries to faithfully execute the job as described by JobConf, however: • f Some configuration parameters may have been marked as final by administrators and hence cannot be altered. • While some job parameters are straight-forward to set (e.g. setNumReduceTasks(int)), other parameters interact subtly with the rest of the framework and/or job configuration and are more complex to set (e.g. setNumMapTasks(int)). JobConf is typically used to specify the Mapper, combiner (if any), Partitioner, Reducer, InputFormat, OutputFormat and OutputCommitter implementations. JobConf also indicates the set of input files (setInputPaths(JobConf, Path...) /addInputPath(JobConf, Path)) and (setInputPaths(JobConf, String) /addInputPaths(JobConf, String)) and where the output files should be written (setOutputPath(Path)). Optionally, JobConf is used to specify other advanced facets of the job such as the Comparator to be used, files to be put in the DistributedCache, whether intermediate and/or job outputs are to be compressed (and how), debugging via user-provided scripts (setMapDebugScript(String)/setReduceDebugScript(String)) , whether job tasks can be executed in a speculative manner (setMapSpeculativeExecution(boolean))/(setReduceSpeculativeExecution(boolean)) , maximum number of attempts per task (setMaxMapAttempts(int)/setMaxReduceAttempts(int)) , percentage of tasks failure which can be tolerated by the job (setMaxMapTaskFailuresPercent(int)/setMaxReduceTaskFailuresPercent(int)) etc. Of course, users can use set(String, String)/get(String, String) to set/get arbitrary parameters needed by applications. However, use the DistributedCache for large amounts of
Page 13
Copyright © 2008 The Apache Software Foundation. All rights reserved.
Map/Reduce Tutorial
(read-only) data.
6.3. Task Execution &amp; Environment
The TaskTracker executes the Mapper/ Reducer task as a child process in a separate jvm. The child-task inherits the environment of the parent TaskTracker. The user can specify additional options to the child-jvm via the mapred.child.java.opts configuration parameter in the JobConf such as non-standard paths for the run-time linker to search shared libraries via -Djava.library.path=&lt;&gt; etc. If the mapred.child.java.opts contains the symbol @taskid@ it is interpolated with value of taskid of the map/reduce task. Here is an example with multiple arguments and substitutions, showing jvm GC logging, and start of a passwordless JVM JMX agent so that it can connect with jconsole and the likes to watch child memory, threads and get thread dumps. It also sets the maximum heap-size of the child jvm to 512MB and adds an additional path to the java.library.path of the child-jvm. <property> <name>mapred.child.java.opts</name> <value> -Xmx512M -Djava.library.path=/home/mycompany/lib -verbose:gc -Xloggc:/tmp/@taskid@.gc -Dcom.sun.management.jmxremote.authenticate=false -Dcom.sun.management.jmxremote.ssl=false </value> </property> 6.3.1. Memory management Users/admins can also specify the maximum virtual memory of the launched child-task, and any sub-process it launches recursively, using mapred.child.ulimit. Note that the value set here is a per process limit. The value for mapred.child.ulimit should be specified in kilo bytes (KB). And also the value must be greater than or equal to the -Xmx passed to JavaVM, else the VM might not start. Note: mapred.child.java.opts are used only for configuring the launched child tasks from task tracker. Configuring the memory options for daemons is documented in cluster_setup.html The memory available to some parts of the framework is also configurable. In map and
Page 14
Copyright © 2008 The Apache Software Foundation. All rights reserved.
Map/Reduce Tutorial
reduce tasks, performance may be influenced by adjusting parameters influencing the concurrency of operations and the frequency with which data will hit disk. Monitoring the filesystem counters for a job- particularly relative to byte counts from the map and into the reduce- is invaluable to the tuning of these parameters. 6.3.2. Map Parameters A record emitted from a map will be serialized into a buffer and metadata will be stored into accounting buffers. As described in the following options, when either the serialization buffer or the metadata exceed a threshold, the contents of the buffers will be sorted and written to disk in the background while the map continues to output records. If either buffer fills completely while the spill is in progress, the map thread will block. When the map is finished, any remaining records are written to disk and all on-disk segments are merged into a single file. Minimizing the number of spills to disk can decrease map time, but a larger buffer also decreases the memory available to the mapper.
Name io.sort.mb int Type Description The cumulative size of the serialization and accounting buffers storing records emitted from the map, in megabytes. The ratio of serialization to accounting space can be adjusted. Each serialized record requires 16 bytes of accounting information in addition to its serialized size to effect the sort. This percentage of space allocated from io.sort.mb affects the probability of a spill to disk being caused by either exhaustion of the serialization buffer or the accounting space. Clearly, for a map outputting small records, a higher value than the default will likely decrease the number of spills to disk. This is the threshold for the accounting and serialization buffers. When this percentage of either buffer has filled, their
io.sort.record.percent
float
io.sort.spill.percent
float
Page 15
Copyright © 2008 The Apache Software Foundation. All rights reserved.
Map/Reduce Tutorial
contents will be spilled to disk in the background. Let io.sort.record.percent be r, io.sort.mb be x, and this value be q. The maximum number of records collected before the collection thread will spill is r /</em> x /<em> q /</em> 2^16. Note that a higher value may decrease the number of- or even eliminate- merges, but will also increase the probability of the map task getting blocked. The lowest average map times are usually obtained by accurately estimating the size of the map output and preventing multiple spills.
Other notes • If either spill threshold is exceeded while a spill is in progress, collection will continue until the spill is finished. For example, if io.sort.buffer.spill.percent is set to 0.33, and the remainder of the buffer is filled while the spill runs, the next spill will include all the collected records, or 0.66 of the buffer, and will not generate additional spills. In other words, the thresholds are defining triggers, not blocking. • A record larger than the serialization buffer will first trigger a spill, then be spilled to a separate file. It is undefined whether or not this record will first pass through the combiner. 6.3.3. Shuffle/Reduce Parameters As described previously, each reduce fetches the output assigned to it by the Partitioner via HTTP into memory and periodically merges these outputs to disk. If intermediate compression of map outputs is turned on, each output is decompressed into memory. The following options affect the frequency of these merges to disk prior to the reduce and the memory allocated to map output during the reduce.
Name io.sort.factor int Type Description Specifies the number of segments on disk to be merged at the same time. It limits the number of open files and compression codecs during the merge. If the number of files
Page 16
Copyright © 2008 The Apache Software Foundation. All rights reserved.
Map/Reduce Tutorial
exceeds this limit, the merge will proceed in several passes. Though this limit also applies to the map, most jobs should be configured so that hitting this limit is unlikely there. mapred.inmem.merge.threshold int The number of sorted map outputs fetched into memory before being merged to disk. Like the spill thresholds in the preceding note, this is not defining a unit of partition, but a trigger. In practice, this is usually set very high (1000) or disabled (0), since merging in-memory segments is often less expensive than merging from disk (see notes following this table). This threshold influences only the frequency of in-memory merges during the shuffle. The memory threshold for fetched map outputs before an in-memory merge is started, expressed as a percentage of memory allocated to storing map outputs in memory. Since map outputs that can&#39;t fit in memory can be stalled, setting this high may decrease parallelism between the fetch and merge. Conversely, values as high as 1.0 have been effective for reduces whose input can fit entirely in memory. This parameter influences only the frequency of in-memory merges during the shuffle. The percentage of memoryrelative to the maximum heapsize as typically specified in mapred.child.java.optsthat can be allocated to storing
mapred.job.shuffle.merge.percentfloat
mapred.job.shuffle.input.buffer.percent float
Page 17
Copyright © 2008 The Apache Software Foundation. All rights reserved.
Map/Reduce Tutorial
map outputs during the shuffle. Though some memory should be set aside for the framework, in general it is advantageous to set this high enough to store large and numerous map outputs. mapred.job.reduce.input.buffer.percent float The percentage of memory relative to the maximum heapsize in which map outputs may be retained during the reduce. When the reduce begins, map outputs will be merged to disk until those that remain are under the resource limit this defines. By default, all map outputs are merged to disk before the reduce begins to maximize the memory available to the reduce. For less memory-intensive reduces, this should be increased to avoid trips to disk.
Other notes • If a map output is larger than 25 percent of the memory allocated to copying map outputs, it will be written directly to disk without first staging through memory. • When running with a combiner, the reasoning about high merge thresholds and large buffers may not hold. For merges started before all map outputs have been fetched, the combiner is run while spilling to disk. In some cases, one can obtain better reduce times by spending resources combining map outputs- making disk spills small and parallelizing spilling and fetching- rather than aggressively increasing buffer sizes. • When merging in-memory map outputs to disk to begin the reduce, if an intermediate merge is necessary because there are segments to spill and at least io.sort.factor segments already on disk, the in-memory map outputs will be part of the intermediate merge. 6.3.4. Directory Structure The task tracker has local directory, ${mapred.local.dir}/taskTracker/ to create localized cache and localized job. It can define multiple local directories (spanning multiple disks) and then each filename is assigned to a semi-random local directory. When the job starts, task tracker creates a localized job directory relative to the local directory specified in
Page 18
Copyright © 2008 The Apache Software Foundation. All rights reserved.
Map/Reduce Tutorial
the configuration. Thus the task tracker directory structure looks the following: • ${mapred.local.dir}/taskTracker/archive/ : The distributed cache. This directory holds the localized distributed cache. Thus localized distributed cache is shared among all the tasks and jobs • ${mapred.local.dir}/taskTracker/jobcache/$jobid/ : The localized job directory • ${mapred.local.dir}/taskTracker/jobcache/$jobid/work/ : The job-specific shared directory. The tasks can use this space as scratch space and share files among them. This directory is exposed to the users through the configuration property job.local.dir. The directory can accessed through api JobConf.getJobLocalDir(). It is available as System property also. So, users (streaming etc.) can call System.getProperty(&quot;job.local.dir&quot;) to access the directory. • ${mapred.local.dir}/taskTracker/jobcache/$jobid/jars/ : The jars directory, which has the job jar file and expanded jar. The job.jar is the application&#39;s jar file that is automatically distributed to each machine. It is expanded in jars directory before the tasks for the job start. The job.jar location is accessible to the application through the api JobConf.getJar() . To access the unjarred directory, JobConf.getJar().getParent() can be called. • ${mapred.local.dir}/taskTracker/jobcache/$jobid/job.xml : The job.xml file, the generic job configuration, localized for the job. • ${mapred.local.dir}/taskTracker/jobcache/$jobid/$taskid : The task directory for each task attempt. Each task directory again has the following structure : • ${mapred.local.dir}/taskTracker/jobcache/$jobid/$taskid/job.xml : A job.xml file, task localized job configuration, Task localization means that properties have been set that are specific to this particular task within the job. The properties localized for each task are described below. • ${mapred.local.dir}/taskTracker/jobcache/$jobid/$taskid/output : A directory for intermediate output files. This contains the temporary map reduce data generated by the framework such as map output files etc. • ${mapred.local.dir}/taskTracker/jobcache/$jobid/$taskid/work : The curernt working directory of the task. With jvm reuse enabled for tasks, this directory will be the directory on which the jvm has started • ${mapred.local.dir}/taskTracker/jobcache/$jobid/$taskid/work/tmp : The temporary directory for the task. (User can specify the property mapred.child.tmp to set the value of temporary directory for map and reduce tasks. This defaults to ./tmp. If the value is not an absolute path, it is prepended with task&#39;s working directory. Otherwise, it is directly assigned. The directory will be created if it doesn&#39;t exist. Then, the child java tasks are executed
Page 19
Copyright © 2008 The Apache Software Foundation. All rights reserved.
Map/Reduce Tutorial
with option -Djava.io.tmpdir=&#39;the absolute path of the tmp dir&#39;. Anp pipes and streaming are set with environment variable, TMPDIR=&#39;the absolute path of the tmp dir&#39;). This directory is created, if mapred.child.tmp has the value ./tmp
6.3.5. Task JVM Reuse Jobs can enable task JVMs to be reused by specifying the job configuration mapred.job.reuse.jvm.num.tasks. If the value is 1 (the default), then JVMs are not reused (i.e. 1 task per JVM). If it is -1, there is no limit to the number of tasks a JVM can run (of the same job). One can also specify some value greater than 1 using the api JobConf.setNumTasksToExecutePerJvm(int) The following properties are localized in the job configuration for each task&#39;s execution:
Name mapred.job.id mapred.jar job.local.dir mapred.tip.id mapred.task.id mapred.task.is.map mapred.task.partition map.input.file map.input.start map.input.length mapred.work.output.dir String String String String String boolean int String long long String Type The job id job.jar location in job directory The job specific shared scratch space The task id The task attempt id Is this a map task The id of the task within the job The filename that the map is reading from The offset of the start of the map input split The number of bytes in the map input split The task&#39;s temporary output directory Description
The standard output (stdout) and error (stderr) streams of the task are read by the TaskTracker and logged to ${HADOOP<em>LOG_DIR}/userlogs
Page 20
Copyright © 2008 The Apache Software Foundation. All rights reserved.
Map/Reduce Tutorial
The DistributedCache can also be used to distribute both jars and native libraries for use in the map and/or reduce tasks. The child-jvm always has its current working directory added to the java.library.path and LD_LIBRARY_PATH. And hence the cached libraries can be loaded via System.loadLibrary or System.load. More details on how to load shared libraries through distributed cache are documented at native_libraries.html
6.4. Job Submission and Monitoring
JobClient is the primary interface by which user-job interacts with the JobTracker. JobClient provides facilities to submit jobs, track their progress, access component-tasks&#39; reports and logs, get the Map/Reduce cluster&#39;s status information and so on. The job submission process involves: 1. Checking the input and output specifications of the job. 2. Computing the InputSplit values for the job. 3. Setting up the requisite accounting information for the DistributedCache of the job, if necessary. 4. Copying the job&#39;s jar and configuration to the Map/Reduce system directory on the FileSystem. 5. Submitting the job to the JobTracker and optionally monitoring it&#39;s status. Job history files are also logged to user specified directory hadoop.job.history.user.location which defaults to job output directory. The files are stored in &quot;_logs/history/&quot; in the specified directory. Hence, by default they will be in mapred.output.dir/_logs/history. User can stop logging by giving the value none for hadoop.job.history.user.location User can view the history logs summary in specified directory using the following command $ bin/hadoop job -history output-dir This command will print job details, failed and killed tip details. More details about the job such as successful tasks and task attempts made for each task can be viewed using the following command $ bin/hadoop job -history all output-dir User can use OutputLogFilter to filter log files from the output directory listing. Normally the user creates the application, describes various facets of the job via JobConf, and then uses the JobClient to submit the job and monitor its progress. 6.4.1. Job Control Users may need to chain Map/Reduce jobs to accomplish complex tasks which cannot be
Page 21
Copyright © 2008 The Apache Software Foundation. All rights reserved.
Map/Reduce Tutorial
done via a single Map/Reduce job. This is fairly easy since the output of the job typically goes to distributed file-system, and the output, in turn, can be used as the input for the next job. However, this also means that the onus on ensuring jobs are complete (success/failure) lies squarely on the clients. In such cases, the various job-control options are: • runJob(JobConf) : Submits the job and returns only after the job has completed. • submitJob(JobConf) : Only submits the job, then poll the returned handle to the RunningJob to query status and make scheduling decisions. • JobConf.setJobEndNotificationURI(String) : Sets up a notification upon job-completion, thus avoiding polling.
6.5. Job Input
InputFormat describes the input-specification for a Map/Reduce job. The Map/Reduce framework relies on the InputFormat of the job to: 1. Validate the input-specification of the job. 2. Split-up the input file(s) into logical InputSplit instances, each of which is then assigned to an individual Mapper. 3. Provide the RecordReader implementation used to glean input records from the logical InputSplit for processing by the Mapper. The default behavior of file-based InputFormat implementations, typically sub-classes of FileInputFormat, is to split the input into logical InputSplit instances based on the total size, in bytes, of the input files. However, the FileSystem blocksize of the input files is treated as an upper bound for input splits. A lower bound on the split size can be set via mapred.min.split.size. Clearly, logical splits based on input-size is insufficient for many applications since record boundaries must be respected. In such cases, the application should implement a RecordReader, who is responsible for respecting record-boundaries and presents a record-oriented view of the logical InputSplit to the individual task. TextInputFormat is the default InputFormat. If TextInputFormat is the InputFormat for a given job, the framework detects input-files with the .gz extensions and automatically decompresses them using the appropriate CompressionCodec. However, it must be noted that compressed files with the above extensions cannot be split and each compressed file is processed in its entirety by a single mapper.
Page 22
Copyright © 2008 The Apache Software Foundation. All rights reserved.
Map/Reduce Tutorial
6.5.1. InputSplit InputSplit represents the data to be processed by an individual Mapper. Typically InputSplit presents a byte-oriented view of the input, and it is the responsibility of RecordReader to process and present a record-oriented view. FileSplit is the default InputSplit. It sets map.input.file to the path of the input file for the logical split. 6.5.2. RecordReader RecordReader reads <key, value> pairs from an InputSplit. Typically the RecordReader converts the byte-oriented view of the input, provided by the InputSplit, and presents a record-oriented to the Mapper implementations for processing. RecordReader thus assumes the responsibility of processing record boundaries and presents the tasks with keys and values.
6.6. Job Output
OutputFormat describes the output-specification for a Map/Reduce job. The Map/Reduce framework relies on the OutputFormat of the job to: 1. Validate the output-specification of the job; for example, check that the output directory doesn&#39;t already exist. 2. Provide the RecordWriter implementation used to write the output files of the job. Output files are stored in a FileSystem. TextOutputFormat is the default OutputFormat. 6.6.1. OutputCommitter OutputCommitter describes the commit of task output for a Map/Reduce job. The Map/Reduce framework relies on the OutputCommitter of the job to: 1. Setup the job during initialization. For example, create the temporary output directory for the job during the initialization of the job. Job setup is done by a separate task when the job is in PREP state and after initializing tasks. Once the setup task completes, the job will be moved to RUNNING state. 2. Cleanup the job after the job completion. For example, remove the temporary output directory after the job completion. Job cleanup is done by a separate task at the end of the
Page 23
Copyright © 2008 The Apache Software Foundation. All rights reserved.
Map/Reduce Tutorial
job. Job is declared SUCCEDED/FAILED/KILLED after the cleanup task completes. 3. Setup the task temporary output. Task setup is done as part of the same task, during task initialization. 4. Check whether a task needs a commit. This is to avoid the commit procedure if a task does not need commit. 5. Commit of the task output. Once task is done, the task will commit it&#39;s output if required. 6. Discard the task commit. If the task has been failed/killed, the output will be cleaned-up. If task could not cleanup (in exception block), a separate task will be launched with same attempt-id to do the cleanup. FileOutputCommitter is the default OutputCommitter. Job setup/cleanup tasks occupy map or reduce slots, whichever is free on the TaskTracker. And JobCleanup task, TaskCleanup tasks and JobSetup task have the highest priority, and in that order. 6.6.2. Task Side-Effect Files In some applications, component tasks need to create and/or write to side-files, which differ from the actual job-output files. In such cases there could be issues with two instances of the same Mapper or Reducer running simultaneously (for example, speculative tasks) trying to open and/or write to the same file (path) on the FileSystem. Hence the application-writer will have to pick unique names per task-attempt (using the attemptid, say attempt_200709221812_0001_m_000000_0), not just per task. To avoid these issues the Map/Reduce framework, when the OutputCommitter is FileOutputCommitter, maintains a special ${mapred.output.dir}/_temporary/</em>${taskid} sub-directory accessible via ${mapred.work.output.dir} for each task-attempt on the FileSystem where the output of the task-attempt is stored. On successful completion of the task-attempt, the files in the ${mapred.output.dir}/<em>temporary/</em>${taskid} (only) are promoted to ${mapred.output.dir}. Of course, the framework discards the sub-directory of unsuccessful task-attempts. This process is completely transparent to the application. The application-writer can take advantage of this feature by creating any side-files required in ${mapred.work.output.dir} during execution of a task via FileOutputFormat.getWorkOutputPath(), and the framework will promote them similarly for succesful task-attempts, thus eliminating the need to pick unique paths per task-attempt. Note: The value of ${mapred.work.output.dir} during execution of a particular task-attempt is actually ${mapred.output.dir}/<em>temporary/</em>{$taskid}, and this value is set by the Map/Reduce framework. So, just create any side-files in the path returned by FileOutputFormat.getWorkOutputPath() from map/reduce task to take advantage
Page 24
Copyright © 2008 The Apache Software Foundation. All rights reserved.
Map/Reduce Tutorial
of this feature. The entire discussion holds true for maps of jobs with reducer=NONE (i.e. 0 reduces) since output of the map, in that case, goes directly to HDFS. 6.6.3. RecordWriter RecordWriter writes the output <key, value> pairs to an output file. RecordWriter implementations write the job outputs to the FileSystem.
6.7. Other Useful Features
6.7.1. Submitting Jobs to Queues Users submit jobs to Queues. Queues, as collection of jobs, allow the system to provide specific functionality. For example, queues use ACLs to control which users who can submit jobs to them. Queues are expected to be primarily used by Hadoop Schedulers. Hadoop comes configured with a single mandatory queue, called &#39;default&#39;. Queue names are defined in the mapred.queue.names property of the Hadoop site configuration. Some job schedulers, such as the Capacity Scheduler, support multiple queues. A job defines the queue it needs to be submitted to through the mapred.job.queue.name property, or through the setQueueName(String) API. Setting the queue name is optional. If a job is submitted without an associated queue name, it is submitted to the &#39;default&#39; queue. 6.7.2. Counters Counters represent global counters, defined either by the Map/Reduce framework or applications. Each Counter can be of any Enum type. Counters of a particular Enum are bunched into groups of type Counters.Group. Applications can define arbitrary Counters (of type Enum) and update them via Reporter.incrCounter(Enum, long) or Reporter.incrCounter(String, String, long) in the map and/or reduce methods. These counters are then globally aggregated by the framework. 6.7.3. DistributedCache DistributedCache distributes application-specific, large, read-only files efficiently. DistributedCache is a facility provided by the Map/Reduce framework to cache files
Page 25
Copyright © 2008 The Apache Software Foundation. All rights reserved.
Map/Reduce Tutorial
(text, archives, jars and so on) needed by applications. Applications specify the files to be cached via urls (hdfs://) in the JobConf. The DistributedCache assumes that the files specified via hdfs:// urls are already present on the FileSystem. The framework will copy the necessary files to the slave node before any tasks for the job are executed on that node. Its efficiency stems from the fact that the files are only copied once per job and the ability to cache archives which are un-archived on the slaves. DistributedCache tracks the modification timestamps of the cached files. Clearly the cache files should not be modified by the application or externally while the job is executing. DistributedCache can be used to distribute simple, read-only data/text files and more complex types such as archives and jars. Archives (zip, tar, tgz and tar.gz files) are un-archived at the slave nodes. Files have execution permissions set. The files/archives can be distributed by setting the property mapred.cache.{files|archives}. If more than one file/archive has to be distributed, they can be added as comma separated paths. The properties can also be set by APIs DistributedCache.addCacheFile(URI,conf)/ DistributedCache.addCacheArchive(URI,conf) and DistributedCache.setCacheFiles(URIs,conf)/ DistributedCache.setCacheArchives(URIs,conf) where URI is of the form hdfs://host:port/absolute-path/#link-name. In Streaming, the files can be distributed through command line option -cacheFile/-cacheArchive. Optionally users can also direct the DistributedCache to symlink the cached file(s) into the current working directory of the task via the DistributedCache.createSymlink(Configuration) api. Or by setting the configuration property mapred.create.symlink as yes. The DistributedCache will use the fragment of the URI as the name of the symlink. For example, the URI hdfs://namenode:port/lib.so.1/#lib.so will have the symlink name as lib.so in task&#39;s cwd for the file lib.so.1 in distributed cache. The DistributedCache can also be used as a rudimentary software distribution mechanism for use in the map and/or reduce tasks. It can be used to distribute both jars and native libraries. The DistributedCache.addArchiveToClassPath(Path, Configuration) or DistributedCache.addFileToClassPath(Path, Configuration) api can be used to cache files/jars and also add them to the classpath of child-jvm. The same can be done by setting the configuration properties mapred.job.classpath.{files|archives}. Similarly the cached files that are symlinked into the working directory of the task can be used to distribute native libraries and load them.
Page 26
Copyright © 2008 The Apache Software Foundation. All rights reserved.
Map/Reduce Tutorial
6.7.4. Tool The Tool interface supports the handling of generic Hadoop command-line options. Tool is the standard for any Map/Reduce tool or application. The application should delegate the handling of standard command-line options to GenericOptionsParser via ToolRunner.run(Tool, String[]) and only handle its custom arguments. The generic Hadoop command-line options are: -conf <configuration file> -D <property=value> -fs <local|namenode:port> -jt <local|jobtracker:port> 6.7.5. IsolationRunner IsolationRunner is a utility to help debug Map/Reduce programs. To use the IsolationRunner, first set keep.failed.tasks.files to true (also see keep.tasks.files.pattern). Next, go to the node on which the failed task ran and go to the TaskTracker&#39;s local directory and run the IsolationRunner: $ cd <local path>/taskTracker/${taskid}/work $ bin/hadoop org.apache.hadoop.mapred.IsolationRunner ../job.xml IsolationRunner will run the failed task in a single jvm, which can be in the debugger, over precisely the same input. 6.7.6. Profiling Profiling is a utility to get a representative (2 or 3) sample of built-in java profiler for a sample of maps and reduces. User can specify whether the system should collect profiler information for some of the tasks in the job by setting the configuration property mapred.task.profile. The value can be set using the api JobConf.setProfileEnabled(boolean). If the value is set true, the task profiling is enabled. The profiler information is stored in the user log directory. By default, profiling is not enabled for the job. Once user configures that profiling is needed, she/he can use the configuration property mapred.task.profile.{maps|reduces} to set the ranges of map/reduce tasks to
Page 27
Copyright © 2008 The Apache Software Foundation. All rights reserved.
Map/Reduce Tutorial
profile. The value can be set using the api JobConf.setProfileTaskRange(boolean,String). By default, the specified range is 0-2.
User can also specify the profiler configuration arguments by setting the configuration property mapred.task.profile.params. The value can be specified using the api JobConf.setProfileParams(String). If the string contains a %s, it will be replaced with the name of the profiling output file when the task runs. These parameters are passed to the task child JVM on the command line. The default value for the profiling parameters is -agentlib:hprof=cpu=samples,heap=sites,force=n,thread=y,verbose=n,file=%s 6.7.7. Debugging The Map/Reduce framework provides a facility to run user-provided scripts for debugging. When a map/reduce task fails, a user can run a debug script, to process task logs for example. The script is given access to the task&#39;s stdout and stderr outputs, syslog and jobconf. The output from the debug script&#39;s stdout and stderr is displayed on the console diagnostics and also as part of the job UI. In the following sections we discuss how to submit a debug script with a job. The script file needs to be distributed and submitted to the framework.
6.7.7.1. How to distribute the script file:
The user needs to use DistributedCache to distribute and symlink the script file.
6.7.7.2. How to submit the script:
A quick way to submit the debug script is to set values for the properties mapred.map.task.debug.script and mapred.reduce.task.debug.script, for debugging map and reduce tasks respectively. These properties can also be set by using APIs JobConf.setMapDebugScript(String) and JobConf.setReduceDebugScript(String) . In streaming mode, a debug script can be submitted with the command-line options -mapdebug and -reducedebug, for debugging map and reduce tasks respectively. The arguments to the script are the task&#39;s stdout, stderr, syslog and jobconf files. The debug command, run on the node where the map/reduce task failed, is: $script $stdout $stderr $syslog $jobconf Pipes programs have the c++ program name as a fifth argument for the command. Thus for the pipes programs the command is $script $stdout $stderr $syslog $jobconf $program
Page 28
Copyright © 2008 The Apache Software Foundation. All rights reserved.
Map/Reduce Tutorial
6.7.7.3. Default Behavior:
For pipes, a default script is run to process core dumps under gdb, prints stack trace and gives info about running threads. 6.7.8. JobControl JobControl is a utility which encapsulates a set of Map/Reduce jobs and their dependencies. 6.7.9. Data Compression Hadoop Map/Reduce provides facilities for the application-writer to specify compression for both intermediate map-outputs and the job-outputs i.e. output of the reduces. It also comes bundled with CompressionCodec implementation for the zlib compression algorithm. The gzip file format is also supported. Hadoop also provides native implementations of the above compression codecs for reasons of both performance (zlib) and non-availability of Java libraries. More details on their usage and availability are available here.
6.7.9.1. Intermediate Outputs
Applications can control compression of intermediate map-outputs via the JobConf.setCompressMapOutput(boolean) api and the CompressionCodec to be used via the JobConf.setMapOutputCompressorClass(Class) api.
6.7.9.2. Job Outputs
Applications can control compression of job-outputs via the FileOutputFormat.setCompressOutput(JobConf, boolean) api and the CompressionCodec to be used can be specified via the FileOutputFormat.setOutputCompressorClass(JobConf, Class) api. If the job outputs are to be stored in the SequenceFileOutputFormat, the required SequenceFile.CompressionType (i.e. RECORD / BLOCK - defaults to RECORD) can be specified via the SequenceFileOutputFormat.setOutputCompressionType(JobConf, SequenceFile.CompressionType) api. 6.7.10. Skipping Bad Records Hadoop provides an option where a certain set of bad input records can be skipped when processing map inputs. Applications can control this feature through the SkipBadRecords
Page 29
Copyright © 2008 The Apache Software Foundation. All rights reserved.
Map/Reduce Tutorial
class. This feature can be used when map tasks crash deterministically on certain input. This usually happens due to bugs in the map function. Usually, the user would have to fix these bugs. This is, however, not possible sometimes. The bug may be in third party libraries, for example, for which the source code is not available. In such cases, the task never completes successfully even after multiple attempts, and the job fails. With this feature, only a small portion of data surrounding the bad records is lost, which may be acceptable for some applications (those performing statistical analysis on very large data, for example). By default this feature is disabled. For enabling it, refer to SkipBadRecords.setMapperMaxSkipRecords(Configuration, long) and SkipBadRecords.setReducerMaxSkipGroups(Configuration, long). With this feature enabled, the framework gets into &#39;skipping mode&#39; after a certain number of map failures. For more details, see SkipBadRecords.setAttemptsToStartSkipping(Configuration, int). In &#39;skipping mode&#39;, map tasks maintain the range of records being processed. To do this, the framework relies on the processed record counter. See SkipBadRecords.COUNTER_MAP_PROCESSED_RECORDS and SkipBadRecords.COUNTER_REDUCE_PROCESSED_GROUPS. This counter enables the framework to know how many records have been processed successfully, and hence, what record range caused a task to crash. On further attempts, this range of records is skipped. The number of records skipped depends on how frequently the processed record counter is incremented by the application. It is recommended that this counter be incremented after every record is processed. This may not be possible in some applications that typically batch their processing. In such cases, the framework may skip additional records surrounding the bad record. Users can control the number of skipped records through SkipBadRecords.setMapperMaxSkipRecords(Configuration, long) and SkipBadRecords.setReducerMaxSkipGroups(Configuration, long). The framework tries to narrow the range of skipped records using a binary search-like approach. The skipped range is divided into two halves and only one half gets executed. On subsequent failures, the framework figures out which half contains bad records. A task will be re-executed till the acceptable skipped value is met or all task attempts are exhausted. To increase the number of task attempts, use JobConf.setMaxMapAttempts(int) and JobConf.setMaxReduceAttempts(int). Skipped records are written to HDFS in the sequence file format, for later analysis. The location can be changed through SkipBadRecords.setSkipOutputPath(JobConf, Path).</li>
<li>Example: WordCount v2.0
Page 30
Copyright © 2008 The Apache Software Foundation. All rights reserved.
Map/Reduce Tutorial
Here is a more complete WordCount which uses many of the features provided by the Map/Reduce framework we discussed so far. This needs the HDFS to be up and running, especially for the DistributedCache-related features. Hence it only works with a pseudo-distributed or fully-distributed Hadoop installation.
7.1. Source Code
WordCount.java 1. 2. 3. 4. 5. 6. 7. 8. 9. 10. 11. 12. 13. 14. 15. public static class Map extends MapReduceBase implements Mapper<LongWritable, Text, Text, IntWritable> { public class WordCount extends Configured implements Tool { import org.apache.hadoop.fs.Path; import org.apache.hadoop.filecache.DistributedCache; import org.apache.hadoop.conf./<em>; import org.apache.hadoop.io./</em>; import org.apache.hadoop.mapred./<em>; import org.apache.hadoop.util./</em>; import java.io./<em>; import java.util./</em>; package org.myorg;</li>
<li><ol>
<li>static enum Counters { INPUT_WORDS }
Page 31
Copyright © 2008 The Apache Software Foundation. All rights reserved.
Map/Reduce Tutorial</li>
</ol>
</li>
<li><ol>
<li><ol>
<li><ol>
<li><ol>
<li><ol>
<li>private boolean caseSensitive = true; private Set<String> patternsToSkip = new HashSet<String>(); private final static IntWritable one = new IntWritable(1); private Text word = new Text();</li>
</ol>
</li>
</ol>
</li>
</ol>
</li>
</ol>
</li>
</ol>
</li>
<li><ol>
<li><ol>
<li><ol>
<li><ol>
<li><ol>
<li>public void configure(JobConf job) { caseSensitive = job.getBoolean(&quot;wordcount.case.sensitive&quot;, true); inputFile = job.get(&quot;map.input.file&quot;); private long numRecords = 0; private String inputFile;</li>
</ol>
</li>
</ol>
</li>
</ol>
</li>
</ol>
</li>
</ol>
</li>
<li><ol>
<li>32.
if (job.getBoolean(&quot;wordcount.skip.patterns&quot;, false)) { Path[] patternsFiles = new Path[0]; try { patternsFiles = DistributedCache.getLocalCacheFiles(job); } catch (IOException ioe) { System.err.println(&quot;Caught</li>
</ol>
</li>
<li><ol>
<li><ol>
<li><ol>
<li>37.
Page 32
Copyright © 2008 The Apache Software Foundation. All rights reserved.
Map/Reduce Tutorial
exception while getting cached files: &quot; + StringUtils.stringifyException(ioe)); 38. 39. 40. 41. 42. 43. 44. 45. 46. 47. private void parseSkipFile(Path patternsFile) { try { BufferedReader fis = new BufferedReader(new FileReader(patternsFile.toString())); String pattern = null; while ((pattern = fis.readLine()) != null) { patternsToSkip.add(pattern); } } catch (IOException ioe) { System.err.println(&quot;Caught exception while parsing the cached file &#39;&quot; + patternsFile + &quot;&#39; : &quot; + StringUtils.stringifyException(ioe)); } } } for (Path patternsFile : patternsFiles) { parseSkipFile(patternsFile); } } }</li>
</ol>
</li>
</ol>
</li>
</ol>
</li>
<li><ol>
<li><ol>
<li><ol>
<li><ol>
<li>53.</li>
</ol>
</li>
</ol>
</li>
</ol>
</li>
</ol>
</li>
<li><ol>
<li><ol>
<li>57.
public void map(LongWritable key,
Page 33
Copyright © 2008 The Apache Software Foundation. All rights reserved.
Map/Reduce Tutorial
Text value, OutputCollector<Text, IntWritable> output, Reporter reporter) throws IOException { 58. String line = (caseSensitive) ? value.toString() : value.toString().toLowerCase();</li>
</ol>
</li>
</ol>
</li>
<li><ol>
<li><ol>
<li>&quot;&quot;); 62. 63. 64. 65. 66. word.set(tokenizer.nextToken()); 67. 68. reporter.incrCounter(Counters.INPUT_WORDS, 1); 69. 70. 71. 72. if ((++numRecords % 100) == 0) { reporter.setStatus(&quot;Finished processing &quot; + numRecords + &quot; records &quot; + &quot;from the input file: &quot; + inputFile); } } } } output.collect(word, one); StringTokenizer tokenizer = new StringTokenizer(line); while (tokenizer.hasMoreTokens()) { } for (String pattern : patternsToSkip) { line = line.replaceAll(pattern,</li>
</ol>
</li>
</ol>
</li>
<li><ol>
<li>75.
Page 34
Copyright © 2008 The Apache Software Foundation. All rights reserved.
Map/Reduce Tutorial</li>
</ol>
</li>
<li><ol>
<li>public static class Reduce extends MapReduceBase implements Reducer<Text, IntWritable, Text, IntWritable> { public void reduce(Text key, Iterator<IntWritable> values, OutputCollector<Text, IntWritable> output, Reporter reporter) throws IOException { int sum = 0; while (values.hasNext()) { sum += values.next().get(); } output.collect(key, new IntWritable(sum)); } }
78.</li>
</ol>
</li>
<li><ol>
<li><ol>
<li><ol>
<li><ol>
<li><ol>
<li><ol>
<li><ol>
<li><ol>
<li><ol>
<li><ol>
<li><ol>
<li>91.
public int run(String[] args) throws Exception { JobConf conf = new JobConf(getConf(), WordCount.class); conf.setJobName(&quot;wordcount&quot;);
conf.setOutputKeyClass(Text.class); 92. conf.setOutputValueClass(IntWritable.class); 93. 94. 95. conf.setMapperClass(Map.class);
Page 35
Copyright © 2008 The Apache Software Foundation. All rights reserved.
Map/Reduce Tutorial
conf.setCombinerClass(Reduce.class); 96. conf.setReducerClass(Reduce.class); 97. 98. conf.setInputFormat(TextInputFormat.class); 99. conf.setOutputFormat(TextOutputFormat.class); 100. 101. 102. 103. 104. DistributedCache.addCacheFile(new Path(args[++i]).toUri(), conf); 105. conf.setBoolean(&quot;wordcount.skip.patterns&quot;, true); 106. 107. 108. 109. 110. 111. FileInputFormat.setInputPaths(conf, new Path(other_args.get(0))); 112. FileOutputFormat.setOutputPath(conf, new Path(other_args.get(1))); 113. } else { other_args.add(args[i]); } } List<String> other_args = new ArrayList<String>(); for (int i=0; i &lt; args.length; ++i) { if (&quot;-skip&quot;.equals(args[i])) {
Page 36
Copyright © 2008 The Apache Software Foundation. All rights reserved.
Map/Reduce Tutorial</li>
</ol>
</li>
</ol>
</li>
</ol>
</li>
</ol>
</li>
</ol>
</li>
</ol>
</li>
</ol>
</li>
</ol>
</li>
</ol>
</li>
</ol>
</li>
</ol>
</li>
<li><ol>
<li><ol>
<li><ol>
<li><ol>
<li>119.
JobClient.runJob(conf); return 0; }
public static void main(String[] args) throws Exception { int res = ToolRunner.run(new Configuration(), new WordCount(), args); System.exit(res); } }</li>
</ol>
</li>
</ol>
</li>
</ol>
</li>
</ol>
</li>
<li><ol>
<li><ol>
<li>123.
7.2. Sample Runs
Sample text-files as input: $ bin/hadoop dfs -ls /usr/joe/wordcount/input/ /usr/joe/wordcount/input/file01 /usr/joe/wordcount/input/file02 $ bin/hadoop dfs -cat /usr/joe/wordcount/input/file01 Hello World, Bye World! $ bin/hadoop dfs -cat /usr/joe/wordcount/input/file02 Hello Hadoop, Goodbye to hadoop. Run the application: $ bin/hadoop jar /usr/joe/wordcount.jar org.myorg.WordCount /usr/joe/wordcount/input /usr/joe/wordcount/output Output: $ bin/hadoop dfs -cat /usr/joe/wordcount/output/part-00000 Bye 1 Goodbye 1 Hadoop, 1 Hello 2
Page 37
Copyright © 2008 The Apache Software Foundation. All rights reserved.
Map/Reduce Tutorial
World! 1 World, 1 hadoop. 1 to 1 Notice that the inputs differ from the first version we looked at, and how they affect the outputs. Now, lets plug-in a pattern-file which lists the word-patterns to be ignored, via the DistributedCache. $ hadoop dfs -cat /user/joe/wordcount/patterns.txt . \, ! to Run it again, this time with more options: $ bin/hadoop jar /usr/joe/wordcount.jar org.myorg.WordCount -Dwordcount.case.sensitive=true /usr/joe/wordcount/input /usr/joe/wordcount/output -skip /user/joe/wordcount/patterns.txt As expected, the output: $ bin/hadoop dfs -cat /usr/joe/wordcount/output/part-00000 Bye 1 Goodbye 1 Hadoop 1 Hello 2 World 2 hadoop 1 Run it once more, this time switch-off case-sensitivity: $ bin/hadoop jar /usr/joe/wordcount.jar org.myorg.WordCount -Dwordcount.case.sensitive=false /usr/joe/wordcount/input /usr/joe/wordcount/output -skip /user/joe/wordcount/patterns.txt Sure enough, the output: $ bin/hadoop dfs -cat /usr/joe/wordcount/output/part-00000 bye 1
Page 38
Copyright © 2008 The Apache Software Foundation. All rights reserved.
Map/Reduce Tutorial
goodbye 1 hadoop 2 hello 2 world 2
7.3. Highlights
The second version of WordCount improves upon the previous one by using some features offered by the Map/Reduce framework: • Demonstrates how applications can access configuration parameters in the configure method of the Mapper (and Reducer) implementations (lines 28-43). • Demonstrates how the DistributedCache can be used to distribute read-only data needed by the jobs. Here it allows the user to specify word-patterns to skip while counting (line 104). • Demonstrates the utility of the Tool interface and the GenericOptionsParser to handle generic Hadoop command-line options (lines 87-116, 119). • Demonstrates how applications can use Counters (line 68) and how they can set application-specific status information via the Reporter instance passed to the map (and reduce) method (line 72). Java and JNI are trademarks or registered trademarks of Sun Microsystems, Inc. in the United States and other countries.
Page 39
Copyright © 2008 The Apache Software Foundation. All rights reserved.</li>
</ol>
</li>
</ol>
</li>
</ol>

      
    </div>
    
    
      
    
    
        <footer id="post-meta">
        <span class="categories">Posted in<span class="breadcrumb fa fa-folder"><li><a href="/categories/hadoop/">hadoop</a></li></span></span> | <span class="tags">Tagged <a href="/tags/hadoop/" class="label label-primary">hadoop</a></span> | <span class="time">recent updated:<time title="2014-03-07 09:54:36"datetime="2014-03-07 09:54:36"> mar. 7 2014</time></span> | <span class="comment-link">
<a href="http://itsolife.com/2014/02/02/2014-02-02-hadoop--mapred_tutorial/#comments" class="ds-thread-count comment-link" data-thread-key="2014-02-02-hadoop--mapred_tutorial" data-count-type="comments">暂无评论</a></span>
        </footer>
    
    <div class="clearfix"></div>
  </div>
</div>
</article>





<ul id="pagination" class="pagination pagination-lg">
  <li><a class="extend prev" href="/page/108/">&laquo;</a></li><li><a class="page-number" href="/">1</a></li><li><a class="page-number" href="/page/2/">2</a></li><li><span class="space">&hellip;</span></li><li><a class="page-number" href="/page/106/">106</a></li><li><a class="page-number" href="/page/107/">107</a></li><li><a class="page-number" href="/page/108/">108</a></li><li class="active"><li><span class="page-number current">109</span></li><li><a class="page-number" href="/page/110/">110</a></li><li><a class="page-number" href="/page/111/">111</a></li><li><a class="page-number" href="/page/112/">112</a></li><li><span class="space">&hellip;</span></li></li><li><a class="page-number" href="/page/164/">164</a></li><li><a class="page-number" href="/page/165/">165</a></li><li><a class="extend next" href="/page/110/">&raquo;</a></li>
  <div class="clearfix"></div>
</ul></div><!--wapper-->
       </div><!-- ID main-col END -->
       <aside id="sidebar" class="alignright col-sx-6 col-sm-4 col-md-3 col-lg-3">
<div id="widget_search" class="widget panel panel-primary">
    <form action="//google.com/search" method="get" accept-charset="utf-8">
  <div class="input-group">
    <input class="form-control" id="searchbox" type="search" name="q" results="0" placeholder="search">
    <span class="input-group-btn">
      <button class="btn btn-default" type="submit">Go!</button>
    </span>
    <input type="hidden" name="q" value="site:itsolife.com">
  </div>
</form>
</div>

<div id="widget_category" class="widget panel panel-primary">
  <div class="panel-heading">category</div>  <div data-src='category' class='ajax_widgets'>正在加载...</div>
</div>

<div id="widget_recent_posts" class="widget panel panel-primary">
  <div class="panel-heading">recent posts</div>  <div data-src='recent_posts' class='ajax_widgets'>正在加载...</div></div>

<div id="widget_tagcloud" class="widget panel panel-primary">
  <div class="panel-heading">tagcloud</div>  <div data-src='tagcloud' class='ajax_widgets'>正在加载...</div></div>

<div id="widget_latest_update_posts" class="widget panel panel-primary">
  <div class="panel-heading">最近更新</div>  <div data-src='latest_update_posts' class='ajax_widgets'>正在加载...</div></div>

<div id="widget_recent_comments" class="widget panel panel-primary">
  <div class="panel-heading">recent comments</div>  

<div class="list-group-item ds-recent-comments" data-num-items="6" data-show-avatars="1" data-show-time="1" data-show-title="1" data-show-admin="1" data-excerpt-length="50"></div>



</div>

</aside>
       <div class="clearfix"></div>
     </div><!-- row END -->
  </div>
  <footer id="footer" class="container">
  <div class="panel panel-info">
  <section id='footer_widget'></section>  <div class="panel-footer">
  <div id="site-info">
    <span class='author'>
  
  &copy; 2014 RobinChia
  
    &nbsp;&nbsp;</span>
  
  <span id='analytics-51la'></span><span id='analytics-google'>
<script type="text/javascript">
  var _gaq = _gaq || [];
  _gaq.push(['_setAccount', 'UA-48559895-1']);
  _gaq.push(['_trackPageview']);
  _js2load.push({src:('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js'});
</script></span><span id='analytics-cnzz'>
<script type="text/javascript">var cnzz_protocol = (("https:" == document.location.protocol) ? " https://" : " http://");document.write(unescape("%3Cspan id='cnzz_stat_icon_5774006'%3E%3C/span%3E%3Cscript src='" + cnzz_protocol + "s17.cnzz.com/stat.php%3Fid%3D5774006%26show%3Dpic' type='text/javascript'%3E%3C/script%3E"));</script>
</span><span id='analytics-baidu'>
<script>
var _hmt = _hmt || [];
_js2load.push({src:"//hm.baidu.com/hm.js?1442f50724afc42380b51f097c43082c"});
</script>
</span>  </div>
  <div id="copyright">Blog powered by <a href='http://zespia.tw/hexo/'><strong>hexo</strong></a> Theme <strong><a href='https://github.com/chenall/hexo-theme-chenall'>chenall</a></strong>(Some change in it)<span class="pull-right"> 更新时间: <em>2014-03-15 13:06:28</em></span></div>
</div>
<div class="clearfix"></div>


  </div>
  </footer>
  
        <script src="http://cdn.bootcss.com/jquery/1.10.2/jquery.min.js"></script>        
        <script src="http://cdn.staticfile.org/twitter-bootstrap/3.1.0/js/bootstrap.min.js"></script>        
                
        <script src="http://cdn.bootcss.com/prettify/r298/prettify.min.js"></script>    <script type="text/javascript">
   var lang=["bsh", "c", "cc", "cpp", "cs", "csh", "cyc", "cv", "htm", "html",
    "java", "js", "m", "mxml", "perl", "pl", "pm", "py", "rb", "sh",
    "xhtml", "xml", "xsl"];
   var pretty_base='';
   $('script').each(function(){
	var c = $(this).attr('src');
	if (!c)
	    return;
	if (c.match(/(\/)?prettify(\.min)?\.js/i))
	{
	    var index = c.lastIndexOf('/');
	    if (index != -1)
		pretty_base = c.substr(0,index + 1);
	    return false;
	}
   })
   $('pre code').each(function(){
	var c = $(this).attr('class')
	if (!c)
	    return;
	c = c.match(/\s?(lang\-\w+)/i);
	if (c && lang.indexOf(c[1]) == -1)
	{
	    lang.push(c[1]);
	    $.getScript(pretty_base + c[1] + '.min.js');
	}
   })

    $(window).load(function(){
       $("pre").addClass("prettyprint");
       prettyPrint();
    })
</script>    
            <script type="text/javascript">
var duoshuoQuery = {short_name:"robinchia"};
_js2load.push({src:'http://static.duoshuo.com/embed.js',charset:'UTF-8'});
</script>
    
            <!--wumii_relatedItems-->
    
        <script src="http://cdn.bootcss.com/fancybox/2.1.5/jquery.fancybox.min.js"></script>    <script type="text/javascript">
(function($){
  $('.entry').each(function(i){
    $(this).find('img').each(function(){
      var alt = this.alt;

      if (alt){
        $(this).before('<span class="caption">' + alt + '</span>');
      }

      $(this).wrap('<a href="' + this.src + '" title="' + alt + '" class="fancybox" rel="fancybox' + i + '" />');
    });
  });

  $('.fancybox').fancybox();
})(jQuery);
</script>    
        <script src="http://cdn.bootcss.com/mathjax/2.3/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>    <script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      processEscapes: true
    }
  });
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>
    
<script type="text/javascript">
$('.ajax_widgets').each(function(){var src=$(this).attr('data-src');if(src)$(this).load('/widgets/'+src+'.html');});
$.each(_js2load,function(index,obj){loadjs(obj.src,obj.charset)});
</script>

<div id="scroll2top">
<img src="/scroll2top/arrow.png"/>
</div>
<script src="/scroll2top/scroll2top.min.js"></script>
<div id="winterland">
  <canvas></canvas>
</div>
<script src="/js/winterland.min.js"></script>

  </body>
</html>
