
<!DOCTYPE HTML>
<html>
<head>
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<title>The 109 Page | It so life</title>
<meta name="author" content="RobinChia">

<meta name="description" content="It so life">


<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">


<meta property="og:site_name" content="It so life"/>

<!--[if IE]><style>.testIE.IE{display:inline;}</style><![endif]-->
<!--[if lte IE 7]><link rel="stylesheet" href="/css/ie7.css" type="text/css"><![endif]-->
<!--[if (lt IE 9)&(gt IE 7)]><style>.testIE.IE8{display:inline;}</style><![endif]-->
<!--[if gt IE 8]><style>.testIE.IE9{display:inline;}</style><![endif]-->

<link href="/favicon.png" rel="icon">
<link rel="alternate" href="/atom.xml" title="It so life Feed" type="application/atom+xml">

<link rel="stylesheet" href="http://cdn.staticfile.org/twitter-bootstrap/3.1.0/css/bootstrap.min.css" type="text/css">
<link rel="stylesheet" href="http://cdn.bootcss.com/font-awesome/4.0.3/css/font-awesome.min.css" type="text/css">
<link rel="stylesheet" href="http://cdn.bootcss.com/prettify/r298/prettify.min.css" type="text/css">
<link rel="stylesheet" href="http://cdn.bootcss.com/fancybox/2.1.5/jquery.fancybox.min.css" type="text/css">
<link rel="stylesheet" href="/css/style.css" type="text/css">
<!--[if lt IE 9]>
   <style>article,aside,dialog,footer,header,section,footer,nav,figure,menu{display:block}</style>
   <script src="http://cdn.staticfile.org/html5shiv/r29/html5.min.js"></script>
   <script src="http://cdn.staticfile.org/respond.js/1.4.2/respond.min.js"></script>
<link href="http://cdn.staticfile.org/respond.js/1.4.2/respond-proxy.html" id="respond-proxy" rel="respond-proxy" />
<link href="/js/respond.proxy.gif" id="respond-redirect" rel="respond-redirect" />
<script src="/js/respond.proxy.js"></script>
   <script src="http://cdn.bootcss.com/selectivizr/1.0.2/selectivizr-min.js"></script>
<![endif]-->
<script type="text/javascript">
function loadjs(c,d){var a=document.createElement("script");a.async=!0;a.type="text/javascript";a.src=c;a.charset=d||"gbk";(document.getElementsByTagName("HEAD")[0]||document.getElementsByTagName("BODY")[0]).appendChild(a)};
var _protocol = (("https:" == document.location.protocol) ? " https://" : " http://");
var _js2load = [];
</script>

</head>
<body>
      <header id="header" class="container"><nav id="main-nav" class="navbar navbar-default navbar-fixed-top " role="navigation">
  <div class="container">
    <div class="navbar-header">
    <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".navbar-collapse">
      <span class="sr-only">Toggle navigation</span>
      <span class="icon-bar"></span>
      <span class="icon-bar"></span>
      <span class="icon-bar"></span>
    </button>
    <a class="navbar-brand" href="/">It so life</a>
    </div>
    <div  class="collapse navbar-collapse">
      <ul  class="nav navbar-nav">
  
        <li><a href="/" title="Home">Home</a></li>      
        <li><a href="/about/" title="About">About</a></li>      
        <li><a href="/archives/" title="Archives">Archives</a></li>      
      <li class='dropdown'>
        <a class='dropdown-toggle' data-toggle='dropdown' href='#'>website<b class='caret'></b></a>
        <ul class='dropdown-menu pure-menu-selected'>
    
          <li><a href="//groups.google.com/forum/#!forum/pongba" title="TopLanguage">TopLanguage</a></li>    
          <li><a href="//itpub.net/" title="ITPub">ITPub</a></li>    
          <li><a href="//blog.jobbole.com/" title="Bole">Bole</a></li>    
          <li><a href="//nosql-database.org/" title="nosql">nosql</a></li>    
          <li><a href="//gitimmersion.googol.im/" title="Git">Git</a></li>    
        </ul>
      </li>
    
      </ul>
      <ul class="nav navbar-nav navbar-right">
      
        <li><a href="/atom.xml">RSS</a></li>
      
      
        <li><a href="https://twitter.com/robinchia">twitter</a></li>
      
      
      
      
        <li><a href="https://github.com/robinchia">github</a></li>
      
      </ul>
    </div>
  </div>
</nav>
<div class="clearfix"></div>
</header>
  <div id='content' class="container">
     <div class="page-header-wrapper">
      <!--[if lt IE 9]><div class="alert alert-warning alert-dismissable"><button type="button" class="close" data-dismiss="alert" aria-hidden="true">&times;</button><strong>提示:</strong>您的浏览器版本太低了,建议升级到 <strong><a href="http://windows.microsoft.com/zh-cn/internet-explorer/download-ie" title="IE9">IE9</a></strong> 以上,本站使用<a href="https://www.google.com/intl/zh-CN/chrome/">Chrome浏览器</a>可以获得最好的显示效果.</div><![endif]-->
      <div class="page-header"><h1 align="center"><big>It so life</big> </h1>
        <h5 align="center"><big>love as life</big></h5>
      </div>
     </div>
     <div class="row">
       <div id="main-col" class="alignleft col-sx-12 col-sm-8 col-md-9 col-lg-9">
      <section id='header_widget'></section>
          <div id="wrapper"><article>
<div id="post" class="post well">
  <div class="post-content">
    <header class="well-sm">
      <i class="fa icon fa-5x pull-left"></i>
      <h1 class="title"><a href="/2014/02/02/2014-02-02-hadoop--Hadoop在CentOS下的单机配置/">Hadoop在CentOS下的单机配置</a></h1>
      
        <span>Posted on<time datetime="2014-02-02T01:54:36.000Z"> <a href="/2014/02/02/2014-02-02-hadoop--Hadoop在CentOS下的单机配置/">feb. 2 2014</a></time></span>      
    </header>
    
    
    <div class="entry">
      
        <h1 id="hadoop-centos-">Hadoop在CentOS下的单机配置</h1>
<p>前言的前言</p>
<p>如果你做某件从未接触过的事的时候很纠结很曲折，那么为你自己高兴吧，你能学到很多东西！</p>
<p>以下的东西都是贴图，所以你们只有手敲了。我也不清楚这个东西是不是应该花很多时间去做，有得有失，某些付出不知道到底值多少。据/<em>/</em>说一下午都能配出来，谁叫我傻呢，谁叫我蠢呢，不过该走的路咱还是踏实点走吧，不去跟人比。所以现在我把细节写出来，供大家参考，让你能在两小时内完成。希望它能帮助你学习，而不是让你变得更依赖。如有不对的地方请指正，我也是初学者。谢谢！</p>
<p>前言</p>
<p>做事总有个原因吧，那么我们为什么安装单机的<a href="http://www.linuxidc.com/topicnews.aspx?tid=13" title="Hadoop" target="_blank">Hadoop</a>呢？因为官网上有安装单机hadoop，因为某权威网站有<a href="http://www.linuxidc.com/topicnews.aspx?tid=2" title="Ubuntu" target="_blank">Ubuntu</a>下安装单机hadoop，但是没有一个网站有<a href="http://www.linuxidc.com/topicnews.aspx?tid=14" title="CentOS" target="_blank">CentOS</a>下单机安装，所以我现在CentOS下面单机配置hadoop。</p>
<p>其实单机hadoop的安装没有什么实质的用处，主要用于初学者熟悉指令，以及对hadoop配置有个大致了解，以便于安装分布式。</p>
<p>首先，我们来理清思路。</p>
<p>目的：安装hadoop</p>
<p>Hadoop是需要在java环境下面运行，所以，首先要保证你的系统下面装有JDK。那么步骤是：配置SSH——安装JDK——安装hadoop（当然你愿意先安装它也完全没问题）——配置java的环境变量（需要知道java的安装路径）——配置namenode下面3个配置文件——格式化hadoop——启动hadoop。</p>
<p>我们用一般用户登录，然后切换到root下面，因为权限的问题，这样相比下会更安全点，注意linux下面尽量不要用root登录。</p>
<p>开始了</p>
<p>所需软件</p>
<p>CentOS、Java、Hadoop安装软件。本人用的版本为Linux Cent OS 5.5、jdk1.6.0_13、hadoop-0.20.2.tar.gz。</p>
<p>我们要提醒一下，linux下面很注意权限问题。我们应该以一般用户登录，然后切换至root用户才能使用某些命令，并能使系统处于相对安全的状态。</p>
<p>所以做如下处理，来切换到root用户。
<img src="" alt="Hadoop在CentOS下的单机配置"></p>
<ol>
<li>SSH无密码验证配置（更建议放到最后一步进行，为非核心步骤，只是方便而已）</li>
</ol>
<p>Hadoop 需要使用SSH 协议。</p>
<p>namenode 将使用SSH 协议启动 namenode和datanode 进程，配置 SSH localhost无密码验证。</p>
<p>(1)生成密钥对
<img src="" alt="Hadoop在CentOS下的单机配置"></p>
<p>前面是为了切换到root下面</p>
<p>通过以上命令将在/root/.ssh/ 目录下生成id_rsa私钥和id_rsa.pub公钥。</p>
<p>（2）进入/root/.ssh目录在namenode节点下做如下配置：
<img src="" alt="Hadoop在CentOS下的单机配置"></p>
<p>可以用键入ssh localhost命令来看已经连接，会有这样的显示</p>
<p><img src="" alt="Hadoop在CentOS下的单机配置"></p>
<p>注意最后一行！跟第一行比较，发现我们用ssh进入到localhost了！但已不需要输入密码了。（这样说你们也一定不知道，如果把这个放到最后一步做就会更懂。）</p>
<p>本人认为这样设置会发现后面操作不会让你老是输入密码，并非核心步骤，大家可以试试先配置其它的，再到这一步，就明白为什么了。
来源： <a href="[http://www.linuxidc.com/Linux/2011-07/37992.htm](http://www.linuxidc.com/Linux/2011-07/37992.htm)">[http://www.linuxidc.com/Linux/2011-07/37992.htm](http://www.linuxidc.com/Linux/2011-07/37992.htm)</a> </p>
<ol>
<li>安装JDK</li>
</ol>
<p>(1)下载JDK</p>
<p>建议到sun的官网上下载,地址如下：<a href="https://cds.sun.com/is-bin/INTERSHOP.enfinity/WFS/CDS-CDS_Developer-Site/en_US/-/USD/ViewFilteredProducts-SingleVariationTypeFilter" target="_blank"><a href="https://cds.sun.com/is-bin/INTERSHOP.enfinity/WFS/CDS-CDS_Developer-Site/en_US/-/USD/ViewFilteredProducts-SingleVariationTypeFilter">https://cds.sun.com/is-bin/INTERSHOP.enfinity/WFS/CDS-CDS_Developer-Site/en_US/-/USD/ViewFilteredProducts-SingleVariationTypeFilter</a></a></p>
<p>选择jdk-6u24-linux-i586.bin</p>
<p>(2)安装JDK</p>
<p>我把它装在/opt里面,所以切换到/opt下面。在命令行输入如下指令来执行JDK文件:</p>
<p><img src="" alt=""></p>
<p>权限有问题！我们看看它的权限
<img src="" alt="Hadoop在CentOS下的单机配置"></p>
<p>没有可执行的x标志，那么我们可以通过命令改变。如下操作：</p>
<p><img src="" alt="Hadoop在CentOS下的单机配置"></p>
<p>看到没，变成绿色的了。有人是把所有者、组、其他用户对该文件的权限都设置为可执行，不过我在这就只让它能被所有者执行就行了。（该文件可能不管紧要，其他重要的文件，我认为不能像他们那样设置。）</p>
<p>现在我们再执行它
<img src="" alt="Hadoop在CentOS下的单机配置"></p>
<p>没有问题了吧，在开始解包了。</p>
<p>(1)Java环境变量配置</p>
<p>输入vim /etc/profile，添加如下的内容（在此我建议所有的都编辑都用vim取代vi，因为它有颜色变化，有语法问题的话很容易发现。）
<img src="" alt="Hadoop在CentOS下的单机配置"></p>
<p>保存好退出后，我们需要改变一下改文件的权限，并执行一下该文件使配置生效。（注：大家一定要小心版本和路径啊，）</p>
<p><img src="" alt="Hadoop在CentOS下的单机配置"></p>
<p>配置完后执行java –version</p>
<p><img src="" alt="Hadoop在CentOS下的单机配置"></p>
<p>显示java的版本</p>
<p>来源： <a href="[http://www.linuxidc.com/Linux/2011-07/37992p2.htm](http://www.linuxidc.com/Linux/2011-07/37992p2.htm)">[http://www.linuxidc.com/Linux/2011-07/37992p2.htm](http://www.linuxidc.com/Linux/2011-07/37992p2.htm)</a> </p>
<ol>
<li>安装<a href="http://www.linuxidc.com/topicnews.aspx?tid=13" title="Hadoop" target="_blank">Hadoop</a></li>
</ol>
<p>（1）下载hadoop</p>
<p>到如下网址下载hadoop，存到/opt中,当然也可以手动点击下载。
<img src="" alt="Hadoop在CentOS下的单机配置"></p>
<p>（2）解压hadoop到/opt/hadoop下面，当然没有现成的opt/hadoop这个目录，所以要新建。</p>
<p><img src="" alt="Hadoop在CentOS下的单机配置"></p>
<p>然后解压到/opt/hadoop下</p>
<p><img src="" alt="Hadoop在CentOS下的单机配置"></p>
<p>3.1   进入/opt/hadoop/hadoop-0.20.2/conf，配置Hadoop配置文件。</p>
<p>（1）配置java环境：修改hadoop-env.sh文件
<img src="" alt="Hadoop在CentOS下的单机配置"></p>
<p>在最后加上这样的内容</p>
<p><img src="" alt="Hadoop在CentOS下的单机配置"></p>
<p>(2)配置Namenode的三个配置文件core-site.xml, hdfs-site.xml, mapred-site.xml。对应于/src/core/core-default.xml，但不能直接修改它，（hadoop启动时先读取src下面的core/core-default.xml,hdfs/hdfs-default.xml,apred/mapred-default.xml，里面缺失的变量由conf下面的三个-site文件提供）</p>
<p>这部分的配置建议参考官方网站（建议大家多上官网），如下：<a href="http://hadoop.apache.org/common/docs/current/single_node_setup.html" target="_blank"><a href="http://hadoop.apache.org/common/docs/current/single_node_setup.html">http://hadoop.apache.org/common/docs/current/single_node_setup.html</a></a></p>
<p>(2.1)配置core
<img src="" alt="Hadoop在CentOS下的单机配置"></p>
<p>（2.2）配置hdfs</p>
<p><img src="" alt="Hadoop在CentOS下的单机配置"></p>
<p>（2.3）配置mapred</p>
<p><img src="" alt="Hadoop在CentOS下的单机配置"></p>
<p>来源： <a href="[http://www.linuxidc.com/Linux/2011-07/37992p3.htm](http://www.linuxidc.com/Linux/2011-07/37992p3.htm)">[http://www.linuxidc.com/Linux/2011-07/37992p3.htm](http://www.linuxidc.com/Linux/2011-07/37992p3.htm)</a></p>
<p>4、启动<a href="http://www.linuxidc.com/topicnews.aspx?tid=13" title="Hadoop" target="_blank">Hadoop</a></p>
<p>(1)格式化namenode，（注意看清路径哦）
<img src="" alt="Hadoop在CentOS下的单机配置"></p>
<p>(2) 启动Hadoop守护进程</p>
<p><img src="" alt="Hadoop在CentOS下的单机配置"></p>
<p>这就表示你配置成功了，上面的一个都不能少</p>
<p>这时候你就可以点击进入下面的网站了。</p>
<p>NameNode - <a href="http://localhost:50070/" target="_blank">http://localhost:50070/</a></p>
<p>JobTracker - <a href="http://localhost:50030/" target="_blank">http://localhost:50030/</a></p>
<p>good luck</p>
<p>其实刚刚接触一个东西可能会觉得不好弄，一旦你弄好了以后就会很顺手。那时候你会告诉自己，这个东西装起来怎么这么白痴啊！赶紧开始下一个工作！加油！
来源： <a href="[http://www.linuxidc.com/Linux/2011-07/37992p4.htm](http://www.linuxidc.com/Linux/2011-07/37992p4.htm)">[http://www.linuxidc.com/Linux/2011-07/37992p4.htm](http://www.linuxidc.com/Linux/2011-07/37992p4.htm)</a> </p>

      
    </div>
    
    
      
    
    
        <footer id="post-meta">
        <span class="categories">Posted in<span class="breadcrumb fa fa-folder"><li><a href="/categories/hadoop/">hadoop</a></li></span></span> | <span class="tags">Tagged <a href="/tags/hadoop/" class="label label-primary">hadoop</a></span> | <span class="time">recent updated:<time title="2014-03-07 09:54:36"datetime="2014-03-07 09:54:36"> mar. 7 2014</time></span> | <span class="comment-link">
<a href="http://itsolife.com/2014/02/02/2014-02-02-hadoop--Hadoop在CentOS下的单机配置/#comments" class="ds-thread-count comment-link" data-thread-key="2014-02-02-hadoop--Hadoop在CentOS下的单机配置" data-count-type="comments">暂无评论</a></span>
        </footer>
    
    <div class="clearfix"></div>
  </div>
</div>
</article>



  <article>
<div id="post" class="post well">
  <div class="post-content">
    <header class="well-sm">
      <i class="fa icon fa-5x pull-left"></i>
      <h1 class="title"><a href="/2014/02/02/2014-02-02-hadoop--hdfs_shell/">hdfs_shell</a></h1>
      
        <span>Posted on<time datetime="2014-02-02T01:54:36.000Z"> <a href="/2014/02/02/2014-02-02-hadoop--hdfs_shell/">feb. 2 2014</a></time></span>      
    </header>
    
    
    <div class="entry">
      
        <h1 id="hdfs_shell">hdfs_shell</h1>
<p>HDFS File System Shell Guide
Table of contents
1
Overview............................................................................................................................3
1.1 1.2 1.3 1.4 1.5 1.6 1.7 1.8 1.9
cat ................................................................................................................................. 3 chgrp ............................................................................................................................. 3 chmod ........................................................................................................................... 3 chown ........................................................................................................................... 4 copyFromLocal..............................................................................................................4 copyToLocal..................................................................................................................4 count ............................................................................................................................. 4 cp .................................................................................................................................. 4 du................................................................................................................................... 5 dus ...............................................................................................................................5 expunge .......................................................................................................................5 get ................................................................................................................................5 getmerge ......................................................................................................................6 ls...................................................................................................................................6 lsr................................................................................................................................. 6 mkdir ...........................................................................................................................7 moveFromLocal ..........................................................................................................7 moveToLocal............................................................................................................... 7 mv ............................................................................................................................... 7 put ............................................................................................................................... 8 rm ................................................................................................................................ 8 rmr ...............................................................................................................................8 setrep ...........................................................................................................................9
Copyright © 2008 The Apache Software Foundation. All rights reserved.
1.10 1.11 1.12 1.13 1.14 1.15 1.16 1.17 1.18 1.19 1.20 1.21 1.22 1.23
HDFS File System Shell Guide
1.24 1.25 1.26 1.27 1.28
stat ...............................................................................................................................9 tail ............................................................................................................................... 9 test .............................................................................................................................10 text ............................................................................................................................ 10 touchz ........................................................................................................................10
Page 2
Copyright © 2008 The Apache Software Foundation. All rights reserved.
HDFS File System Shell Guide</p>
<ol>
<li>Overview
The FileSystem (FS) shell is invoked by bin/hadoop fs <args>. All FS shell commands take path URIs as arguments. The URI format is scheme://autority/path. For HDFS the scheme is hdfs, and for the local filesystem the scheme is file. The scheme and authority are optional. If not specified, the default scheme specified in the configuration is used. An HDFS file or directory such as /parent/child can be specified as hdfs://namenodehost/parent/child or simply as /parent/child (given that your configuration is set to point to hdfs://namenodehost). Most of the commands in FS shell behave like corresponding Unix commands. Differences are described with each of the commands. Error information is sent to stderr and the output is sent to stdout.
1.1. cat
Usage: hadoop fs -cat URI [URI …] Copies source paths to stdout. Example: • hadoop fs -cat hdfs://nn1.example.com/file1 hdfs://nn2.example.com/file2 • hadoop fs -cat file:///file3 /user/hadoop/file4 Exit Code: Returns 0 on success and -1 on error.
1.2. chgrp
Usage: hadoop fs -chgrp [-R] GROUP URI [URI …] Change group association of files. With -R, make the change recursively through the directory structure. The user must be the owner of files, or else a super-user. Additional information is in the HDFS Admin Guide: Permissions.
1.3. chmod
Usage: hadoop fs -chmod [-R] <MODE[,MODE]... | OCTALMODE> URI [URI …] Change the permissions of files. With -R, make the change recursively through the directory structure. The user must be the owner of the file, or else a super-user. Additional information is in the HDFS Admin Guide: Permissions.
Page 3
Copyright © 2008 The Apache Software Foundation. All rights reserved.
HDFS File System Shell Guide
1.4. chown
Usage: hadoop fs -chown [-R] [OWNER][:[GROUP]] URI [URI ] Change the owner of files. With -R, make the change recursively through the directory structure. The user must be a super-user. Additional information is in the HDFS Admin Guide: Permissions.
1.5. copyFromLocal
Usage: hadoop fs -copyFromLocal <localsrc> URI Similar to put command, except that the source is restricted to a local file reference.
1.6. copyToLocal
Usage: hadoop fs -copyToLocal [-ignorecrc] [-crc] URI <localdst> Similar to get command, except that the destination is restricted to a local file reference.
1.7. count
Usage: hadoop fs -count [-q] <paths> Count the number of directories, files and bytes under the paths that match the specified file pattern. The output columns are: DIR_COUNT, FILE_COUNT, CONTENT_SIZE FILE_NAME. The output columns with -q are: QUOTA, REMAINING_QUATA, SPACE_QUOTA, REMAINING_SPACE_QUOTA, DIR_COUNT, FILE_COUNT, CONTENT_SIZE, FILE_NAME. Example: • hadoop fs -count hdfs://nn1.example.com/file1 hdfs://nn2.example.com/file2 • hadoop fs -count -q hdfs://nn1.example.com/file1 Exit Code: Returns 0 on success and -1 on error.
1.8. cp
Page 4
Copyright © 2008 The Apache Software Foundation. All rights reserved.
HDFS File System Shell Guide
Usage: hadoop fs -cp URI [URI …] <dest> Copy files from source to destination. This command allows multiple sources as well in which case the destination must be a directory. Example: • hadoop fs -cp /user/hadoop/file1 /user/hadoop/file2 • hadoop fs -cp /user/hadoop/file1 /user/hadoop/file2 /user/hadoop/dir Exit Code: Returns 0 on success and -1 on error.
1.9. du
Usage: hadoop fs -du URI [URI …] Displays aggregate length of files contained in the directory or the length of a file in case its just a file. Example: hadoop fs -du /user/hadoop/dir1 /user/hadoop/file1 hdfs://nn.example.com/user/hadoop/dir1 Exit Code: Returns 0 on success and -1 on error.
1.10. dus
Usage: hadoop fs -dus <args> Displays a summary of file lengths.
1.11. expunge
Usage: hadoop fs -expunge Empty the Trash. Refer to HDFS Architecture for more information on Trash feature.
1.12. get
Usage: hadoop fs -get [-ignorecrc] [-crc] <src> <localdst> Copy files to the local file system. Files that fail the CRC check may be copied with the -ignorecrc option. Files and CRCs may be copied using the -crc option.
Page 5
Copyright © 2008 The Apache Software Foundation. All rights reserved.
HDFS File System Shell Guide
Example: • hadoop fs -get /user/hadoop/file localfile • hadoop fs -get hdfs://nn.example.com/user/hadoop/file localfile Exit Code: Returns 0 on success and -1 on error.
1.13. getmerge
Usage: hadoop fs -getmerge <src> <localdst> [addnl] Takes a source directory and a destination file as input and concatenates files in src into the destination local file. Optionally addnl can be set to enable adding a newline character at the end of each file.
1.14. ls
Usage: hadoop fs -ls <args> For a file returns stat on the file with the following format: permissions number_of_replicas userid groupid filesize modification_date modification_time filename For a directory it returns list of its direct children as in unix.A directory is listed as: permissions userid groupid modification_date modification_time dirname Example: hadoop fs -ls /user/hadoop/file1 Exit Code: Returns 0 on success and -1 on error.
1.15. lsr
Usage: hadoop fs -lsr <args> Recursive version of ls. Similar to Unix ls -R.
Page 6
Copyright © 2008 The Apache Software Foundation. All rights reserved.
HDFS File System Shell Guide
1.16. mkdir
Usage: hadoop fs -mkdir <paths> Takes path uri&#39;s as argument and creates directories. The behavior is much like unix mkdir -p creating parent directories along the path. Example: • hadoop fs -mkdir /user/hadoop/dir1 /user/hadoop/dir2 • hadoop fs -mkdir hdfs://nn1.example.com/user/hadoop/dir hdfs://nn2.example.com/user/hadoop/dir Exit Code: Returns 0 on success and -1 on error.
1.17. moveFromLocal
Usage: dfs -moveFromLocal <localsrc> <dst> Similar to put command, except that the source localsrc is deleted after it&#39;s copied.
1.18. moveToLocal
Usage: hadoop fs -moveToLocal [-crc] <src> <dst> Displays a &quot;Not implemented yet&quot; message.
1.19. mv
Usage: hadoop fs -mv URI [URI …] <dest> Moves files from source to destination. This command allows multiple sources as well in which case the destination needs to be a directory. Moving files across filesystems is not permitted. Example: • hadoop fs -mv /user/hadoop/file1 /user/hadoop/file2 • hadoop fs -mv hdfs://nn.example.com/file1 hdfs://nn.example.com/file2 hdfs://nn.example.com/file3 hdfs://nn.example.com/dir1 Exit Code:
Page 7
Copyright © 2008 The Apache Software Foundation. All rights reserved.
HDFS File System Shell Guide
Returns 0 on success and -1 on error.
1.20. put
Usage: hadoop fs -put <localsrc> ... <dst> Copy single src, or multiple srcs from local file system to the destination filesystem. Also reads input from stdin and writes to destination filesystem. • hadoop fs -put localfile /user/hadoop/hadoopfile • hadoop fs -put localfile1 localfile2 /user/hadoop/hadoopdir • hadoop fs -put localfile hdfs://nn.example.com/hadoop/hadoopfile • hadoop fs -put - hdfs://nn.example.com/hadoop/hadoopfile Reads the input from stdin. Exit Code: Returns 0 on success and -1 on error.
1.21. rm
Usage: hadoop fs -rm [-skipTrash] URI [URI …] Delete files specified as args. Only deletes non empty directory and files. If the -skipTrash option is specified, the trash, if enabled, will be bypassed and the specified file(s) deleted immediately. This can be useful when it is necessary to delete files from an over-quota directory. Refer to rmr for recursive deletes. Example: • hadoop fs -rm hdfs://nn.example.com/file /user/hadoop/emptydir Exit Code: Returns 0 on success and -1 on error.
1.22. rmr
Usage: hadoop fs -rmr [-skipTrash] URI [URI …] Recursive version of delete. If the -skipTrash option is specified, the trash, if enabled, will be bypassed and the specified file(s) deleted immediately. This can be useful when it is necessary to delete files from an over-quota directory.
Page 8
Copyright © 2008 The Apache Software Foundation. All rights reserved.
HDFS File System Shell Guide
Example: • hadoop fs -rmr /user/hadoop/dir • hadoop fs -rmr hdfs://nn.example.com/user/hadoop/dir Exit Code: Returns 0 on success and -1 on error.
1.23. setrep
Usage: hadoop fs -setrep [-R] <path> Changes the replication factor of a file. -R option is for recursively increasing the replication factor of files within a directory. Example: • hadoop fs -setrep -w 3 -R /user/hadoop/dir1 Exit Code: Returns 0 on success and -1 on error.
1.24. stat
Usage: hadoop fs -stat URI [URI …] Returns the stat information on the path. Example: • hadoop fs -stat path Exit Code: Returns 0 on success and -1 on error.
1.25. tail
Usage: hadoop fs -tail [-f] URI Displays last kilobyte of the file to stdout. -f option can be used as in Unix. Example: • hadoop fs -tail pathname Exit Code:
Page 9
Copyright © 2008 The Apache Software Foundation. All rights reserved.
HDFS File System Shell Guide
Returns 0 on success and -1 on error.
1.26. test
Usage: hadoop fs -test -[ezd] URI Options: -e check to see if the file exists. Return 0 if true. -z check to see if the file is zero length. Return 0 if true. -d check to see if the path is directory. Return 0 if true. Example: • hadoop fs -test -e filename
1.27. text
Usage: hadoop fs -text <src> Takes a source file and outputs the file in text format. The allowed formats are zip and TextRecordInputStream.
1.28. touchz
Usage: hadoop fs -touchz URI [URI …] Create a file of zero length. Example: • hadoop -touchz pathname Exit Code: Returns 0 on success and -1 on error.
Page 10
Copyright © 2008 The Apache Software Foundation. All rights reserved.</li>
</ol>

      
    </div>
    
    
      
    
    
        <footer id="post-meta">
        <span class="categories">Posted in<span class="breadcrumb fa fa-folder"><li><a href="/categories/hadoop/">hadoop</a></li></span></span> | <span class="tags">Tagged <a href="/tags/hadoop/" class="label label-primary">hadoop</a></span> | <span class="time">recent updated:<time title="2014-03-07 09:54:36"datetime="2014-03-07 09:54:36"> mar. 7 2014</time></span> | <span class="comment-link">
<a href="http://itsolife.com/2014/02/02/2014-02-02-hadoop--hdfs_shell/#comments" class="ds-thread-count comment-link" data-thread-key="2014-02-02-hadoop--hdfs_shell" data-count-type="comments">暂无评论</a></span>
        </footer>
    
    <div class="clearfix"></div>
  </div>
</div>
</article>



  <article>
<div id="post" class="post well">
  <div class="post-content">
    <header class="well-sm">
      <i class="fa icon fa-5x pull-left"></i>
      <h1 class="title"><a href="/2014/02/02/2014-02-02-hadoop--HadoopSummit2013见闻：看完基本了解整个Hadoop生态圈格局和趋势了/">Hadoop Summit 2013见闻：看完基本了解整个Hadoop生态圈格局和趋势了</a></h1>
      
        <span>Posted on<time datetime="2014-02-02T01:54:36.000Z"> <a href="/2014/02/02/2014-02-02-hadoop--HadoopSummit2013见闻：看完基本了解整个Hadoop生态圈格局和趋势了/">feb. 2 2014</a></time></span>      
    </header>
    
    
    <div class="entry">
      
        <h1 id="hadoop-summit-2013-hadoop-">Hadoop Summit 2013见闻：看完基本了解整个Hadoop生态圈格局和趋势了</h1>
<p>原文出处： <a href="http://blog.sina.com.cn/s/blog_53a5366c0101dp0q.html" target="_blank">钱五哥の共享空间</a></p>
<p>今天参加了3个keynotes，42个session中的8个，和一大堆厂商讨论技术，真是信息大爆炸的一天。</p>
<p><a href="http://cdn2.jobbole.com/2013/06/53a5366c4e020b68545d6amp.jpg" title="Hadoop Summit 2013见闻：看完基本了解整个Hadoop生态圈格局和趋势了" target="_blank"><img src="&quot;Hadoop Summit 2013见闻：看完基本了解整个Hadoop生态圈格局和趋势了&quot;" alt="Hadoop Summit 2013见闻：看完基本了解整个Hadoop生态圈格局和趋势了"></a></p>
<p>Hadoop从诞生到今年已经有7个年头，今年出现了很多新的变化：</p>
<p>1、Hadoop被公认是一套行业大数据标准开源软件，在分布式环境下提供了海量数据的处理能力（Gartner）。几乎所有主流厂商都围绕Hadoop开发工具、开源软件、商业化工具和技术服务。今年大型IT公司，如EMC、Microsoft、Intel、Teradata、Cisco都明显增加了Hadoop方面的投入，Teradata还公开展示了一个一体机；另一方面创业型Hadoop公司层出不穷，这次看到的几个是Sqrrl、Wandisco、GridGain、InMobi等等，都推出了开源的或者商用的软件。</p>
<p><a href="http://cdn2.jobbole.com/2013/06/53a5366c4e020b79dcc7bamp.jpg" title="Hadoop Summit 2013见闻：看完基本了解整个Hadoop生态圈格局和趋势了" target="_blank"><img src="&quot;Hadoop Summit 2013见闻：看完基本了解整个Hadoop生态圈格局和趋势了&quot;" alt="Hadoop Summit 2013见闻：看完基本了解整个Hadoop生态圈格局和趋势了"></a></p>
<p>2、Hadoop生态系统丰富多彩，但是核心已经被Cloudera、HortonWorks牢牢掌控，基本上没有撼动之可能。今年Hortonworks的宣传是100% open source，Cloudera只好干着急，谁叫他不开放Cloudera Enterprise Manager的源代码呢？Hortonworks介绍Ambari的时候，会场至少5个Cloudera的工程师在仔细聆听，有个小伙不停地在iPad上面速记，竞争可见一斑，个人估计，Cloudera早晚将Enterprise Manager开源。Hortonworks目前Ambari的committer是20+，Contributor 50+，后一个数字可能有些水，但是第一个是没有问题的。目前每天有update，1.25版本比1.0x版本明显好用了。其他大小厂商的生存之道就是搞插件，如Wandisco、vmware、mellanox、GridGain，而且插件均是不用修改内核的外挂 – 这些厂商是没有能力动内核的，持续投入可能会有一些作用，如vmware，但是一线hadoop厂商是绝不会松手的。</p>
<p>3、Hadoop 2.0转型基本上无可阻挡。Hortonworks的VPArun在介绍Tez的时候，给出了很多有趣的ppt，主旨就是一个：MapReduce已经是昨日黄花，Yarn将是未来并行计算的基础设施。我自己还没有使用Yarn，但是Hortonworks已经围绕Yarn开发了很多工具，尤其是Tez，这个玩意可以提升查询计划的执行时间，PIG和Hive将被改写并重装上阵。Hortonworks虽然没有搞出来Impala，但是从更底层的技术上包围Impala，两个老大的布局和较量始终没有停止。</p>
<p><a href="http://cdn2.jobbole.com/2013/06/53a5366c4e020b9af392eamp.jpg" title="Hadoop Summit 2013见闻：看完基本了解整个Hadoop生态圈格局和趋势了" target="_blank"><img src="&quot;Hadoop Summit 2013见闻：看完基本了解整个Hadoop生态圈格局和趋势了&quot;" alt="Hadoop Summit 2013见闻：看完基本了解整个Hadoop生态圈格局和趋势了"></a></p>
<p><a href="http://cdn2.jobbole.com/2013/06/53a5366c4e020bac324d8amp.jpg" title="Hadoop Summit 2013见闻：看完基本了解整个Hadoop生态圈格局和趋势了" target="_blank"><img src="&quot;Hadoop Summit 2013见闻：看完基本了解整个Hadoop生态圈格局和趋势了&quot;" alt="Hadoop Summit 2013见闻：看完基本了解整个Hadoop生态圈格局和趋势了"></a></p>
<p>4、SQL over Hadoop是一个重要的技术趋势。去年Hadoop World时，MPP还吹嘘自己如何牛X。但是Google发布了Dremel和PowerDrill，EMC搞出来HAWQ，Cloudera搞出来Impala之后，所有的MPP都开始反思自己的技术路线。和Parccel技术人员（感觉是售前）讨论了一下，她找出一张卡片说Parccel速度是Hive的100X，领先Impala10年。我感觉这个说话很快就会失灵，首先是Hive的优化一直没有停止，Hortonworks搞出来Tez、Stinger（与Facebook合作）。虽然MPP领先Hadoop很多年，根据80：20原则，如果hadoopSQL只做用户需要的20%特性，那么这个差距最多2年，2年内，hadoopSQL将在部分领域超越MPP。MPP企业的出路就是学习HAWQ。列存储也是推陈出新，近期主要是ORC（MS和Hortonworks合作）、Parquet（Twitter和Cloudera合作），有木有看出来两个巨头PK的身影？有木有看到抱团PK？这些技术在测试中均显示出很大的优势</p>
<p><a href="http://cdn2.jobbole.com/2013/06/53a5366c4e020bc314b2bamp.jpg" title="Hadoop Summit 2013见闻：看完基本了解整个Hadoop生态圈格局和趋势了" target="_blank"><img src="&quot;Hadoop Summit 2013见闻：看完基本了解整个Hadoop生态圈格局和趋势了&quot;" alt="Hadoop Summit 2013见闻：看完基本了解整个Hadoop生态圈格局和趋势了"></a></p>
<p><a href="http://cdn2.jobbole.com/2013/06/53a5366c4e020bd3a31e7amp.jpg" title="Hadoop Summit 2013见闻：看完基本了解整个Hadoop生态圈格局和趋势了" target="_blank"><img src="&quot;Hadoop Summit 2013见闻：看完基本了解整个Hadoop生态圈格局和趋势了&quot;" alt="Hadoop Summit 2013见闻：看完基本了解整个Hadoop生态圈格局和趋势了"></a></p>
<p><a href="http://cdn2.jobbole.com/2013/06/53a5366c4e020be5d05c3amp.jpg" title="Hadoop Summit 2013见闻：看完基本了解整个Hadoop生态圈格局和趋势了" target="_blank"><img src="&quot;Hadoop Summit 2013见闻：看完基本了解整个Hadoop生态圈格局和趋势了&quot;" alt="Hadoop Summit 2013见闻：看完基本了解整个Hadoop生态圈格局和趋势了"></a></p>
<p>5、IT和开源单位合作广泛。这个不仅仅存在IT厂商和开源之间，实际上开源之间也在密切合作。不太清楚合作的内部信息，但是基本上有两种模式：产品/软件交叉集成（含管理系统集成）；合作开发和推广。在技术方面就要求软件有很好的架构，提供开放的接口，这一点Ambari的设计和俺对HT的要求一模一样，可以俺未能如愿，而Amabri已经开发了好几个版本。</p>
<p>6、技术上看，大数据和云的整合也是一个选项（注意，不是趋势，而是选项）。今年新增了OpenStack相关议题，一些集成商和厂商也提出了云上Hadoop的适用场景。这个并不是适用于所有人，但是部分用户可以因此获益。Netflix是一个典型的例子，他们的实例都在AWS上面，显然他们的hadoop是基于虚拟机的，和一个Netflix小伙子（日本人）交流，他们大约有2000个虚拟实例，基于EMR，并开发了Gennie管理系统。</p>
<p><a href="http://cdn2.jobbole.com/2013/06/53a5366c4e020bf6e8c61amp.jpg" title="Hadoop Summit 2013见闻：看完基本了解整个Hadoop生态圈格局和趋势了" target="_blank"><img src="&quot;Hadoop Summit 2013见闻：看完基本了解整个Hadoop生态圈格局和趋势了&quot;" alt="Hadoop Summit 2013见闻：看完基本了解整个Hadoop生态圈格局和趋势了"></a></p>
<p>要睡觉了，4小时后还有一场信息大爆炸！贴一张在宾馆小院乘凉，看到的小松鼠吧，也就距离我5米不到，真要赞一声美帝的环境！</p>
<p><a href="http://cdn2.jobbole.com/2013/06/53a5366c07cd01360d282amp.jpg" title="Hadoop Summit 2013见闻：看完基本了解整个Hadoop生态圈格局和趋势了" target="_blank"><img src="&quot;Hadoop Summit 2013见闻：看完基本了解整个Hadoop生态圈格局和趋势了&quot;" alt="Hadoop Summit 2013见闻：看完基本了解整个Hadoop生态圈格局和趋势了"></a></p>
<p>相关信息：</p>
<ol>
<li><a href="http://blog.sina.com.cn/s/blog_53a5366c0101doch.html" title="http://blog.sina.com.cn/s/blog_53a5366c0101doch.html" target="_blank">Hadoop Summit 2013 Day1：BOF&amp;Meetup</a>
<img src="&quot;1 vote, average: 5.00 out of 5&quot;" alt="1 vote, average: 5.00 out of 5"><img src="&quot;1 vote, average: 5.00 out of 5&quot;" alt="1 vote, average: 5.00 out of 5"><img src="&quot;1 vote, average: 5.00 out of 5&quot;" alt="1 vote, average: 5.00 out of 5"><img src="&quot;1 vote, average: 5.00 out of 5&quot;" alt="1 vote, average: 5.00 out of 5"><img src="&quot;1 vote, average: 5.00 out of 5&quot;" alt="1 vote, average: 5.00 out of 5"> (<strong>*1</strong> 个评分，平均: <strong>5.00*</strong>)</li>
</ol>
<p><img src="&quot;Loading ...&quot;" alt="Loading ..."> Loading ...</p>

      
    </div>
    
    
      
    
    
        <footer id="post-meta">
        <span class="categories">Posted in<span class="breadcrumb fa fa-folder"><li><a href="/categories/hadoop/">hadoop</a></li></span></span> | <span class="tags">Tagged <a href="/tags/hadoop/" class="label label-primary">hadoop</a></span> | <span class="time">recent updated:<time title="2014-03-07 09:54:36"datetime="2014-03-07 09:54:36"> mar. 7 2014</time></span> | <span class="comment-link">
<a href="http://itsolife.com/2014/02/02/2014-02-02-hadoop--HadoopSummit2013见闻：看完基本了解整个Hadoop生态圈格局和趋势了/#comments" class="ds-thread-count comment-link" data-thread-key="2014-02-02-hadoop--HadoopSummit2013见闻：看完基本了解整个Hadoop生态圈格局和趋势了" data-count-type="comments">暂无评论</a></span>
        </footer>
    
    <div class="clearfix"></div>
  </div>
</div>
</article>



  <article>
<div id="post" class="post well">
  <div class="post-content">
    <header class="well-sm">
      <i class="fa icon fa-5x pull-left"></i>
      <h1 class="title"><a href="/2014/02/02/2014-02-02-hadoop--Hadoop121编译Eclipse插件/">Hadoop 1.2.1编译Eclipse插件</a></h1>
      
        <span>Posted on<time datetime="2014-02-02T01:54:36.000Z"> <a href="/2014/02/02/2014-02-02-hadoop--Hadoop121编译Eclipse插件/">feb. 2 2014</a></time></span>      
    </header>
    
    
    <div class="entry">
      
        <h1 id="hadoop-1-2-1-eclipse-">Hadoop 1.2.1编译Eclipse插件</h1>
<h2 id="-src-contrib-eclipse-plugin-build-xml">/src/contrib/eclipse-plugin/build.xml</h2>
<h3 id="1-ivy-download-">1）取消ivy-download：</h3>
<p><a href="http://static.oschina.net/uploads/space/2013/0520/194613_IBLw_256028.png" target="_blank"><img src="" alt=""></a></p>
<h3 id="2-plugin-jar-">2）添加将要打包到plugin中的第三方jar包列表：</h3>
<p>01</p>
<!-- Override jar target to specify manifest -->


<p>02</p>
<p>&lt;</p>
<p>target</p>
<p>name</p>
<p>=</p>
<p>&quot;jar&quot;</p>
<p>depends</p>
<p>=</p>
<p>&quot;compile&quot;</p>
<p>unless</p>
<p>=</p>
<p>&quot;skip.contrib&quot;</p>
<p>&gt;
03</p>
<p>&lt;</p>
<p>mkdir</p>
<p>dir</p>
<p>=</p>
<p>&quot;${build.dir}/lib&quot;</p>
<p>/&gt; </p>
<p>04</p>
<p>05</p>
<!-- 自定义的修改内容：begin -->


<p>06</p>
<!--
07

<copy file="${hadoop.root}/build/hadoop-core-${version}.jar"

08



tofile="${build.dir}/lib/hadoop-core.jar" verbose="true"/>
09

<copy file="${hadoop.root}/build/ivy/lib/Hadoop/common/commons-cli-${commons-cli.version}.jar" 

10



todir="${build.dir}/lib" verbose="true"/> 
11

-->


<p>12</p>
<p>&lt;</p>
<p>copy</p>
<p>file</p>
<p>=</p>
<p>&quot;${hadoop.root}/hadoop-core-${version}.jar&quot;</p>
<p>tofile</p>
<p>=</p>
<p>&quot;${build.dir}/lib/hadoop-core.jar&quot;</p>
<p>verbose</p>
<p>=</p>
<p>&quot;true&quot;</p>
<p>/&gt;
13</p>
<p>&lt;</p>
<p>copy</p>
<p>file</p>
<p>=</p>
<p>&quot;${hadoop.root}/lib/commons-cli-1.2.jar&quot;</p>
<p>todir</p>
<p>=</p>
<p>&quot;${build.dir}/lib&quot;</p>
<p>verbose</p>
<p>=</p>
<p>&quot;true&quot;</p>
<p>/&gt; </p>
<p>14</p>
<p>&lt;</p>
<p>copy</p>
<p>file</p>
<p>=</p>
<p>&quot;${hadoop.root}/lib/commons-configuration-1.6.jar&quot;</p>
<p>todir</p>
<p>=</p>
<p>&quot;${build.dir}/lib&quot;</p>
<p>verbose</p>
<p>=</p>
<p>&quot;true&quot;</p>
<p>/&gt; 
15</p>
<p>&lt;</p>
<p>copy</p>
<p>file</p>
<p>=</p>
<p>&quot;${hadoop.root}/lib/commons-httpclient-3.0.1.jar&quot;</p>
<p>todir</p>
<p>=</p>
<p>&quot;${build.dir}/lib&quot;</p>
<p>verbose</p>
<p>=</p>
<p>&quot;true&quot;</p>
<p>/&gt; </p>
<p>16</p>
<p>&lt;</p>
<p>copy</p>
<p>file</p>
<p>=</p>
<p>&quot;${hadoop.root}/lib/commons-lang-2.4.jar&quot;</p>
<p>todir</p>
<p>=</p>
<p>&quot;${build.dir}/lib&quot;</p>
<p>verbose</p>
<p>=</p>
<p>&quot;true&quot;</p>
<p>/&gt;<br>17</p>
<p>&lt;</p>
<p>copy</p>
<p>file</p>
<p>=</p>
<p>&quot;${hadoop.root}/lib/jackson-core-asl-1.8.8.jar&quot;</p>
<p>todir</p>
<p>=</p>
<p>&quot;${build.dir}/lib&quot;</p>
<p>verbose</p>
<p>=</p>
<p>&quot;true&quot;</p>
<p>/&gt;</p>
<p>18</p>
<p>&lt;</p>
<p>copy</p>
<p>file</p>
<p>=</p>
<p>&quot;${hadoop.root}/lib/jackson-mapper-asl-1.8.8.jar&quot;</p>
<p>todir</p>
<p>=</p>
<p>&quot;${build.dir}/lib&quot;</p>
<p>verbose</p>
<p>=</p>
<p>&quot;true&quot;</p>
<p>/&gt;<br>19</p>
<!-- 自定义的修改内容：end -->


<p>20</p>
<p>&lt;</p>
<p>jar
21</p>
<p>jarfile</p>
<p>=</p>
<p>&quot;${build.dir}/hadoop-${name}-${version}.jar&quot;</p>
<p>22</p>
<p>manifest</p>
<p>=</p>
<p>&quot;${root}/META-INF/MANIFEST.MF&quot;</p>
<blockquote>
<p>23</p>
</blockquote>
<p>&lt;</p>
<p>fileset</p>
<p>dir</p>
<p>=</p>
<p>&quot;${build.dir}&quot;</p>
<p>includes</p>
<p>=</p>
<p>&quot;classes/ lib/&quot;</p>
<p>/&gt; </p>
<p>24</p>
<p>&lt;</p>
<p>fileset</p>
<p>dir</p>
<p>=</p>
<p>&quot;${root}&quot;</p>
<p>includes</p>
<p>=</p>
<p>&quot;resources/ plugin.xml&quot;</p>
<p>/&gt; 
25</p>
<p>&lt;/</p>
<p>jar</p>
<blockquote>
</blockquote>
<p>26</p>
<p>&lt;/</p>
<p>target</p>
<blockquote>
<p>&lt;</p>
</blockquote>
<p>span</p>
<p>style</p>
<p>=</p>
<p>&quot;font-size:10pt;line-height:1.5;font-family:&#39;sans serif&#39;, tahoma, verdana, helvetica;&quot;</p>
<blockquote>
<p> &lt;/</p>
</blockquote>
<p>span</p>
<p>&gt;</p>
<p><a href="http://static.oschina.net/uploads/space/2013/0520/194752_bBaX_256028.png" target="_blank"><img src="" alt=""></a> </p>
<h2 id="3-hadoop-src-contrib-build-contrib-xml-">3.%hadoop%/src/contrib/build-contrib.xml ：</h2>
<h3 id="1-hadoop-version-eclipse-eclipse-home-">1）添加hadoop的version和eclipse的eclipse.home属性：</h3>
<p>1</p>
<p>&lt;?</p>
<p>xml</p>
<p>version</p>
<p>=</p>
<p>&quot;1.0&quot;</p>
<p>?&gt;</p>
<p>2</p>
<!-- Imported by contrib//*/build.xml files to share generic targets. -->
3

&lt;

project


name

=

&quot;hadoopbuildcontrib&quot;


xmlns:ivy

=

&quot;antlib:org.apache.ivy.ant&quot;

&gt;

4

&lt;

property


name

=

&quot;name&quot;


value

=

&quot;${ant.project.name}&quot;

/&gt;
5

&lt;

property


name

=

&quot;root&quot;


value

=

&quot;${basedir}&quot;

/&gt;

6

&lt;

property


name

=

&quot;hadoop.root&quot;


location

=

&quot;${root}/../../../&quot;

/&gt;
7

<!-- hadoop版本、eclipse安装路径 -->

<p>8</p>
<p>&lt;</p>
<p>property</p>
<p>name</p>
<p>=</p>
<p>&quot;version&quot;</p>
<p>value</p>
<p>=</p>
<p>&quot;1.1.2&quot;</p>
<p>/&gt;
9</p>
<p>&lt;</p>
<p>property</p>
<p>name</p>
<p>=</p>
<p>&quot;eclipse.home&quot;</p>
<p>location</p>
<p>=</p>
<p>&quot;%eclipse%&quot;</p>
<p>/&gt;</p>
<p><a href="http://static.oschina.net/uploads/space/2013/0520/194838_mpUD_256028.png" target="_blank"><img src="" alt=""></a>  </p>
<h3 id="2-ivy-download-">2）取消ivy-download：</h3>
<p><a href="http://static.oschina.net/uploads/space/2013/0520/194859_Cc5J_256028.png" target="_blank"><img src="" alt=""></a> 
来源： <a href="[http://my.oschina.net/vigiles/blog/132238](http://my.oschina.net/vigiles/blog/132238)">[http://my.oschina.net/vigiles/blog/132238](http://my.oschina.net/vigiles/blog/132238)</a></p>
<p>配置/${hadoop-home}/src/contrib/eclipse-plugins/build.xml</p>
<p>找到</p>
<path id=”classpath”>

<p>然后添加</p>
<fileset dir=”${hadoop.root}/”>
      <include name=”/*.jar”/>
</fileset>

<p><strong>这一步很重要，如果不添加的话会出现找不到相应的程序包，错误如下（给出部分）</strong> ：</p>
<p>[javac]      Counters.Group group = counters.getGroup(groupName);
    [javac]              ^
    [javac] /home/summerdg/hadoop_src/hadoop-1.2.1/src/contrib/eclipse-plugin/src/java/org/apache/hadoop/eclipse/server/HadoopJob.java:305: 错误: 程序包Counters不存在
    [javac]      for (Counters.Counter counter : group) {
    [javac]                    ^
    [javac] /home/summerdg/hadoop_src/hadoop-1.2.1/src/contrib/eclipse-plugin/src/java/org/apache/hadoop/eclipse/dfs/DFSFile.java:74: 错误: 找不到符号
    [javac]      FileStatus fs = getDFS().getFileStatus(path);
    [javac]      ^
    [javac]  符号:  类 FileStatus
    [javac]  位置: 类 DFSFile
    [javac] 注: 某些输入文件使用或覆盖了已过时的 API。
    [javac] 注: 有关详细信息, 请使用 -Xlint:deprecation 重新编译。
    [javac] 注: 某些输入文件使用了未经检查或不安全的操作。
    [javac] 注: 有关详细信息, 请使用 -Xlint:unchecked 重新编译。
    [javac] 100 个错误</p>
<p>当然，出现这个错误还有个原因，就是你已经设置了这句，但你依然出现这个错误，那就是你eclipse版本的问题了。</p>
<p>找到
来源： <a href="[http://www.kankanews.com/ICkengine/archives/63441.shtml](http://www.kankanews.com/ICkengine/archives/63441.shtml)">[http://www.kankanews.com/ICkengine/archives/63441.shtml](http://www.kankanews.com/ICkengine/archives/63441.shtml)</a> </p>
<h2 id="4-hadoop_home-build-xml-">4.编辑%HADOOP_HOME%/build.xml：</h2>
<h3 id="1-hadoop-">1）修改hadoop版本号：</h3>
<p><a href="http://static.oschina.net/uploads/space/2013/0520/194950_g5cw_256028.png" target="_blank"><img src="" alt=""></a></p>
<h3 id="2-ivy-download-">2）取消ivy-download：</h3>
<p><a href="http://static.oschina.net/uploads/space/2013/0520/195011_l8BK_256028.png" target="_blank"><img src="" alt=""></a> </p>
<h2 id="5-hadoop-src-contrib-eclipse-plugin-meta-inf-manifest-mf-">5.修改%hadoop%/src/contrib/eclipse-plugin/META-INF/MANIFEST.MF：</h2>
<p>修改${HADOOP_HOME}/src/contrib/eclipse-plugin/META-INF/MANIFEST.MF的Bundle-ClassPath：
1</p>
<p>Bundle-ClassPath: classes/,</p>
<p>2</p>
<p>lib/hadoop-core.jar,
3</p>
<p>lib/commons-cli-1.2.jar,</p>
<p>4</p>
<p>lib/commons-configuration-1.6.jar,
5</p>
<p>lib/commons-httpclient-3.0.1.jar,</p>
<p>6</p>
<p>lib/commons-lang-2.4.jar,
7</p>
<p>lib/jackson-core-asl-1.8.8.jar,</p>
<p>8</p>
<p>lib/jackson-mapper-asl-1.8.8.jar</p>
<p><a href="http://static.oschina.net/uploads/space/2013/0520/195106_mqwV_256028.png" target="_blank"><img src="" alt=""></a> </p>
<h2 id="-build-contrib-eclipse-plugin-ant-jar-">进入到 /build/contrib/eclipse-plugin/执行 ant jar ：</h2>
<p>01</p>
<p>hep@hep-ubuntu:~$ </p>
<p>cd</p>
<p>~/hadoop/src/contrib/eclipse-plugin</p>
<p>02</p>
<p>hep@hep-ubuntu:~/hadoop/src/contrib/eclipse-plugin$ ant jar
03</p>
<p>Buildfile: /home/hep/hadoop/src/contrib/eclipse-plugin/build.xml</p>
<p>04</p>
<p>05</p>
<p>check-contrib:</p>
<p>06</p>
<p>07</p>
<p>init:</p>
<p>08</p>
<p>[</p>
<p>echo</p>
<p>] contrib: eclipse-plugin
09</p>
<p>10</p>
<p>init-contrib:
11</p>
<p>12</p>
<p>ivy-probe-antlib:
13</p>
<p>14</p>
<p>ivy-init-antlib:
15</p>
<p>16</p>
<p>ivy-init:
17</p>
<p>[ivy:configure] :: Ivy 2.1.0 - 20090925235825 :: <a href="http://ant.apache.org/ivy/" target="_blank">http://ant.apache.org/ivy/</a> ::</p>
<p>18</p>
<p>[ivy:configure] :: loading settings :: </p>
<p>file</p>
<p>= /home/hep/hadoop/ivy/ivysettings.xml
19</p>
<p>20</p>
<p>ivy-resolve-common:
21</p>
<p>[ivy:resolve] :: resolving dependencies :: org.apache.hadoop</p>
<p>/#eclipse-plugin;working@hep-ubuntu</p>
<p>22</p>
<p>[ivy:resolve]   confs: [common]
23</p>
<p>[ivy:resolve]   found commons-logging</p>
<p>/#commons-logging;1.0.4 in maven2</p>
<p>24</p>
<p>[ivy:resolve]   found log4j</p>
<p>/#log4j;1.2.15 in maven2
25</p>
<p>[ivy:resolve] :: resolution report :: resolve 171ms :: artifacts dl 4ms</p>
<p>26</p>
<hr>
<p>27</p>
<p>|                  |            modules            ||   artifacts   |</p>
<p>28</p>
<p>|       conf       | number| search|dwnlded|evicted|| number|dwnlded|
29</p>
<hr>
<p>30</p>
<p>|      common      |   2   |   0   |   0   |   0   ||   2   |   0   |
31</p>
<hr>
<p>32</p>
<p>33</p>
<p>ivy-retrieve-common:</p>
<p>34</p>
<p>[ivy:retrieve] :: retrieving :: org.apache.hadoop</p>
<p>/#eclipse-plugin [sync]
35</p>
<p>[ivy:retrieve]  confs: [common]</p>
<p>36</p>
<p>[ivy:retrieve]  0 artifacts copied, 2 already retrieved (0kB/6ms)
37</p>
<p>[ivy:cachepath] DEPRECATED: </p>
<p>&#39;ivy.conf.file&#39;</p>
<p>is deprecated, use </p>
<p>&#39;ivy.settings.file&#39;</p>
<p>instead</p>
<p>38</p>
<p>[ivy:cachepath] :: loading settings :: </p>
<p>file</p>
<p>= /home/hep/hadoop/ivy/ivysettings.xml
39</p>
<p>40</p>
<p>compile:
41</p>
<p>[</p>
<p>echo</p>
<p>] contrib: eclipse-plugin</p>
<p>42</p>
<p>[javac] /home/hep/hadoop/src/contrib/eclipse-plugin/build.xml:61: warning: </p>
<p>&#39;includeantruntime&#39;</p>
<p>was not </p>
<p>set</p>
<p>, defaulting to build.sysclasspath=last; </p>
<p>set</p>
<p>to </p>
<p>false</p>
<p>for</p>
<p>repeatable builds
43</p>
<p>44</p>
<p>jar:
45</p>
<p>[</p>
<p>mkdir</p>
<p>] Created </p>
<p>dir</p>
<p>: /home/hep/hadoop/build/contrib/eclipse-plugin/lib</p>
<p>46</p>
<p>[copy] Copying 1 </p>
<p>file</p>
<p>to /home/hep/hadoop/build/contrib/eclipse-plugin/lib
47</p>
<p>[copy] Copying /home/hep/hadoop/hadoop-core-1.1.2.jar to /home/hep/hadoop/build/contrib/eclipse-plugin/lib/hadoop-core.jar</p>
<p>48</p>
<p>[copy] Copying 1 </p>
<p>file</p>
<p>to /home/hep/hadoop/build/contrib/eclipse-plugin/lib
49</p>
<p>[copy] Copying /home/hep/hadoop/lib/commons-cli-1.2.jar to /home/hep/hadoop/build/contrib/eclipse-plugin/lib/commons-cli-1.2.jar</p>
<p>50</p>
<p>[copy] Copying 1 </p>
<p>file</p>
<p>to /home/hep/hadoop/build/contrib/eclipse-plugin/lib
51</p>
<p>[copy] Copying /home/hep/hadoop/lib/commons-configuration-1.6.jar to /home/hep/hadoop/build/contrib/eclipse-plugin/lib/commons-configuration-1.6.jar</p>
<p>52</p>
<p>[copy] Copying 1 </p>
<p>file</p>
<p>to /home/hep/hadoop/build/contrib/eclipse-plugin/lib
53</p>
<p>[copy] Copying /home/hep/hadoop/lib/commons-httpclient-3.0.1.jar to /home/hep/hadoop/build/contrib/eclipse-plugin/lib/commons-httpclient-3.0.1.jar</p>
<p>54</p>
<p>[copy] Copying 1 </p>
<p>file</p>
<p>to /home/hep/hadoop/build/contrib/eclipse-plugin/lib
55</p>
<p>[copy] Copying /home/hep/hadoop/lib/commons-lang-2.4.jar to /home/hep/hadoop/build/contrib/eclipse-plugin/lib/commons-lang-2.4.jar</p>
<p>56</p>
<p>[copy] Copying 1 </p>
<p>file</p>
<p>to /home/hep/hadoop/build/contrib/eclipse-plugin/lib
57</p>
<p>[copy] Copying /home/hep/hadoop/lib/jackson-core-asl-1.8.8.jar to /home/hep/hadoop/build/contrib/eclipse-plugin/lib/jackson-core-asl-1.8.8.jar</p>
<p>58</p>
<p>[copy] Copying 1 </p>
<p>file</p>
<p>to /home/hep/hadoop/build/contrib/eclipse-plugin/lib
59</p>
<p>[copy] Copying /home/hep/hadoop/lib/jackson-mapper-asl-1.8.8.jar to /home/hep/hadoop/build/contrib/eclipse-plugin/lib/jackson-mapper-asl-1.8.8.jar</p>
<p>60</p>
<p>[jar] Building jar: /home/hep/hadoop/build/contrib/eclipse-plugin/hadoop-eclipse-plugin-1.1.2.jar
61</p>
<p>62</p>
<p>BUILD SUCCESSFUL
63</p>
<p>Total </p>
<p>time</p>
<p>: 3 seconds</p>
<p>生成的插件jar就在本目录中。</p>
<p>把编译好的插件放入 $eclipse_home/dropins/hadoop/plugins/ (mkdir -p 创建)</p>
<p>启动eclipse 新建mapreduce project， 配置好map/reduce location, 就可以看到hdfs中的文件了</p>

      
    </div>
    
    
      
    
    
        <footer id="post-meta">
        <span class="categories">Posted in<span class="breadcrumb fa fa-folder"><li><a href="/categories/hadoop/">hadoop</a></li></span></span> | <span class="tags">Tagged <a href="/tags/hadoop/" class="label label-primary">hadoop</a></span> | <span class="time">recent updated:<time title="2014-03-07 09:54:36"datetime="2014-03-07 09:54:36"> mar. 7 2014</time></span> | <span class="comment-link">
<a href="http://itsolife.com/2014/02/02/2014-02-02-hadoop--Hadoop121编译Eclipse插件/#comments" class="ds-thread-count comment-link" data-thread-key="2014-02-02-hadoop--Hadoop121编译Eclipse插件" data-count-type="comments">暂无评论</a></span>
        </footer>
    
    <div class="clearfix"></div>
  </div>
</div>
</article>



  <article>
<div id="post" class="post well">
  <div class="post-content">
    <header class="well-sm">
      <i class="fa icon fa-5x pull-left"></i>
      <h1 class="title"><a href="/2014/02/02/2014-02-02-hadoop--CentOS的Hadoop集群配置/">CentOS的Hadoop集群配置</a></h1>
      
        <span>Posted on<time datetime="2014-02-02T01:54:36.000Z"> <a href="/2014/02/02/2014-02-02-hadoop--CentOS的Hadoop集群配置/">feb. 2 2014</a></time></span>      
    </header>
    
    
    <div class="entry">
      
        <h1 id="centos-hadoop-">CentOS的Hadoop集群配置</h1>
<h3 id="-centos-hadoop-http-blog-csdn-net-inte_sleeper-article-details-6569985-"><a href="http://blog.csdn.net/inte_sleeper/article/details/6569985" target="_blank">CentOS的Hadoop集群配置（一）</a></h3>
<h3 id="-"> </h3>
<p>参考资料：</p>
<p><a href="http://www.michael-noll.com/tutorials/running-hadoop-on-ubuntu-linux-single-node-cluster/" target="_blank"><a href="http://www.michael-noll.com/tutorials/running-hadoop-on-ubuntu-linux-single-node-cluster/">http://www.michael-noll.com/tutorials/running-hadoop-on-ubuntu-linux-single-node-cluster/</a></a></p>
<p><a href="http://www.michael-noll.com/tutorials/running-hadoop-on-ubuntu-linux-multi-node-cluster/" target="_blank"><a href="http://www.michael-noll.com/tutorials/running-hadoop-on-ubuntu-linux-multi-node-cluster/">http://www.michael-noll.com/tutorials/running-hadoop-on-ubuntu-linux-multi-node-cluster/</a></a></p>
<p><a href="http://www.michael-noll.com/tutorials/writing-an-hadoop-mapreduce-program-in-python/" target="_blank"><a href="http://www.michael-noll.com/tutorials/writing-an-hadoop-mapreduce-program-in-python/">http://www.michael-noll.com/tutorials/writing-an-hadoop-mapreduce-program-in-python/</a></a></p>
<p><a href="http://hadoop.apache.org/common/docs/current/cluster_setup.html" target="_blank"><a href="http://hadoop.apache.org/common/docs/current/cluster_setup.html">http://hadoop.apache.org/common/docs/current/cluster_setup.html</a></a></p>
<p>以下集群配置内容，以两台机器为例。其中一台是 master ，另一台是 slave1 。</p>
<p>master 上运行 name node, data node, task tracker, job tracker ， secondary name node ；</p>
<p>slave1 上运行 data node, task tracker 。</p>
<p>前面加 /* 表示对两台机器采取相同的操作</p>
<ol>
<li>安装 JDK /*</li>
</ol>
<p>yum install java-1.6.0-openjdk-devel</p>
<ol>
<li>设置环境变量 /*</li>
</ol>
<p>编辑 /etc/profile 文件，设置 JAVA_HOME 环境变量以及类路径：</p>
<p>export JAVA_HOME=&quot;/usr/lib/jvm/java-1.6.0-openjdk-1.6.0.0.x86_64&quot;</p>
<p>export PATH=$PATH:$JAVA_HOME/bin</p>
<p>export CLASSPATH=.:$JAVA_HOME/lib/tools.jar:$JAVA_HOME/lib/dt.jar</p>
<ol>
<li>添加 hosts 的映射 /*</li>
</ol>
<p>编辑 /etc/hosts 文件，<strong>注意</strong> <strong>host name **</strong>不要有下划线，见下步骤 9**</p>
<p>192.168.225.16 master</p>
<p>192.168.225.66 slave1</p>
<ol>
<li>配置 SSH /*</li>
</ol>
<p>cd /root &amp; mkdir .ssh</p>
<p>chmod 700 .ssh &amp; cd .ssh</p>
<p>创建密码为空的 RSA 密钥对：</p>
<p>ssh-keygen -t rsa -P &quot;&quot;</p>
<p>在提示的对称密钥名称中输入 id_rsa</p>
<p>将公钥添加至 authorized_keys 中：</p>
<p>cat id_rsa.pub &gt;&gt; authorized_keys</p>
<p>chmod 644 authorized_keys <strong>/#</strong> <strong>重要</strong></p>
<p>编辑 sshd 配置文件 /etc/ssh/sshd_config ，把 /#AuthorizedKeysFile  .ssh/authorized_keys 前面的注释取消掉。</p>
<p>重启 sshd 服务：</p>
<p>service sshd restart</p>
<p>测试 SSH 连接。连接时会提示是否连接，按回车后会将此公钥加入至 knows_hosts 中：</p>
<p>ssh localhost</p>
<ol>
<li>配置 master 和 slave1 的 ssh 互通</li>
</ol>
<p>在 slave1 中重复步骤 4 ，然后把 slave1 中的 .ssh/authorized_keys 复制至 master 的 .ssh/authorized_keys中。注意复制过去之后，要看最后的类似 root@localhost 的字符串，修改成 root@slave1 。同样将 master的 key 也复制至 slave1 ，并将最后的串修改成 root@master 。</p>
<p>或者使用如下命令：</p>
<p>ssh-copy-id -i ~/.ssh/id_rsa.pub root@slave1</p>
<p>测试 SSH 连接：</p>
<p>在 master 上运行：</p>
<p>ssh slave1</p>
<p>在 slave1 上运行：</p>
<p>ssh master</p>
<ol>
<li>安装 Hadoop</li>
</ol>
<p>下载 hadoop 安装包：</p>
<p>wget <a href="http://mirror.bjtu.edu.cn/apache/hadoop/common/hadoop-0.20.203.0/hadoop-0.20.203.0rc1.tar.gz" target="_blank"><a href="http://mirror.bjtu.edu.cn/apache/hadoop/common/hadoop-0.20.203.0/hadoop-0.20.203.0rc1.tar.gz">http://mirror.bjtu.edu.cn/apache/hadoop/common/hadoop-0.20.203.0/hadoop-0.20.203.0rc1.tar.gz</a></a></p>
<p>复制安装包至 slave1 ：</p>
<p>scp hadoop-0.20.203.0rc1.tar.gz root@slave1:/root/</p>
<p>解压：</p>
<p>tar xzvf hadoop-0.20.203.0rc1.tar.gz</p>
<p>mkdir /usr/local/hadoop</p>
<p>mv hadoop-0.20.203.0//* /usr/local/hadoop</p>
<pre><code>     修改 .bashrc 文件（位于用户目录下，即 ~/.bashrc ，对于 root ，即为 /root/.bashrc ）

     添加环境变量：

     export HADOOP_HOME=/usr/local/hadoop

export PATH=$PATH:$HADOOP_HOME/bin
</code></pre><ol>
<li>配置 Hadoop 环境变量 /*</li>
</ol>
<p><strong>以下所有 hadoop **</strong>目录下的文件，均以相对路径 hadoop <strong>**开始</strong></p>
<p>修改 hadoop/conf/hadoop-env.sh 文件，将里面的 JAVA_HOME 改成步骤 2 中设置的值。</p>
<ol>
<li>创建 Hadoop 本地临时文件夹 /*</li>
</ol>
<p>mkdir /root/hadoop_tmp （<strong>注意这一步，千万不要放在</strong> <strong>/tmp **</strong>目录下面！！因为 <strong><strong>/tmp </strong></strong>默认分配的空间是很小的，往 <strong><strong>hdfs </strong></strong>里放几个大文件就会导致空间满了，就会报错）**</p>
<p>修改权限：</p>
<p>chown -R hadoop:hadoop /root/hadoop_tmp</p>
<p>更松地，也可以这样：</p>
<p>chmod –R 777 /root/hadoop_tmp</p>
<ol>
<li>配置 Hadoop</li>
</ol>
<p>修改 master 的 hadoop/conf/core-site.xml ，在 <configuration> 节中添加如下内容：</p>
<p>注意： <strong>fs.default.name **</strong>的值不能带下划线**</p>
<property>

    <name>hadoop.tmp.dir</name>

    <value>/root/hadoop<em>tmp/hadoop</em>${user.name}</value>

</property>

<property>

    <name>fs.default.name</name>

    <value>hdfs://localhost:54310</value> 

</property>

<property>

    <name>io.sort.mb</name>

    <value>1024</value> 

</property>

<pre><code>     其中 io.sort.mb 值，指定了排序使用的内存，大的内存可以加快 job 的处理速度。



     修改 hadoop/conf/mapred-site.xml ，在 &lt;configuration&gt; 节中添加如下内容：
</code></pre><property>

    <name>mapred.job.tracker</name>

    <value>localhost:54311</value>

</property>

<property>

    <name>mapred.map.child.java.opts</name>

    <value>-Xmx4096m</value>

</property>

<property>

    <name>mapred.reduce.child.java.opts</name>

    <value>-Xmx4096m</value>

</property>

<pre><code>     其中 mapred.map.child.java.opts, mapred.reduce.child.java.opts 分别指定 map/reduce 任务使用的最大堆内存。较小的内存可能导致程序抛出 OutOfMemoryException 。
</code></pre><p>修改 conf/hdfs -site.xml ，在 <configuration> 节中添加如下内容：</p>
<property>

    <name>dfs.replication</name>

    <value>2</value>

</property>



<p>同样，修改 slave1 的 /usr/local/hadoop/conf/core-site.xml ，在 <configuration> 节中添加如下内容：</p>
<property>

    <name>hadoop.tmp.dir</name>

    <value>/root/hadoop<em>tmp/hadoop</em>${user.name}</value>

</property>

<property>

    <name>fs.default.name</name>

    <value>hdfs://localhost:54310</value> 

</property>

<property>

    <name>io.sort.mb</name>

    <value>1024</value> 

</property>



<pre><code>     修改 conf/mapred-site.xml ，在 &lt;configuration&gt; 节中添加如下内容：
</code></pre><property>

    <name>mapred.job.tracker</name>

    <value>localhost:54311</value>

</property>

<property>

    <name>mapred.map.child.java.opts</name>

    <value>-Xmx4096m</value>

</property>

<property>

    <name>mapred.reduce.child.java.opts</name>

    <value>-Xmx4096m</value>

</property>



<pre><code>     修改 conf/hdfs -site.xml ，在 &lt;configuration&gt; 节中添加如下内容：
</code></pre><property>

    <name>dfs.replication</name>

    <value>2</value>

    </property>



<ol>
<li>修改 hadoop/bin/hadoop 文件</li>
</ol>
<p>把 221 行修改成如下。因为对于 root 用户， -jvm 参数是有问题的，所以需要加一个判断 ( 或者以非 root 用户运行这个脚本也没问题 )</p>
<p>HADOOP_OPTS=&quot;$HADOOP_OPTS -jvm server $HADOOP_DATANODE_OPTS&quot;  à</p>
<pre><code>/#for root, -jvm option is invalid.

CUR_USER=`whoami`

if [ &quot;$CUR_USER&quot; = &quot;root&quot; ]; then

    HADOOP_OPTS=&quot;$HADOOP_OPTS -server $HADOOP_DATANODE_OPTS&quot;

else

    HADOOP_OPTS=&quot;$HADOOP_OPTS -jvm server $HADOOP_DATANODE_OPTS&quot;

fi 
</code></pre><p>unset $CUR_USER</p>
<p>至此， master 和 slave1 都已经完成了 single_node 的搭建，可以分别在两台机器上测试单节点。</p>
<p>启动节点：</p>
<p>hadoop/bin/start-all.sh</p>
<p>运行 jps 命令，应能看到类似如下的输出：</p>
<p>937 DataNode</p>
<p>9232 Jps</p>
<p>8811 NameNode</p>
<p>12033 JobTracker</p>
<p>12041 TaskTracker
来源： <a href="[http://blog.csdn.net/inte_sleeper/article/details/6569985](http://blog.csdn.net/inte_sleeper/article/details/6569985)">[http://blog.csdn.net/inte_sleeper/article/details/6569985](http://blog.csdn.net/inte_sleeper/article/details/6569985)</a> </p>
<p><a href="http://blog.csdn.net/inte_sleeper/article/details/6569990" target="_blank">CentOS的Hadoop集群配置（二）</a>
下面的教程把它们合并至 multi-node cluster 。</p>
<ol>
<li>合并 single-node 至 multi-node cluster</li>
</ol>
<p>修改 master 的 hadoop/conf/core-site.xml ：</p>
<property>

    <name>hadoop.tmp.dir</name>

    <value>/root/hadoop<em>tmp/hadoop</em>${user.name}</value>

</property>

<property>

    <name>fs.default.name</name>

    <value>hdfs://<strong>master</strong> :54310</value>

</property>

<property>

    <name>io.sort.mb</name>

    <value>1024</value> 

</property>



<pre><code>     修改 conf/mapred-site.xml ，在 &lt;configuration&gt; 节中添加如下内容：
</code></pre><property>

    <name>mapred.job.tracker</name>

    <value><strong>master</strong> :54311</value>

</property>

<property>

    <name>mapred.map.child.java.opts</name>

    <value>-Xmx4096m</value>

</property>

<property>

    <name>mapred.reduce.child.java.opts</name>

    <value>-Xmx4096m</value>

</property>

<pre><code>     修改 conf/hdfs -site.xml ，在 &lt;configuration&gt; 节中添加如下内容：
</code></pre><property>

    <name>dfs.replication</name>

    <value>2</value>

</property>

<p>把这三个文件复制至 slave1 相应的目录 hadoop/conf 中 <strong>( **</strong>即 master <strong><strong>和 slave1 </strong></strong>的内容完全一致 )**</p>
<pre><code>     修改所有节点的 hadoop/conf/masters ，把文件内容改成： master

     修改所有节点的 hadoop/conf/slaves ，把文件内容改成：
</code></pre><p>master</p>
<pre><code>slave1



     分别删除 master 和 slave1 的 dfs/data 文件：

     rm –rf /root/hadoop_tmp/hadoop_root/dfs/data
</code></pre><p>重新格式化 namenode ：</p>
<pre><code>     hadoop/bin/hadoop namenode -format



     测试，在 master 上运行：

     hadoop/bin/start-all.sh

     在 master 上运行 jps 命令
</code></pre><p>此时输出应类似于：</p>
<pre><code>     11648 TaskTracker
</code></pre><p>11166 NameNode</p>
<p>11433 SecondaryNameNode</p>
<p>12552 Jps</p>
<p>11282 DataNode</p>
<p>11525 JobTracker</p>
<p>在 slave1 上运行 jps</p>
<p>此时输出应包含 ( 即至少有 DataNode, 否则即为出错 ) ：</p>
<p>3950 Jps</p>
<p>3121 TaskTracker</p>
<p>3044 DataNode</p>
<ol>
<li>测试一个 JOB</li>
</ol>
<p>首先升级 python( 可选，如果 JOB 是 python 写的 ) ：</p>
<p>cd /etc/yum.repos.d/</p>
<p>wget <a href="http://mirrors.geekymedia.com/centos/geekymedia.repo" target="_blank">http://mirrors.geekymedia.com/centos/geekymedia.repo</a></p>
<p>yum makecache</p>
<p>yum -y install python26</p>
<p><strong>升级 python **</strong>的教程，见另外一篇文档。如果已经通过以上方法安装了 python2.6 <strong>**，那需要先卸载：</strong></p>
<p>yum remove python26 python26-devel</p>
<pre><code>     CentOS 的 yum 依赖于 python2.4 ，而 /usr/bin 中 python 程序即为 python2.4 。我们需要把它修改成python2.6 。



     cd /usr/bin/

     编辑 yum 文件，把第一行的
</code></pre><p>/#!/usr/bin/python   à   /#!/usr/bin/python2.4  </p>
<p>保存文件。</p>
<pre><code>     删除旧版本的 python 可执行文件（这个文件跟该目录下 python2.4 其实是一样的，所以可以直接删除）

     rm -f python

     让 python 指向 python2.6 的可执行程序。

     ln -s python26 python  
</code></pre><ol>
<li>Word count python 版本</li>
</ol>
<p><strong>Map.py</strong></p>
<p>/#! /usr/bin/python</p>
<p>import sys;</p>
<p>for line in sys.stdin:</p>
<p>  line =  line.strip();</p>
<p>  words = line.split();</p>
<p>  for word in words:</p>
<pre><code>  print &#39;%s/t%s&#39; % (word,1);
</code></pre><p><strong>Reduce.py</strong></p>
<p>/#!/usr/bin/python</p>
<p>import sys;</p>
<p>wc = {};</p>
<p>for line in sys.stdin:</p>
<p>  line = line.strip();</p>
<p>  word,count = line.split(&#39;/t&#39;,1);</p>
<p>  try:</p>
<pre><code>  count = int(count);
</code></pre><p>  except Error:</p>
<pre><code>  pass;
</code></pre><p>  if wc.has_key(word):</p>
<pre><code>  wc[word] += count;
</code></pre><p>  else: wc[word] = count;</p>
<p>for key in wc.keys():</p>
<p>  print &#39;%s/t%s&#39; % (key, wc[key]);</p>
<p>本机测试：</p>
<p>echo &quot;foo foo bar bar foo abc&quot; | map.py</p>
<p>echo &quot;foo foo bar bar foo abc&quot; | map.py | sort | reduce.py</p>
<p>在 hadoop 中测试：</p>
<p>hadoop jar /usr/local/hadoop/contrib/streaming/hadoop-streaming-0.20.203.0.jar -file mapper.py -mapper mapper.py -file reducer.py -reducer reducer.py -input wc//* -output wc-out</p>
<p>Job 成功后，会在 HDFS 中生成 wc-out 目录。</p>
<p>查看结果：</p>
<p>hadoop fs –ls wc-out</p>
<p>hadoop fs –cat wc-out/part-00000</p>
<ol>
<li>集群增加新节点</li>
</ol>
<p>a.       执行步骤 1 ， 2.</p>
<p>b.       修改 hosts 文件，将集群中的 hosts 加入本身 /etc/hosts 中。并修改集群中其他节点的 hosts ，将新节点加入。</p>
<p>c.       master 的 conf/slaves 文件中，添加新节点。</p>
<p>d.       启动 datanode 和 task tracker 。</p>
<p>hadoop-daemon.sh start datanode</p>
<p>hadoop-daemon.sh start tasktracker</p>
<ol>
<li>Trouble-shooting</li>
</ol>
<p>hadoop 的日志在 hadoop/logs 中。</p>
<p>其中， logs 根目录包含的是 namenode, datanode, jobtracker, tasktracker 等的日志。分别以 hadoop-{username}-namenode/datanode/jobtracker/tasktracker-hostname.log 命名。</p>
<p>userlogs 目录里包含了具体的 job 日志，每个 job 有一个单独的目录，以 job<em>YYYYmmddHHmm_xxxx 命名。里面包含数个 attempt</em>{jobname}<em>m_xxxxx 或 attempt</em>{jobname}_r_xxxx 等数个目录。其中目录名中的m 表示 map 任务的日志， r 表示 reduce 任务的日志。因此，出错时，可以有针对性地查看特定的日志。</p>
<p>常见错误：</p>
<ol>
<li>出现类似：</li>
</ol>
<p><em>ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: java.io.IOException: Incompatible namespaceIDs …</em></p>
<p>的异常，是因为先格式化了 namenode ，后来又修改了配置导致。将 dfs/data 文件夹内容删除，再重新格式化 namenode 即可。</p>
<ol>
<li>出现类似：</li>
</ol>
<p><em>INFO org.apache.hadoop.ipc.Client: Retrying connect to server:…</em></p>
<p>的异常，首先确认 name node 是否启动。如果已经启动，有可能是 master 或 slave1 中的配置出错，集群配置参考步骤 11 。也有可能是防火墙问题，需添加以下例外：</p>
<p>50010 端口用于数据传输， 50020 用于 RPC 调用， 50030 是 WEB 版的 JOB 状态监控， 54311 是job tracker ， 54310 是与 master 通信的端口。</p>
<p>完整的端口列表见：</p>
<p><a href="http://www.cloudera.com/blog/2009/08/hadoop-default-ports-quick-reference/" target="_blank"><a href="http://www.cloudera.com/blog/2009/08/hadoop-default-ports-quick-reference/">http://www.cloudera.com/blog/2009/08/hadoop-default-ports-quick-reference/</a></a></p>
<p>iptables -A RH-Firewall-1-INPUT -p tcp -m tcp --dport 50010 -j ACCEPT</p>
<p>iptables -A RH-Firewall-1-INPUT -p tcp -m tcp --dport 50020 -j ACCEPT</p>
<p>iptables -A RH-Firewall-1-INPUT -p tcp -m tcp --dport 50030 -j ACCEPT</p>
<p>iptables -A RH-Firewall-1-INPUT -p tcp -m tcp --dport 50060 -j ACCEPT</p>
<p>iptables -A RH-Firewall-1-INPUT -p tcp -m tcp --dport 54310 -j ACCEPT</p>
<p>iptables -A RH-Firewall-1-INPUT -p tcp -m tcp --dport 54311 -j ACCEPT</p>
<p>iptables -A OUTPUT -p tcp -m tcp --dport 50010 -j ACCEPT</p>
<p>iptables -A OUTPUT -p tcp -m tcp --dport 50020 -j ACCEPT</p>
<p>iptables -A OUTPUT -p tcp -m tcp --sport 50010 -j ACCEPT</p>
<p>iptables -A OUTPUT -p tcp -m tcp --sport 50020 -j ACCEPT</p>
<p>iptables -A OUTPUT -p tcp -m tcp --dport 50030 -j ACCEPT</p>
<p>iptables -A OUTPUT -p tcp -m tcp --sport 50030 -j ACCEPT</p>
<p>iptables -A OUTPUT -p tcp -m tcp --dport 50060 -j ACCEPT</p>
<p>iptables -A OUTPUT -p tcp -m tcp --sport 50060 -j ACCEPT</p>
<p>iptables -A OUTPUT -p tcp -m tcp --dport 54310 -j ACCEPT</p>
<p>iptables -A OUTPUT -p tcp -m tcp --sport 54310 -j ACCEPT</p>
<p>iptables -A OUTPUT -p tcp -m tcp --dport 54311 -j ACCEPT</p>
<p>iptables -A OUTPUT -p tcp -m tcp --sport 54311 -j ACCEPT</p>
<p>保存规则：</p>
<p>/etc/init.d/iptables save</p>
<p>重启 iptables 服务：</p>
<p>service iptables restart</p>
<p>如果还是出现问题 2 的错误，那可能需要手工修改 /etc/sysconfig/iptables 的规则。手动添加这些规则。若有 ”reject-with icmp-host-prohibited” 的规则，需将规则加到它的前面。注意修改配置文件的时候，不需要带 iptables 命令。直接为类似于：</p>
<p>-A OUTPUT -p tcp -m tcp --sport 54311 -j ACCEPT</p>
<p>或关闭防火墙 <strong>( **</strong>建议，因为端口太多，要加的例外很多 )**</p>
<p>service iptables stop</p>
<ol>
<li><p>在 /etc/hosts 文件中，确保一个 host 只对应一个 IP ，否则会出错（如同时将 slave1 指向 127.0.0.1 和192.168.225.66 ），<strong>可能导致数据无法从一个节点复制至另一节点。</strong></p>
</li>
<li><p>出现类似：</p>
</li>
</ol>
<p><em>FATAL org.apache.hadoop.mapred.TaskTracker: Error running child : java.lang.OutOfMemoryError: Java heap space…</em></p>
<p>的异常，是因为堆内存不够。有以下几个地方可以考虑配置：</p>
<p>a.       conf/hadoop-env.sh 中， export HADOOP_HEAPSIZE=1000 这一行，默认为注释掉，堆大小为1000M ，可以取消注释，将这个值调大一些（对于 16G 的内存，可以调至 8G ）。</p>
<p>b.       conf/mapred-site.xml 中，添加 mapred.map.child.java.opts 属性，手动指定 JAVA 堆的参数值为 -Xmx2048m 或更大。这个值调整 map 任务的堆大小。即：</p>
<property>

    <name>mapred.map.child.java.opts </name>

    <value>-Xmx2048m</value>

</property>

<p>c.       conf/mapred-site.xml 中，添加 mapred.reduce.child.java.opts 属性，手动指定 JAVA 堆的参数值为 -Xmx2048m 或更大。这个值调整 reduce 任务的堆大小。即：</p>
<property>

    <name>mapred.reduce.child.java.opts </name>

    <value>-Xmx2048m</value>

</property>

<pre><code>               注意调整这些值之后，要重启 name node 。
</code></pre><p><em>5.       </em>出现类似： <em>java.io.IOException: File /user/root/pv_product_110124 could only be replicated to 0 nodes, instead of 1…</em></p>
<p>的异常，首先确保 hadoop 临时文件夹中有足够的空间，空间不够会导致这个错误。</p>
<p>如果空间没问题，那就尝试把临时文件夹中 dfs/data 目录删除，然后重新格式化 name node ：</p>
<p>hadoop namenode -format</p>
<p>注意：此命令会删除 hdfs 上的文件</p>
<p><em>6.       </em>出现类似： <em>java.io.IOException: Broken pipe…</em></p>
<p>的异常，检查你的程序吧，没准输出了不该输出的信息，如调试信息等。
来源： <a href="[http://blog.csdn.net/inte_sleeper/article/details/6569990](http://blog.csdn.net/inte_sleeper/article/details/6569990)">[http://blog.csdn.net/inte_sleeper/article/details/6569990](http://blog.csdn.net/inte_sleeper/article/details/6569990)</a> </p>

      
    </div>
    
    
      
    
    
        <footer id="post-meta">
        <span class="categories">Posted in<span class="breadcrumb fa fa-folder"><li><a href="/categories/hadoop/">hadoop</a></li></span></span> | <span class="tags">Tagged <a href="/tags/hadoop/" class="label label-primary">hadoop</a></span> | <span class="time">recent updated:<time title="2014-03-07 09:54:36"datetime="2014-03-07 09:54:36"> mar. 7 2014</time></span> | <span class="comment-link">
<a href="http://itsolife.com/2014/02/02/2014-02-02-hadoop--CentOS的Hadoop集群配置/#comments" class="ds-thread-count comment-link" data-thread-key="2014-02-02-hadoop--CentOS的Hadoop集群配置" data-count-type="comments">暂无评论</a></span>
        </footer>
    
    <div class="clearfix"></div>
  </div>
</div>
</article>





<ul id="pagination" class="pagination pagination-lg">
  <li><a class="extend prev" href="/page/108/">&laquo;</a></li><li><a class="page-number" href="/">1</a></li><li><a class="page-number" href="/page/2/">2</a></li><li><span class="space">&hellip;</span></li><li><a class="page-number" href="/page/106/">106</a></li><li><a class="page-number" href="/page/107/">107</a></li><li><a class="page-number" href="/page/108/">108</a></li><li class="active"><li><span class="page-number current">109</span></li><li><a class="page-number" href="/page/110/">110</a></li><li><a class="page-number" href="/page/111/">111</a></li><li><a class="page-number" href="/page/112/">112</a></li><li><span class="space">&hellip;</span></li></li><li><a class="page-number" href="/page/164/">164</a></li><li><a class="page-number" href="/page/165/">165</a></li><li><a class="extend next" href="/page/110/">&raquo;</a></li>
  <div class="clearfix"></div>
</ul></div><!--wapper-->
       </div><!-- ID main-col END -->
       <aside id="sidebar" class="alignright col-sx-6 col-sm-4 col-md-3 col-lg-3">
<div id="widget_search" class="widget panel panel-primary">
    <form action="//google.com/search" method="get" accept-charset="utf-8">
  <div class="input-group">
    <input class="form-control" id="searchbox" type="search" name="q" results="0" placeholder="search">
    <span class="input-group-btn">
      <button class="btn btn-default" type="submit">Go!</button>
    </span>
    <input type="hidden" name="q" value="site:itsolife.com">
  </div>
</form>
</div>

<div id="widget_category" class="widget panel panel-primary">
  <div class="panel-heading">category</div>  <div data-src='category' class='ajax_widgets'>正在加载...</div>
</div>

<div id="widget_recent_posts" class="widget panel panel-primary">
  <div class="panel-heading">recent posts</div>  <div data-src='recent_posts' class='ajax_widgets'>正在加载...</div></div>

<div id="widget_tagcloud" class="widget panel panel-primary">
  <div class="panel-heading">tagcloud</div>  <div data-src='tagcloud' class='ajax_widgets'>正在加载...</div></div>

<div id="widget_latest_update_posts" class="widget panel panel-primary">
  <div class="panel-heading">最近更新</div>  <div data-src='latest_update_posts' class='ajax_widgets'>正在加载...</div></div>

<div id="widget_recent_comments" class="widget panel panel-primary">
  <div class="panel-heading">recent comments</div>  

<div class="list-group-item ds-recent-comments" data-num-items="6" data-show-avatars="1" data-show-time="1" data-show-title="1" data-show-admin="1" data-excerpt-length="50"></div>



</div>

</aside>
       <div class="clearfix"></div>
     </div><!-- row END -->
  </div>
  <footer id="footer" class="container">
  <div class="panel panel-info">
  <section id='footer_widget'></section>  <div class="panel-footer">
  <div id="site-info">
    <span class='author'>
  
  &copy; 2014 RobinChia
  
    &nbsp;&nbsp;</span>
  
  <span id='analytics-51la'></span><span id='analytics-google'>
<script type="text/javascript">
  var _gaq = _gaq || [];
  _gaq.push(['_setAccount', 'UA-48559895-1']);
  _gaq.push(['_trackPageview']);
  _js2load.push({src:('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js'});
</script></span><span id='analytics-cnzz'>
<script type="text/javascript">var cnzz_protocol = (("https:" == document.location.protocol) ? " https://" : " http://");document.write(unescape("%3Cspan id='cnzz_stat_icon_5774006'%3E%3C/span%3E%3Cscript src='" + cnzz_protocol + "s17.cnzz.com/stat.php%3Fid%3D5774006%26show%3Dpic' type='text/javascript'%3E%3C/script%3E"));</script>
</span><span id='analytics-baidu'>
<script>
var _hmt = _hmt || [];
_js2load.push({src:"//hm.baidu.com/hm.js?1442f50724afc42380b51f097c43082c"});
</script>
</span>  </div>
  <div id="copyright">Blog powered by <a href='http://zespia.tw/hexo/'><strong>hexo</strong></a> Theme <strong><a href='https://github.com/chenall/hexo-theme-chenall'>chenall</a></strong>(Some change in it)<span class="pull-right"> 更新时间: <em>2014-03-15 17:28:50</em></span></div>
</div>
<div class="clearfix"></div>


  </div>
  </footer>
  
        <script src="http://cdn.bootcss.com/jquery/1.10.2/jquery.min.js"></script>        
        <script src="http://cdn.staticfile.org/twitter-bootstrap/3.1.0/js/bootstrap.min.js"></script>        
                
        <script src="http://cdn.bootcss.com/prettify/r298/prettify.min.js"></script>    <script type="text/javascript">
   var lang=["bsh", "c", "cc", "cpp", "cs", "csh", "cyc", "cv", "htm", "html",
    "java", "js", "m", "mxml", "perl", "pl", "pm", "py", "rb", "sh",
    "xhtml", "xml", "xsl"];
   var pretty_base='';
   $('script').each(function(){
	var c = $(this).attr('src');
	if (!c)
	    return;
	if (c.match(/(\/)?prettify(\.min)?\.js/i))
	{
	    var index = c.lastIndexOf('/');
	    if (index != -1)
		pretty_base = c.substr(0,index + 1);
	    return false;
	}
   })
   $('pre code').each(function(){
	var c = $(this).attr('class')
	if (!c)
	    return;
	c = c.match(/\s?(lang\-\w+)/i);
	if (c && lang.indexOf(c[1]) == -1)
	{
	    lang.push(c[1]);
	    $.getScript(pretty_base + c[1] + '.min.js');
	}
   })

    $(window).load(function(){
       $("pre").addClass("prettyprint");
       prettyPrint();
    })
</script>    
            <script type="text/javascript">
var duoshuoQuery = {short_name:"robinchia"};
_js2load.push({src:'http://static.duoshuo.com/embed.js',charset:'UTF-8'});
</script>
    
            <!--wumii_relatedItems-->
    
        <script src="http://cdn.bootcss.com/fancybox/2.1.5/jquery.fancybox.min.js"></script>    <script type="text/javascript">
(function($){
  $('.entry').each(function(i){
    $(this).find('img').each(function(){
      var alt = this.alt;

      if (alt){
        $(this).before('<span class="caption">' + alt + '</span>');
      }

      $(this).wrap('<a href="' + this.src + '" title="' + alt + '" class="fancybox" rel="fancybox' + i + '" />');
    });
  });

  $('.fancybox').fancybox();
})(jQuery);
</script>    
        <script src="http://cdn.bootcss.com/mathjax/2.3/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>    <script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      processEscapes: true
    }
  });
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>
    
<script type="text/javascript">
$('.ajax_widgets').each(function(){var src=$(this).attr('data-src');if(src)$(this).load('/widgets/'+src+'.html');});
$.each(_js2load,function(index,obj){loadjs(obj.src,obj.charset)});
</script>

<div id="scroll2top">
<img src="/scroll2top/arrow.png"/>
</div>
<script src="/scroll2top/scroll2top.min.js"></script>
<div id="winterland">
  <canvas></canvas>
</div>
<script src="/js/winterland.min.js"></script>

  </body>
</html>
