
<!DOCTYPE HTML>
<html>
<head>
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<title>The 114 Page | It so life</title>
<meta name="author" content="RobinChia">

<meta name="description" content="It so life">


<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">


<meta property="og:site_name" content="It so life"/>

<!--[if IE]><style>.testIE.IE{display:inline;}</style><![endif]-->
<!--[if lte IE 7]><link rel="stylesheet" href="/css/ie7.css" type="text/css"><![endif]-->
<!--[if (lt IE 9)&(gt IE 7)]><style>.testIE.IE8{display:inline;}</style><![endif]-->
<!--[if gt IE 8]><style>.testIE.IE9{display:inline;}</style><![endif]-->

<link href="/favicon.png" rel="icon">
<link rel="alternate" href="/atom.xml" title="It so life Feed" type="application/atom+xml">

<link rel="stylesheet" href="http://cdn.staticfile.org/twitter-bootstrap/3.1.0/css/bootstrap.min.css" type="text/css">
<link rel="stylesheet" href="http://cdn.bootcss.com/font-awesome/4.0.3/css/font-awesome.min.css" type="text/css">
<link rel="stylesheet" href="http://cdn.bootcss.com/prettify/r298/prettify.min.css" type="text/css">
<link rel="stylesheet" href="http://cdn.bootcss.com/fancybox/2.1.5/jquery.fancybox.min.css" type="text/css">
<link rel="stylesheet" href="/css/style.css" type="text/css">
<!--[if lt IE 9]>
   <style>article,aside,dialog,footer,header,section,footer,nav,figure,menu{display:block}</style>
   <script src="http://cdn.staticfile.org/html5shiv/r29/html5.min.js"></script>
   <script src="http://cdn.staticfile.org/respond.js/1.4.2/respond.min.js"></script>
<link href="http://cdn.staticfile.org/respond.js/1.4.2/respond-proxy.html" id="respond-proxy" rel="respond-proxy" />
<link href="/js/respond.proxy.gif" id="respond-redirect" rel="respond-redirect" />
<script src="/js/respond.proxy.js"></script>
   <script src="http://cdn.bootcss.com/selectivizr/1.0.2/selectivizr-min.js"></script>
<![endif]-->
<script type="text/javascript">
function loadjs(c,d){var a=document.createElement("script");a.async=!0;a.type="text/javascript";a.src=c;a.charset=d||"gbk";(document.getElementsByTagName("HEAD")[0]||document.getElementsByTagName("BODY")[0]).appendChild(a)};
var _protocol = (("https:" == document.location.protocol) ? " https://" : " http://");
var _js2load = [];
</script>

</head>
<body>
      <header id="header" class="container"><nav id="main-nav" class="navbar navbar-default navbar-fixed-top " role="navigation">
  <div class="container">
    <div class="navbar-header">
    <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".navbar-collapse">
      <span class="sr-only">Toggle navigation</span>
      <span class="icon-bar"></span>
      <span class="icon-bar"></span>
      <span class="icon-bar"></span>
    </button>
    <a class="navbar-brand" href="/">It so life</a>
    </div>
    <div  class="collapse navbar-collapse">
      <ul  class="nav navbar-nav">
  
        <li><a href="/" title="Home">Home</a></li>      
        <li><a href="/about/" title="About">About</a></li>      
        <li><a href="/archives/" title="Archives">Archives</a></li>      
      <li class='dropdown'>
        <a class='dropdown-toggle' data-toggle='dropdown' href='#'>website<b class='caret'></b></a>
        <ul class='dropdown-menu pure-menu-selected'>
    
          <li><a href="//groups.google.com/forum/#!forum/pongba" title="TopLanguage">TopLanguage</a></li>    
          <li><a href="//itpub.net/" title="ITPub">ITPub</a></li>    
          <li><a href="//blog.jobbole.com/" title="Bole">Bole</a></li>    
          <li><a href="//nosql-database.org/" title="nosql">nosql</a></li>    
          <li><a href="//gitimmersion.googol.im/" title="Git">Git</a></li>    
        </ul>
      </li>
    
      </ul>
      <ul class="nav navbar-nav navbar-right">
      
        <li><a href="/atom.xml">RSS</a></li>
      
      
        <li><a href="https://twitter.com/robinchia">twitter</a></li>
      
      
      
      
        <li><a href="https://github.com/robinchia">github</a></li>
      
      </ul>
    </div>
  </div>
</nav>
<div class="clearfix"></div>
</header>
  <div id='content' class="container">
     <div class="page-header-wrapper">
      <!--[if lt IE 9]><div class="alert alert-warning alert-dismissable"><button type="button" class="close" data-dismiss="alert" aria-hidden="true">&times;</button><strong>提示:</strong>您的浏览器版本太低了,建议升级到 <strong><a href="http://windows.microsoft.com/zh-cn/internet-explorer/download-ie" title="IE9">IE9</a></strong> 以上,本站使用<a href="https://www.google.com/intl/zh-CN/chrome/">Chrome浏览器</a>可以获得最好的显示效果.</div><![endif]-->
      <div class="page-header"><h1 align="center"><big>It so life</big> </h1>
        <h5 align="center"><big>love as life</big></h5>
      </div>
     </div>
     <div class="row">
       <div id="main-col" class="alignleft col-sx-12 col-sm-8 col-md-9 col-lg-9">
      <section id='header_widget'></section>
          <div id="wrapper"><article>
<div id="post" class="post well">
  <div class="post-content">
    <header class="well-sm">
      <i class="fa icon fa-5x pull-left"></i>
      <h1 class="title"><a href="/2014/02/02/2014-02-02-zhiya-IT外企--IT外企那点儿事3：奇怪的面试/">IT外企那点儿事(3)：奇怪的面试</a></h1>
      
        <span>Posted on<time datetime="2014-02-02T01:54:36.000Z"> <a href="/2014/02/02/2014-02-02-zhiya-IT外企--IT外企那点儿事3：奇怪的面试/">feb. 2 2014</a></time></span>      
    </header>
    
    
    <div class="entry">
      
        <h1 id="it-3-">IT外企那点儿事(3)：奇怪的面试</h1>
<p><a href=""></a></p>
<h1 id="-http-www-cnblogs-com-forfuture1978-"><a href="http://www.cnblogs.com/forfuture1978/" target="_blank">觉先</a></h1>
<p>  <a href="http://www.cnblogs.com/" target="_blank">博客园</a> :: <a href="http://www.cnblogs.com/forfuture1978/" target="_blank">首页</a> :: <a href="http://q.cnblogs.com/" target="_blank">博问</a> :: <a href="http://home.cnblogs.com/ing/" target="_blank">闪存</a> :: <a href="http://www.cnblogs.com/forfuture1978/admin/EditPosts.aspx?opt=1" target="_blank">新随笔</a> :: <a href="http://space.cnblogs.com/msg/send/%e8%a7%89%e5%85%88" target="_blank">联系</a> :: <a href="http://www.cnblogs.com/forfuture1978/rss" target="_blank">订阅</a> <a href="http://www.cnblogs.com/forfuture1978/rss" target="_blank"><img src="" alt="订阅"></a> :: <a href="http://www.cnblogs.com/forfuture1978/admin/EditPosts.aspx" target="_blank">管理</a> :: <img src="" alt="">   130 随笔 :: 0 文章 :: 544 评论 :: 0 引用
<a href="">&lt;</a>2010年5月<a href="">&gt;</a>日一二三四五六252627282930<a href="http://www.cnblogs.com/forfuture1978/archive/2010/05/01.html" target="_blank">1</a><a href="http://www.cnblogs.com/forfuture1978/archive/2010/05/02.html" target="_blank">2</a><a href="http://www.cnblogs.com/forfuture1978/archive/2010/05/03.html" target="_blank">3</a>4<a href="http://www.cnblogs.com/forfuture1978/archive/2010/05/05.html" target="_blank">5</a>67<a href="http://www.cnblogs.com/forfuture1978/archive/2010/05/08.html" target="_blank">8</a><a href="http://www.cnblogs.com/forfuture1978/archive/2010/05/09.html" target="_blank">9</a>101112<a href="http://www.cnblogs.com/forfuture1978/archive/2010/05/13.html" target="_blank">13</a>1415<a href="http://www.cnblogs.com/forfuture1978/archive/2010/05/16.html" target="_blank">16</a>1718<a href="http://www.cnblogs.com/forfuture1978/archive/2010/05/19.html" target="_blank">19</a>2021<a href="http://www.cnblogs.com/forfuture1978/archive/2010/05/22.html" target="_blank">22</a>23<a href="http://www.cnblogs.com/forfuture1978/archive/2010/05/24.html" target="_blank">24</a><a href="http://www.cnblogs.com/forfuture1978/archive/2010/05/25.html" target="_blank">25</a><a href="http://www.cnblogs.com/forfuture1978/archive/2010/05/26.html" target="_blank">26</a><a href="http://www.cnblogs.com/forfuture1978/archive/2010/05/27.html" target="_blank">27</a>2829303112345</p>
<h3 id="-">公告</h3>
<p>昵称：<a href="http://home.cnblogs.com/u/forfuture1978/" target="_blank">觉先</a>
园龄：<a href="http://home.cnblogs.com/u/forfuture1978/" title="入园时间：2009-12-10" target="_blank">3年7个月</a>
荣誉：<a href="http://www.cnblogs.com/expert/" target="_blank">推荐博客</a>
粉丝：<a href="http://home.cnblogs.com/u/forfuture1978/followers/" target="_blank">560</a>
关注：<a href="http://home.cnblogs.com/u/forfuture1978/followees/" target="_blank">3</a></p>
<p><a href="">+加关注</a></p>
<h3 id="-">搜索</h3>
<h3 id="-">常用链接</h3>
<ul>
<li><a href="http://www.cnblogs.com/forfuture1978/MyPosts.html" target="_blank">我的随笔</a></li>
<li><a href="http://www.cnblogs.com/forfuture1978/MyComments.html" target="_blank">我的评论</a></li>
<li><a href="http://www.cnblogs.com/forfuture1978/OtherPosts.html" title="我发表过评论的随笔" target="_blank">我的参与</a></li>
<li><a href="http://www.cnblogs.com/forfuture1978/RecentComments.html" target="_blank">最新评论</a></li>
<li><a href="http://www.cnblogs.com/forfuture1978/tag/" target="_blank">我的标签</a></li>
</ul>
<h3 id="-">随笔分类</h3>
<ul>
<li><a href="http://www.cnblogs.com/forfuture1978/category/300670.html" target="_blank">Hadoop原理与代码分析(7)</a></li>
<li><a href="http://www.cnblogs.com/forfuture1978/category/300669.html" target="_blank">IT外企那点儿事(12)</a></li>
<li><a href="http://www.cnblogs.com/forfuture1978/category/345798.html" target="_blank">Java(2)</a></li>
<li><a href="http://www.cnblogs.com/forfuture1978/category/345797.html" target="_blank">Linux(14)</a></li>
<li><a href="http://www.cnblogs.com/forfuture1978/category/300665.html" target="_blank">Lucene原理与代码分析(38)</a></li>
<li><a href="http://www.cnblogs.com/forfuture1978/category/300666.html" target="_blank">长尾理论(16)</a></li>
<li><a href="http://www.cnblogs.com/forfuture1978/category/345794.html" target="_blank">管理学(10)</a></li>
<li><a href="http://www.cnblogs.com/forfuture1978/category/345800.html" target="_blank">经济学(4)</a></li>
<li><a href="http://www.cnblogs.com/forfuture1978/category/345796.html" target="_blank">算法(1)</a></li>
<li><a href="http://www.cnblogs.com/forfuture1978/category/345795.html" target="_blank">闲话IT业(3)</a></li>
<li><a href="http://www.cnblogs.com/forfuture1978/category/300667.html" target="_blank">心理学与管理学效应(9)</a></li>
<li><a href="http://www.cnblogs.com/forfuture1978/category/300668.html" target="_blank">组织行为学(15)</a></li>
</ul>
<h3 id="-">随笔档案</h3>
<ul>
<li><a href="http://www.cnblogs.com/forfuture1978/archive/2012/11.html" target="_blank">2012年11月 (3)</a></li>
<li><a href="http://www.cnblogs.com/forfuture1978/archive/2012/01.html" target="_blank">2012年1月 (5)</a></li>
<li><a href="http://www.cnblogs.com/forfuture1978/archive/2011/12.html" target="_blank">2011年12月 (6)</a></li>
<li><a href="http://www.cnblogs.com/forfuture1978/archive/2011/10.html" target="_blank">2011年10月 (3)</a></li>
<li><a href="http://www.cnblogs.com/forfuture1978/archive/2011/09.html" target="_blank">2011年9月 (1)</a></li>
<li><a href="http://www.cnblogs.com/forfuture1978/archive/2010/11.html" target="_blank">2010年11月 (8)</a></li>
<li><a href="http://www.cnblogs.com/forfuture1978/archive/2010/10.html" target="_blank">2010年10月 (1)</a></li>
<li><a href="http://www.cnblogs.com/forfuture1978/archive/2010/09.html" target="_blank">2010年9月 (1)</a></li>
<li><a href="http://www.cnblogs.com/forfuture1978/archive/2010/08.html" target="_blank">2010年8月 (1)</a></li>
<li><a href="http://www.cnblogs.com/forfuture1978/archive/2010/07.html" target="_blank">2010年7月 (1)</a></li>
<li><a href="http://www.cnblogs.com/forfuture1978/archive/2010/06.html" target="_blank">2010年6月 (6)</a></li>
<li><a href="http://www.cnblogs.com/forfuture1978/archive/2010/05.html" target="_blank">2010年5月 (22)</a></li>
<li><a href="http://www.cnblogs.com/forfuture1978/archive/2010/04.html" target="_blank">2010年4月 (18)</a></li>
<li><a href="http://www.cnblogs.com/forfuture1978/archive/2010/03.html" target="_blank">2010年3月 (8)</a></li>
<li><a href="http://www.cnblogs.com/forfuture1978/archive/2010/02.html" target="_blank">2010年2月 (39)</a></li>
<li><a href="http://www.cnblogs.com/forfuture1978/archive/2010/01.html" target="_blank">2010年1月 (1)</a></li>
<li><a href="http://www.cnblogs.com/forfuture1978/archive/2009/12.html" target="_blank">2009年12月 (6)</a></li>
</ul>
<h3 id="-">相册</h3>
<ul>
<li><a href="http://www.cnblogs.com/forfuture1978/gallery/247104.html" target="_blank">IT外企那点儿事</a></li>
</ul>
<h3 id="-">最新评论</h3>
<ul>
<li><a href="http://www.cnblogs.com/forfuture1978/archive/2012/11/26/2788610.html#2727561" target="_blank">1. Re:IT外企那点儿事(12)：也说跳槽</a></li>
<li>楼主怎么之后没有更新hadoop的相关信息了呢？是没有再研究了吗？</li>
<li>--lyeoswu</li>
<li><a href="http://www.cnblogs.com/forfuture1978/archive/2010/06/13/1757479.html#2713121" target="_blank">2. Re:Lucene 原理与代码分析完整版</a></li>
<li>提个建议，你生成的pdf中没有目录，影响阅读，用office转制的过程中其实设置一下即可，方便大众嘛~，还望能发我一份，谢谢！
sendreams@hotmail.com</li>
<li>--sendreams</li>
<li><a href="http://www.cnblogs.com/forfuture1978/archive/2012/11/26/2788610.html#2712415" target="_blank">3. Re:IT外企那点儿事(12)：也说跳槽</a></li>
<li><a href="&quot;查看所回复的评论&quot;">@</a>mojunbin
现在这公司，本来做的Siverlight，我进去后没多久就转JAVA了，最近在公司折腾JAVA的一些东西，业余时间玩玩游戏，看看CLR、并折腾linux。现在观点有所转变，觉得学技术更多的是为了扩宽思维、提高眼界</li>
<li>--峰顶飞龙</li>
<li><a href="http://www.cnblogs.com/forfuture1978/archive/2012/11/26/2788610.html#2711923" target="_blank">4. Re:IT外企那点儿事(12)：也说跳槽</a></li>
<li><a href="&quot;查看所回复的评论&quot;">@</a>峰顶飞龙
您的经历和我差不多，呵呵。不晓得现在兄弟在搞C/C++呢？</li>
<li>--mojunbin</li>
<li><a href="http://www.cnblogs.com/forfuture1978/archive/2010/11/23/1884967.html#2708814" target="_blank">5. Re:Hadoop学习总结之五：Hadoop的运行痕迹</a></li>
<li>楼主你好，在远程调试MapReduce时，本地代码进入不了自定义的job类，而是进入到Credentials class中，此类在hadoop-core-1.0.4.jar中，请问楼主在调试过程可否遇到此问题？</li>
<li>--彭莉珊</li>
</ul>
<h3 id="-">阅读排行榜</h3>
<ul>
<li><a href="http://www.cnblogs.com/forfuture1978/archive/2010/08/04/1791660.html" target="_blank">1. IT外企那点儿事(8)：又是一年加薪时(26798)</a></li>
<li><a href="http://www.cnblogs.com/forfuture1978/archive/2010/06/13/1757479.html" target="_blank">2. Lucene 原理与代码分析完整版(25615)</a></li>
<li><a href="http://www.cnblogs.com/forfuture1978/archive/2010/02/23/1671909.html" target="_blank">3. 从技术生命周期看IT历史(20877)</a></li>
<li><a href="http://www.cnblogs.com/forfuture1978/archive/2009/12/21/1628546.html" target="_blank">4. 101个著名的管理学及心理学效应(20827)</a></li>
<li><a href="http://www.cnblogs.com/forfuture1978/archive/2010/11/14/1877086.html" target="_blank">5. Hadoop学习总结之三：Map-Reduce入门(18674)</a></li>
</ul>
<h3 id="-">评论排行榜</h3>
<ul>
<li><a href="http://www.cnblogs.com/forfuture1978/archive/2010/06/13/1757479.html" target="_blank">1. Lucene 原理与代码分析完整版(68)</a></li>
<li><a href="http://www.cnblogs.com/forfuture1978/archive/2010/05/05/1727644.html" target="_blank">2. IT外企那点儿事(4)：激动人心的入职演讲(39)</a></li>
<li><a href="http://www.cnblogs.com/forfuture1978/archive/2010/05/13/1734162.html" target="_blank">3. IT外企那点儿事(6)：管理路线和技术路线(37)</a></li>
<li><a href="http://www.cnblogs.com/forfuture1978/archive/2010/08/04/1791660.html" target="_blank">4. IT外企那点儿事(8)：又是一年加薪时(35)</a></li>
<li><a href="http://www.cnblogs.com/forfuture1978/archive/2012/11/26/2788610.html" target="_blank">5. IT外企那点儿事(12)：也说跳槽(33)</a></li>
</ul>
<h3 id="-">推荐排行榜</h3>
<ul>
<li><a href="http://www.cnblogs.com/forfuture1978/archive/2010/06/13/1757479.html" target="_blank">1. Lucene 原理与代码分析完整版(55)</a></li>
<li><a href="http://www.cnblogs.com/forfuture1978/archive/2010/05/03/1726200.html" target="_blank">2. IT外企那点儿事(3)：奇怪的面试(39)</a></li>
<li><a href="http://www.cnblogs.com/forfuture1978/archive/2010/08/04/1791660.html" target="_blank">3. IT外企那点儿事(8)：又是一年加薪时(36)</a></li>
<li><a href="http://www.cnblogs.com/forfuture1978/archive/2012/11/26/2788610.html" target="_blank">4. IT外企那点儿事(12)：也说跳槽(34)</a></li>
<li><a href="http://www.cnblogs.com/forfuture1978/archive/2010/05/13/1734162.html" target="_blank">5. IT外企那点儿事(6)：管理路线和技术路线(27)</a></li>
</ul>
<p><a href="http://www.cnblogs.com/forfuture1978/archive/2010/05/03/1726200.html" target="_blank">IT外企那点儿事(3)：奇怪的面试</a></p>
<p>外企的面试都面写啥？不同的企业也是不一样的，总的来说可以归结为以下几句话：</p>
<h2 id="-">三类企业面实战，二类企业面基础，一类企业面算法。</h2>
<p>在此声明，此处所谓的一二三类，绝没有轻视其他企业的意思，这里的一二三类基本上是按照本科毕业的时候起薪来划分的，一类企业指的是年薪15万以上的企业，二类企业指的是年薪10万左右的企业，三类企业指的是年薪5万左右的企业。当然按照上两次的描述大家可以知道，并不是起薪高的企业的程序员一定最好发展的最好，而进入创业企业的人最后可能后来居上，成为IT达人。当然此规律也不仅仅适用于外企。</p>
<h2 id="-">三类企业</h2>
<p>三类企业起薪不高，招聘的目的也相对的明确，是要找那种来了就能真枪实弹的把东西作出来的人。</p>
<p>他们多不太关心员工的培训和成长，不太关心员工是否对技术有浓厚的兴趣和深入的钻研，他们就是一个想法，他们要做一个东西，做这个东西需要某方面的技术，所以要找这会方面的人。</p>
<p>他们不知道，大多数的程序员其实喜欢做一些在自己能力以上20%的东西，也即研究研究可以做出来，但不是太熟练，而不喜欢做一些自己已经非常熟，毫无挑战的东西。</p>
<p>但是他们需要这样的人，所以在面试中，面试的问题比较具体，甚至具体到一个个的配置项，也有当场给你环境，让你搭一个框架，做一个东西的。</p>
<p>他们希望，最好你以前做过的项目和他们现在的项目十分相似，来了就能够上手。</p>
<p>其实很多程序员跳槽，就是因为原来的工作已经没有了挑战，想找一个更有挑战的，有更多大牛的地方，如果原来的项目我干的不亦乐乎，还来你这里干什么？</p>
<p>但是现在工作难找啊，所以他们总是能够找到需要的人，毕竟出来混，大家都是混口饭吃，不容易啊。</p>
<p>要想进入此类企业，一个最好的办法就是上手做，在学校里就可以找个实习的公司，哪怕不给钱也去(强烈谴责这种企业，剥夺劳动者的基本权利，也就在中国他们能干的出来，放到欧美罚不死他们)，先混些实践经验，做些边角料的活，然后跟着lead一步一步进入核心模块，相信只要认认真真的做过，面过这类企业应该不成问题。</p>
<p>此类企业的流动性相对较大，往往被用作程序员的跳板，跳到二类甚至一类的企业中去。所以不幸进入此类企业的兄弟们，在实战的过程中，别忘了多看看源码，多想想背后的原理，多补充一下计算机科学的基本知识，早日脱离苦海。</p>
<h2 id="-">二类企业</h2>
<p>二类企业其实薪水已经非常不错了，毕业就能进入此类企业的程序员也多是学校中的优秀分子。</p>
<p>此类企业注重程序员的基础，认为只要基础好，他们愿意培训并培养程序员，给你机会进行学习。</p>
<p>此类企业招聘的时候，职位有可能是不太确定的，可能是Java，可能是C++，可能是windows，可能是Linux，他们认为只要你基础好，语言不是问题，平台不是问题，培训一下上手会很快。</p>
<p>记得面试一家与通信有关的欧企，面试官开始问了很多C/C++的基础知识，后来问了很多操作系统和计算机网络的基础知识，最后说，他们是需要有通信背景的，然后连问我三个有关通信方面的问题，我都说不知道，最后只有坦然承认，通信我确实一点都不懂。后来我认为我是彻底没希望了，没想到后来竟收到了他们的offer，并在入职后进行了长达两个月的通信方面的培训，后来我问我的面试官怎么回事，他说，你的C/C++，操作系统，计算机网络的面试题几乎都对了，我觉得你的基础不错。</p>
<p>所以要进入此类的企业，有关基础方面的书还是要认认真真，仔仔细细的看，下面推荐一部分：</p>
<ul>
<li>C： 《The c programming langage》</li>
<li>C++：《Thinking in C++》，《The c++ programming language》，《effective c++》，《more effective c++》，《exceptional c++》，《more exceptional c++》，《inside the c++ object model》</li>
<li>Java：《Thinking in java》，《Core Java》，《effective java》，《Java Puzzlers》，《Java Network Programming》，《java concurrency in practice》，《深入Java虚拟机》</li>
<li>windows：《Windows核心编程》，《Windows Internals》</li>
<li>linux：《Advanced Programming in the UNIX.Environment》，《Understanding Linux Network Internals》，《UNIX Network Programming》</li>
<li>network：《TCPIP Illustrated Volume I》，《The Linux Networking Architecture》</li>
</ul>
<p>我没有在装B，也不是看过以上所有的书，不过上述书籍的确是程序员必藏书，我也只不过是在用到的时候翻开相关章节看看。</p>
<p>然而给大家的建议是，在做项目的时候，千万不能够做什么就只知道什么，与此相关基础知识也应该多看一些。面试的时候也经常遇到这种情况，就是面试者号称做过socket，问到tcp/ip拥塞控制却一无所知，会简单使用socket client端和server端几个简单函数人太多了，如何保证你能够脱颖而出呢？</p>
<p>其实很多事情我们觉得不可能，但是这个世界上就是有牛人确实做到了，比如英语六级能够考99分(满分100)，就是把答案全给我，就让我写作文，我也做不到啊，再如高考满分750分，山东的状元730+分，也就意味着数理化全对，语文140+，英语140+，我的天，也是把答案给我，就让我写语文和英语的作文，我也做不到啊。</p>
<p>然而读以上书籍却没有上面两个例子难的不可想象，我所知道的身边的人就有C, C++, linux, network这几个分支全读过的，而且不止一个。</p>
<p>能进入二类的企业，混个中层，也能过上满不错的生活了。</p>
<h2 id="-">一类企业</h2>
<p>一类企业薪水非常高，毕业就能进入的可以说是学校中的佼佼者了，一般会名校背景，名企实习，甚至有过获奖的才能够进入。</p>
<p>此类企业除了注重程序员的基础之外，更加重视程序员的思想，算法及聪明程度。</p>
<p>所以很多奇奇怪怪的面试题在网上都流传出来了，这些题目真可谓费尽心机。面试过程长达n轮，每轮都可能因为疏漏和状态不佳被刷掉，最后剩下的几近完美。</p>
<p>在面试中，程序是要当场在黑板上写出来的，很短的时间，要求很强的健壮性，面试官还会在旁边施加心理压力，你确定吗？要注意XXX。</p>
<p>虽然问题是经常外流的，然而新的问题却是不断的会出，可能是因为工作中有些需要解决的问题，自己想了一天多才想出的解决方案，却抽象出来考别人，让别人在很短的时间作出来，这种心理开始很爽，后来觉得很罪恶，多少有些原来自己穷，受富人欺负，后来富了又欺负穷人的味道。</p>
<p>有些人会质疑，这些精巧的算法在工作中真的能够用到很多吗？答案当然不是。</p>
<p>这其实是一个供需的问题。马克思告诉我们，商品的价格是由价值量决定的，商品应该以价值量为基础，实行等价交换。西方经济学告诉我们商品的价格会随着供需关系的变化而变化。当供需矛盾相当大的时候，商品的价格就会远离价值量。</p>
<p>《经济学的思维方式》一书中写到，所有的稀缺品都需要以某种方式分配，必须建立某种规则和制度，对那些要求得到稀缺品的人加以甄别，决定谁该得到多少。价格只是最常用的一种方式。</p>
<p>想想我们的高考吧，那些千辛万苦考上清华的学子毕业后又有多少高中的知识留在脑子里呢？学到的东西又有多少是能够在实际中用到的呢？其实很少，高考分数不过是进入清华的一个价格而已，已经由于清华只有一所，考生却有千百万这样的供需差别远远的偏离了使用价值，毕竟能够轻松看懂教科书的人太多了，他们只能够不但要全会，还要全对。</p>
<p>进入一类企业也是同样的，能把我上述书籍都看完的人是大有人在的，仅仅基础知识已经不能够甄别想进入一类企业的人们，所以需要奇奇怪怪的算法题。</p>
<p>要进入一类企业，《算法导论》这本书必不可少，要前前后后仔细的看，而且应该不止一遍。《编程珠玑》也是一本不错的书，其中的例子可以常常的回味。《编程之美》也不错，更贴近面试，更实用一些。其实更重要的是Top coder，就是多看多练。</p>
<p>其实考入名校基本就是一种方法，多做题，以便在考场中看到题目就能够有思路，考场的时间仅仅用于保证正确率就可以了。</p>
<p>进入一类企业也是一样，要想很短的时间，在很大的压力下写出健壮的程序，其实只有一种方法，就是类似的题目遇到过，思路是马上就有的，在会议室的时间仅仅用于保证健壮性就可以了。</p>
<p>曾经一段时间，对精巧的算法十分的崇尚，甚至引以为豪，然而后来慢慢发现，天天沉浸在算法之中，沉浸在计算机的小天地里面，又对社会做了什么贡献呢？难道自己的才能，抱负就仅仅放在这些数字的技巧当中吗？</p>
<p>我们不应该像孔乙己一样研究茴香豆有几种写法，而是应该如阿朱《走出软件作坊》中描述的一样，虽然方案不是完美和精巧，然而逢山开路，遇水搭桥，真正的解决一个个的问题，作出一些可以影响人们生活的软件。</p>
<p>先写到这里，下一章要开始写入职了。</p>
<p>分类: <a href="http://www.cnblogs.com/forfuture1978/category/300669.html" target="_blank">IT外企那点儿事</a></p>
<p>绿色通道： <a href="">好文要顶</a> <a href="">关注我</a> <a href="">收藏该文</a><a href="http://space.cnblogs.com/msg/send/%e8%a7%89%e5%85%88" target="_blank">与我联系</a> <a href="&quot;分享至新浪微博&quot;"><img src="" alt=""></a>
<a href="http://home.cnblogs.com/u/forfuture1978/" target="_blank"><img src="" alt=""></a></p>
<p><a href="http://home.cnblogs.com/u/forfuture1978/" target="_blank">觉先</a>
<a href="http://home.cnblogs.com/u/forfuture1978/followees" target="_blank">关注 - 3</a>
<a href="http://home.cnblogs.com/u/forfuture1978/followers" target="_blank">粉丝 - 560</a></p>
<p>荣誉：<a href="http://www.cnblogs.com/expert/" target="_blank">推荐博客</a>
<a href="">+加关注</a></p>
<p>39</p>
<p>0
(请您对文章做出评价)</p>
<p><a href="http://www.cnblogs.com/forfuture1978/archive/2010/05/02/1725948.html" target="_blank">«</a> 上一篇：<a href="http://www.cnblogs.com/forfuture1978/archive/2010/05/02/1725948.html" title="发布于2010-05-02 12:42" target="_blank">IT外企那点儿事(2)：多种多样的外企</a>
<a href="http://www.cnblogs.com/forfuture1978/archive/2010/05/05/1727644.html" target="_blank">»</a> 下一篇：<a href="http://www.cnblogs.com/forfuture1978/archive/2010/05/05/1727644.html" title="发布于2010-05-05 02:35" target="_blank">IT外企那点儿事(4)：激动人心的入职演讲</a>
posted on 2010-05-03 01:05 <a href="http://www.cnblogs.com/forfuture1978/" target="_blank">觉先</a> 阅读(14392) 评论(24) <a href="http://www.cnblogs.com/forfuture1978/admin/EditPosts.aspx?postid=1726200" target="_blank">编辑</a> <a href="">收藏</a></p>
<p><a href=""></a></p>
<h3 id="-">评论</h3>
<p><a href="">/#1楼</a><a href=""></a>  2010-05-03 09:12  <a href="http://www.cnblogs.com/rupeng/">杨中科</a> <a href="http://space.cnblogs.com/msg/send/%e6%9d%a8%e4%b8%ad%e7%a7%91" title="发送站内短消息" target="_blank"> </a></p>
<p>这么好的文章怎么上不了主页？</p>
<p><a href="">支持(0)</a><a href="">反对(0)</a>
<a href="http://pic.cnitblog.com/face/u130406.jpg" target="_blank">http://pic.cnitblog.com/face/u130406.jpg</a></p>
<p><a href="">/#2楼</a><a href=""></a>  2010-05-03 09:34  <a href="http://www.cnblogs.com/s021368/">阿龍</a> <a href="http://space.cnblogs.com/msg/send/%e9%98%bf%e9%be%8d" title="发送站内短消息" target="_blank"> </a></p>
<p>很多单位都把程序员当成了挣钱的工具，培训，门都没有。就连基本的系统里面的很多设计文档都不给你看，作为公司机密。</p>
<p><a href="">支持(0)</a><a href="">反对(0)</a>
<a href="http://pic.cnitblog.com/face/u23974.jpg" target="_blank">http://pic.cnitblog.com/face/u23974.jpg</a></p>
<p><a href="">/#3楼</a><a href=""></a>[楼主]  2010-05-03 10:50  <a href="http://www.cnblogs.com/forfuture1978/">觉先</a> <a href="http://space.cnblogs.com/msg/send/%e8%a7%89%e5%85%88" title="发送站内短消息" target="_blank"> </a></p>
<p><a href="&quot;查看所回复的评论&quot;">@</a>杨中科
你好，不知道为什么，我是在windows live writer里面写的，提交上来后，原来再编辑的话可以选择发布在精华区，现在只能选择发布在候选区了。
另一点，此类的文章在博客园容易被拍啊，不是技术类文章，现在都不敢发在首页了。</p>
<p><a href="">支持(0)</a><a href="">反对(0)</a>
<a href="http://pic.cnitblog.com/face/u103165.jpg" target="_blank">http://pic.cnitblog.com/face/u103165.jpg</a></p>
<p><a href="">/#4楼</a><a href=""></a>  2010-05-03 13:05  <a href="http://www.cnblogs.com/ilovedotnet/">ilovedotnet</a> <a href="http://space.cnblogs.com/msg/send/ilovedotnet" title="发送站内短消息" target="_blank"> </a></p>
<p>不错，排版大有进步啊！</p>
<p><a href="">支持(0)</a><a href="">反对(0)</a>
<a href="http://pic.cnitblog.com/face/u26921.jpg" target="_blank">http://pic.cnitblog.com/face/u26921.jpg</a></p>
<p><a href="">/#5楼</a><a href=""></a>  2010-05-03 23:16  <a href="http://www.cnblogs.com/isyd/">依落の守候</a> <a href="http://space.cnblogs.com/msg/send/%e4%be%9d%e8%90%bd%e3%81%ae%e5%ae%88%e5%80%99" title="发送站内短消息" target="_blank"> </a></p>
<p><a href="&quot;查看所回复的评论&quot;">@</a>杨中科
居然在这看到如鹏的老大，幸会幸会。</p>
<p><a href="">支持(0)</a><a href="">反对(0)</a>
<a href="http://pic.cnitblog.com/face/u102636.jpg" target="_blank">http://pic.cnitblog.com/face/u102636.jpg</a></p>
<p><a href="">/#6楼</a><a href=""></a>  2010-05-03 23:17  <a href="http://www.cnblogs.com/isyd/">依落の守候</a> <a href="http://space.cnblogs.com/msg/send/%e4%be%9d%e8%90%bd%e3%81%ae%e5%ae%88%e5%80%99" title="发送站内短消息" target="_blank"> </a></p>
<p>好文章。至少是能改变人观念的文章。谢谢lz了。</p>
<p><a href="">支持(0)</a><a href="">反对(0)</a>
<a href="http://pic.cnitblog.com/face/u102636.jpg" target="_blank">http://pic.cnitblog.com/face/u102636.jpg</a></p>
<p><a href="">/#7楼</a><a href=""></a>  2010-05-04 00:32  <a href="http://home.cnblogs.com/u/122265/">琛</a> <a href="http://space.cnblogs.com/msg/send/%e7%90%9b" title="发送站内短消息" target="_blank"> </a></p>
<p>对我的目标信念更加的坚定
谢谢</p>
<p><a href="">支持(0)</a><a href="">反对(0)</a></p>
<p><a href="">/#8楼</a><a href=""></a>  2010-05-04 09:06  <a href="http://www.cnblogs.com/Eson/">ESON</a> <a href="http://space.cnblogs.com/msg/send/ESON" title="发送站内短消息" target="_blank"> </a></p>
<p>挺好，不过我这人不太喜欢看书，应该找个时间挑战一下自己。</p>
<p><a href="">支持(0)</a><a href="">反对(0)</a>
<a href="http://pic.cnitblog.com/face/u109470.jpg?id=02124325" target="_blank">http://pic.cnitblog.com/face/u109470.jpg?id=02124325</a></p>
<p><a href="">/#9楼</a><a href=""></a>  2010-05-04 13:36  <a href="http://www.cnblogs.com/yanshaoli/">JacksonChina</a> <a href="http://space.cnblogs.com/msg/send/JacksonChina" title="发送站内短消息" target="_blank"> </a></p>
<p>呵呵。LZ可以直接发到首页啊。
说得很实在，不是通篇大道理，大部分人还是认可的。
非技术类没关系啊。毕竟园子很多人排斥的不是这类型
的文章，而是某个人的风格。</p>
<p><a href="">支持(0)</a><a href="">反对(0)</a></p>
<p><a href="">/#10楼</a><a href=""></a>  2010-05-05 13:09  <a href="http://www.cnblogs.com/dooom/">clound</a> <a href="http://space.cnblogs.com/msg/send/clound" title="发送站内短消息" target="_blank"> </a></p>
<p>这个系列不错 ，好文章。
LZ也很谦虚，刚看了评论，都没发到首页。</p>
<p><a href="">支持(0)</a><a href="">反对(0)</a></p>
<p><a href="">/#11楼</a><a href=""></a>  2010-05-05 13:49  <a href="http://home.cnblogs.com/u/72808/">hechaner</a> <a href="http://space.cnblogs.com/msg/send/hechaner" title="发送站内短消息" target="_blank"> </a></p>
<p><a href="&quot;查看引用原文&quot;">引用</a>觉先：
@杨中科
你好，不知道为什么，我是在windows live writer里面写的，提交上来后，原来再编辑的话可以选择发布在精华区，现在只能选择发布在候选区了。
另一点，此类的文章在博客园容易被拍啊，不是技术类文章，现在都不敢发在首页了。
不是非技术的文章就会被拍砖，我觉得写的很中肯，对大家也有用，帮楼主推荐了</p>
<p><a href="">支持(0)</a><a href="">反对(0)</a></p>
<p><a href="">/#12楼</a><a href=""></a>[楼主]  2010-05-05 14:39  <a href="http://www.cnblogs.com/forfuture1978/">觉先</a> <a href="http://space.cnblogs.com/msg/send/%e8%a7%89%e5%85%88" title="发送站内短消息" target="_blank"> </a></p>
<p><a href="&quot;查看所回复的评论&quot;">@</a>hechaner
非常感谢啊</p>
<p><a href="">支持(0)</a><a href="">反对(0)</a>
<a href="http://pic.cnitblog.com/face/u103165.jpg" target="_blank">http://pic.cnitblog.com/face/u103165.jpg</a></p>
<p><a href="">/#13楼</a><a href=""></a>  2010-05-05 15:21  <a href="http://www.cnblogs.com/lonely_rain/">lonely_rain</a> <a href="http://space.cnblogs.com/msg/send/lonely_rain" title="发送站内短消息" target="_blank"> </a></p>
<p>顶一下。</p>
<p><a href="">支持(0)</a><a href="">反对(0)</a>
<a href="http://pic.cnitblog.com/face/u105358.jpg?id=24155351" target="_blank">http://pic.cnitblog.com/face/u105358.jpg?id=24155351</a></p>
<p><a href="">/#14楼</a><a href=""></a>  2010-05-05 16:03  <a href="http://www.cnblogs.com/aqhistory/">blackcat</a> <a href="http://space.cnblogs.com/msg/send/blackcat" title="发送站内短消息" target="_blank"> </a></p>
<p>还好。呵呵。不错。
非常生动。
有助于职业规划。</p>
<p><a href="">支持(0)</a><a href="">反对(0)</a>
<a href="http://pic.cnitblog.com/face/u106668.jpg" target="_blank">http://pic.cnitblog.com/face/u106668.jpg</a></p>
<p><a href="">/#15楼</a><a href=""></a>  2010-05-05 21:55  <a href="http://www.cnblogs.com/Jong/">Caspar Jiong</a> <a href="http://space.cnblogs.com/msg/send/Caspar+Jiong" title="发送站内短消息" target="_blank"> </a></p>
<p>好文！</p>
<p><a href="">支持(0)</a><a href="">反对(0)</a></p>
<p><a href="">/#16楼</a><a href=""></a>  2010-05-06 08:59  <a href="http://www.cnblogs.com/elwin/">elwin.wang</a> <a href="http://space.cnblogs.com/msg/send/elwin.wang" title="发送站内短消息" target="_blank"> </a></p>
<p>LZ,我对你的佩服如滔滔江水，连绵不绝啊</p>
<p><a href="">支持(0)</a><a href="">反对(0)</a></p>
<p><a href="">/#17楼</a><a href=""></a>  2010-05-12 00:48  <a href="http://home.cnblogs.com/u/132441/">学飞的菜鸟</a> <a href="http://space.cnblogs.com/msg/send/%e5%ad%a6%e9%a3%9e%e7%9a%84%e8%8f%9c%e9%b8%9f" title="发送站内短消息" target="_blank"> </a></p>
<p>嗯，说得很有道理啊。博客园里真是高手如云啊，以后得多来拜会才是。</p>
<p><a href="">支持(0)</a><a href="">反对(0)</a></p>
<p><a href="">/#18楼</a><a href=""></a>  2010-05-13 09:55  <a href="http://www.cnblogs.com/TomToDo/">Yes!加菲猫</a> <a href="http://space.cnblogs.com/msg/send/Yes!%e5%8a%a0%e8%8f%b2%e7%8c%ab" title="发送站内短消息" target="_blank"> </a></p>
<p>这篇标记下，虽然不进外企，但是文章提到的东西还是要多留意</p>
<p><a href="">支持(0)</a><a href="">反对(0)</a>
<a href="http://pic.cnitblog.com/face/u33631.jpg" target="_blank">http://pic.cnitblog.com/face/u33631.jpg</a></p>
<p><a href="">/#19楼</a><a href=""></a>  2010-05-13 16:11  <a href="http://www.cnblogs.com/shanjsh/">坐井观天</a> <a href="http://space.cnblogs.com/msg/send/%e5%9d%90%e4%ba%95%e8%a7%82%e5%a4%a9" title="发送站内短消息" target="_blank"> </a></p>
<p>这文章要顶</p>
<p><a href="">支持(0)</a><a href="">反对(0)</a></p>
<p><a href="">/#20楼</a><a href=""></a>  2010-05-13 22:51  <a href="http://www.cnblogs.com/likwo/">Likwo</a> <a href="http://space.cnblogs.com/msg/send/Likwo" title="发送站内短消息" target="_blank"> </a></p>
<p>文章非常好，辛苦啦</p>
<p><a href="">支持(0)</a><a href="">反对(0)</a></p>
<p><a href="">/#21楼</a><a href=""></a>  2010-05-14 10:30  <a href="http://www.cnblogs.com/jianjialin/">MyCoolDog</a> <a href="http://space.cnblogs.com/msg/send/MyCoolDog" title="发送站内短消息" target="_blank"> </a></p>
<p>ding!!</p>
<p><a href="">支持(0)</a><a href="">反对(0)</a>
<a href="http://pic.cnitblog.com/face/u32601.jpg" target="_blank">http://pic.cnitblog.com/face/u32601.jpg</a></p>
<p><a href="">/#22楼</a><a href=""></a>  2012-11-27 23:40  <a href="http://www.cnblogs.com/pinopino/">我不是AI</a> <a href="http://space.cnblogs.com/msg/send/%e6%88%91%e4%b8%8d%e6%98%afAI" title="发送站内短消息" target="_blank"> </a></p>
<p>请问, 你说的Top coder是这个么: <a href="http://www.topcoder.com/?" target="_blank"><a href="http://www.topcoder.com?">http://www.topcoder.com?</a></a></p>
<p><a href="">支持(0)</a><a href="">反对(0)</a>
<a href="http://pic.cnitblog.com/face/u271595.jpg?id=24154145" target="_blank">http://pic.cnitblog.com/face/u271595.jpg?id=24154145</a></p>
<p><a href="">/#23楼</a><a href=""></a>  2012-12-05 11:10  <a href="http://www.cnblogs.com/huzi007/">全力以赴001</a> <a href="http://space.cnblogs.com/msg/send/%e5%85%a8%e5%8a%9b%e4%bb%a5%e8%b5%b4001" title="发送站内短消息" target="_blank"> </a></p>
<p>给人感悟的....真给力!</p>
<p><a href="">支持(0)</a><a href="">反对(0)</a></p>
<p><a href="">/#24楼</a><a href=""></a>25739772012/12/10 10:17:29  2012-12-10 10:17  <a href="http://home.cnblogs.com/u/476653/">Mamamiya</a> <a href="http://space.cnblogs.com/msg/send/Mamamiya" title="发送站内短消息" target="_blank"> </a></p>
<p>博主的文章很能发人深省。收益良多。</p>
<p><a href="">支持(0)</a><a href="">反对(0)</a></p>
<p><a href="">刷新评论</a><a href="">刷新页面</a><a href="">返回顶部</a></p>
<p>注册用户登录后才能发表评论，请 <a href="">登录</a> 或 <a href="">注册</a>，<a href="http://www.cnblogs.com/" target="_blank">访问</a>网站首页。
<a href="http://www.cnblogs.com/" title="程序员的网上家园" target="_blank">博客园首页</a><a href="http://q.cnblogs.com/" title="程序员问答社区" target="_blank">博问</a><a href="http://news.cnblogs.com/" title="IT新闻" target="_blank">新闻</a><a href="http://home.cnblogs.com/ing/" target="_blank">闪存</a><a href="http://job.cnblogs.com/" target="_blank">程序员招聘</a><a href="http://kb.cnblogs.com/" target="_blank">知识库</a></p>
<p><strong>最新IT新闻</strong>:
· <a href="http://news.cnblogs.com/n/182456/" target="_blank">Facebook推出一体化数据中心管理软件</a>
· <a href="http://news.cnblogs.com/n/182455/" target="_blank">顺丰掌门人王卫：马云最佩服的人</a>
· <a href="http://news.cnblogs.com/n/182454/" target="_blank">2013年美国十大最佳科技雇主</a>
· <a href="http://news.cnblogs.com/n/182453/" target="_blank">这四年 Google中国落寂了 刘允尽力了</a>
· <a href="http://news.cnblogs.com/n/182452/" target="_blank">在线课程“慕课”来袭 专家称大学应主动参与</a>
» <a href="http://news.cnblogs.com/" title="IT新闻" target="_blank">更多新闻...</a></p>
<p><strong>最新知识库文章</strong>:
· <a href="http://kb.cnblogs.com/page/141892/" target="_blank">阿里巴巴集团去IOE运动的思考与总结</a>
· <a href="http://kb.cnblogs.com/page/182265/" target="_blank">硅谷归来7点分享：创业者，做你自己</a>
· <a href="http://kb.cnblogs.com/page/182200/" target="_blank">我为什么不能坚持？</a>
· <a href="http://kb.cnblogs.com/page/168725/" target="_blank">成为高效程序员的7个重要习惯</a>
· <a href="http://kb.cnblogs.com/page/182047/" target="_blank">谈谈对BPM的理解</a>
» <a href="http://kb.cnblogs.com/" target="_blank">更多知识库文章...</a>
Powered by:
<a href="http://www.cnblogs.com/" target="_blank">博客园</a>
Copyright © 觉先</p>

      
    </div>
    
    
      
    
    
        <footer id="post-meta">
        <span class="categories">Posted in<span class="breadcrumb fa fa-folder"><li><a href="/categories/zhiya/">zhiya</a></li></span><span class="breadcrumb"><li><a href="/categories/职涯/">职涯</a></li><li><a href="/categories/职涯/IT外企/">IT外企</a></li></span></span> | <span class="tags">Tagged <a href="/tags/IT外企/" class="label label-primary">IT外企</a><a href="/tags/zhiya/" class="label label-success">zhiya</a></span> | <span class="time">recent updated:<time title="2014-03-07 09:54:36"datetime="2014-03-07 09:54:36"> mar. 7 2014</time></span> | <span class="comment-link">
<a href="http://itsolife.com/2014/02/02/2014-02-02-zhiya-IT外企--IT外企那点儿事3：奇怪的面试/#comments" class="ds-thread-count comment-link" data-thread-key="2014-02-02-zhiya-IT外企--IT外企那点儿事3：奇怪的面试" data-count-type="comments">暂无评论</a></span>
        </footer>
    
    <div class="clearfix"></div>
  </div>
</div>
</article>



  <article>
<div id="post" class="post well">
  <div class="post-content">
    <header class="well-sm">
      <i class="fa icon fa-5x pull-left"></i>
      <h1 class="title"><a href="/2014/02/02/2014-02-02-zhiya-IT外企--IT外企那点儿事1：外企也就那么回事/">IT外企那点儿事(1)：外企也就那么回事</a></h1>
      
        <span>Posted on<time datetime="2014-02-02T01:54:36.000Z"> <a href="/2014/02/02/2014-02-02-zhiya-IT外企--IT外企那点儿事1：外企也就那么回事/">feb. 2 2014</a></time></span>      
    </header>
    
    
    <div class="entry">
      
        <h1 id="it-1-">IT外企那点儿事(1)：外企也就那么回事</h1>
<p><a href=""></a></p>
<h1 id="-http-www-cnblogs-com-forfuture1978-"><a href="http://www.cnblogs.com/forfuture1978/" target="_blank">觉先</a></h1>
<p>  <a href="http://www.cnblogs.com/" target="_blank">博客园</a> :: <a href="http://www.cnblogs.com/forfuture1978/" target="_blank">首页</a> :: <a href="http://q.cnblogs.com/" target="_blank">博问</a> :: <a href="http://home.cnblogs.com/ing/" target="_blank">闪存</a> :: <a href="http://www.cnblogs.com/forfuture1978/admin/EditPosts.aspx?opt=1" target="_blank">新随笔</a> :: <a href="http://space.cnblogs.com/msg/send/%e8%a7%89%e5%85%88" target="_blank">联系</a> :: <a href="http://www.cnblogs.com/forfuture1978/rss" target="_blank">订阅</a> <a href="http://www.cnblogs.com/forfuture1978/rss" target="_blank"><img src="" alt="订阅"></a> :: <a href="http://www.cnblogs.com/forfuture1978/admin/EditPosts.aspx" target="_blank">管理</a> :: <img src="" alt="">   130 随笔 :: 0 文章 :: 544 评论 :: 0 引用
<a href="">&lt;</a>2010年4月<a href="">&gt;</a>日一二三四五六28293031123<a href="http://www.cnblogs.com/forfuture1978/archive/2010/04/04.html" target="_blank">4</a>56<a href="http://www.cnblogs.com/forfuture1978/archive/2010/04/07.html" target="_blank">7</a><a href="http://www.cnblogs.com/forfuture1978/archive/2010/04/08.html" target="_blank">8</a>9101112<a href="http://www.cnblogs.com/forfuture1978/archive/2010/04/13.html" target="_blank">13</a>14<a href="http://www.cnblogs.com/forfuture1978/archive/2010/04/15.html" target="_blank">15</a>161718192021<a href="http://www.cnblogs.com/forfuture1978/archive/2010/04/22.html" target="_blank">22</a>2324<a href="http://www.cnblogs.com/forfuture1978/archive/2010/04/25.html" target="_blank">25</a>26<a href="http://www.cnblogs.com/forfuture1978/archive/2010/04/27.html" target="_blank">27</a>28<a href="http://www.cnblogs.com/forfuture1978/archive/2010/04/29.html" target="_blank">29</a><a href="http://www.cnblogs.com/forfuture1978/archive/2010/04/30.html" target="_blank">30</a>12345678</p>
<h3 id="-">公告</h3>
<p>昵称：<a href="http://home.cnblogs.com/u/forfuture1978/" target="_blank">觉先</a>
园龄：<a href="http://home.cnblogs.com/u/forfuture1978/" title="入园时间：2009-12-10" target="_blank">3年7个月</a>
荣誉：<a href="http://www.cnblogs.com/expert/" target="_blank">推荐博客</a>
粉丝：<a href="http://home.cnblogs.com/u/forfuture1978/followers/" target="_blank">560</a>
关注：<a href="http://home.cnblogs.com/u/forfuture1978/followees/" target="_blank">3</a></p>
<p><a href="">+加关注</a></p>
<h3 id="-">搜索</h3>
<h3 id="-">常用链接</h3>
<ul>
<li><a href="http://www.cnblogs.com/forfuture1978/MyPosts.html" target="_blank">我的随笔</a></li>
<li><a href="http://www.cnblogs.com/forfuture1978/MyComments.html" target="_blank">我的评论</a></li>
<li><a href="http://www.cnblogs.com/forfuture1978/OtherPosts.html" title="我发表过评论的随笔" target="_blank">我的参与</a></li>
<li><a href="http://www.cnblogs.com/forfuture1978/RecentComments.html" target="_blank">最新评论</a></li>
<li><a href="http://www.cnblogs.com/forfuture1978/tag/" target="_blank">我的标签</a></li>
</ul>
<h3 id="-">随笔分类</h3>
<ul>
<li><a href="http://www.cnblogs.com/forfuture1978/category/300670.html" target="_blank">Hadoop原理与代码分析(7)</a></li>
<li><a href="http://www.cnblogs.com/forfuture1978/category/300669.html" target="_blank">IT外企那点儿事(12)</a></li>
<li><a href="http://www.cnblogs.com/forfuture1978/category/345798.html" target="_blank">Java(2)</a></li>
<li><a href="http://www.cnblogs.com/forfuture1978/category/345797.html" target="_blank">Linux(14)</a></li>
<li><a href="http://www.cnblogs.com/forfuture1978/category/300665.html" target="_blank">Lucene原理与代码分析(38)</a></li>
<li><a href="http://www.cnblogs.com/forfuture1978/category/300666.html" target="_blank">长尾理论(16)</a></li>
<li><a href="http://www.cnblogs.com/forfuture1978/category/345794.html" target="_blank">管理学(10)</a></li>
<li><a href="http://www.cnblogs.com/forfuture1978/category/345800.html" target="_blank">经济学(4)</a></li>
<li><a href="http://www.cnblogs.com/forfuture1978/category/345796.html" target="_blank">算法(1)</a></li>
<li><a href="http://www.cnblogs.com/forfuture1978/category/345795.html" target="_blank">闲话IT业(3)</a></li>
<li><a href="http://www.cnblogs.com/forfuture1978/category/300667.html" target="_blank">心理学与管理学效应(9)</a></li>
<li><a href="http://www.cnblogs.com/forfuture1978/category/300668.html" target="_blank">组织行为学(15)</a></li>
</ul>
<h3 id="-">随笔档案</h3>
<ul>
<li><a href="http://www.cnblogs.com/forfuture1978/archive/2012/11.html" target="_blank">2012年11月 (3)</a></li>
<li><a href="http://www.cnblogs.com/forfuture1978/archive/2012/01.html" target="_blank">2012年1月 (5)</a></li>
<li><a href="http://www.cnblogs.com/forfuture1978/archive/2011/12.html" target="_blank">2011年12月 (6)</a></li>
<li><a href="http://www.cnblogs.com/forfuture1978/archive/2011/10.html" target="_blank">2011年10月 (3)</a></li>
<li><a href="http://www.cnblogs.com/forfuture1978/archive/2011/09.html" target="_blank">2011年9月 (1)</a></li>
<li><a href="http://www.cnblogs.com/forfuture1978/archive/2010/11.html" target="_blank">2010年11月 (8)</a></li>
<li><a href="http://www.cnblogs.com/forfuture1978/archive/2010/10.html" target="_blank">2010年10月 (1)</a></li>
<li><a href="http://www.cnblogs.com/forfuture1978/archive/2010/09.html" target="_blank">2010年9月 (1)</a></li>
<li><a href="http://www.cnblogs.com/forfuture1978/archive/2010/08.html" target="_blank">2010年8月 (1)</a></li>
<li><a href="http://www.cnblogs.com/forfuture1978/archive/2010/07.html" target="_blank">2010年7月 (1)</a></li>
<li><a href="http://www.cnblogs.com/forfuture1978/archive/2010/06.html" target="_blank">2010年6月 (6)</a></li>
<li><a href="http://www.cnblogs.com/forfuture1978/archive/2010/05.html" target="_blank">2010年5月 (22)</a></li>
<li><a href="http://www.cnblogs.com/forfuture1978/archive/2010/04.html" target="_blank">2010年4月 (18)</a></li>
<li><a href="http://www.cnblogs.com/forfuture1978/archive/2010/03.html" target="_blank">2010年3月 (8)</a></li>
<li><a href="http://www.cnblogs.com/forfuture1978/archive/2010/02.html" target="_blank">2010年2月 (39)</a></li>
<li><a href="http://www.cnblogs.com/forfuture1978/archive/2010/01.html" target="_blank">2010年1月 (1)</a></li>
<li><a href="http://www.cnblogs.com/forfuture1978/archive/2009/12.html" target="_blank">2009年12月 (6)</a></li>
</ul>
<h3 id="-">相册</h3>
<ul>
<li><a href="http://www.cnblogs.com/forfuture1978/gallery/247104.html" target="_blank">IT外企那点儿事</a></li>
</ul>
<h3 id="-">最新评论</h3>
<ul>
<li><a href="http://www.cnblogs.com/forfuture1978/archive/2012/11/26/2788610.html#2727561" target="_blank">1. Re:IT外企那点儿事(12)：也说跳槽</a></li>
<li>楼主怎么之后没有更新hadoop的相关信息了呢？是没有再研究了吗？</li>
<li>--lyeoswu</li>
<li><a href="http://www.cnblogs.com/forfuture1978/archive/2010/06/13/1757479.html#2713121" target="_blank">2. Re:Lucene 原理与代码分析完整版</a></li>
<li>提个建议，你生成的pdf中没有目录，影响阅读，用office转制的过程中其实设置一下即可，方便大众嘛~，还望能发我一份，谢谢！
sendreams@hotmail.com</li>
<li>--sendreams</li>
<li><a href="http://www.cnblogs.com/forfuture1978/archive/2012/11/26/2788610.html#2712415" target="_blank">3. Re:IT外企那点儿事(12)：也说跳槽</a></li>
<li><a href="&quot;查看所回复的评论&quot;">@</a>mojunbin
现在这公司，本来做的Siverlight，我进去后没多久就转JAVA了，最近在公司折腾JAVA的一些东西，业余时间玩玩游戏，看看CLR、并折腾linux。现在观点有所转变，觉得学技术更多的是为了扩宽思维、提高眼界</li>
<li>--峰顶飞龙</li>
<li><a href="http://www.cnblogs.com/forfuture1978/archive/2012/11/26/2788610.html#2711923" target="_blank">4. Re:IT外企那点儿事(12)：也说跳槽</a></li>
<li><a href="&quot;查看所回复的评论&quot;">@</a>峰顶飞龙
您的经历和我差不多，呵呵。不晓得现在兄弟在搞C/C++呢？</li>
<li>--mojunbin</li>
<li><a href="http://www.cnblogs.com/forfuture1978/archive/2010/11/23/1884967.html#2708814" target="_blank">5. Re:Hadoop学习总结之五：Hadoop的运行痕迹</a></li>
<li>楼主你好，在远程调试MapReduce时，本地代码进入不了自定义的job类，而是进入到Credentials class中，此类在hadoop-core-1.0.4.jar中，请问楼主在调试过程可否遇到此问题？</li>
<li>--彭莉珊</li>
</ul>
<h3 id="-">阅读排行榜</h3>
<ul>
<li><a href="http://www.cnblogs.com/forfuture1978/archive/2010/08/04/1791660.html" target="_blank">1. IT外企那点儿事(8)：又是一年加薪时(26799)</a></li>
<li><a href="http://www.cnblogs.com/forfuture1978/archive/2010/06/13/1757479.html" target="_blank">2. Lucene 原理与代码分析完整版(25616)</a></li>
<li><a href="http://www.cnblogs.com/forfuture1978/archive/2010/02/23/1671909.html" target="_blank">3. 从技术生命周期看IT历史(20878)</a></li>
<li><a href="http://www.cnblogs.com/forfuture1978/archive/2009/12/21/1628546.html" target="_blank">4. 101个著名的管理学及心理学效应(20828)</a></li>
<li><a href="http://www.cnblogs.com/forfuture1978/archive/2010/11/14/1877086.html" target="_blank">5. Hadoop学习总结之三：Map-Reduce入门(18681)</a></li>
</ul>
<h3 id="-">评论排行榜</h3>
<ul>
<li><a href="http://www.cnblogs.com/forfuture1978/archive/2010/06/13/1757479.html" target="_blank">1. Lucene 原理与代码分析完整版(68)</a></li>
<li><a href="http://www.cnblogs.com/forfuture1978/archive/2010/05/05/1727644.html" target="_blank">2. IT外企那点儿事(4)：激动人心的入职演讲(39)</a></li>
<li><a href="http://www.cnblogs.com/forfuture1978/archive/2010/05/13/1734162.html" target="_blank">3. IT外企那点儿事(6)：管理路线和技术路线(37)</a></li>
<li><a href="http://www.cnblogs.com/forfuture1978/archive/2010/08/04/1791660.html" target="_blank">4. IT外企那点儿事(8)：又是一年加薪时(35)</a></li>
<li><a href="http://www.cnblogs.com/forfuture1978/archive/2012/11/26/2788610.html" target="_blank">5. IT外企那点儿事(12)：也说跳槽(33)</a></li>
</ul>
<h3 id="-">推荐排行榜</h3>
<ul>
<li><a href="http://www.cnblogs.com/forfuture1978/archive/2010/06/13/1757479.html" target="_blank">1. Lucene 原理与代码分析完整版(55)</a></li>
<li><a href="http://www.cnblogs.com/forfuture1978/archive/2010/05/03/1726200.html" target="_blank">2. IT外企那点儿事(3)：奇怪的面试(39)</a></li>
<li><a href="http://www.cnblogs.com/forfuture1978/archive/2010/08/04/1791660.html" target="_blank">3. IT外企那点儿事(8)：又是一年加薪时(36)</a></li>
<li><a href="http://www.cnblogs.com/forfuture1978/archive/2012/11/26/2788610.html" target="_blank">4. IT外企那点儿事(12)：也说跳槽(34)</a></li>
<li><a href="http://www.cnblogs.com/forfuture1978/archive/2010/05/13/1734162.html" target="_blank">5. IT外企那点儿事(6)：管理路线和技术路线(27)</a></li>
</ul>
<p><a href="http://www.cnblogs.com/forfuture1978/archive/2010/04/30/1725341.html" target="_blank">IT外企那点儿事(1)：外企也就那么回事</a></p>
<p>外企，一个听起来似乎充满光环的名字，每年众多大学毕业生向往的地方。</p>
<p>说起外企，总能让人联想到很多令人心动的名词：高薪，人性化，浮动工作制，年假，完善的流程，各种福利如：旅游，室内乒乓球台，健身房，按摩椅，小食品，酸奶……</p>
<p>然而真正进入了外企，时间长了，也就发现，其实外企也就那么回事。</p>
<h2 id="-">高薪</h2>
<p>所谓高薪，严格意义上来讲是高起薪，也即刚毕业的时候每个企业公开的秘密，同学们总能够从师哥师姐那里打听到这个数字，有的企业甚至爆出较去年惊人的数字来做宣传。一个个光鲜的数字吸引着尚未毕业的大学生们，宣讲会的人数是基本和这个数字成正比的。</p>
<p>然而由于大多数的外企，由于规模比较大，机构也相对的稳定，高起薪的背后是稳定的加薪，每年7%~10%是常道，20%则是皇恩浩荡了，除非你能够取得整个Team都认可的成就，然而如果不幸参与的项目是一个多年的产品，至多是修改一些Bug或者增加一些边边角角的功能，又有多少这样的机会呢？大约在下看到的是这样的，也许并不符合所有外企的情形。</p>
<p>于是当毕业生中的佼佼者很幸运的加入大的外企的时候，不如你的同学只有默默的加入了不算太大的民企。</p>
<p>这一直是你引以为豪的资本，并总在同学聚会的时候大说特说你们公司的薪水，福利，在你的同学抱怨民企的加班声中附和着，心中却莫名的产生了一种优越感。</p>
<p>这种优越感使得你进一步沉浸在美好的外企生活中，却发现越来越没有那么优越了。三年，五年，你一次次的听说你的同学升职了，又升职了，而你还是一个普通的engineer，因为外企的升职基本是由严格的年限的，有时候多少有些按资排辈的味道。你一次一次听说你的同学加薪了，又加薪了，薪水直逼你当前的薪水，甚至在五年的关头超过你。</p>
<p>你越来越发现你的同学逐渐的掌握了一个系统前前后后的模块，能够完整的负责起一个项目的时候，你却还是螺丝钉，每天接受外国人的指示，在yes, ok, no problem, i am 100% agree的声音中继续做你的螺丝钉般的小功能。</p>
<p>我不知道十年后会如何，在参加了多次的开发者大会后，我发现几乎所有的外企的演讲者都是外国人，中国的演讲者则多来自本土的创业企业，当听着他们如数家珍的谈着自己的创业企业如何一步步做大，系统如何一步步改进，直到今天的架构，他们外企的同学能有这种机会吗？</p>
<h2 id="-">人性化</h2>
<p>所谓人性化，用外企的语言就是我们是很Open的。</p>
<p>Open体现在很多方面，诸如高管的办公室的门始终是开着的，你可以在任何时刻走到任何的高官的办公室里发表自己的看法，只是你必须保证，当你满怀激情的走进高官的办公室，关上门，半个小时后同样满怀激情走出办公室，你的顶头上司对你没有看法，即便你确实没有说什么，仅仅谈论了一下午餐而已。</p>
<p>所以除非高层主动安排和你谈话，尽量不要没事跑到高层那里，在你的顶头上司控制范围之外和他的上司进行私密的谈话，要知道有一种关系叫表面上支持，心中的隔阂。即便是高层主动要和你谈话，最好事先和你的顶头上司事先沟通，当然不用太正式，比如在闲聊的时间抱怨一下：&quot;今天下午又要被老大找去One on One，项目这么忙，不知道有啥事情可谈的&quot;，呵呵，一些术而已，姑妄言之姑听之吧。</p>
<p>对你最重要的永远是你的顶头上司，当高层听完你的建议，OK, I will take it into consideration之后，便和你没有啥关系了，绝不会存在当你的顶头上司决定给你涨薪7%的时候，高层会出来说一句，我觉得他表现还不错，涨10%吧。</p>
<p>当然，按照公司的规定，你的顶头上司也会过一段时间和你来一次One on One，问问当前的情况，问问有啥意见等等，这可不是推心置腹的时候，需要把握火候，对当前的情况说的太满意，感觉不真诚，太不满意自然领导不爱听，说没意见显得对Team不够关心，说太多意见会让人感觉你不安全。</p>
<p>所以总的原则是：</p>
<ul>
<li>要多提改善性意见(&quot;code review预留的时间应该更长一些&quot;)，少提颠覆性意见(&quot;现在的项目流程有很大问题&quot;)，</li>
<li>多提有证据的具体意见(&quot;我们有几十个Bug，可能一个星期确实做不完&quot;)，少提抽象型意见(&quot;Team之间的沟通有问题&quot;)，</li>
<li>多说与项目相关的意见，少说与自己相关的意见(尤其不要太真实的说自己的人生规划)，</li>
<li>多说在领导意料范围之内的意见(这样会给领导以对Team的控制感，比如说天天加班到10点，领导也看在眼中，可以提一下)，少说在领导意料之外的意见(即便有，请事先沟通，让领导在One on One之前就心里有数)。</li>
</ul>
<p>Open还体现来另外的方面，比如领导会和员工一起参加各种工作之外的活动，比如打球，比如年会表演，比如一起健身等等，而且在此过程中，往往是充满的欢声笑语的，但一定不要忘记领导就是领导，哪怕不在项目中，千万不要因为你曾经是学校的篮球高手，或是文艺主干，就能在此类的活动中充当领袖角色，在你的项目领导面前指手画脚，虽然在活动中他会夸你，没想到你还有这方面的才能，但是在领导面前充老大，这笔账是迟早要还的，比如在项目的后期不能够完成美国派来的任务的时候，你会被冠以虽然前一阵成功组织了活动，但是耽误了一些项目进度的罪名，从而影响你的绩效。</p>
<p>如果你在健身房遇到领导，和你一起健身，你们可以边健身边聊的很开心，但是领导的心中的第一个想法一定是，这小子项目干完了吗，还有空工作时间健身？，并且会在以后的工作中反映出来，比如时常关心你的工作进度，加大你的工作量等。</p>
<h2 id="-">浮动工作制</h2>
<p>所谓浮动工作制，很好听的名字，就是你早上可以推迟来，晚上可以早些走，只要能够完成任务，每天工作6个小时都可以。</p>
<p>初入外企的时候，看到很多前辈可以早上十点，甚至十一点才到公司，认为浮动工作制太好了，于是拼命的工作，企图在6小时干完10个小时的活，然后有时间或学习或休息。然而最后发现，活是永远干不完的，资本家花钱请了你，会让你轻松应对？</p>
<p>浮动工作制，其实就是加班不给加班费的另一种说法，也即合同中也许会写着&quot;所有的加班费已经被计入了薪水中&quot;。只要能够完成任务，每天工作12个小时也是应该的。晚上留下来很晚，或是早上很早被拉起来和老美开会，也是浮动的时间之中，你无话可说。为了改美国客户的一个Bug，深夜加班，你无话可说。在中国是休息日，但美国不是休息日的时候派去美国，并不补偿你的休息日，也不给三倍工资，你无话可说。</p>
<h2 id="-">年假</h2>
<p>外企的年假是相对较多的，也是外企在校园宣讲中经常引以为豪的一点。然而年假又有多少真正能够落到实处呢？其时大部分是休不到的，项目不允许，领导不允许，外国人也不允许。</p>
<p>不允许当然不是显式的，而是潜规则的。项目永远是紧的，即便不那么紧，也会被人们喊得使大家觉得很紧，如果一个Team有很多人休很多假，对领导来说，好像对上面不太好交代。</p>
<p>如果Team中你单独休假，你会被提醒，现在大家都在赶进度，不要因为你这个模块把项目block了。</p>
<p>如果Team中大家想一起休假，领导会说，大家都在这个时候休，连backup都没有，出了事情找不到人啊。</p>
<p>如果你平时想休息一天，领导会说，有什么事情吗？没什么事情可以等项目闲了些集中休息一下，明天早上可以晚来些，可能这一阵确实太累了。</p>
<p>如果你想连着长假一起休，领导会说，本来就有一个星期了，还另外请，不如平时累的时候休息一天，效果好。</p>
<p>如果美国人放假(如圣诞)，中国不放假，美国人会在放假前有很多任务布置过来，要在这个期间赶上美国的进度。</p>
<p>如果美国不放假，中国放假(如过年)，总不能让美国老板找不到人吧。</p>
<p>当然以上借口只是在你提出请假的时候，以商量的口气被提及，如果你真想请假，领导还是会毫不犹豫的批准的，因为我们是Open的嘛。然而以上借口却会使得多数员工不太敢于请假，因为大家都明白，有一种关系叫表面上支持，心中的隔阂。</p>
<p>当然即便假期被批准，还是有条件的，比如&quot;没问题，好好休息，走之前把文档(报告，邮件，代码)发出来(提交到svn)就行了&quot;。一般这个附加条件都会耗费一些时间的，一般是第二天休，前一天晚上至少九十点走，早上请，中午才能走，中午请，下午三点多才能走。</p>
<h2 id="-">完善的流程</h2>
<p>外企的流程是非常完善的，甚至是极度的完善，过分的完善。</p>
<p>所以外企一般都会有会议室预定系统，会议室永远是被占着的，一天一天的总是开会，讨论。</p>
<p>例会就有模块组的，开发组的(包含多个模块)，项目组的(开发和测试)，Group的(同一个大老板的多个项目)，all-hands的(整个公司)。</p>
<p>写一篇文档要模块组review，开发组review，测试组review，和美国开会review，重新改了第二轮review。以及code review，bug review。</p>
<p>每个项目组作了一个阶段后给整个项目组的demo，甚至给整个group及老外demo，说是增加visualbility。</p>
<p>一般要到下午晚些时候才能够清净些写代码，晚餐后才是代码的高峰期。</p>
<p>这也是为什么小公司半年作出来的东西，大公司要做几年。当然大公司这样做自然有它的道理，大公司稳定，不愁客户资源，不差钱，今年做出来或是明年做出来，客户别无选择，员工也养得起。这些小公司都做不到，必须尽快的满足客户的需要，必须在钱花完之前拉到下一个项目。</p>
<p>然而这对程序员的职业生涯来说好么，我不敢评价。只是在和很多朋友讨论的时候，他们发现，自己一直在忙啊忙，当跳槽试图总结自己做了啥的时候，却发现就不多的东西，不多的技术，当他们去面创业公司的时候，经常会被问，你们这么长时间，怎么就做了这么个东西？</p>
<p>大公司完善的流程还有一个特点，就是这个流程是完全为此公司定制的，当然公司大，自然可以有钱从头到尾弄自己的东西，既不用常用的，也不用开源的，无论是开发工具，测试工具，代码管理工具。这也导致了员工的粘性特别强，当走出这家公司，就像换了一片天地，原来会的别人用不到，别人常用的，却不怎么会，最后只好在公司养老，好在薪水也不错，福利也不错。</p>
<h2 id="-">设施</h2>
<p>最后提及的是各种美好的设施，这是很有吸引力的。然而为了您的前途，虽不能说敬而远之，也要注意享用的时间，如中午，晚上。</p>
<p>尽量不要在工作时间娱乐，甚至喧哗，人民的眼睛是雪亮的，领导的眼睛也是雪亮的，尤其是对于软件这种成果极难量化的产品，有时候表现和态度反而成了一种指标，不像销售一样，给公司带来的是真金白银，我无论怎么玩，能拿回单子就行，然而对于软件，你有绝对的证据证明成果超越别人吗？</p>
<p>所以外企有个很有意思的现象，一个团队的座位，离食品的距离越近越好，离娱乐设备的距离越远越好。离食品近，取用方便，领导看到你拿吃的也不会说什么，然而离娱乐设备近，领导办公室的门都开着，有谁胆敢长时间玩耍啊。所以娱乐设备上面玩耍的人一般都是座位离得比较远的。</p>
<p>此篇就写到这里的，在外企多年，其实发生了很多有趣的事情和现象，当走过几个外企的时候，发现有很多相似的潜规则。</p>
<p>进入中国的外企，其实是有中国特色的外企。中华文化的强大，使得所有的东西一到中国就会中国化，甚至改变了味道。很多民族如满族，回族的很多人都失去了原来民族的特色。也只有在中国，才可能存在儒释道三教合一的说法，不知道释迦摩尼有何感想。上学的时候，一个我很佩服的大物老师，年纪很大，他是坚定的马克思主义者，但是他曾经说，上个星期我病的厉害，差点就去见马克思了。我笑道，马克思是唯物的，是不相信死后有鬼的，死后去见阎王是迷信，去见马克思就不是了？</p>
<p>等有空的时候，再接着给大家讲外企的故事。</p>
<p>分类: <a href="http://www.cnblogs.com/forfuture1978/category/300669.html" target="_blank">IT外企那点儿事</a></p>
<p>绿色通道： <a href="">好文要顶</a> <a href="">关注我</a> <a href="">收藏该文</a><a href="http://space.cnblogs.com/msg/send/%e8%a7%89%e5%85%88" target="_blank">与我联系</a> <a href="&quot;分享至新浪微博&quot;"><img src="" alt=""></a>
<a href="http://home.cnblogs.com/u/forfuture1978/" target="_blank"><img src="" alt=""></a></p>
<p><a href="http://home.cnblogs.com/u/forfuture1978/" target="_blank">觉先</a>
<a href="http://home.cnblogs.com/u/forfuture1978/followees" target="_blank">关注 - 3</a>
<a href="http://home.cnblogs.com/u/forfuture1978/followers" target="_blank">粉丝 - 560</a></p>
<p>荣誉：<a href="http://www.cnblogs.com/expert/" target="_blank">推荐博客</a>
<a href="">+加关注</a></p>
<p>18</p>
<p>0
(请您对文章做出评价)</p>
<p><a href="http://www.cnblogs.com/forfuture1978/archive/2010/04/29/1723417.html" target="_blank">«</a> 上一篇：<a href="http://www.cnblogs.com/forfuture1978/archive/2010/04/29/1723417.html" title="发布于2010-04-29 00:24" target="_blank">高级Linux程序设计第五章：进程间通信</a>
<a href="http://www.cnblogs.com/forfuture1978/archive/2010/05/01/1725761.html" target="_blank">»</a> 下一篇：<a href="http://www.cnblogs.com/forfuture1978/archive/2010/05/01/1725761.html" title="发布于2010-05-01 20:57" target="_blank">信息检索导论(译)：第一章 布尔检索(1)</a>
posted on 2010-04-30 21:30 <a href="http://www.cnblogs.com/forfuture1978/" target="_blank">觉先</a> 阅读(8943) 评论(26) <a href="http://www.cnblogs.com/forfuture1978/admin/EditPosts.aspx?postid=1725341" target="_blank">编辑</a> <a href="">收藏</a></p>
<p><a href=""></a></p>
<h3 id="-">评论</h3>
<p><a href="">/#1楼</a><a href=""></a>  2010-04-30 21:54  <a href="http://www.cnblogs.com/lguyss/">土星的狗狗</a> <a href="http://space.cnblogs.com/msg/send/%e5%9c%9f%e6%98%9f%e7%9a%84%e7%8b%97%e7%8b%97" title="发送站内短消息" target="_blank"> </a></p>
<p>写的太好了，又学习了~哈哈，直接映射了我之前的日本公司。</p>
<p><a href="">支持(0)</a><a href="">反对(0)</a>
<a href="http://pic.cnitblog.com/face/u23929.jpg" target="_blank">http://pic.cnitblog.com/face/u23929.jpg</a></p>
<p><a href="">/#2楼</a><a href=""></a>  2010-04-30 23:57  <a href="http://www.cnblogs.com/wenjl520/">温景良(Jason)</a> <a href="http://space.cnblogs.com/msg/send/%e6%b8%a9%e6%99%af%e8%89%af(Jason" target="_blank"> </a> &quot;发送站内短消息&quot;)</p>
<p>没去过,期待中</p>
<p><a href="">支持(0)</a><a href="">反对(0)</a>
<a href="http://pic.cnitblog.com/face/u33118.jpg" target="_blank">http://pic.cnitblog.com/face/u33118.jpg</a></p>
<p><a href="">/#3楼</a><a href=""></a>  2010-05-01 10:41  <a href="http://www.cnblogs.com/ilovedotnet/">ilovedotnet</a> <a href="http://space.cnblogs.com/msg/send/ilovedotnet" title="发送站内短消息" target="_blank"> </a></p>
<p><a href="&quot;查看所回复的评论&quot;">@</a>土星的狗狗
我觉得日企和韩企不能算是真正的外企，大家通常说的外企只包括欧美企业。</p>
<p><a href="">支持(0)</a><a href="">反对(0)</a>
<a href="http://pic.cnitblog.com/face/u26921.jpg" target="_blank">http://pic.cnitblog.com/face/u26921.jpg</a></p>
<p><a href="">/#4楼</a><a href=""></a>  2010-05-01 21:59  <a href="http://www.cnblogs.com/skyyang/">DarroldYang</a> <a href="http://space.cnblogs.com/msg/send/DarroldYang" title="发送站内短消息" target="_blank"> </a></p>
<p>说的很像
加入这样的工作环境没多久
他们一个项目也是做了几年了
后面就一直在bug</p>
<p><a href="">支持(0)</a><a href="">反对(0)</a>
<a href="http://pic.cnitblog.com/face/u23975.png" target="_blank">http://pic.cnitblog.com/face/u23975.png</a></p>
<p><a href="">/#5楼</a><a href=""></a>  2010-05-04 09:26  <a href="http://www.cnblogs.com/peon/">加菲猫</a> <a href="http://space.cnblogs.com/msg/send/%e5%8a%a0%e8%8f%b2%e7%8c%ab" title="发送站内短消息" target="_blank"> </a></p>
<p>作者深得外企三味，不过假如你有心，在薪水上超过大部分民企的同学问题不大，华为腾讯的除外</p>
<p><a href="">支持(0)</a><a href="">反对(0)</a>
<a href="http://pic.cnitblog.com/face/u28333.jpg" target="_blank">http://pic.cnitblog.com/face/u28333.jpg</a></p>
<p><a href="">/#6楼</a><a href=""></a>  2010-05-05 09:28  <a href="http://www.cnblogs.com/jciwolf/">Jerry Qian</a> <a href="http://space.cnblogs.com/msg/send/Jerry+Qian" title="发送站内短消息" target="_blank"> </a></p>
<p>楼主打击人啦，我现在在学英语啊。</p>
<p><a href="">支持(0)</a><a href="">反对(0)</a></p>
<p><a href="">/#7楼</a><a href=""></a>  2010-05-05 11:43  <a href="http://www.cnblogs.com/bobliu/">Bob Liu</a> <a href="http://space.cnblogs.com/msg/send/Bob+Liu" title="发送站内短消息" target="_blank"> </a></p>
<p>看看，了解一下外企～</p>
<p><a href="">支持(0)</a><a href="">反对(0)</a>
<a href="http://pic.cnitblog.com/face/u126058.jpg" target="_blank">http://pic.cnitblog.com/face/u126058.jpg</a></p>
<p><a href="">/#8楼</a><a href=""></a>  2010-05-05 12:01  <a href="http://www.cnblogs.com/Aaron_Anubis/">Aaron_Aanubis</a> <a href="http://space.cnblogs.com/msg/send/Aaron_Aanubis" title="发送站内短消息" target="_blank"> </a></p>
<p>楼主，太好了···把这些说出来，叫我们更加了解外企，我们就更能根据自身来进行职业规划了！！！3q</p>
<p><a href="">支持(0)</a><a href="">反对(0)</a></p>
<p><a href="">/#9楼</a><a href=""></a>  2010-05-05 12:25  <a href="http://www.cnblogs.com/Jong/">Caspar Jiong</a> <a href="http://space.cnblogs.com/msg/send/Caspar+Jiong" title="发送站内短消息" target="_blank"> </a></p>
<p>深有同感！</p>
<p><a href="">支持(0)</a><a href="">反对(0)</a></p>
<p><a href="">/#10楼</a><a href=""></a>  2010-05-05 12:51  <a href="http://www.cnblogs.com/jerry_cong/">鸟瞰</a> <a href="http://space.cnblogs.com/msg/send/%e9%b8%9f%e7%9e%b0" title="发送站内短消息" target="_blank"> </a></p>
<p>对外企的经营模式，如果想老板的朋友们，某些地方还是比较值得借鉴的</p>
<p><a href="">支持(0)</a><a href="">反对(0)</a>
<a href="http://pic.cnitblog.com/face/u122874.jpg?id=16080143" target="_blank">http://pic.cnitblog.com/face/u122874.jpg?id=16080143</a></p>
<p><a href="">/#11楼</a><a href=""></a>  2010-05-05 15:13  <a href="http://www.cnblogs.com/facingwaller/">撞破南墙</a> <a href="http://space.cnblogs.com/msg/send/%e6%92%9e%e7%a0%b4%e5%8d%97%e5%a2%99" title="发送站内短消息" target="_blank"> </a></p>
<p>看了觉得果然很中国化，不知道GG也是这样的吗？</p>
<p><a href="">支持(0)</a><a href="">反对(0)</a>
<a href="http://pic.cnitblog.com/face/u69696.jpg?id=16133304" target="_blank">http://pic.cnitblog.com/face/u69696.jpg?id=16133304</a></p>
<p><a href="">/#12楼</a><a href=""></a>  2010-05-05 15:46  <a href="http://www.cnblogs.com/ArthasCui/">Arthas-Cui</a> <a href="http://space.cnblogs.com/msg/send/Arthas-Cui" title="发送站内短消息" target="_blank"> </a></p>
<p>你知道的太多了。。。</p>
<p><a href="">支持(0)</a><a href="">反对(0)</a>
<a href="http://pic.cnitblog.com/face/u37616.jpg?id=29112918" target="_blank">http://pic.cnitblog.com/face/u37616.jpg?id=29112918</a></p>
<p><a href="">/#13楼</a><a href=""></a>  2010-05-05 16:11  <a href="http://www.cnblogs.com/daxianren/">卡通一下</a> <a href="http://space.cnblogs.com/msg/send/%e5%8d%a1%e9%80%9a%e4%b8%80%e4%b8%8b" title="发送站内短消息" target="_blank"> </a></p>
<p>大家可能不太清楚，在美国顶头上司才是你真正的老板。他要开你走，董事会是不会举行表决的，哈哈！</p>
<p><a href="">支持(0)</a><a href="">反对(0)</a></p>
<p><a href="">/#14楼</a><a href=""></a>  2010-05-05 16:15  <a href="http://www.cnblogs.com/daxianren/">卡通一下</a> <a href="http://space.cnblogs.com/msg/send/%e5%8d%a1%e9%80%9a%e4%b8%80%e4%b8%8b" title="发送站内短消息" target="_blank"> </a></p>
<p>楼主文章的题目，<strong>“外企那点儿事”；“也就那么回事”</strong>，可以看出楼主当前的心态！
就好象世上有人说的，一不小心发财了！一不小心成功了......
呵呵！</p>
<p><a href="">支持(0)</a><a href="">反对(0)</a></p>
<p><a href="">/#15楼</a><a href=""></a>[楼主]  2010-05-05 18:36  <a href="http://www.cnblogs.com/forfuture1978/">觉先</a> <a href="http://space.cnblogs.com/msg/send/%e8%a7%89%e5%85%88" title="发送站内短消息" target="_blank"> </a></p>
<p><a href="&quot;查看所回复的评论&quot;">@</a>卡通一下
外企那点儿事是学了当前的流行语，明朝那点儿事，Java那点儿事...
及历史是什么玩意儿等，吸引人注意的一个噱头而已。</p>
<p><a href="">支持(0)</a><a href="">反对(0)</a>
<a href="http://pic.cnitblog.com/face/u103165.jpg" target="_blank">http://pic.cnitblog.com/face/u103165.jpg</a></p>
<p><a href="">/#16楼</a><a href=""></a>  2010-05-05 19:20  <a href="http://www.cnblogs.com/daxianren/">卡通一下</a> <a href="http://space.cnblogs.com/msg/send/%e5%8d%a1%e9%80%9a%e4%b8%80%e4%b8%8b" title="发送站内短消息" target="_blank"> </a></p>
<p><a href="&quot;查看引用原文&quot;">引用</a>觉先：
@卡通一下
外企那点儿事是学了当前的流行语，明朝那点儿事，Java那点儿事...
及历史是什么玩意儿等，吸引人注意的一个噱头而已。
朋友间聊天我们也经常地说，只是在正式场合是不说的，呵呵！</p>
<p><a href="">支持(0)</a><a href="">反对(0)</a></p>
<p><a href="">/#17楼</a><a href=""></a>  2010-05-09 21:17  <a href="http://www.cnblogs.com/dytes/">dytes</a> <a href="http://space.cnblogs.com/msg/send/dytes" title="发送站内短消息" target="_blank"> </a></p>
<p>不要一概而论，就我的经历而言，还是相当宽松的。-</p>
<p><a href="">支持(0)</a><a href="">反对(0)</a></p>
<p><a href="">/#18楼</a><a href=""></a>  2010-05-10 13:02  <a href="http://www.cnblogs.com/Koy/">Koy</a> <a href="http://space.cnblogs.com/msg/send/Koy" title="发送站内短消息" target="_blank"> </a></p>
<p>講得好好，頂一下。</p>
<p><a href="">支持(0)</a><a href="">反对(0)</a></p>
<p><a href="">/#19楼</a><a href=""></a>  2010-05-10 22:53  <a href="http://home.cnblogs.com/u/127908/">小飞哥</a> <a href="http://space.cnblogs.com/msg/send/%e5%b0%8f%e9%a3%9e%e5%93%a5" title="发送站内短消息" target="_blank"> </a></p>
<p>哈哈楼主你知道得太多了</p>
<p><a href="">支持(0)</a><a href="">反对(0)</a></p>
<p><a href="">/#20楼</a><a href=""></a>  2010-05-13 10:56  <a href="http://www.cnblogs.com/KissKnife/">SnowToday</a> <a href="http://space.cnblogs.com/msg/send/SnowToday" title="发送站内短消息" target="_blank"> </a></p>
<p>基本上是这个样子，不过外企跟外企也不太一样，部门跟部门不太一样，项目跟项目不太一样，比如请假放假，我们这请假绝大多数不会有问题，请假就请了，不会有那么多顾虑，还有老外的一些重要节日我们也会跟着放比如圣诞、复活节。</p>
<p><a href="">支持(0)</a><a href="">反对(0)</a>
<a href="http://pic.cnitblog.com/face/u10907.jpg" target="_blank">http://pic.cnitblog.com/face/u10907.jpg</a></p>
<p><a href="">/#21楼</a><a href=""></a>  2010-05-13 11:11  <a href="http://home.cnblogs.com/u/132808/">足球王子</a> <a href="http://space.cnblogs.com/msg/send/%e8%b6%b3%e7%90%83%e7%8e%8b%e5%ad%90" title="发送站内短消息" target="_blank"> </a></p>
<p>在外企参与项目的机会会少很多，但是没有进过外企，就好像没怎么见过世面一样。</p>
<p><a href="">支持(0)</a><a href="">反对(0)</a></p>
<p><a href="">/#22楼</a><a href=""></a>  2010-05-13 15:46  <a href="http://www.cnblogs.com/Abbott/">Abbott zhao</a> <a href="http://space.cnblogs.com/msg/send/Abbott+zhao" title="发送站内短消息" target="_blank"> </a></p>
<p>这也是为什么小公司半年作出来的东西，大公司要做几年。当然大公司这样做自然有它的道理，大公司稳定，不愁客户资源，不差钱，今年做出来或是明年做出来，客户别无选择，员工也养得起。这些小公司都做不到，必须尽快的满足客户的需要，必须在钱花完之前拉到下一个项目。
这句话有点偏。小公司做的产品真的不能恭维。</p>
<p><a href="">支持(1)</a><a href="">反对(0)</a></p>
<p><a href="">/#23楼</a><a href=""></a>  2010-05-13 21:28  <a href="http://www.cnblogs.com/417533880/">Forrest Liu</a> <a href="http://space.cnblogs.com/msg/send/Forrest+Liu" title="发送站内短消息" target="_blank"> </a></p>
<p>我也在一家小外企工作，最近遇到点事很郁闷。前两天女朋友来公司附近办事，我就带她在公司里待会，等着和我一起吃午饭。。结果她待了没有十分钟，公司老板从我身边过就看到她了，过了一会我的team leader就用communicater跟我说，让她出去，我就带她出去了。。回来后我的leader跟我说老板看到我女朋友了，很生气。因为我之前的公司很随便，以前也带朋友同学什么的进去过。当时也就觉得没什么。今天早上leader又跟我谈话，说客户那边反应对我的工作不满意。我当时很奇怪，因为我跟我的客户一直保持沟通，而且分配给我的task我也都完成的很不错。前一个月的时候我还特意问过客户对我的工作有什么意见，我在哪方面可以做的更好，结果客户给我回复说对我的工作很满意。我不知道是因为我得罪了谁或者我做错了什么。。现在很困惑，前辈给我指点一下吧~~</p>
<p><a href="">支持(0)</a><a href="">反对(0)</a>
<a href="http://pic.cnitblog.com/face/u39386.jpg" target="_blank">http://pic.cnitblog.com/face/u39386.jpg</a></p>
<p><a href="">/#24楼</a><a href=""></a>  2010-05-14 18:32  <a href="http://www.cnblogs.com/cjc1021/">开心每一天ㄨ</a> <a href="http://space.cnblogs.com/msg/send/%e5%bc%80%e5%bf%83%e6%af%8f%e4%b8%80%e5%a4%a9%e3%84%a8" title="发送站内短消息" target="_blank"> </a></p>
<p><a href="&quot;查看所回复的评论&quot;">@</a>Forrest Liu
感觉比较没有人情味就是。做事都是规规矩矩的。
如果你在民企或者是小企，哪怕是国企可能都不一定。外企业也是不一样，但你目前所处的就是挺没人情味的感觉~</p>
<p><a href="">支持(0)</a><a href="">反对(0)</a>
<a href="http://pic.cnitblog.com/face/u25669.jpg" target="_blank">http://pic.cnitblog.com/face/u25669.jpg</a></p>
<p><a href="">/#25楼</a><a href=""></a>  2010-05-14 23:16  <a href="http://www.cnblogs.com/qingteng1983/">无待</a> <a href="http://space.cnblogs.com/msg/send/%e6%97%a0%e5%be%85" title="发送站内短消息" target="_blank"> </a></p>
<p>有意思，受教了。</p>
<p><a href="">支持(0)</a><a href="">反对(0)</a>
<a href="http://pic.cnitblog.com/face/u131056.jpg?id=02141056" target="_blank">http://pic.cnitblog.com/face/u131056.jpg?id=02141056</a></p>
<p><a href="">/#26楼</a><a href=""></a>18306582010/5/22 21:08:21  2010-05-22 21:08  <a href="http://home.cnblogs.com/u/134973/">fyljf</a> <a href="http://space.cnblogs.com/msg/send/fyljf" title="发送站内短消息" target="_blank"> </a></p>
<p>有些点真是深有体会</p>
<p><a href="">支持(0)</a><a href="">反对(0)</a></p>
<p><a href="">刷新评论</a><a href="">刷新页面</a><a href="">返回顶部</a></p>
<p>注册用户登录后才能发表评论，请 <a href="">登录</a> 或 <a href="">注册</a>，<a href="http://www.cnblogs.com/" target="_blank">访问</a>网站首页。
<a href="http://www.cnblogs.com/" title="程序员的网上家园" target="_blank">博客园首页</a><a href="http://q.cnblogs.com/" title="程序员问答社区" target="_blank">博问</a><a href="http://news.cnblogs.com/" title="IT新闻" target="_blank">新闻</a><a href="http://home.cnblogs.com/ing/" target="_blank">闪存</a><a href="http://job.cnblogs.com/" target="_blank">程序员招聘</a><a href="http://kb.cnblogs.com/" target="_blank">知识库</a></p>
<p><strong>最新IT新闻</strong>:
· <a href="http://news.cnblogs.com/n/182486/" target="_blank">新硬硬整合时代</a>
· <a href="http://news.cnblogs.com/n/182485/" target="_blank">狗血的百度91并购案啊 阿里和周鸿祎都曾掺和</a>
· <a href="http://news.cnblogs.com/n/182483/" target="_blank">如何让搜索引擎抓取AJAX内容？</a>
· <a href="http://news.cnblogs.com/n/182482/" target="_blank">避免代码注释的五大理由</a>
· <a href="http://news.cnblogs.com/n/182481/" target="_blank">OpenWrt——适用于路由器的Linux系统</a>
» <a href="http://news.cnblogs.com/" title="IT新闻" target="_blank">更多新闻...</a></p>
<p><strong>最新知识库文章</strong>:
· <a href="http://kb.cnblogs.com/page/141892/" target="_blank">阿里巴巴集团去IOE运动的思考与总结</a>
· <a href="http://kb.cnblogs.com/page/182265/" target="_blank">硅谷归来7点分享：创业者，做你自己</a>
· <a href="http://kb.cnblogs.com/page/182200/" target="_blank">我为什么不能坚持？</a>
· <a href="http://kb.cnblogs.com/page/168725/" target="_blank">成为高效程序员的7个重要习惯</a>
· <a href="http://kb.cnblogs.com/page/182047/" target="_blank">谈谈对BPM的理解</a>
» <a href="http://kb.cnblogs.com/" target="_blank">更多知识库文章...</a>
Powered by:
<a href="http://www.cnblogs.com/" target="_blank">博客园</a>
Copyright © 觉先</p>

      
    </div>
    
    
      
    
    
        <footer id="post-meta">
        <span class="categories">Posted in<span class="breadcrumb fa fa-folder"><li><a href="/categories/zhiya/">zhiya</a></li></span><span class="breadcrumb"><li><a href="/categories/职涯/">职涯</a></li><li><a href="/categories/职涯/IT外企/">IT外企</a></li></span></span> | <span class="tags">Tagged <a href="/tags/IT外企/" class="label label-primary">IT外企</a><a href="/tags/zhiya/" class="label label-success">zhiya</a></span> | <span class="time">recent updated:<time title="2014-03-07 09:54:36"datetime="2014-03-07 09:54:36"> mar. 7 2014</time></span> | <span class="comment-link">
<a href="http://itsolife.com/2014/02/02/2014-02-02-zhiya-IT外企--IT外企那点儿事1：外企也就那么回事/#comments" class="ds-thread-count comment-link" data-thread-key="2014-02-02-zhiya-IT外企--IT外企那点儿事1：外企也就那么回事" data-count-type="comments">暂无评论</a></span>
        </footer>
    
    <div class="clearfix"></div>
  </div>
</div>
</article>



  <article>
<div id="post" class="post well">
  <div class="post-content">
    <header class="well-sm">
      <i class="fa icon fa-5x pull-left"></i>
      <h1 class="title"><a href="/2014/02/02/2014-02-02-hadoop--Hadoop知识分享文稿byquqi99-技术并艺术着/">Hadoop知识分享文稿 ( by quqi99 ) </a></h1>
      
        <span>Posted on<time datetime="2014-02-02T01:54:36.000Z"> <a href="/2014/02/02/2014-02-02-hadoop--Hadoop知识分享文稿byquqi99-技术并艺术着/">feb. 2 2014</a></time></span>      
    </header>
    
    
    <div class="entry">
      
        <h1 id="hadoop-by-quqi99-">Hadoop知识分享文稿 ( by quqi99 ) - 技术并艺术着</h1>
<p>您还未登录！|<a href="https://passport.csdn.net/account/login" target="_blank">登录</a>|<a href="https://passport.csdn.net/account/register" target="_blank">注册</a>|<a href="https://passport.csdn.net/help/faq" target="_blank">帮助</a></p>
<ul>
<li><a href="http://www.csdn.net/" target="_blank">首页</a></li>
<li><a href="http://news.csdn.net/" target="_blank">业界</a></li>
<li><a href="http://mobile.csdn.net/" target="_blank">移动</a></li>
<li><a href="http://cloud.csdn.net/" target="_blank">云计算</a></li>
<li><a href="http://sd.csdn.net/" target="_blank">研发</a></li>
<li><a href="http://bbs.csdn.net/" target="_blank">论坛</a></li>
<li><a href="http://blog.csdn.net/" target="_blank">博客</a></li>
<li><a href="http://download.csdn.net/" target="_blank">下载</a></li>
<li><h2 id="-"><a href="">更多</a></h2>
</li>
</ul>
<h1 id="-http-blog-csdn-net-quqi99-"><a href="http://blog.csdn.net/quqi99" target="_blank">技术并艺术着</a></h1>
<h2 id="-blog">张华的技术Blog</h2>
<ul>
<li><a href="http://blog.csdn.net/quqi99?viewmode=contents" target="_blank"><img src="" alt="">目录视图</a></li>
<li><a href="http://blog.csdn.net/quqi99?viewmode=list" target="_blank"><img src="" alt="">摘要视图</a></li>
<li><a href="http://blog.csdn.net/quqi99/rss/list" target="_blank"><img src="" alt="">订阅</a>
<a href="https://code.csdn.net/blog/12" target="_blank">公告：博客新增直接引用代码功能</a>        <a href="http://www.csdn.net/article/2013-08-06/2816471" target="_blank">专访李铁军：从医生到金山首席安全专家的转变</a>      <a href="http://blog.csdn.net/csdnproduct/article/details/9495139" target="_blank">CSDN博客频道自定义域名、标签搜索功能上线啦！</a>      <a href="http://blog.csdn.net/adali/article/details/9813651" target="_blank">独一无二的职位：开源社区经理</a></li>
</ul>
<h3 id="-hadoop-by-quqi99-"><a href="">[置顶] Hadoop知识分享文稿 ( by quqi99 )</a></h3>
<p>分类： <a href="http://blog.csdn.net/quqi99/article/category/328029" target="_blank">Seach Engine</a>  2011-03-31 15:19 1977人阅读 <a href="">评论</a>(0) <a href="&quot;收藏&quot;">收藏</a> <a href="&quot;举报&quot;">举报</a>
<a href="http://blog.csdn.net/tag/details.html?tag=hadoop" target="_blank">hadoop</a><a href="http://blog.csdn.net/tag/details.html?tag=%e4%bb%bb%e5%8a%a1" target="_blank">任务</a><a href="http://blog.csdn.net/tag/details.html?tag=mapreduce" target="_blank">mapreduce</a><a href="http://blog.csdn.net/tag/details.html?tag=%e4%bb%bb%e5%8a%a1%e8%b0%83%e5%ba%a6" target="_blank">任务调度</a><a href="http://blog.csdn.net/tag/details.html?tag=%e9%9b%86%e7%be%a4" target="_blank">集群</a><a href="http://blog.csdn.net/tag/details.html?tag=%e4%bd%9c%e4%b8%9a" target="_blank">作业</a></p>
<p>目录<a href="&quot;系统根据文章中H1到H6标签自动生成文章目录&quot;">(?)</a><a href="&quot;展开&quot;">[+]</a></p>
<ol>
<li><a href="">作者张华 写于2010-08-15   发表于2011-03-31 版权声明可以任意转载转载时请务必以超链接形式标明文章原始出处和作者信息及本版权声明</a></li>
<li><a href="">httpblogcsdnnetquqi99</a></li>
<li><p><a href="">hadoop 理论基础</a></p>
</li>
<li><p><a href="">hadoop 是什么</a></p>
</li>
<li><a href="">hadoop 项目</a></li>
<li><a href="">MapReduce 任务的运行流程</a></li>
<li><p><a href="">MapReduce 任务的数据流图</a></p>
</li>
<li><p><a href="">hadoop 入门实战</a></p>
</li>
<li><p><a href="">测试环境</a></p>
</li>
<li><a href="">测试程序</a></li>
<li><a href="">属性配置</a></li>
<li><a href="">免密码 SSH 设置</a></li>
<li><a href="">配置 hosts</a></li>
<li><a href="">格式化 HDFS 文件系统</a></li>
<li><a href="">启动守护进程</a></li>
<li><p><a href="">运行程序</a></p>
</li>
<li><p><a href="">hadoop 高级进阶</a></p>
</li>
<li><a href="">hadoop 应用案例</a></li>
<li><a href="">参考文献</a><pre><code>                       **Hadoop知识分享文稿 ( by quqi99 )**
</code></pre></li>
</ol>
<h2 id="-2010-08-15-2011-03-31"><a href=""></a>作者：张华 写于：2010-08-15   发表于：2011-03-31</h2>
<p>版权声明：可以任意转载，转载时请务必以超链接形式标明文章原始出处和作者信息及本版权声明</p>
<h2 id="-http-blog-csdn-net-quqi99-"><a href=""></a>( <a href="http://blog.csdn.net/quqi99">http://blog.csdn.net/quqi99</a> )</h2>
<p><strong>内容目录</strong></p>
<p>目 录</p>
<p>1 hadoop 理论基础 3</p>
<p>1.1 hadoop 是什么 3</p>
<p>1.2 hadoop 项目 3</p>
<p>1.3 Map/Reduce 任务的运行流程 4</p>
<p>1.4 Map/Reduce 任务的数据流图 5</p>
<p>2 hadoop 入门实战 7</p>
<p>2.1  测试环境 7</p>
<p>2.2  测试程序 7</p>
<p>2.3  属性配置 9</p>
<p>2.4  免密码SSH 设置 10</p>
<p>2.5  配置hosts 11</p>
<p>2.6  格式化HDFS 文件系统 11</p>
<p>2.7  启动守护进程 11</p>
<p>2.8  运行程序 11</p>
<p>3 hadoop 高级进阶 12</p>
<p>4 hadoop 应用案例 12</p>
<p>5  参考文献 12</p>
<h1 id="-1-hadoop-"><a href=""></a>1 hadoop  理论基础</h1>
<h2 id="-1-1-hadoop-"><a href=""></a>1.1 hadoop  是什么</h2>
<p>Hadoop  是 Doug Cutting  开发的，他是一个相当牛的哥们，他同时是大名鼎鼎的 Lucene  及 Nutch  的作者。</p>
<p>我是这样理解 hadoop  的，它就是用来对海量数据进行存储与分析的一个开源软件。它包括两块：</p>
<p>1  ） HDFS ( Hadoop Distrubuted File System )  ，可以对重要数据进行冗余存储，有点类似于冗余磁盘陈列。</p>
<p>2  ）对 Map/Reduce  编程模型的一个实现。当然，关系型数据库（ RDBMS  ）也能做类似的事情，但为什么不用 RDBMS  呢？我们知道，让计算移动于数据上比让数据移动到计算更有效率。这使得 Map/Reduce  适合数据被一次写入和多次读取的应用，而 RDBMS  更适合持续更新的数据集。</p>
<h2 id="-1-2-hadoop-"><a href=""></a>1.2 hadoop  项目</h2>
<p>如今，广义上的 Hadoop  已经发展成为一个分布式计算基础架构这把“大伞”下相关子项目的集合，其技术栈如下图所示：</p>
<p>图：</p>
<pre><code>                                     ![]()
</code></pre><p><img src="http://blog.csdn.net/root/Library/Caches/TemporaryItems/moz-screenshot-4.png" alt=""></p>
<pre><code>                                                图1 hadoop 的子项目
</code></pre><ul>
<li>Core ： 一系列分布式文件系统和通用I/O 的组件和接口( 序列化、Java RPC 和持久化数据结构) 。</li>
<li>Avro ： 用于数据的序列化，当然，JDK 中也有Seriable 接口，但hadoop 中有它自己的序列化方式，具说更有效率。</li>
<li>MapReduce ： 分布式数据处理模式和执行环境，运行于大型商用机集群。</li>
<li>HDFS ： 分布式文件系统，运行于大型商用机集群。</li>
<li>Pig ： HDFS 上的数据检索语言，类似于RDBMS 中的SQL 语言。</li>
<li>Hbase ： 一个分布式的、列存储数据库。HBase 使用HDFS 作为底层存储，同时支持MapReduce 的批量式计算和点查询( 随机读取) 。</li>
<li>ZooKeeper ： 一个分布式的、高可用性的协调服务。ZooKeeper 提供分布式锁之类的基本服务用于构建分布式应用。</li>
<li>Hive ： 分布式数据仓库。Hive 管理HDFS 中存储的数据，并提供基于SQL 的查询语言( 由运行时引擎翻译成MapReduce 作业) 用以查询数据。</li>
<li>Chukwa ： 分布式数据收集和分析系统。Chukwa 运行HDFS 中存储数据的收集器，它使用MapReduce 来生成报告。</li>
</ul>
<h2 id="-1-3-map-reduce-"><a href=""></a> 1.3 Map/Reduce  任务的运行流程</h2>
<pre><code>                 ![]()
</code></pre><p>JobClient  的  submitJob()  方法的作业提交过程如下：</p>
<p>1  ）向 Jobtraker  请求一个新作业 ID</p>
<p>2  ） 调用 JobTracker  的 getNewJobId()</p>
<p>3  ）  JobClient  进行作业划分，并将划分后的输入及作业的 JAR  文件、配置文件等复制到 HDFS  中去</p>
<p>4  ） 提交作业，会把此调用放入到一个内部的队列中，交由作业调度器进行调度。值得一提的是，针对  Map  任务与 Reduce  任务，任务调度器是优先选择 Map  任务的，另外，任务调度器在选择 Reduce  任务时并没有考虑数据的本地化。然而，针对一个 Map  任务，它考虑的是 Tasktracker  网络位置和选取一个距离其输入划分文件最近的 Tasktracker  ，它可能是数据本地化的，也可能是机架本地化的，还可能得到不同的机架上取数据。</p>
<p>5  ） 初始化包括创建一个代表该正在运行的作业的对象，它封装任务和记录信息，以便跟踪任务的状态和进度。</p>
<p>6  ） JobTracker  任务调度器首先从共享文件系统中获取 JobClient  已计算好的输入划分信息，然后为每个划分创建一个 Map  任务。创建 的 reduce  任务的数量是由 JobConf  的 Mapred.reduce.tasks  属性决定，它是用 setNumReduceTask()  方法来设置的。</p>
<p>7  ） TaskTracker  执行一个简单的循环，定期发送心跳（ Heartbeat  ）方法调用 Jobtracker  告诉是否还活着，同时，心跳还会报告任务运行的是否已经准备运行新的任务。</p>
<p>8  ） TaskTracker  已经被分配了任务，下一步是运行任务。首先它需要将它所需的全部文件从 HDFS  中复制到本地磁盘。</p>
<p>9  ）紧接着，它要启动一个新的 Java  虚拟机来运行每个任务，这使得用户所定义的 Map  和 Reduce  函数的任务缺陷都不会影响 TaskTracker  （比如导致它崩溃或者挂起）</p>
<p>10  ）运行 Map  任务或者 Reduce  任务，值得一提的是，这些任务使用标准输入与输出流，换句话说，你可以用任务语言（如 JAVA  ， C++  ， Shell  等）来实现 Map  和 Reduce  ，只要保证它们也使用标准输入与输出流，就可以将输出的键值对传回给 JAVA  进程了。</p>
<h2 id="-1-4-map-reduce-"><a href=""></a> 1.4 Map/Reduce  任务的数据流图</h2>
<p><img src="" alt=""></p>
<pre><code>    图3  Map/Reduce  中单一 Reduce  任务的数据流图
</code></pre><p><img src="" alt=""></p>
<pre><code>             图4  Map/Reduce  中多个 Reduce  任务的数据流图
</code></pre><p><img src="" alt=""></p>
<pre><code>            图5  MapReduce  中没有 Reduce  任务的数据流图
</code></pre><p><strong>任务粒度</strong>   ： 分片的个数，在将原始大数据切割成小数据集时，通常让小数据集小于或等于 HDFS  中的一个 Block  的大小（缺省是 64M)  ，这样能够保证一个小数据集位于一台计算机上，便于本地计算。 有 M   个 小数据集 待处理，就启动 M   个 Map   任务，注意这 M   个 Map   任务分布于 N   台计算机上并行运行，Reduce   任务的数量 R   则可由用户指定 。</p>
<p><strong>Map</strong>   ： 输入 <k1, v1>   输出 List(<k2,v2>)</p>
<p><strong>Reduce</strong>   ： 输入 <k2,List(v2)>   输出 <k3,v3></p>
<p><strong>分区（</strong>  <strong>Partition)</strong>  :   把 Map   任务输出的中间结果按 key   的范围划分成 R   份 ( R  是预先定义的 Reduce  任务的个数) ，划分时通常使用 hash  函数如: hash(key) mod R ，这样可以保证某一段范围内的 key ，一定是由一个 Reduce  任务来处理，可以简化 Reduce  的过程。</p>
<p><strong>Combine</strong>   :   在  partition   之前，还可以对中间结果先做  combine  ，即将中间结果中有相同  key  的  <key, value>   对合并成一对。 combine   的过程与  Reduce   的过程类似，很多情况下就可以直接使用  Reduce   函数，但  combine   是作为  Map   任务的一部分，在执行完  Map   函数后紧接着执行的。 Combine   能够减少中间结果中  <key, value>   对的数目，从而减少网络流量。</p>
<p>下面举个例子来着重说明 Combine  ， hadoop  允许用户声明一个 combiner  运行在 Map  的输出上，它的输出再作为 Reduce  的输入。例如，找出每一年的最调气温：</p>
<p>假如用户的输入的分片数是 2  ，那么：</p>
<p>1  ）第一个 Map  的输出如下：</p>
<p>（ 1950  ， 0  ）</p>
<p>（ 1950  ， 20  ）</p>
<p>（ 1950  ， 10  ）</p>
<p>2  ） 第二个 Map  的输出如下：</p>
<p>（ 1950  ， 25  ）</p>
<p>（ 1950  ， 15  ）</p>
<p>3  ） Reduce  的输入如下：</p>
<p>（ 1950  ，［ 0  ， 20  ， 10  ， 25  ， 15  ］）</p>
<p><strong>注意：如果有</strong>   <strong>combine</strong>    <strong>的话，此时</strong>   <strong>Reduce</strong>    <strong>的输入应该是：</strong></p>
<p><strong>max(0, 20, 10, 25, 15) = max(max(0,20,10), max(25,15)) = max(20,25)</strong></p>
<p><strong>combine</strong>    <strong>并不能取代</strong>   <strong>reduce,</strong>    <strong>例如，如果我们计算平均气温，便不能使用</strong>   <strong>combine</strong>    <strong>，因为：</strong></p>
<p><strong>mean(0,20,10,25,15) = 14</strong></p>
<p><strong>但是：</strong></p>
<p><strong>mean(mean(0,20,10), mean(25,15)) = mean(10,20) = 15</strong></p>
<p>4  ） Reduce  的输出如下：</p>
<p>（ 1950  ， 25  ）</p>
<h1 id="-2-hadoop-"><a href=""></a>2 hadoop  入门实战</h1>
<p>hadoop  有三种部署模式：</p>
<ul>
<li>单机模式：没有守护进程，一切都运行在单个 JVM  上，适合测试与调试。</li>
<li>伪集群模式：守护进程在本地运行，适合模拟集群。</li>
<li>集群模式：守护进程运行在集群的某台机器上。</li>
</ul>
<p>所以，在以上任一特定模式运行 hadoop  时，只需要做两件事情：</p>
<p>1  ） 设置适当属性</p>
<p>2  ）启动 hadoop  的守护进程（名称节点，二级名称节名，数据节点）</p>
<p>hadoop  默认的是单机模式，下面，我们将着重介绍在集群模式是如何部署？</p>
<h2 id="-2-1-"><a href=""></a>2.1   测试环境</h2>
<p>用两台机器做为测试环境 ,   通常，集群里的一台机器被指定为  NameNode  ，另一台不同的机器被指定为 JobTracker  ，这些机器是 <strong>masters;</strong>  余下的机器即作为 DataNode  <strong>也</strong> 作为 TaskTracker  ，这些机器是 <strong>slaves</strong>  <strong>。</strong></p>
<p>1  ）  master (JobTracker &amp; NameNode)  ：我的工作机  ( zhanghua  .quqi.com)</p>
<p>2  ）  slave (TaskTracker &amp; DataNode)  ：我的开发机 ( tadev03  .quqi.com)</p>
<p>3)   两机均已安装 ssh   与  rsync</p>
<h2 id="-2-2-"><a href=""></a>2.2   测试程序</h2>
<p>1  ）  /home/workspace/hadoopExample/input/file01:</p>
<p>Hello World Bye World</p>
<p>2) /home/workspace/hadoopExample/input/file02:</p>
<p>Hello  Hadoop    Goodbye  Hadoop</p>
<ol>
<li>WordCount.java</li>
</ol>
<p><strong>package</strong>    com.TripResearch.hadoop;</p>
<p><strong>import</strong>   java.io.IOException;</p>
<p><strong>import</strong>   java.util.Iterator;</p>
<p><strong>import</strong>   java.util.StringTokenizer;</p>
<p><strong>import</strong>   org.apache.hadoop.fs.Path;</p>
<p><strong>import</strong>   org.apache.hadoop.io.IntWritable;</p>
<p><strong>import</strong>   org.apache.hadoop.io.LongWritable;</p>
<p><strong>import</strong>   org.apache.hadoop.io.Text;</p>
<p><strong>import</strong>   org.apache.hadoop.mapred. FileInputFormat  ;</p>
<p><strong>import</strong>   org.apache.hadoop.mapred.FileOutputFormat;</p>
<p><strong>import</strong>   org.apache.hadoop.mapred.JobClient;</p>
<p><strong>import</strong>   org.apache.hadoop.mapred. JobConf  ;</p>
<p><strong>import</strong>   org.apache.hadoop.mapred. MapReduceBase  ;</p>
<p><strong>import</strong>   org.apache.hadoop.mapred. Mapper  ;</p>
<p><strong>import</strong>   org.apache.hadoop.mapred.OutputCollector;</p>
<p><strong>import</strong>   org.apache.hadoop.mapred. Reducer  ;</p>
<p><strong>import</strong>   org.apache.hadoop.mapred.Reporter;</p>
<p><strong>import</strong>   org.apache.hadoop.mapred. TextInputFormat  ;</p>
<p><strong>import</strong>   org.apache.hadoop.mapred. TextOutputFormat  ;</p>
<p>//<em>/</em></p>
<p>/<em>  <em>*@author</em></em>    huazhang</p>
<p>/*/</p>
<p>@SuppressWarnings ( &quot;deprecation&quot; )</p>
<p><strong>public</strong>    <strong>class</strong>   WordCount {</p>
<p><strong>public</strong>    <strong>static</strong>    <strong>class</strong>   MyMap  <strong>extends</strong>    MapReduceBase    <strong>implements</strong></p>
<p>Mapper <LongWritable, Text, Text, IntWritable> {</p>
<p><strong>private</strong>    <strong>final</strong>    <strong>static</strong>   IntWritable  <em>one</em>   =  <strong>new</strong>   IntWritable(1);</p>
<p><strong>private</strong>   Text  word  =  <strong>new</strong>   Text();</p>
<p><strong>public</strong>    <strong>void</strong>   map(LongWritable key, Text value,</p>
<p>OutputCollector<Text, IntWritable> output, Reporter reporter)</p>
<p><strong>throws</strong>   IOException {</p>
<p>String line = value.toString();</p>
<p>StringTokenizer tokenizer =  <strong>new</strong>   StringTokenizer(line);</p>
<p><strong>while</strong>   (tokenizer.hasMoreTokens()) {</p>
<p>word .set(tokenizer.nextToken());</p>
<p>output.collect( word ,  <em>one</em>  );</p>
<p>}</p>
<p>}</p>
<p>}</p>
<p><strong>public</strong>    <strong>static</strong>    <strong>class</strong>   MyReduce  <strong>extends</strong>    MapReduceBase    <strong>implements</strong></p>
<p>Reducer <Text, IntWritable, Text, IntWritable> {</p>
<p><strong>public</strong>    <strong>void</strong>   reduce(Text key, Iterator<IntWritable> values,</p>
<p>OutputCollector<Text, IntWritable> output, Reporter reporter)</p>
<p><strong>throws</strong>   IOException {</p>
<p><strong>int</strong>   sum = 0;</p>
<p><strong>while</strong>   (values.hasNext()) {</p>
<p>sum += values.next().get();</p>
<p>}</p>
<p>output.collect(key,  <strong>new</strong>   IntWritable(sum));</p>
<p>}</p>
<p>}</p>
<p><strong>public</strong>    <strong>static</strong>    <strong>void</strong>   main(String[] args)  <strong>throws</strong>   Exception {</p>
<p>JobConf   conf =  <strong>new</strong>   JobConf(WordCount. <strong>class</strong>  );</p>
<p>conf.setJobName( &quot;wordcount&quot; );</p>
<p>conf.setOutputKeyClass(Text. <strong>class</strong>  );</p>
<p>conf.setOutputValueClass(IntWritable. <strong>class</strong>  );</p>
<p>conf.setMapperClass(MyMap. <strong>class</strong>  );</p>
<p>conf.setCombinerClass(MyReduce. <strong>class</strong>  );</p>
<p>conf.setReducerClass(MyReduce. <strong>class</strong>  );</p>
<p>conf.setInputFormat( TextInputFormat  . <strong>class</strong>  );</p>
<p>conf.setOutputFormat( TextOutputFormat  . <strong>class</strong>  );</p>
<p>FileInputFormat  . <em>setInputPaths</em>  (conf,  <strong>new</strong>   Path(args[0]));</p>
<p>FileOutputFormat. <em>setOutputPath</em>  (conf,  <strong>new</strong>   Path(args[1]));</p>
<p>JobClient.<em>runJob</em> (conf);</p>
<p>}</p>
<p>}</p>
<p><img src="" alt=""></p>
<h2 id="-2-3-"><a href=""></a>2.3   属性配置</h2>
<p>按下图所示修改至少 3  个属性, 如下图所示：</p>
<p>   <img src="" alt=""></p>
<ol>
<li></li>
<li><p>conf/core-site.xml</p>
</li>
</ol>
<configuration>

<property>

<name>fs.default.name</name>

<value>hdfs://zhanghua  .quqi.com:9000</value>

</property>

</configuration>

<p>注意：此处如果是伪集群模式可配置为  hdfs://localhost:9000 ,    是本地模式则为：  localhost:9000   。另外，其他输入输入路径，是本地模式是本地文件系统的路径，是非地模式，用 hdfs  文件系统的路径格式。</p>
<ol>
<li>conf/hdfs-site.xml</li>
</ol>
<configuration>

<property>

<name>dfs.replication</name>

<value>1</value>

</property>

<p></configuration></p>
<ol>
<li>conf/mapred-site.xml</li>
</ol>
<configuration>

<property>

<name>mapred.job.tracker</name>

<value>zhanghua  .quqi.com:8021</value>

</property>

<p></configuration></p>
<ol>
<li>masters</li>
</ol>
<p>zhanghua  .quqi.com (   伪分布模式就配成  localhost)</p>
<ol>
<li>slaves</li>
</ol>
<p>tadev03  .quqi.com  (   伪分布模式就配成 localhost)</p>
<ol>
<li>将以上配置好的 hadoop  文件夹拷到所有机器的相同目录下：</li>
</ol>
<p>scp -r /home/soft/hadoop-0.20.2 <a href="mailto:root@tadev03.daodao.com">root@tadev03</a>   <a href="mailto:root@tadev03.daodao.com">.quqi.com</a>  :/home/soft/hadoop-0.20.2</p>
<p>注意：确保两台机器的  JAVA_HOME   的路径一致，如果不一致，就要改 。</p>
<p>hadoop  所有可配置的配置文件说明如下：</p>
<p>hadoop-env.sh   运行 hadoop  的脚本中使用的环境变量</p>
<p>core-site.xml hadoop  的核心配置，如 HDFS  和 MapReduce  中很普遍的 I/O  设置</p>
<p>hdfs-site.xml HDFS  后台程序设置的配置：名称节点，第二名称节点及数据节点</p>
<p>mapred-site.xml MapReduce  后台程序设置的配置： jobtracker  和 tasktracker</p>
<p>masters   记录运行第二名称节点 的机器（一行一个）的列表</p>
<p>slaves   记录运行数据节点的机器（一行一个）的列表</p>
<h2 id="-2-4-ssh-"><a href=""></a>2.4   免密码 SSH  设置</h2>
<p>免密码  ssh   设置， 保证至少从   master    可以不用口令登陆所有的   slaves    。</p>
<p>1  ）生成密钥对： ssh-keygen -t rsa -P &#39;&#39; -f /root/.ssh/id_rsa (  这样密钥就留在了客户端 )</p>
<p>2)   将公钥拷到要连接的服务器，</p>
<p>scp /root/.ssh/id_rsa.pub root@tadev03  .quqi.com:/tmp</p>
<p>ssh -l root tadev03  .quqi.com</p>
<p>more /tmp/id_rsa.pub &gt;&gt; /root/.ssh/authorized_keys</p>
<ol>
<li>ssh tadev03  .quqi.com   不需要输入密码即为成功。</li>
</ol>
<p>（注意：伪分布模式也要配置  ssh localhost   无密码登录，如果是  mac   ，请将  ssh   打开）</p>
<p>(  另外，在 mac  中请在 hadoop-config.sh  文件中配置  export JAVA_HOME=/System/Library/Frameworks/JavaVM.framework/Versions/1.6/Home)</p>
<p>三条控制线线：</p>
<p>SSH →   这样就可以直接从主节点远程启动从节点上的脚本，如  ssh tadev03  .quqi.com &#39;/var/aa.sh&#39;</p>
<p>NameNode (<a href="http://localhost:50030/" target="_blank"><a href="http://localhost:50070">http://localhost:50070</a></a> ) → DataNode</p>
<p>JobTracker ( <a href="http://localhost:50030/" target="_blank"><a href="http://localhost:50030">http://localhost:50030</a></a> )→ TaskTracker (<a href="http://localhost:50030/" target="_blank"><a href="http://localhost:50060">http://localhost:50060</a></a> )</p>
<h2 id="-2-5-hosts"><a href=""></a>2.5   配置 hosts</h2>
<p>必须配置 master   和 slaves   之间的双向 hosts.   修改 /etc/hosts   进行配置，略。</p>
<h2 id="-2-6-hdfs-"><a href=""></a>2.6   格式化 HDFS  文件系统</h2>
<p>和我们常见的 NTFS  ， FAT32  文件系统一样， NDFS  最开始也是需要格式化的。格式化过程用来创建存储目录以及名称节点的永久数据结构的初始版本来创建一个空的文件系统。命令如下：</p>
<p>hadoop namenode -format</p>
<p>已知问题：在重新格式化时，可能会报： SHUTDOWN_MSG: Shutting down NameNode</p>
<p>解决办法： rm -rf /tmp/hadoop-root/dfs/name</p>
<h2 id="-2-7-"><a href=""></a>2.7   启动守护进程</h2>
<p>1    ）启动   HDFS    守护进程：    start-dfs.sh</p>
<p>(      start-dfs.sh    脚本会参照 NameNode    上 ${HADOOP_CONF_DIR}/slaves    文件的内容，在所有列出的 slave    上启动 DataNode    守护进程。   )</p>
<p>已知问题：在已设置   JAVA_HOME    的情况下仍会报：   Error: JAVA_HOME is not set</p>
<p>解决办法：我是在  hadoop.sh  文件中加下面一句解决的：</p>
<p>JAVA_HOME=/System/Library/Frameworks/JavaVM.framework/Versions/1.6/Home</p>
<p>2  ）启动  Map/Reduce  守护进程：   start-mapred.sh</p>
<p>(      start-mapred.sh   脚本会参照 JobTracker   上 ${HADOOP_CONF_DIR}/slaves   文件的内容，在所有列出的 slave   上启动 TaskTracker   守护进程  )</p>
<p>3)   启动成功后，可以通过访问  <a href="http://localhost:50030" target="_blank">http://localhost:50030</a>   验证。</p>
<p>注意：也可直接使用  start-all.sh       与  stop-all.sh       脚本  ,       在主节点   master    上面启动   hadoop    ，主节点会启动  /    停止所有从节点的   hadoop    。会启动  5       个   java        进程  ,        同时会在   /tmp        目录下创建五个   pid        文件记录这些进程   ID        号。通过这五个文件，可以得知   namenode, datanode, secondary namenode, jobtracker, tasktracker        分别对应于哪一个   Java        进程。</p>
<p>已知问题：启动后，日志中报：  java.io.IOException: File /tmp/hadoop-root/mapred/system/jobtracker.info could only be replicated to 0 nodes, instead of 1</p>
<p>解决办法：原因是    从  tadev03     .quqi.com       机器上无法  ping zhanghua     .quqi.com</p>
<h2 id="-2-8-"><a href=""></a>2.8   运行程序</h2>
<p>先将测试数据及其他输入由本地文件系统拷到  HFDS  文件系统中去（注意：   jar   除外 ）</p>
<ol>
<li></li>
<li><p>hadoop fs -mkdir input</p>
</li>
<li>hadoop fs -ls .</li>
<li>hadoop fs -copyFromLocal /home/workspace/hadoopExample/input/file01 input/file01</li>
<li>hadoop fs -copyFromLocal /home/workspace/hadoopExample/input/file02 input/file02</li>
</ol>
<p>这时候就可以执行下列命令运行程序了，注意：后面的input , output  等目录都是HDFS  文件系统的路径。(  如果是本地模式，就用本地文件系统的绝对路径）</p>
<ol>
<li></li>
</ol>
<p>hadoop     jar   /home/workspace/hadoopExample/hadoopExample.jar com.TripResearch.hadoop.WordCount input/ output</p>
<p>已知问题：在集群模式下运行时任务会Pending</p>
<p>最后，运行下列命令查看结果：</p>
<p>/home/soft/hadoop-0.20.2/bin/hadoop fs -cat output/part-00000</p>
<p>也可访问下列地址查看状态：</p>
<p>NameNode – <a href="http://localhost:50070/" target="_blank"><a href="http://zhanghua">http://zhanghua</a></a>   <a href="http://localhost:50070/" target="_blank">.quqi.com</a> <a href="http://localhost:50070/" target="_blank">:50070/</a></p>
<p>JobTracker - <a href="http://localhost:50030/" target="_blank"><a href="http://zhanghua">http://zhanghua</a></a>   <a href="http://localhost:50030/" target="_blank">.quqi.com</a> <a href="http://localhost:50030/" target="_blank">:50030/</a></p>
<p>常用命令说明如下：</p>
<p>hadoop dfs –ls   查看 /usr/root  目录下的内容径；
hadoop dfs –rmr xxx xxx  就是删除目录；
hadoop dfsadmin -report   这个命令可以全局的查看 DataNode  的情况；
hadoop job -list   后面增加参数是对于当前运行的 Job  的操作，例如 list,kill  等；
hadoop balancer   均衡磁盘负载的命令。</p>
<h1 id="-3-hadoop-"><a href=""></a>3 hadoop  高级进阶</h1>
<h1 id="-4-hadoop-"><a href=""></a>4 hadoop  应用案例</h1>
<h1 id="-5-"><a href=""></a>5   参考文献</h1>
<ol>
<li><a href="http://hadoop.apache.org/common/docs/r0.18.2/cn/" target="_blank"><a href="http://hadoop.apache.org/common/docs/r0.18.2/cn/">http://hadoop.apache.org/common/docs/r0.18.2/cn/</a></a></li>
<li>hadoop 0.20.2  集群配置入门 <a href="http://dev.firnow.com/course/3_program/java/javajs/20100719/453042.html" target="_blank"><a href="http://dev.firnow.com/course/3_program/java/javajs/">http://dev.firnow.com/course/3_program/java/javajs/</a></a></li>
<li>Hadoop 分布式文件系统（HDFS ）初步实践 <a href="http://huatai.me/?p=352" target="_blank"><a href="http://huatai.me/?p=352">http://huatai.me/?p=352</a></a></li>
<li>Hadoop 分布式部署实验2_ 格式化分布式文件系统 <a href="http://hi.baidu.com/thinke365/blog/item/15602aa8f9074cf41e17a235.html" target="_blank"><a href="http://hi.baidu.com/thinke365/blog/item/15602aa8f9074cf41e17a235.html">http://hi.baidu.com/thinke365/blog/item/15602aa8f9074cf41e17a235.html</a></a></li>
<li>hadoop 安装出现问题（紧急），请前辈指教 <a href="http://forum.hadoop.tw/viewtopic.php?f=4&amp;t=90" target="_blank"><a href="http://forum.hadoop.tw/viewtopic.php?f=4&amp;t=90">http://forum.hadoop.tw/viewtopic.php?f=4&amp;t=90</a></a></li>
<li>用 Hadoop  进行分布式并行编程 <a href="http://www.ibm.com/developerworks/cn/opensource/os-cn-hadoop1/index.html" target="_blank"><a href="http://www.ibm.com/developerworks/cn/opensource/os-cn-hadoop1/index.html">http://www.ibm.com/developerworks/cn/opensource/os-cn-hadoop1/index.html</a></a></li>
<li>用 Hadoop  进行分布式数据处理 <a href="http://tech.ddvip.com/2010-06/1275983295155033.html" target="_blank"><a href="http://tech.ddvip.com/2010-06/1275983295155033.html">http://tech.ddvip.com/2010-06/1275983295155033.html</a></a></li>
</ol>
<p>分享到： <a href="&quot;分享到新浪微博&quot;"></a><a href="&quot;分享到腾讯微博&quot;"></a></p>
<ol>
<li>上一篇：<a href="http://blog.csdn.net/quqi99/article/details/6160846" target="_blank">Lucene Scoring 评分机制 （ by quqi99 )</a></li>
<li><p>下一篇：<a href="http://blog.csdn.net/quqi99/article/details/6292472" target="_blank">深入理解各JEE服务器Web层集群原理 ( by quqi99 )</a>
查看评论<a href=""></a></p>
<p>暂无评论
您还没有登录,请<a href="">[登录]</a>或<a href="http://passport.csdn.net/account/register?from=http%3A%2F%2Fblog.csdn.net%2Fquqi99%2Farticle%2Fdetails%2F6291788" target="_blank">[注册]</a></p>
</li>
</ol>
<p>/* 以上用户言论只代表其个人观点，不代表CSDN网站的观点或立场<a href=""></a><a href=""></a>
<a href="&quot;回到顶部&quot;"><img src="" alt="TOP"></a></p>
<p>个人资料</p>
<p><a href="http://my.csdn.net/quqi99" target="_blank"><img src="&quot;访问我的空间&quot;" alt=""></a>
<a href="http://my.csdn.net/quqi99" target="_blank">quqi99</a></p>
<p><a href="&quot;[加关注]&quot;"></a> <a href="&quot;[发私信]&quot;"></a>
<a href="http://medal.blog.csdn.net/allmedal.aspx" target="_blank"><img src="" alt=""></a></p>
<ul>
<li>访问：198660次</li>
<li>积分：3337分</li>
<li><p>排名：第1895名</p>
</li>
<li><p>原创：146篇</p>
</li>
<li>转载：23篇</li>
<li>译文：0篇</li>
<li>评论：123条</li>
</ul>
<p>文章搜索</p>
<p><a href=""></a></p>
<p>文章分类</p>
<ul>
<li><a href="http://blog.csdn.net/quqi99/article/category/875141" target="_blank">VM / Cloud</a>(7)</li>
<li><a href="http://blog.csdn.net/quqi99/article/category/557281" target="_blank">Middleware / Java AppServer</a>(13)</li>
<li><a href="http://blog.csdn.net/quqi99/article/category/328029" target="_blank">Seach Engine</a>(5)</li>
<li><a href="http://blog.csdn.net/quqi99/article/category/674417" target="_blank">Linux / Unix / Shell</a>(24)</li>
<li><a href="http://blog.csdn.net/quqi99/article/category/328188" target="_blank">J2SE / JEE</a>(40)</li>
<li><a href="http://blog.csdn.net/quqi99/article/category/347580" target="_blank">DB / NoSQL</a>(9)</li>
<li><a href="http://blog.csdn.net/quqi99/article/category/803236" target="_blank">Architecture</a>(0)</li>
<li><a href="http://blog.csdn.net/quqi99/article/category/351802" target="_blank">Android</a>(3)</li>
<li><a href="http://blog.csdn.net/quqi99/article/category/803239" target="_blank">Life</a>(4)</li>
<li><a href="http://blog.csdn.net/quqi99/article/category/689016" target="_blank">Other</a>(9)</li>
<li><a href="http://blog.csdn.net/quqi99/article/category/1112756" target="_blank">OpenStack</a>(37)</li>
<li><a href="http://blog.csdn.net/quqi99/article/category/1139084" target="_blank">Python</a>(1)</li>
<li><a href="http://blog.csdn.net/quqi99/article/category/1167554" target="_blank">C / C++</a>(2)</li>
<li><p><a href="http://blog.csdn.net/quqi99/article/category/1490633" target="_blank">Networking</a>(1)
文章存档</p>
</li>
<li><p><a href="http://blog.csdn.net/quqi99/article/month/2013/08" target="_blank">2013年08月</a>(4)</p>
</li>
<li><a href="http://blog.csdn.net/quqi99/article/month/2013/07" target="_blank">2013年07月</a>(13)</li>
<li><a href="http://blog.csdn.net/quqi99/article/month/2013/06" target="_blank">2013年06月</a>(6)</li>
<li><a href="http://blog.csdn.net/quqi99/article/month/2013/05" target="_blank">2013年05月</a>(2)</li>
<li><a href="http://blog.csdn.net/quqi99/article/month/2013/04" target="_blank">2013年04月</a>(2)</li>
<li><a href="http://blog.csdn.net/quqi99/article/month/2013/03" target="_blank">2013年03月</a>(3)</li>
<li><a href="http://blog.csdn.net/quqi99/article/month/2013/02" target="_blank">2013年02月</a>(1)</li>
<li><a href="http://blog.csdn.net/quqi99/article/month/2013/01" target="_blank">2013年01月</a>(9)</li>
<li><a href="http://blog.csdn.net/quqi99/article/month/2012/12" target="_blank">2012年12月</a>(3)</li>
<li><a href="http://blog.csdn.net/quqi99/article/month/2012/11" target="_blank">2012年11月</a>(2)</li>
<li><a href="http://blog.csdn.net/quqi99/article/month/2012/08" target="_blank">2012年08月</a>(1)</li>
<li><a href="http://blog.csdn.net/quqi99/article/month/2012/07" target="_blank">2012年07月</a>(1)</li>
<li><a href="http://blog.csdn.net/quqi99/article/month/2012/06" target="_blank">2012年06月</a>(7)</li>
<li><a href="http://blog.csdn.net/quqi99/article/month/2012/05" target="_blank">2012年05月</a>(2)</li>
<li><a href="http://blog.csdn.net/quqi99/article/month/2012/04" target="_blank">2012年04月</a>(6)</li>
<li><a href="http://blog.csdn.net/quqi99/article/month/2012/03" target="_blank">2012年03月</a>(2)</li>
<li><a href="http://blog.csdn.net/quqi99/article/month/2012/02" target="_blank">2012年02月</a>(1)</li>
<li><a href="http://blog.csdn.net/quqi99/article/month/2011/12" target="_blank">2011年12月</a>(1)</li>
<li><a href="http://blog.csdn.net/quqi99/article/month/2011/09" target="_blank">2011年09月</a>(5)</li>
<li><a href="http://blog.csdn.net/quqi99/article/month/2011/08" target="_blank">2011年08月</a>(2)</li>
<li><a href="http://blog.csdn.net/quqi99/article/month/2011/06" target="_blank">2011年06月</a>(2)</li>
<li><a href="http://blog.csdn.net/quqi99/article/month/2011/04" target="_blank">2011年04月</a>(5)</li>
<li><a href="http://blog.csdn.net/quqi99/article/month/2011/03" target="_blank">2011年03月</a>(2)</li>
<li><a href="http://blog.csdn.net/quqi99/article/month/2011/01" target="_blank">2011年01月</a>(4)</li>
<li><a href="http://blog.csdn.net/quqi99/article/month/2010/12" target="_blank">2010年12月</a>(1)</li>
<li><a href="http://blog.csdn.net/quqi99/article/month/2010/07" target="_blank">2010年07月</a>(1)</li>
<li><a href="http://blog.csdn.net/quqi99/article/month/2010/05" target="_blank">2010年05月</a>(1)</li>
<li><a href="http://blog.csdn.net/quqi99/article/month/2010/04" target="_blank">2010年04月</a>(2)</li>
<li><a href="http://blog.csdn.net/quqi99/article/month/2010/03" target="_blank">2010年03月</a>(2)</li>
<li><a href="http://blog.csdn.net/quqi99/article/month/2010/02" target="_blank">2010年02月</a>(4)</li>
<li><a href="http://blog.csdn.net/quqi99/article/month/2010/01" target="_blank">2010年01月</a>(6)</li>
<li><a href="http://blog.csdn.net/quqi99/article/month/2009/12" target="_blank">2009年12月</a>(6)</li>
<li><a href="http://blog.csdn.net/quqi99/article/month/2009/11" target="_blank">2009年11月</a>(5)</li>
<li><a href="http://blog.csdn.net/quqi99/article/month/2009/10" target="_blank">2009年10月</a>(6)</li>
<li><a href="http://blog.csdn.net/quqi99/article/month/2009/09" target="_blank">2009年09月</a>(1)</li>
<li><a href="http://blog.csdn.net/quqi99/article/month/2009/08" target="_blank">2009年08月</a>(1)</li>
<li><a href="http://blog.csdn.net/quqi99/article/month/2009/06" target="_blank">2009年06月</a>(4)</li>
<li><a href="http://blog.csdn.net/quqi99/article/month/2009/03" target="_blank">2009年03月</a>(1)</li>
<li><a href="http://blog.csdn.net/quqi99/article/month/2008/11" target="_blank">2008年11月</a>(1)</li>
<li><a href="http://blog.csdn.net/quqi99/article/month/2008/10" target="_blank">2008年10月</a>(2)</li>
<li><a href="http://blog.csdn.net/quqi99/article/month/2008/08" target="_blank">2008年08月</a>(1)</li>
<li><a href="http://blog.csdn.net/quqi99/article/month/2008/06" target="_blank">2008年06月</a>(3)</li>
<li><a href="http://blog.csdn.net/quqi99/article/month/2008/04" target="_blank">2008年04月</a>(1)</li>
<li><a href="http://blog.csdn.net/quqi99/article/month/2008/03" target="_blank">2008年03月</a>(1)</li>
<li><a href="http://blog.csdn.net/quqi99/article/month/2008/02" target="_blank">2008年02月</a>(1)</li>
<li><a href="http://blog.csdn.net/quqi99/article/month/2008/01" target="_blank">2008年01月</a>(3)</li>
<li><a href="http://blog.csdn.net/quqi99/article/month/2007/12" target="_blank">2007年12月</a>(4)</li>
<li><a href="http://blog.csdn.net/quqi99/article/month/2007/11" target="_blank">2007年11月</a>(1)</li>
<li><a href="http://blog.csdn.net/quqi99/article/month/2007/10" target="_blank">2007年10月</a>(4)</li>
<li><a href="http://blog.csdn.net/quqi99/article/month/2007/08" target="_blank">2007年08月</a>(5)</li>
<li><a href="http://blog.csdn.net/quqi99/article/month/2007/07" target="_blank">2007年07月</a>(6)</li>
<li><a href="http://blog.csdn.net/quqi99/article/month/2007/06" target="_blank">2007年06月</a>(1)</li>
<li><a href="http://blog.csdn.net/quqi99/article/month/2007/05" target="_blank">2007年05月</a>(8)</li>
</ul>
<p>展开</p>
<p>阅读排行</p>
<ul>
<li><a href="http://blog.csdn.net/quqi99/article/details/1916553" title="Android学习笔记 ( by quqi99 )" target="_blank">Android学习笔记 ( by quqi99 )</a>(22132)</li>
<li><a href="http://blog.csdn.net/quqi99/article/details/1680647" title="java.lang.NoClassDefFoundError: org/apache/commons/io/output/DeferredFileOutputStream" target="_blank">java.lang.NoClassDefFoundError: org/apache/commons/io/output/DeferredFileOutputStream</a>(17851)</li>
<li><a href="http://blog.csdn.net/quqi99/article/details/7433285" title="建立openstack quantum开发环境" target="_blank">建立openstack quantum开发环境</a>(6747)</li>
<li><a href="http://blog.csdn.net/quqi99/article/details/1755642" title="在linux中安装字库( by quqi99 )" target="_blank">在linux中安装字库( by quqi99 )</a>(5151)</li>
<li><a href="http://blog.csdn.net/quqi99/article/details/1930304" title="Android手机客户端与Servlet交换数据(by quqi99)" target="_blank">Android手机客户端与Servlet交换数据(by quqi99)</a>(5140)</li>
<li><a href="http://blog.csdn.net/quqi99/article/details/5298017" title="ReentrantLock与synchronized的区别 ( by quqi99 )" target="_blank">ReentrantLock与synchronized的区别 ( by quqi99 )</a>(4864)</li>
<li><a href="http://blog.csdn.net/quqi99/article/details/5604732" title="个人户口档案转移笔记（适用北京集体户口）" target="_blank">个人户口档案转移笔记（适用北京集体户口）</a>(4802)</li>
<li><a href="http://blog.csdn.net/quqi99/article/details/1624225" title="Magnolia学习笔记（一个基于JSR170的内容管理系统） ( by quqi99 )" target="_blank">Magnolia学习笔记（一个基于JSR170的内容管理系统） ( by quqi99 )</a>(4342)</li>
<li><a href="http://blog.csdn.net/quqi99/article/details/1624218" title="JSpider学习笔记 ( by quqi99 )" target="_blank">JSpider学习笔记 ( by quqi99 )</a>(4149)</li>
<li><p><a href="http://blog.csdn.net/quqi99/article/details/3099945" title="Plone学习笔记 ( by quqi99 )" target="_blank">Plone学习笔记 ( by quqi99 )</a>(4057)
评论排行</p>
</li>
<li><p><a href="http://blog.csdn.net/quqi99/article/details/1916553" title="Android学习笔记 ( by quqi99 )" target="_blank">Android学习笔记 ( by quqi99 )</a>(21)</p>
</li>
<li><a href="http://blog.csdn.net/quqi99/article/details/1680647" title="java.lang.NoClassDefFoundError: org/apache/commons/io/output/DeferredFileOutputStream" target="_blank">java.lang.NoClassDefFoundError: org/apache/commons/io/output/DeferredFileOutputStream</a>(18)</li>
<li><a href="http://blog.csdn.net/quqi99/article/details/1930304" title="Android手机客户端与Servlet交换数据(by quqi99)" target="_blank">Android手机客户端与Servlet交换数据(by quqi99)</a>(12)</li>
<li><a href="http://blog.csdn.net/quqi99/article/details/5604732" title="个人户口档案转移笔记（适用北京集体户口）" target="_blank">个人户口档案转移笔记（适用北京集体户口）</a>(8)</li>
<li><a href="http://blog.csdn.net/quqi99/article/details/1624225" title="Magnolia学习笔记（一个基于JSR170的内容管理系统） ( by quqi99 )" target="_blank">Magnolia学习笔记（一个基于JSR170的内容管理系统） ( by quqi99 )</a>(7)</li>
<li><a href="http://blog.csdn.net/quqi99/article/details/2591768" title="使用itext生成word格式的报表(by quqi99)" target="_blank">使用itext生成word格式的报表(by quqi99)</a>(5)</li>
<li><a href="http://blog.csdn.net/quqi99/article/details/1755642" title="在linux中安装字库( by quqi99 )" target="_blank">在linux中安装字库( by quqi99 )</a>(4)</li>
<li><a href="http://blog.csdn.net/quqi99/article/details/6305061" title="Android分享文稿 ( by quqi99 )" target="_blank">Android分享文稿 ( by quqi99 )</a>(4)</li>
<li><a href="http://blog.csdn.net/quqi99/article/details/9156497" title="OpenDaylight学习 ( by quqi99 )" target="_blank">OpenDaylight学习 ( by quqi99 )</a>(3)</li>
<li><a href="http://blog.csdn.net/quqi99/article/details/2590703" title="使用jacob生成word(by quqi99)" target="_blank">使用jacob生成word(by quqi99)</a>(3)</li>
</ul>
<p>推荐文章
最新评论</p>
<ul>
<li><a href="http://blog.csdn.net/quqi99/article/details/9156497#comments" target="_blank">OpenDaylight学习 ( by quqi99 )</a></li>
</ul>
<p><a href="http://blog.csdn.net/quqi99" target="_blank">quqi99</a>: hi whoeversucks, 谢谢你的实时信息，非常有用，我已经更新到博客里了。另外，问个问题，...</p>
<ul>
<li><a href="http://blog.csdn.net/quqi99/article/details/9156497#comments" target="_blank">OpenDaylight学习 ( by quqi99 )</a></li>
</ul>
<p><a href="http://blog.csdn.net/whoeversucks" target="_blank">whoeversucks</a>: 注意，OpenDayLight Controller和OSCP实际上2个独立的SDN控制器项目（分别...</p>
<ul>
<li><a href="http://blog.csdn.net/quqi99/article/details/9102745#comments" target="_blank">给Linux虚机扩充硬盘空间 ( by quqi99 )</a></li>
</ul>
<p><a href="http://blog.csdn.net/quqi99" target="_blank">quqi99</a>: hi dalinhuang, 谢谢你的回复，你给的这个方法是只适合LVM场景的啊，我没有使用LVM。</p>
<ul>
<li><a href="http://blog.csdn.net/quqi99/article/details/9102745#comments" target="_blank">给Linux虚机扩充硬盘空间 ( by quqi99 )</a></li>
</ul>
<p><a href="http://blog.csdn.net/dalinhuang" target="_blank">dalinhuang</a>: 给根（/）扩充的步骤：（以你的virtualbox并使用LVM为例）1. 新增一块虚拟硬盘，给虚机。...</p>
<ul>
<li><a href="http://blog.csdn.net/quqi99/article/details/9156497#comments" target="_blank">OpenDaylight学习 ( by quqi99 )</a></li>
</ul>
<p><a href="http://blog.csdn.net/piaochenping" target="_blank">piaochenping</a>: 你好，为什么我安装时老是出现这个错误呢？ Failed to execute goal org.co...</p>
<ul>
<li><a href="http://blog.csdn.net/quqi99/article/details/7659905#comments" target="_blank">OpenStack CI测试之devstack-gate ( by quqi99 )</a></li>
</ul>
<p><a href="http://blog.csdn.net/quqi99" target="_blank">quqi99</a>: @sunyilong2012: 这种错误应该是差模块吧，可以单独安装一下试试, sudo pip i...</p>
<ul>
<li><a href="http://blog.csdn.net/quqi99/article/details/7411091#comments" target="_blank">Fedora 16上源码建立pydev + eclipse的OpenStack开发环境笔记草稿 ( by quqi99 )</a></li>
</ul>
<p><a href="http://blog.csdn.net/quqi99" target="_blank">quqi99</a>: openstack因为用到了一些linux特有的东西，如iptables，所以目前只能跑在linux...</p>
<ul>
<li><a href="http://blog.csdn.net/quqi99/article/details/7411091#comments" target="_blank">Fedora 16上源码建立pydev + eclipse的OpenStack开发环境笔记草稿 ( by quqi99 )</a></li>
</ul>
<p><a href="http://blog.csdn.net/javaerss" target="_blank">javaerss</a>: 大神...看哭了，为此特地跑去下载fedora 16来做实验。之前用ubuntu下用eclipse ...</p>
<ul>
<li><a href="http://blog.csdn.net/quqi99/article/details/6576375#comments" target="_blank">玩转play framework ( by quqi99 )</a></li>
</ul>
<p><a href="http://blog.csdn.net/nanfu08" target="_blank">nanfu08</a>: 你能看得清，如果只是自己看的话我没话说，这样的文字叫人怎么读？？</p>
<ul>
<li><a href="http://blog.csdn.net/quqi99/article/details/7659905#comments" target="_blank">OpenStack CI测试之devstack-gate ( by quqi99 )</a></li>
</ul>
<p><a href="http://blog.csdn.net/sunyilong2012" target="_blank">dragonsun</a>: 您好，我在做这个测试的时候遇到了无法导入statsd的问题，请问您有解决的方法吗？+ /home/j...</p>
<p><a href="http://www.csdn.net/company/about.html" target="_blank">公司简介</a>|<a href="http://www.csdn.net/company/recruit.html" target="_blank">招贤纳士</a>|<a href="http://www.csdn.net/company/marketing.html" target="_blank">广告服务</a>|<a href="http://www.csdn.net/company/account.html" target="_blank">银行汇款帐号</a>|<a href="http://www.csdn.net/company/contact.html" target="_blank">联系方式</a>|<a href="http://www.csdn.net/company/statement.html" target="_blank">版权声明</a>|<a href="http://www.csdn.net/company/layer.html" target="_blank">法律顾问</a>|<a href="mailto:webmaster@csdn.net">问题报告</a><a href="http://wpa.qq.com/msgrd?v=3&amp;uin=2355263776&amp;site=qq&amp;menu=yes" target="_blank">QQ客服</a> <a href="http://e.weibo.com/csdnsupport/profile" target="_blank">微博客服</a> <a href="http://bbs.csdn.net/forums/Service" target="_blank">论坛反馈</a> <a href="mailto:webmaster@csdn.net">联系邮箱：webmaster@csdn.net</a> 服务热线：400-600-2320京 ICP 证 070598 号北京创新乐知信息技术有限公司 版权所有世纪乐知(北京)网络技术有限公司 提供技术支持江苏乐知网络技术有限公司 提供商务支持Copyright © 1999-2012, CSDN.NET, All Rights Reserved <a href="http://www.hd315.gov.cn/beian/view.asp?bianhao=010202001032100010" target="_blank"><img src="" alt="GongshangLogo"></a>
<img src="http://counter.csdn.net/pv.aspx?id=24" alt=""></p>

      
    </div>
    
    
      
    
    
        <footer id="post-meta">
        <span class="categories">Posted in<span class="breadcrumb fa fa-folder"><li><a href="/categories/hadoop/">hadoop</a></li></span></span> | <span class="tags">Tagged <a href="/tags/hadoop/" class="label label-primary">hadoop</a></span> | <span class="time">recent updated:<time title="2014-03-07 09:54:36"datetime="2014-03-07 09:54:36"> mar. 7 2014</time></span> | <span class="comment-link">
<a href="http://itsolife.com/2014/02/02/2014-02-02-hadoop--Hadoop知识分享文稿byquqi99-技术并艺术着/#comments" class="ds-thread-count comment-link" data-thread-key="2014-02-02-hadoop--Hadoop知识分享文稿byquqi99-技术并艺术着" data-count-type="comments">暂无评论</a></span>
        </footer>
    
    <div class="clearfix"></div>
  </div>
</div>
</article>



  <article>
<div id="post" class="post well">
  <div class="post-content">
    <header class="well-sm">
      <i class="fa icon fa-5x pull-left"></i>
      <h1 class="title"><a href="/2014/02/02/2014-02-02-hadoop--Hadoop集群_Hadoop安装配置/">Hadoop集群_Hadoop安装配置</a></h1>
      
        <span>Posted on<time datetime="2014-02-02T01:54:36.000Z"> <a href="/2014/02/02/2014-02-02-hadoop--Hadoop集群_Hadoop安装配置/">feb. 2 2014</a></time></span>      
    </header>
    
    
    <div class="entry">
      
        <h1 id="hadoop-_hadoop-">Hadoop集群_Hadoop安装配置</h1>
<h1 id="-hadoop-5-_hadoop-http-www-cnblogs-com-xia520pi-archive-2012-05-16-2503949-html-"><a href="http://www.cnblogs.com/xia520pi/archive/2012/05/16/2503949.html" target="_blank">Hadoop集群（第5期）_Hadoop安装配置</a></h1>
<h2 id="1-">1、集群部署介绍</h2>
<h3 id="1-1-hadoop-">1.1 Hadoop简介</h3>
<p><img src="" alt="">　　Hadoop是Apache软件基金会旗下的一个开源分布式计算平台。以Hadoop分布式文件系统（HDFS，Hadoop Distributed Filesystem）和MapReduce（Google MapReduce的开源实现）为<strong>核心</strong>的Hadoop为用户提供了系统底层细节透明的分布式基础架构。</p>
<p>对于Hadoop的集群来讲，可以分成两大类角色：Master和Salve。一个<strong>HDFS</strong>集群是由一个NameNode和若干个DataNode组成的。其中NameNode作为主服务器，管理文件系统的命名空间和客户端对文件系统的访问操作；集群中的DataNode管理存储的数据。<strong>MapReduce</strong>框架是由一个单独运行在主节点上的JobTracker和运行在每个集群从节点的TaskTracker共同组成的。主节点负责调度构成一个作业的所有任务，这些任务分布在不同的从节点上。主节点监控它们的执行情况，并且重新执行之前的失败任务；从节点仅负责由主节点指派的任务。当一个Job被提交时，JobTracker接收到提交作业和配置信息之后，就会将配置信息等分发给从节点，同时调度任务并监控TaskTracker的执行。</p>
<p>从上面的介绍可以看出，HDFS和MapReduce共同组成了Hadoop分布式系统体系结构的核心。<strong>HDFS</strong>在集群上实现分布式文件系统，<strong>MapReduce</strong>在集群上实现了分布式计算和任务处理。HDFS在MapReduce任务处理过程中提供了文件操作和存储等支持，MapReduce在HDFS的基础上实现了任务的分发、跟踪、执行等工作，并收集结果，二者相互作用，完成了Hadoop分布式集群的主要任务。</p>
<h3 id="1-2-">1.2 环境说明</h3>
<p>集群中包括4个节点：1个Master，3个Salve，节点之间局域网连接，可以相互ping通，具体集群信息可以查看&quot;<strong>Hadoop集群（第2期）</strong>&quot;。节点IP地址分布如下：</p>
<p><strong>机器名称</strong></p>
<p><strong>IP地址</strong>Master.Hadoop</p>
<p>192.168.1.2Salve1.Hadoop</p>
<p>192.168.1.3Salve2.Hadoop</p>
<p>192.168.1.4Salve3.Hadoop</p>
<p>192.168.1.5</p>
<p>四个节点上均是CentOS6.0系统，并且有一个相同的用户<strong>hadoop</strong>。Master机器主要配置NameNode和JobTracker的角色，负责总管分布式数据和分解任务的执行；3个Salve机器配置DataNode和TaskTracker的角色，负责分布式数据存储以及任务的执行。其实应该还应该有1个Master机器，用来作为<strong>备用</strong>，以防止Master服务器<strong>宕机</strong>，还有一个备用马上启用。后续经验积累一定阶段后<strong>补上</strong>一台备用Master机器。</p>
<h3 id="1-3-">1.3 网络配置</h3>
<p>Hadoop集群要按照<strong>1.2小节</strong>表格所示进行配置，我们在&quot;<strong>Hadoop集群（第1期）</strong>&quot;的CentOS6.0安装过程就按照提前规划好的主机名进行安装和配置。如果实验室后来人在安装系统时，没有配置好，不要紧，没有必要重新安装，在安装完系统之后仍然可以根据后来的规划对机器的主机名进行修改。</p>
<p>下面的例子我们将以Master机器为例，即主机名为&quot;Master.Hadoop&quot;，IP为&quot;192.168.1.2&quot;进行一些主机名配置的相关操作。其他的Slave机器以此为依据进行修改。</p>
<p><strong>1）查看当前机器名称</strong></p>
<p>用下面命令进行显示机器名称，如果跟规划的不一致，要按照下面进行修改。</p>
<p>hostname</p>
<p><img src="" alt=""></p>
<p>上图中，用&quot;hostname&quot;查&quot;Master&quot;机器的名字为&quot;Master.Hadoop&quot;，与我们预先规划的一致。</p>
<p><strong>2）修改当前机器名称</strong></p>
<p><strong>假定</strong>我们发现我们的机器的主机名不是我们想要的，通过对&quot;<strong>/etc/sysconfig/network</strong>&quot;文件修改其中&quot;<strong>HOSTNAME</strong>&quot;后面的值，改成我们规划的名称。</p>
<p>这个&quot;<strong>/etc/sysconfig/network</strong>&quot;文件是定义hostname和是否利用网络的不接触网络设备的对系统全体定义的文件。</p>
<p><strong>设定形式</strong>：设定值=值</p>
<p>&quot;/etc/sysconfig/network&quot;的<strong>设定项目</strong>如下：</p>
<p>NETWORKING 是否利用网络</p>
<p>GATEWAY 默认网关</p>
<p>IPGATEWAYDEV 默认网关的接口名</p>
<p>HOSTNAME 主机名</p>
<p>DOMAIN 域名</p>
<p>用下面命令进行修改当前机器的主机名（<strong>备注：</strong>修改系统文件一般用<strong>root</strong>用户）</p>
<p>vim /etc/sysconfig/network</p>
<p><img src="" alt=""></p>
<p>通过上面的命令我们从&quot;/etc/sysconfig/network&quot;中找到&quot;HOSTNAME&quot;进行修改，查看内容如下：</p>
<p><img src="" alt=""></p>
<p><strong>3）修改当前机器IP</strong></p>
<p><strong>假定</strong>我们的机器连IP在当时安装机器时都没有配置好，那此时我们需要对&quot;<strong>ifcfg-eth0</strong>&quot;文件进行配置，该文件位于&quot;<strong>/etc/sysconfig/network-scripts</strong>&quot;文件夹下。</p>
<p>在这个目录下面，存放的是网络接口（网卡）的制御脚本文件（控制文件），ifcfg- eth0是默认的第一个网络接口，如果机器中有多个网络接口，那么名字就将依此类推ifcfg-eth1，ifcfg-eth2，ifcfg- eth3，……。</p>
<p>这里面的文件是相当重要的，涉及到网络能否正常工作。</p>
<p>设定形式：设定值=值</p>
<p>设定项目项目如下：</p>
<p>DEVICE 接口名（设备,网卡）</p>
<p>BOOTPROTO IP的配置方法（static:固定IP， dhcpHCP， none:手动）</p>
<p>HWADDR MAC地址</p>
<p>ONBOOT 系统启动的时候网络接口是否有效（yes/no）</p>
<p>TYPE 网络类型（通常是Ethemet）</p>
<p>NETMASK 网络掩码</p>
<p><strong>IPADDR</strong> IP地址</p>
<p>IPV6INIT IPV6是否有效（yes/no）</p>
<p>GATEWAY 默认网关IP地址</p>
<p>查看&quot;/etc/sysconfig/network-scripts/ifcfg-eth0&quot;内容，如果IP不复核，就行修改。</p>
<p><img src="" alt=""></p>
<p>如果上图中IP与规划不相符，用下面命令进行修改：</p>
<p>vim /etc/sysconfig/network-scripts/ifcgf-eth0</p>
<p>修改完之后可以用&quot;ifconfig&quot;进行查看。</p>
<p><img src="" alt=""></p>
<p><strong>4）配置hosts文件（必须）</strong></p>
<p>&quot;<strong>/etc/hosts</strong>&quot;这个文件是用来配置主机将用的<strong>DNS</strong>服务器信息，是记载LAN内接续的各主机的对应[HostName和IP]用的。当用户在进行网络连接时，首先查找该文件，寻找对应主机名（或域名）对应的IP地址。</p>
<p>我们要测试两台机器之间知否连通，一般用&quot;ping 机器的IP&quot;，如果想用&quot;ping 机器的主机名&quot;发现找不见该名称的机器，解决的办法就是修改&quot;<strong>/etc/hosts</strong>&quot;这个文件，通过把LAN内的各主机的IP地址和HostName的<strong>一一对应</strong>写入这个文件的时候，就可以解决问题。</p>
<p>例如：机器为&quot;Master.Hadoop:192.168.1.2&quot;对机器为&quot;Salve1.Hadoop:192.168.1.3&quot;用命令&quot;ping&quot;记性连接测试。测试结果如下：</p>
<p><img src="" alt=""></p>
<p>从上图中的值，直接对IP地址进行测试，能够ping通，但是对主机名进行测试，发现没有ping通，提示&quot;unknown host——未知主机&quot;，这时查看&quot;Master.Hadoop&quot;的&quot;/etc/hosts&quot;文件内容。</p>
<p><img src="" alt=""></p>
<p>发现里面没有&quot;192.168.1.3 Slave1.Hadoop&quot;内容，故而本机器是无法对机器的主机名为&quot;Slave1.Hadoop&quot; 解析。</p>
<p>在进行<strong>Hadoop集群</strong>配置中，需要在&quot;/etc/hosts&quot;文件中添加集群中所有机器的IP与主机名，这样Master与所有的Slave机器之间不仅可以通过IP进行通信，而且还可以通过主机名进行通信。所以在所有的机器上的&quot;/etc/hosts&quot;文件<strong>末尾</strong>中都要添加如下内容：</p>
<p>192.168.1.2 Master.Hadoop</p>
<p>192.168.1.3 Slave1.Hadoop</p>
<p>192.168.1.4 Slave2.Hadoop</p>
<p>192.168.1.5 Slave3.Hadoop</p>
<p>用以下命令进行添加：</p>
<p>vim /etc/hosts</p>
<p><img src="" alt=""></p>
<p>添加结果如下：</p>
<p><img src="" alt=""></p>
<p>现在我们在进行对机器为&quot;Slave1.Hadoop&quot;的主机名进行ping通测试，看是否能测试成功。</p>
<p><img src="" alt=""></p>
<p>从上图中我们已经能用主机名进行ping通了，说明我们刚才添加的内容，在局域网内能进行DNS解析了，那么现在剩下的事儿就是在其余的Slave机器上进行相同的配置。然后进行测试。（<strong>备注：</strong>当设置SSH无密码验证后，可以&quot;scp&quot;进行复制，然后把原来的&quot;hosts&quot;文件执行覆盖即可。）</p>
<h3 id="1-4-">1.4 所需软件</h3>
<p><strong>1）JDK软件</strong></p>
<p>下载地址：<a href="http://www.oracle.com/technetwork/java/javase/index.html" target="_blank"><a href="http://www.oracle.com/technetwork/java/javase/index.html">http://www.oracle.com/technetwork/java/javase/index.html</a></a></p>
<p>JDK版本：jdk-6u31-linux-i586.bin</p>
<p><strong>2）Hadoop软件</strong></p>
<p>下载地址：<a href="http://hadoop.apache.org/common/releases.html" target="_blank"><a href="http://hadoop.apache.org/common/releases.html">http://hadoop.apache.org/common/releases.html</a></a></p>
<p>Hadoop版本：hadoop-1.0.0.tar.gz</p>
<h3 id="1-5-vsftp-">1.5 VSFTP上传</h3>
<p>在&quot;<strong>Hadoop集群（第3期）</strong>&quot;讲了VSFTP的安装及配置，如果没有安装VSFTP可以按照该文档进行安装。如果安装好了，就可以通过<strong>FlashFXP.exe</strong>软件把我们下载的JDK6.0和Hadoop1.0软件上传到&quot;<strong>Master.Hadoop:192.168.1.2</strong>&quot;服务器上。</p>
<p><img src="" alt=""></p>
<p>刚才我们用一般用户（hadoop）通过FlashFXP软件把所需的两个软件上传了跟目下，我们通过命令查看下一下是否已经上传了。</p>
<p><img src="" alt=""></p>
<p>从图中，我们的所需软件已经准备好了。</p>
<h2 id="2-ssh-">2、SSH无密码验证配置</h2>
<p>Hadoop运行过程中需要管理远端Hadoop守护进程，在Hadoop启动以后，NameNode是通过SSH（Secure Shell）来启动和停止各个DataNode上的各种守护进程的。这就必须在节点之间执行指令的时候是不需要输入密码的形式，故我们需要配置SSH运用无密码公钥认证的形式，这样NameNode使用SSH无密码登录并启动DataName进程，同样原理，DataNode上也能使用SSH无密码登录到NameNode。</p>
<h3 id="2-1-ssh-">2.1 安装和启动SSH协议</h3>
<p>在&quot;Hadoop集群（第1期）&quot;安装CentOS6.0时，我们选择了一些基本安装包，所以我们需要两个服务：ssh和rsync已经安装了。可以通过下面命令查看结果显示如下：</p>
<p>rpm –qa | grep openssh</p>
<p>rpm –qa | grep rsync</p>
<p><img src="" alt=""></p>
<p><strong>假设</strong>没有安装ssh和rsync，可以通过下面命令进行安装。</p>
<p>yum install ssh 安装SSH协议</p>
<p>yum install rsync （rsync是一个远程数据同步工具，可通过LAN/WAN快速同步多台主机间的文件）</p>
<p>service sshd restart 启动服务</p>
<p>确保所有的服务器都安装，上面命令执行完毕，各台机器之间可以通过密码验证相互登。</p>
<h3 id="2-2-master-salve">2.2 配置Master无密码登录所有Salve</h3>
<p><strong>1）SSH无密码原理</strong></p>
<p>Master（NameNode | JobTracker）作为客户端，要实现无密码公钥认证，连接到服务器Salve（DataNode | Tasktracker）上时，需要在Master上生成一个密钥对，包括一个公钥和一个私钥，而后将公钥复制到所有的Slave上。当Master通过SSH连接Salve时，Salve就会生成一个随机数并用Master的公钥对随机数进行加密，并发送给Master。Master收到加密数之后再用私钥解密，并将解密数回传给Slave，Slave确认解密数无误之后就允许Master进行连接了。这就是一个公钥认证过程，其间不需要用户手工输入密码。重要过程是将客户端Master复制到Slave上。</p>
<p><strong>2）Master机器上生成密码对</strong></p>
<p>在Master节点上执行以下命令：</p>
<p>ssh-keygen –t rsa –P &#39;&#39;</p>
<p>这条命是生成其<strong>无密码密钥对</strong>，询问其保存路径时<strong>直接回车</strong>采用默认路径。生成的密钥对：id_rsa和id_rsa.pub，默认存储在&quot;<strong>/home/hadoop/.ssh</strong>&quot;目录下。</p>
<p><img src="" alt=""></p>
<p>查看&quot;/home/hadoop/&quot;下是否有&quot;.ssh&quot;文件夹，且&quot;.ssh&quot;文件下是否有两个刚生产的无密码密钥对。</p>
<p><img src="" alt=""></p>
<p>接着在Master节点上做如下配置，把id_rsa.pub追加到授权的key里面去。</p>
<p>cat ~/.ssh/id_rsa.pub &gt;&gt; ~/.ssh/authorized_keys</p>
<p><img src="" alt=""></p>
<p>在验证前，需要做两件事儿。第一件事儿是修改文件&quot;<strong>authorized_keys</strong>&quot;权限（<strong>权限的设置非常重要，因为不安全的设置安全设置，会让你不能使用RSA功能</strong>），另一件事儿是用root用户设置&quot;<strong>/etc/ssh/sshd_config</strong>&quot;的内容。使其无密码登录有效。</p>
<p><strong>1）修改文件&quot;authorized_keys&quot;</strong></p>
<p>chmod 600 ~/.ssh/authorized_keys</p>
<p><img src="" alt=""></p>
<p><strong>备注：</strong>如果不进行设置，在验证时，扔提示你输入密码，在这里花费了将近半天时间来查找原因。在网上查到了几篇不错的文章，把作为&quot;<strong>Hadoop集群_第5期副刊_JDK和SSH无密码配置</strong>&quot;来帮助额外学习之用。</p>
<p><strong>2）设置SSH配置</strong></p>
<p>用<strong>root</strong>用户登录服务器修改SSH配置文件&quot;/etc/ssh/sshd_config&quot;的下列内容。</p>
<p><img src="" alt=""></p>
<p>RSAAuthentication yes /# 启用 RSA 认证</p>
<p>PubkeyAuthentication yes /# 启用公钥私钥配对认证方式</p>
<p>AuthorizedKeysFile .ssh/authorized_keys /# 公钥文件路径（和上面生成的文件同）</p>
<p>设置完之后记得<strong>重启SSH服务</strong>，才能使刚才设置有效。</p>
<p>service sshd restart</p>
<p><strong>退出root登录</strong>，使用<strong>hadoop</strong>普通用户验证是否成功。</p>
<p>ssh localhost</p>
<p><img src="" alt=""></p>
<p>从上图中得知无密码登录本级已经设置完毕，接下来的事儿是把<strong>公钥</strong>复制<strong>所有</strong>的Slave机器上。使用下面的命令格式进行复制公钥：</p>
<p>scp ~/.ssh/id_rsa.pub 远程用户名@远程服务器IP:~/</p>
<p>例如：</p>
<p>scp ~/.ssh/id_rsa.pub hadoop@192.168.1.3:~/</p>
<p>上面的命令是<strong>复制</strong>文件&quot;<strong>id_rsa.pub</strong>&quot;到服务器IP为&quot;<strong>192.168.1.3</strong>&quot;的用户为&quot;<strong>hadoop</strong>&quot;的&quot;<strong>/home/hadoop/</strong>&quot;下面。</p>
<p>下面就针对IP为&quot;192.168.1.3&quot;的Slave1.Hadoop的节点进行配置。</p>
<p><strong>1）把Master.Hadoop上的公钥复制到Slave1.Hadoop上</strong></p>
<hr>
<p><img src="" alt=""></p>
<p>从上图中我们得知，已经把文件&quot;id_rsa.pub&quot;传过去了，因为并没有建立起无密码连接，所以在连接时，仍然要提示输入输入Slave1.Hadoop服务器用户hadoop的密码。为了确保确实已经把文件传过去了，用SecureCRT登录Slave1.Hadoop:192.168.1.3服务器，查看&quot;/home/hadoop/&quot;下是否存在这个文件。</p>
<p><img src="" alt=""></p>
<p>从上面得知我们已经成功把公钥复制过去了。</p>
<p><strong>2）在&quot;/home/hadoop/&quot;下创建&quot;.ssh&quot;文件夹</strong></p>
<p>这一步<strong>并不是必须</strong>的，如果在Slave1.Hadoop的&quot;/home/hadoop&quot;<strong>已经存在</strong>就不需要创建了，因为我们之前并没有对Slave机器做过无密码登录配置，所以该文件是不存在的。用下面命令进行创建。（<strong>备注：</strong>用hadoop登录系统，如果不涉及系统文件修改，一般情况下都是用我们之前建立的普通用户hadoop进行执行命令。）</p>
<p>mkdir ~/.ssh</p>
<p>然后是修改文件夹&quot;<strong>.ssh</strong>&quot;的用户权限，把他的权限修改为&quot;<strong>700</strong>&quot;，用下面命令执行：</p>
<p>chmod 700 ~/.ssh</p>
<p><strong>备注：</strong>如果不进行，即使你按照前面的操作设置了&quot;authorized_keys&quot;权限，并配置了&quot;/etc/ssh/sshd_config&quot;，还重启了sshd服务，在Master能用&quot;ssh localhost&quot;进行无密码登录，但是对Slave1.Hadoop进行登录仍然需要输入密码，就是因为&quot;.ssh&quot;文件夹的权限设置不对。这个文件夹&quot;.ssh&quot;在配置SSH无密码登录时系统自动生成时，权限自动为&quot;700&quot;，如果是自己手动创建，它的组权限和其他权限都有，这样就会导致RSA无密码远程登录失败。</p>
<p><img src="" alt=""></p>
<p><img src="" alt=""></p>
<p>对比上面两张图，发现文件夹&quot;.ssh&quot;权限已经变了。</p>
<p><strong>3）追加到授权文件&quot;authorized_keys&quot;</strong></p>
<p>到目前为止Master.Hadoop的公钥也有了，文件夹&quot;.ssh&quot;也有了，且权限也修改了。这一步就是把Master.Hadoop的公钥<strong>追加</strong>到Slave1.Hadoop的授权文件&quot;authorized_keys&quot;中去。使用下面命令进行追加并修改&quot;authorized_keys&quot;文件权限：</p>
<p>cat ~/id_rsa.pub &gt;&gt; ~/.ssh/authorized_keys</p>
<p>chmod 600 ~/.ssh/authorized_keys</p>
<p><img src="" alt=""></p>
<p><strong>4）用root用户修改&quot;/etc/ssh/sshd_config&quot;</strong></p>
<p><strong>**具体步骤参考前面Master.Hadoop的&quot;</strong>设置SSH配置**&quot;，具体分为两步：第1是修改配置文件；第2是重启SSH服务。</p>
<p><strong>5）用Master.Hadoop使用SSH无密码登录Slave1.Hadoop</strong></p>
<p>当前面的步骤设置完毕，就可以使用下面命令格式进行SSH无密码登录了。</p>
<p>ssh 远程服务器IP</p>
<p><img src="" alt=""></p>
<p>从上图我们主要3个地方，第1个就是SSH无密码登录命令，第2、3个就是登录前后&quot;<strong>@</strong>&quot;后面的<strong>机器名</strong>变了，由&quot;<strong>Master</strong>&quot;变为了&quot;<strong>Slave1</strong>&quot;，这就说明我们已经成功实现了SSH无密码登录了。</p>
<p>最后记得把&quot;/home/hadoop/&quot;目录下的&quot;id_rsa.pub&quot;文件删除掉。</p>
<p>rm –r ~/id_rsa.pub</p>
<p><img src="" alt=""></p>
<p>到此为止，我们经过前5步已经实现了从&quot;Master.Hadoop&quot;到&quot;Slave1.Hadoop&quot;SSH无密码登录，下面就是重复上面的步骤把剩余的两台（Slave2.Hadoop和Slave3.Hadoop）Slave服务器进行配置。<strong>这样</strong>，我们就完成了&quot;配置Master无密码登录所有的Slave服务器&quot;。</p>
<h3 id="2-3-slave-master">2.3 配置所有Slave无密码登录Master</h3>
<p>和Master无密码登录所有Slave原理一样，就是把Slave的公钥<strong>追加</strong>到Master的&quot;.ssh&quot;文件夹下的&quot;authorized_keys&quot;中，记得是<strong>追加（&gt;&gt;）</strong>。</p>
<p>为了说明情况，我们现在就以&quot;Slave1.Hadoop&quot;无密码登录&quot;Master.Hadoop&quot;为例，进行一遍操作，也算是<strong>巩固</strong>一下前面所学知识，剩余的&quot;Slave2.Hadoop&quot;和&quot;Slave3.Hadoop&quot;就按照这个示例进行就可以了。</p>
<p>首先创建&quot;Slave1.Hadoop&quot;自己的公钥和私钥，并把自己的公钥追加到&quot;authorized_keys&quot;文件中。用到的命令如下：</p>
<p>ssh-keygen –t rsa –P &#39;&#39;</p>
<p>cat ~/.ssh/id_rsa.pub &gt;&gt; ~/.ssh/authorized_keys</p>
<p><img src="" alt=""></p>
<p>接着是用命令&quot;<strong>scp</strong>&quot;复制&quot;Slave1.Hadoop&quot;的公钥&quot;id_rsa.pub&quot;到&quot;Master.Hadoop&quot;的&quot;/home/hadoop/&quot;目录下，并<strong>追加</strong>到&quot;Master.Hadoop&quot;的&quot;authorized_keys&quot;中。</p>
<p><strong>1）在&quot;Slave1.Hadoop&quot;服务器的操作</strong></p>
<p>用到的命令如下：</p>
<p>scp ~/.ssh/id_rsa.pub hadoop@192.168.1.2:~/</p>
<p><img src="" alt=""></p>
<hr>
<p><strong>2）在&quot;Master.Hadoop&quot;服务器的操作</strong></p>
<p>用到的命令如下：</p>
<p>cat ~/id_rsa.pub &gt;&gt; ~/.ssh/authorized_keys</p>
<p><img src="" alt=""></p>
<p>然后删除掉刚才复制过来的&quot;id_rsa.pub&quot;文件。</p>
<p><img src="" alt=""></p>
<p>最后是测试从&quot;Slave1.Hadoop&quot;到&quot;Master.Hadoop&quot;无密码登录。</p>
<p><img src="" alt=""></p>
<p>从上面结果中可以看到已经成功实现了，再试下从&quot;Master.Hadoop&quot;到&quot;Slave1.Hadoop&quot;无密码登录。</p>
<p><img src="" alt=""></p>
<p>至此&quot;Master.Hadoop&quot;与&quot;Slave1.Hadoop&quot;之间可以互相无密码登录了，剩下的就是按照上面的步骤把剩余的&quot;Slave2.Hadoop&quot;和&quot;Slave3.Hadoop&quot;与&quot;Master.Hadoop&quot;之间建立起无密码登录。这样，Master能无密码验证登录每个Slave，每个Slave也能无密码验证登录到Master。</p>
<h2 id="3-java-">3、Java环境安装</h2>
<p>所有的机器上都要安装JDK，现在就先在Master服务器安装，然后其他服务器按照步骤重复进行即可。安装JDK以及配置环境变量，需要以&quot;<strong>root</strong>&quot;的身份进行。</p>
<h3 id="3-1-jdk">3.1 安装JDK</h3>
<p>首先用<strong>root</strong>身份登录&quot;Master.Hadoop&quot;后在&quot;/usr&quot;下创建&quot;java&quot;文件夹，再把用FTP上传到&quot;/home/hadoop/&quot;下的&quot;jdk-6u31-linux-i586.bin&quot;复制到&quot;/usr/java&quot;文件夹中。</p>
<p>mkdir /usr/java</p>
<p>cp /home/hadoop/ jdk-6u31-linux-i586.bin /usr/java</p>
<p><img src="" alt=""></p>
<p>接着<strong>进入</strong>&quot;<strong>/usr/java</strong>&quot;目录<strong>下</strong>通过下面命令使其JDK获得可执行权限，并安装JDK。</p>
<p>chmod +x jdk-6u31-linux-i586.bin</p>
<p>./jdk-6u31-linux-i586.bin</p>
<p><img src="" alt=""></p>
<p>按照上面几步进行操作，最后点击&quot;<strong>Enter</strong>&quot;键开始安装，安装完会提示你按&quot;<strong>Enter</strong>&quot;键退出，然后查看&quot;<strong>/usr/java</strong>&quot;下面会发现多了一个名为&quot;<strong>jdk1.6.0_31</strong>&quot;文件夹，说明我们的JDK安装结束，删除&quot;jdk-6u31-linux-i586.bin&quot;文件，进入下一个&quot;配置环境变量&quot;环节。</p>
<p><img src="" alt=""></p>
<h3 id="3-2-">3.2 配置环境变量</h3>
<p>编辑&quot;/etc/profile&quot;文件，在后面添加Java的&quot;JAVA_HOME&quot;、&quot;CLASSPATH&quot;以及&quot;PATH&quot;内容。</p>
<p><strong>1）编辑&quot;/etc/profile&quot;文件</strong></p>
<p>vim /etc/profile</p>
<p><img src="" alt=""></p>
<p><strong>2）添加Java环境变量</strong></p>
<p>在&quot;<strong>/etc/profile</strong>&quot;文件的<strong>尾部</strong>添加以下内容：</p>
<p>/# set java environment</p>
<p>export JAVA_HOME=/usr/java/jdk1.6.0_31/</p>
<p>export JRE_HOME=/usr/java/jdk1.6.0_31/jre</p>
<p>export CLASSPATH=.:$CLASSPATH:$JAVA_HOME/lib:$JRE_HOME/lib</p>
<p>export PATH=$PATH:$JAVA_HOME/bin:$JRE_HOME/bin</p>
<p>或者</p>
<p>/# set java environment</p>
<p>export JAVA_HOME=/usr/java/jdk1.6.0_31</p>
<p>export CLASSPATH=.:$CLASSPATH:$JAVA_HOME/lib:$JAVA_HOME/jre/lib</p>
<p>export PATH=$PATH:$JAVA_HOME/bin:$JAVA_HOME/jre/bin</p>
<p>以上两种意思一样，那么我们就选择<strong>第2种</strong>来进行设置。</p>
<p><img src="" alt=""></p>
<p><strong>3）使配置生效</strong></p>
<p>保存并退出，执行下面命令使其配置立即生效。</p>
<p>source /etc/profile</p>
<p><img src="" alt=""></p>
<h3 id="3-3-">3.3 验证安装成功</h3>
<p>配置完毕并生效后，用下面命令判断是否成功。</p>
<p>java -version</p>
<p><img src="" alt=""></p>
<p>从上图中得知，我们以确定JDK已经安装成功。</p>
<h3 id="3-4-">3.4 安装剩余机器</h3>
<p>这时用<strong>普通用户hadoop</strong>通过下面命令格式把&quot;Master.Hadoop&quot;文件夹&quot;/home/hadoop/&quot;的JDK复制到其他Slave的&quot;/home/hadoop/&quot;下面，剩下的事儿就是在其余的Slave服务器上按照上图的步骤安装JDK。</p>
<p>scp /home/hadoop/jdk-6u31-linux-i586.bin 远程用户名@远程服务器IP:~/</p>
<p>或者</p>
<p>scp ~/jdk-6u31-linux-i586.bin 远程用户名@远程服务器IP:~/</p>
<p><strong>备注：</strong>&quot;<strong>~</strong>&quot;代表<strong>当前</strong>用户的主目录，当<strong>前用户为hadoop</strong>，所以&quot;<strong>~</strong>&quot;代表&quot;<strong>/home/hadoop</strong>&quot;。</p>
<p><strong>例如：</strong>把JDK从&quot;Master.Hadoop&quot;复制到&quot;Slave1.Hadoop&quot;的命令如下。</p>
<p>scp ~/jdk-6u31-linux-i586 hadoop@192.168.1.3:~/</p>
<p><img src="" alt=""></p>
<p>然后查看&quot;Slave1.Hadoop&quot;的&quot;/home/hadoop&quot;查看是否已经复制成功了。</p>
<p><img src="" alt=""></p>
<p>从上图中得知，我们已经成功复制了，现在我们就用<strong>最高权限用户root</strong>进行安装了。其他的与这个一样。</p>
<h2 id="4-hadoop-">4、Hadoop集群安装</h2>
<p>所有的机器上都要安装hadoop，现在就先在Master服务器安装，然后其他服务器按照步骤重复进行即可。安装和配置hadoop需要以&quot;<strong>root</strong>&quot;的身份进行。</p>
<h3 id="4-1-hadoop">4.1 安装hadoop</h3>
<p>首先用<strong>root</strong>用户登录&quot;Master.Hadoop&quot;机器，查看我们之前用FTP上传至&quot;/home/Hadoop&quot;上传的&quot;<strong>hadoop-1.0.0.tar.gz</strong>&quot;。</p>
<p><img src="" alt=""></p>
<p>接着把&quot;<strong>hadoop-1.0.0.tar.gz</strong>&quot;复制到&quot;/usr&quot;目录下面。</p>
<p>cp /home/hadoop/hadoop-1.0.0.tar.gz /usr</p>
<p><img src="" alt=""></p>
<p>下一步进入&quot;/usr&quot;目录下，用下面命令把&quot;<strong>hadoop-1.0.0.tar.gz</strong>&quot;进行解压，并将其命名为&quot;hadoop&quot;，把该文件夹的<strong>读权限</strong>分配给普通用户<strong>hadoop</strong>，然后删除&quot;<strong>hadoop-1.0.0.tar.gz</strong>&quot;安装包。</p>
<p>cd /usr /#进入&quot;/usr&quot;目录</p>
<p>tar –zxvf hadoop-1.0.0.tar.gz /#解压&quot;<strong>hadoop-1.0.0.tar.gz</strong>&quot;安装包</p>
<p>mv hadoop-1.0.0 hadoop /#将&quot;hadoop-1.0.0&quot;文件夹<strong>重命名</strong>&quot;hadoop&quot;</p>
<p>chown <strong>–R</strong> hadoop:hadoop hadoop /#<strong>将文件夹&quot;hadoop&quot;读权限分配给hadoop用户</strong></p>
<p>rm –rf hadoop-1.0.0.tar.gz /#删除&quot;<strong>hadoop-1.0.0.tar.gz</strong>&quot;安装包</p>
<p><img src="" alt=""></p>
<p>解压后，并重命名。</p>
<p><img src="" alt=""></p>
<p>把&quot;/usr/hadoop&quot;<strong>读权</strong>限分配给<strong>hadoop</strong>用户（<strong>非常重要</strong>）</p>
<p><img src="" alt=""></p>
<p>删除&quot;<strong>hadoop-1.0.0.tar.gz</strong>&quot;安装包</p>
<p><img src="" alt=""></p>
<p>最后在&quot;<strong>/usr/hadoop</strong>&quot;下面创建<strong>tmp</strong>文件夹，把Hadoop的安装路径添加到&quot;<strong>/etc/profile</strong>&quot;中，修改&quot;/etc/profile&quot;文件（配置java环境变量的文件），将以下语句添加到<strong>末尾</strong>，并使其有效：</p>
<p>/# set hadoop path</p>
<p>export HADOOP_HOME=/usr/hadoop</p>
<p>export PATH=$PATH :$HADOOP_HOME/bin</p>
<p><strong>1）在&quot;/usr/hadoop&quot;创建&quot;tmp&quot;文件夹</strong></p>
<p>mkdir /usr/hadoop/tmp</p>
<p><img src="" alt=""></p>
<p><strong>2）配置&quot;/etc/profile&quot;</strong></p>
<p>vim /etc/profile</p>
<p><img src="" alt=""></p>
<p>配置后的文件如下：</p>
<p><img src="" alt=""></p>
<p><strong>3）重启&quot;/etc/profile&quot;</strong></p>
<p>source /etc/profile</p>
<p><img src="" alt=""></p>
<h3 id="4-2-hadoop">4.2 配置hadoop</h3>
<p><strong>1）配置hadoop-env.sh</strong></p>
<p>该&quot;<strong>hadoop-env.sh</strong>&quot;文件位于&quot;<strong>/usr/hadoop/conf</strong>&quot;目录下。</p>
<p><img src="" alt=""></p>
<p>在文件的末尾添加下面内容。</p>
<p>/# set java environment</p>
<p>export JAVA_HOME=/usr/java/jdk1.6.0_31</p>
<p><img src="" alt=""></p>
<p>Hadoop配置文件在conf目录下，之前的版本的配置文件主要是Hadoop-default.xml和Hadoop-site.xml。由于Hadoop发展迅速，代码量急剧增加，代码开发分为了core，hdfs和map/reduce三部分，配置文件也被分成了三个core-site.xml、hdfs-site.xml、mapred-site.xml。core-site.xml和hdfs-site.xml是站在HDFS角度上配置文件；core-site.xml和mapred-site.xml是站在MapReduce角度上配置文件。</p>
<p><strong>2）配置core-site.xml文件</strong></p>
<p>修改Hadoop核心配置文件core-site.xml，这里配置的是HDFS的地址和端口号。</p>
<configuration>

<property>

<name>hadoop.tmp.dir</name>

<value>/usr/hadoop/tmp</value>

（<strong>备注：</strong>请先在 /usr/hadoop 目录下建立 tmp 文件夹）

<description>A base for other temporary directories.</description>

</property>

<!-- file system properties -->

<property>

<name>fs.default.name</name>

<value>hdfs://<strong>192.168.1.2</strong>:<strong>9000</strong></value>

</property>

</configuration>

<p><strong>备注：</strong>如没有配置hadoop.tmp.dir参数，此时系统默认的临时目录为：/tmp/hadoo-hadoop。而这个目录在每次重启后都会被干掉，必须重新执行format才行，否则会出错。</p>
<p>用下面命令进行编辑：</p>
<p><img src="" alt=""></p>
<p>编辑结果显示如下：</p>
<p><img src="" alt=""></p>
<p><strong>3）配置hdfs-site.xml文件</strong></p>
<p>修改Hadoop中HDFS的配置，配置的备份方式默认为3。</p>
<configuration>

<property>

<name>dfs.replication</name>

<value><strong>1</strong></value>

(<strong>备注：</strong>replication 是数据副本数量，默认为3，salve少于3台就会报错)

</property>

<configuration>

用下面命令进行编辑：

<img src="" alt="">

编辑结果显示如下：

<img src="" alt="">

<strong>4）配置mapred-site.xml文件</strong>

修改Hadoop中MapReduce的配置文件，配置的是JobTracker的地址和端口。

<configuration>

<property>

<name>mapred.job.tracker</name>

<value><a href="http://**192.168.1.2**:**9001**" target="_blank">http://**192.168.1.2**:**9001**</a></value>

</property>

</configuration>

<p>用下面命令进行编辑：</p>
<p><img src="" alt=""></p>
<p>编辑结果显示如下：</p>
<p><img src="" alt=""></p>
<p><strong>5）配置masters文件</strong></p>
<p>有两种方案：</p>
<p>（1）第一种</p>
<p>修改localhost为Master.Hadoop</p>
<p>（2）第二种</p>
<p>去掉&quot;localhost&quot;，加入Master机器的IP：192.168.1.2</p>
<p>为保险起见，启用第二种，因为万一忘记配置&quot;/etc/hosts&quot;局域网的DNS失效，这样就会出现意想不到的错误，但是一旦IP配对，网络畅通，就能通过IP找到相应主机。</p>
<p>用下面命令进行修改：</p>
<p><img src="" alt=""></p>
<p>编辑结果显示如下：</p>
<p><img src="" alt=""></p>
<p><strong>6）配置slaves文件（Master主机特有</strong>）</p>
<p>有两种方案：</p>
<p>（1）第一种</p>
<p>去掉&quot;localhost&quot;，每行只添加一个主机名，把剩余的Slave主机名都填上。</p>
<p>例如：添加形式如下</p>
<p>Slave1.Hadoop</p>
<p>Slave2.Hadoop</p>
<p>Slave3.Hadoop</p>
<p>（2）第二种</p>
<p>去掉&quot;localhost&quot;，加入集群中所有Slave机器的IP，也是每行一个。</p>
<p>例如：添加形式如下</p>
<p>192.168.1.3</p>
<p>192.168.1.4</p>
<p>192.168.1.5</p>
<p>原因和添加&quot;masters&quot;文件一样，选择第二种方式。</p>
<p>用下面命令进行修改：</p>
<p><img src="" alt=""></p>
<p>编辑结果如下：</p>
<p><img src="" alt=""></p>
<p>现在在Master机器上的Hadoop配置就结束了，剩下的就是配置Slave机器上的Hadoop。</p>
<p><strong>一种方式</strong>是按照上面的步骤，把Hadoop的安装包在用<strong>普通用户hadoop</strong>通过&quot;<strong>scp</strong>&quot;复制到其他机器的&quot;/home/hadoop&quot;目录下，然后根据实际情况进行安装配置，<strong>除了第6步，那是Master特有的</strong>。用下面命令格式进行。（<strong>备注：</strong>此时切换到普通用户hadoop）</p>
<p>scp ~/hadoop-1.0.0.tar.gz hadoop@服务器IP:~/</p>
<p>例如：从&quot;Master.Hadoop&quot;到&quot;Slave1.Hadoop&quot;复制Hadoop的安装包。</p>
<p><img src="" alt=""></p>
<p><strong>另一种方式</strong>是将 Master上配置好的hadoop所在文件夹&quot;<strong>/usr/hadoop</strong>&quot;复制到所有的Slave的&quot;/usr&quot;目录下（实际上Slave机器上的slavers文件是不必要的， 复制了也没问题）。用下面命令格式进行。（<strong>备注：</strong>此时用户可以为hadoop也可以为root）</p>
<p>scp <strong>-r</strong> /usr/hadoop <strong>root</strong>@服务器IP:/usr/</p>
<p>例如：从&quot;Master.Hadoop&quot;到&quot;Slave1.Hadoop&quot;复制配置Hadoop的文件。</p>
<p><img src="" alt=""></p>
<p>上图中以root用户进行复制，当然不管是用户root还是hadoop，虽然Master机器上的&quot;/usr/hadoop&quot;文件夹用户hadoop有权限，但是Slave1上的hadoop用户却没有&quot;/usr&quot;权限，所以没有创建文件夹的权限。所以无论是哪个用户进行拷贝，右面都是&quot;root@机器IP&quot;格式。因为我们只是建立起了hadoop用户的SSH无密码连接，所以用root进行&quot;scp&quot;时，扔提示让你输入&quot;Slave1.Hadoop&quot;服务器用户root的密码。</p>
<p>查看&quot;Slave1.Hadoop&quot;服务器的&quot;/usr&quot;目录下是否已经存在&quot;hadoop&quot;文件夹，确认已经复制成功。查看结果如下：</p>
<p><img src="" alt=""></p>
<p>从上图中知道，hadoop文件夹确实已经复制了，但是我们发现hadoop权限是root，所以我们现在要给&quot;Slave1.Hadoop&quot;服务器上的用户hadoop添加对&quot;/usr/hadoop&quot;读权限。</p>
<p>以<strong>root</strong>用户登录&quot;Slave1.Hadoop&quot;，执行下面命令。</p>
<p>chown <strong>-R</strong> hadoop:hadoop（<strong>用户名：用户组</strong>） hadoop（<strong>文件夹</strong>）</p>
<p><img src="" alt=""></p>
<p>接着在&quot;Slave1 .Hadoop&quot;上修改&quot;/etc/profile&quot;文件（配置 java 环境变量的文件），将以下语句添加到末尾，并使其有效（source /etc/profile）：</p>
<p>/# set hadoop environment</p>
<p>export HADOOP_HOME=/usr/hadoop</p>
<p>export PATH=$PATH :$HADOOP_HOME/bin</p>
<p>如果不知道怎么设置，可以查看前面&quot;Master.Hadoop&quot;机器的&quot;/etc/profile&quot;文件的配置，到此为此在一台Slave机器上的Hadoop配置就结束了。剩下的事儿就是照葫芦画瓢把剩余的几台Slave机器按照《<strong>从&quot;Master.Hadoop&quot;到&quot;Slave1.Hadoop&quot;复制Hadoop的安装包。</strong>》这个例子进行部署Hadoop。</p>
<h3 id="4-3-">4.3 启动及验证</h3>
<p><strong>1）格式化HDFS文件系统</strong></p>
<p>在&quot;Master.Hadoop&quot;上使用<strong>普通</strong>用户<strong>hadoop</strong>进行操作。（<strong>备注：</strong>只需一次，下次启动不再需要格式化，只需 start-all.sh）</p>
<p>hadoop namenode -format</p>
<p>某些书上和网上的某些资料中用下面命令执行。</p>
<p><img src="" alt=""></p>
<p>我们在看好多文档包括有些书上，按照他们的hadoop环境变量进行配置后，并立即使其生效，但是执行发现没有找见&quot;bin/hadoop&quot;这个命令。</p>
<p><img src="" alt=""></p>
<p>其实我们会发现我们的环境变量配置的是&quot;<strong>$HADOOP_HOME/bin</strong>&quot;，我们已经把bin包含进入了，所以执行时，加上&quot;bin&quot;反而找不到该命令，除非我们的hadoop坏境变量如下设置。</p>
<p>/# set hadoop path</p>
<p>export HADOOP_HOME=/usr/hadoop</p>
<p>export PATH=$PATH :<strong>$HADOOP_HOME</strong>:<strong>$HADOOP_HOME/bin</strong></p>
<p>这样就能直接使用&quot;bin/hadoop&quot;也可以直接使用&quot;hadoop&quot;，现在不管哪种情况，hadoop命令都能找见了。我们也没有必要重新在设置hadoop环境变量了，只需要记住执行Hadoop命令时不需要在前面加&quot;bin&quot;就可以了。</p>
<p><img src="" alt=""></p>
<p><img src="" alt=""></p>
<p>从上图中知道我们已经成功格式话了，但是美中不足就是出现了一个<strong>警告</strong>，从网上的得知这个警告并不影响hadoop执行，但是也有办法解决，详情看后面的&quot;常见问题FAQ&quot;。</p>
<p><strong>2）启动hadoop</strong></p>
<p>在启动前关闭集群中所有机器的防火墙，不然会出现datanode开后又自动关闭。</p>
<p>service iptables stop</p>
<p>使用下面命令启动。</p>
<p>start-all.sh</p>
<p><img src="" alt=""></p>
<p>执行结果如下：</p>
<p><img src="" alt=""></p>
<p>可以通过以下启动日志看出，首先启动namenode 接着启动datanode1，datanode2，…，然后启动secondarynamenode。再启动jobtracker，然后启动tasktracker1，tasktracker2，…。</p>
<p>启动 hadoop成功后，在 Master 中的 tmp 文件夹中生成了 dfs 文件夹，在Slave 中的 tmp 文件夹中均生成了 dfs 文件夹和 mapred 文件夹。</p>
<p>查看Master中&quot;/usr/hadoop/tmp&quot;文件夹内容</p>
<p><img src="" alt=""></p>
<p>查看Slave1中&quot;/usr/hadoop/tmp&quot;文件夹内容。</p>
<p><img src="" alt=""></p>
<p><strong>3）验证hadoop</strong></p>
<p>（1）验证方法一：用&quot;jps&quot;命令</p>
<p>在Master上用 java自带的小工具<strong>jps</strong>查看进程。</p>
<p><img src="" alt=""></p>
<p>在Slave1上用jps查看进程。</p>
<p><img src="" alt=""></p>
<p>如果在查看Slave机器中发现&quot;DataNode&quot;和&quot;TaskTracker&quot;没有起来时，先查看一下日志的，如果是&quot;namespaceID&quot;不一致问题，采用&quot;常见问题FAQ6.2&quot;进行解决，如果是&quot;No route to host&quot;问题，采用&quot;常见问题FAQ6.3&quot;进行解决。</p>
<p>（2）验证方式二：用&quot;hadoop dfsadmin -report&quot;</p>
<p>用这个命令可以查看Hadoop集群的状态。</p>
<p>Master服务器的状态：</p>
<p><img src="" alt=""></p>
<p>Slave服务器的状态</p>
<p><img src="" alt=""></p>
<h3 id="4-4-">4.4 网页查看集群</h3>
<p><strong>1）访问&quot;http:192.168.1.2:50030&quot;</strong></p>
<p><img src="" alt=""></p>
<p>2）访问&quot;<strong>http:192.168.1.2:50070</strong>&quot;</p>
<p><img src="" alt=""></p>
<h2 id="5-faq">5、常见问题FAQ</h2>
<h3 id="5-1-warning-hadoop_home-is-deprecated-">5.1 关于 Warning: $HADOOP_HOME is deprecated.</h3>
<p>hadoop 1.0.0版本，安装完之后敲入hadoop命令时，<strong>老</strong>是提示这个警告：</p>
<p>Warning: $HADOOP_HOME is deprecated.</p>
<p>经查hadoop-1.0.0/bin/hadoop脚本和&quot;hadoop-config.sh&quot;脚本，发现脚本中对HADOOP_HOME的环境变量设置做了判断，笔者的环境根本不需要设置HADOOP_HOME环境变量。</p>
<p>解决方案一：编辑&quot;/etc/profile&quot;文件，去掉HADOOP_HOME的变量设定，重新输入hadoop fs命令，警告消失。</p>
<p>解决方案二：编辑&quot;/etc/profile&quot;文件，添加一个环境变量，之后警告消失：</p>
<p>export HADOOP_HOME_WARN_SUPPRESS=1</p>
<p>解决方案三：编辑&quot;hadoop-config.sh&quot;文件，把下面的&quot;if - fi&quot;功能注释掉。</p>
<p><img src="" alt=""></p>
<p>我们这里本着不动Hadoop原配置文件的前提下，采用&quot;<strong>方案二</strong>&quot;，在&quot;/etc/profile&quot;文件添加上面内容，并用命令&quot;source /etc/profile&quot;使之有效。</p>
<p><strong>1）切换至root用户</strong></p>
<p><img src="" alt=""></p>
<p><strong>2）添加内容</strong></p>
<p><img src="" alt=""></p>
<p><strong>3）重新生效</strong></p>
<p><img src="" alt=""></p>
<h3 id="5-2-no-datanode-to-stop-">5.2 解决&quot;no datanode to stop&quot;问题</h3>
<p>当我停止Hadoop时发现如下信息：</p>
<p><img src="" alt=""></p>
<p>原因：每次namenode format会重新创建一个namenodeId，而tmp/dfs/data下包含了上次format下的id，namenode format清空了namenode下的数据，但是没有清空datanode下的数据，导致启动时失败，所要做的就是每次fotmat前，清空tmp一下的所有目录。</p>
<p><strong>第一种解决方案如下：</strong></p>
<p><strong>1）先删除&quot;/usr/hadoop/tmp&quot;</strong></p>
<p>rm -rf /usr/hadoop/tmp</p>
<p><strong>2）创建&quot;/usr/hadoop/tmp&quot;文件夹</strong></p>
<p>mkdir /usr/hadoop/tmp</p>
<p><strong>3）删除&quot;/tmp&quot;下以&quot;hadoop&quot;开头文件</strong></p>
<p>rm -rf /tmp/hadoop/*</p>
<p><strong>4）重新格式化hadoop</strong></p>
<p>hadoop namenode -format</p>
<p><strong>5）启动hadoop</strong></p>
<p>start-all.sh</p>
<p>使用第一种方案，有种不好处就是原来集群上的重要数据全没有了。假如说Hadoop集群已经运行了一段时间。建议采用第二种。</p>
<p><strong>第二种方案如下：</strong></p>
<p>1）修改每个Slave的namespaceID使其与Master的namespaceID一致。</p>
<p>或者</p>
<p>2）修改Master的namespaceID使其与Slave的namespaceID一致。</p>
<p>该&quot;namespaceID&quot;位于&quot;<strong>/usr/hadoop/tmp/dfs/data/current/VERSION</strong>&quot;文件中，前面<strong>蓝色</strong>的可能根据实际情况变化，但后面<strong>红色</strong>是不变的。</p>
<p>例如：查看&quot;Master&quot;下的&quot;<strong>VERSION</strong>&quot;文件</p>
<p><img src="" alt=""></p>
<p>本人建议采用<strong>第二种</strong>，这样方便快捷，而且还能防止误删。</p>
<h3 id="5-3-slave-datanode-">5.3 Slave服务器中datanode启动后又自动关闭</h3>
<p>查看日志发下如下错误。</p>
<p><strong>ERROR</strong> org.apache.hadoop.hdfs.server.datanode.DataNode: java.io.IOException: Call to ... failed on local exception: java.net.NoRouteToHostException: <strong>No route to host</strong></p>
<p>解决方案是：关闭防火墙</p>
<p>service iptables stop</p>
<h3 id="5-4-hdfs-">5.4 从本地往hdfs文件系统上传文件</h3>
<p>出现如下错误：</p>
<p>INFO hdfs.DFSClient: Exception in createBlockOutputStream java.io.IOException: <strong>Bad connect ack with firstBadLink</strong></p>
<p>INFO hdfs.DFSClient: Abandoning block blk_-1300529705803292651_37023</p>
<p>WARN hdfs.DFSClient: DataStreamer Exception: java.io.IOException: <strong>Unable to create new block.</strong></p>
<p>解决方案是：</p>
<p><strong>1）关闭防火墙</strong></p>
<p>service iptables stop</p>
<p><strong>2）禁用selinux</strong></p>
<p>编辑 &quot;<strong>/etc/selinux/config</strong>&quot;文件，设置&quot;<strong>SELINUX</strong>=<strong>disabled</strong>&quot;</p>
<h3 id="5-5-">5.5 安全模式导致的错误</h3>
<p>出现如下错误：</p>
<p>org.apache.hadoop.dfs.SafeModeException: <strong>Cannot delete ..., Name node is in safe mode</strong></p>
<p>在分布式文件系统启动的时候，开始的时候会有安全模式，当分布式文件系统处于安全模式的情况下，文件系统中的内容不允许修改也不允许删除，直到安全模式结束。安全模式主要是为了系统启动的时候检查各个DataNode上数据块的有效性，同时根据策略必要的复制或者删除部分数据块。运行期通过命令也可以进入安全模式。在实践过程中，系统启动的时候去修改和删除文件也会有安全模式不允许修改的出错提示，只需要等待一会儿即可。</p>
<p>解决方案是：关闭安全模式</p>
<p>hadoop dfsadmin -safemode leave</p>
<h3 id="5-6-exceeded-max_failed_unique_fetches">5.6 解决Exceeded MAX_FAILED_UNIQUE_FETCHES</h3>
<p>出现错误如下：</p>
<p>Shuffle Error: Exceeded MAX_FAILED_UNIQUE_FETCHES; bailing-out</p>
<p>程序里面需要打开多个文件，进行分析，系统一般默认数量是1024，（用ulimit -a可以看到）对于正常使用是够了，但是对于程序来讲，就太少了。</p>
<p>解决方案是：修改2个文件。</p>
<p><strong>1）&quot;/etc/security/limits.conf&quot;</strong></p>
<p>vim /etc/security/limits.conf</p>
<p>加上：</p>
<p>soft nofile 102400</p>
<p>hard nofile 409600</p>
<p><strong>2）&quot;/etc/pam.d/login&quot;</strong></p>
<p>vim /etc/pam.d/login</p>
<p>添加：</p>
<p>session required /lib/security/pam_limits.so</p>
<p>针对第一个问题我纠正下答案：</p>
<p>这是reduce预处理阶段shuffle时获取已完成的map的输出失败次数超过上限造成的，上限默认为5。引起此问题的方式可能会有很多种，比如网络连接不正常，连接超时，带宽较差以及端口阻塞等。通常框架内网络情况较好是不会出现此错误的。</p>
<h3 id="5-7-too-many-fetch-failures-">5.7 解决&quot;Too many fetch-failures&quot;</h3>
<p>出现这个问题主要是结点间的连通不够全面。</p>
<p>解决方案是：</p>
<p><strong>1）检查&quot;/etc/hosts&quot;</strong></p>
<p>要求本机ip 对应 服务器名</p>
<p>要求要包含所有的服务器ip +服务器名</p>
<p><strong>2）检查&quot;.ssh/authorized_keys&quot;</strong></p>
<p>要求包含所有服务器（包括其自身）的public key</p>
<h3 id="5-8-">5.8 处理速度特别的慢</h3>
<p>出现<strong>map</strong>很<strong>快</strong>，但是<strong>reduce</strong>很<strong>慢</strong>，而且反复出现&quot;<strong>reduce=0%</strong>&quot;。</p>
<p>解决方案如下：</p>
<p>结合解决方案5.7，然后修改&quot;conf/hadoop-env.sh&quot;中的&quot;export HADOOP_HEAPSIZE=4000&quot;</p>
<h3 id="5-9-hadoop-outofmemoryerror-">5.9解决hadoop OutOfMemoryError问题</h3>
<p>出现这种异常，明显是jvm内存不够得原因。</p>
<p>解决方案如下：要修改所有的datanode的jvm内存大小。</p>
<p>Java –Xms 1024m -Xmx 4096m</p>
<p>一般jvm的最大内存使用应该为总内存大小的一半，我们使用的8G内存，所以设置为4096m，这一值可能依旧不是最优的值。</p>
<h3 id="5-10-namenode-in-safe-mode">5.10 Namenode in safe mode</h3>
<p>解决方案如下：</p>
<p>bin/hadoop dfsadmin -safemode leave</p>
<h3 id="5-11-io-">5.11 IO写操作出现问题</h3>
<p>0-1246359584298, infoPort=50075, ipcPort=50020):Got exception while serving blk_-5911099437886836280_1292 to /172.16.100.165:</p>
<p>java.net.SocketTimeoutException: 480000 millis timeout while waiting for channel to be ready for write. ch : java.nio.channels.SocketChannel[connected local=/</p>
<p>172.16.100.165:50010 remote=/172.16.100.165:50930]</p>
<p>at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:185)</p>
<p>at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:159)</p>
<p>……</p>
<p>It seems there are many reasons that it can timeout, the example given in HADOOP-3831 is a slow reading client.</p>
<p>解决方案如下：</p>
<p>在hadoop-site.xml中设置dfs.datanode.socket.write.timeout=0</p>
<h3 id="5-12-status-of-255-error">5.12 status of 255 error</h3>
<p>错误类型：</p>
<p>java.io.IOException: Task process exit with nonzero status of 255.</p>
<p>at org.apache.hadoop.mapred.TaskRunner.run(TaskRunner.java:424)</p>
<p>错误原因：</p>
<p>Set mapred.jobtracker.retirejob.interval and mapred.userlog.retain.hours to higher value. By default, their values are 24 hours. These might be the reason for failure, though I&#39;m not sure restart.</p>
<p>解决方案如下：单个datanode</p>
<p>如果一个datanode 出现问题，解决之后需要重新加入cluster而不重启cluster，方法如下：</p>
<p>bin/hadoop-daemon.sh start datanode</p>
<p>bin/hadoop-daemon.sh start jobtracker</p>
<h2 id="6-linux-">6、用到的Linux命令</h2>
<h3 id="6-1-chmod-">6.1 chmod命令详解</h3>
<p><strong>使用权限：</strong>所有使用者</p>
<p><strong>使用方式：</strong>chmod [-cfvR] [--help] [--version] mode file...</p>
<p><strong>说明：</strong></p>
<p>Linux/Unix 的档案存取权限分为三级 : 档案拥有者、群组、其他。利用 chmod 可以藉以控制档案如何被他人所存取。</p>
<p>mode ：权限设定字串，格式如下 ：[ugoa...][[+-=][rwxX]...][,...]，其中u 表示该档案的拥有者，g 表示与该档案的拥有者属于同一个群体(group)者，o 表示其他以外的人，a 表示这三者皆是。</p>
<ul>
<li>表示增加权限、- 表示取消权限、= 表示唯一设定权限。</li>
</ul>
<p>r 表示可读取，w 表示可写入，x 表示可执行，X 表示只有当该档案是个子目录或者该档案已经被设定过为可执行。</p>
<p>-c : 若该档案权限确实已经更改，才显示其更改动作</p>
<p>-f : 若该档案权限无法被更改也不要显示错误讯息</p>
<p>-v : 显示权限变更的详细资料</p>
<p>-R : 对目前目录下的所有档案与子目录进行相同的权限变更(即以递回的方式逐个变更)</p>
<p>--help : 显示辅助说明</p>
<p>--version : 显示版本</p>
<p><strong>范例：</strong></p>
<p>将档案 file1.txt 设为所有人皆可读取</p>
<p>chmod ugo+r file1.txt</p>
<p>将档案 file1.txt 设为所有人皆可读取</p>
<p>chmod a+r file1.txt</p>
<p>将档案 file1.txt 与 file2.txt 设为该档案拥有者，与其所属同一个群体者可写入，但其他以外的人则不可写入</p>
<p>chmod ug+w,o-w file1.txt file2.txt</p>
<p>将 ex1.py 设定为只有该档案拥有者可以执行</p>
<p>chmod u+x ex1.py</p>
<p>将目前目录下的所有档案与子目录皆设为任何人可读取</p>
<p>chmod -R a+r /*</p>
<p>此外chmod也可以用数字来表示权限如 chmod 777 file</p>
<p><strong>语法为：</strong>chmod abc file</p>
<p>其中a,b,c各为一个数字，分别表示User、Group、及Other的权限。</p>
<p>r=4，w=2，x=1</p>
<p>若要rwx属性则4+2+1=7；</p>
<p>若要rw-属性则4+2=6；</p>
<p>若要r-x属性则4+1=7。</p>
<p><strong>范例：</strong></p>
<p>chmod a=rwx file 和 chmod 777 file 效果相同</p>
<p>chmod ug=rwx,o=x file 和 chmod 771 file 效果相同</p>
<p>若用chmod 4755 filename可使此程式具有<strong>root</strong>的权限</p>
<h3 id="6-2-chown-">6.2 chown命令详解</h3>
<p><strong>使用权限：</strong>root</p>
<p><strong>使用方式：</strong>chown [-cfhvR] [--help] [--version] user[:group] file...</p>
<p><strong>说明：</strong></p>
<p>Linux/Unix 是多人多工作业系统，所有的档案皆有拥有者。利用 chown 可以将档案的拥有者加以改变。一般来说，这个指令只有是由系统管理者(root)所使用，一般使用者没有权限可以改变别人的档案拥有者，也没有权限可以自己的档案拥有者改设为别人。只有系统管理者(root)才有这样的权限。</p>
<p>user : 新的档案拥有者的使用者</p>
<p>IDgroup : 新的档案拥有者的使用者群体(group)</p>
<p>-c : 若该档案拥有者确实已经更改，才显示其更改动作</p>
<p>-f : 若该档案拥有者无法被更改也不要显示错误讯息</p>
<p>-h : 只对于连结(link)进行变更，而非该 link 真正指向的档案</p>
<p>-v : 显示拥有者变更的详细资料</p>
<p>-R : 对目前目录下的所有档案与子目录进行相同的拥有者变更(即以递回的方式逐个变更)</p>
<p>--help : 显示辅助说明</p>
<p>--version : 显示版本</p>
<p><strong>范例：</strong></p>
<p>将档案 file1.txt 的拥有者设为 users 群体的使用者 jessie</p>
<p>chown jessie:users file1.txt</p>
<p>将目前目录下的所有档案与子目录的拥有者皆设为 users 群体的使用者 lamport</p>
<p>chown -R lamport:users /*</p>
<p>-rw------- (600) -- 只有属主有读写权限。</p>
<p>-rw-r--r-- (644) -- 只有属主有读写权限；而属组用户和其他用户只有读权限。</p>
<p>-rwx------ (700) -- 只有属主有读、写、执行权限。</p>
<p>-rwxr-xr-x (755) -- 属主有读、写、执行权限；而属组用户和其他用户只有读、执行权限。</p>
<p>-rwx--x--x (711) -- 属主有读、写、执行权限；而属组用户和其他用户只有执行权限。</p>
<p>-rw-rw-rw- (666) -- 所有用户都有文件读、写权限。这种做法不可取。</p>
<p>-rwxrwxrwx (777) -- 所有用户都有读、写、执行权限。更不可取的做法。</p>
<p>以下是对目录的两个普通设定：</p>
<p>drwx------ (700) - 只有属主可在目录中读、写。</p>
<p>drwxr-xr-x (755) - 所有用户可读该目录，但只有属主才能改变目录中的内容</p>
<p>suid的代表数字是4，比如4755的结果是-rwsr-xr-x</p>
<p>sgid的代表数字是2，比如6755的结果是-rwsr-sr-x</p>
<p>sticky位代表数字是1，比如7755的结果是-rwsr-sr-t</p>
<h3 id="6-3-scp-">6.3 scp命令详解</h3>
<p>scp是 secure copy的缩写，scp是linux系统下基于ssh登陆进行安全的远程文件拷贝命令。linux的scp命令可以在linux服务器之间复制文件和目录。</p>
<p><strong>scp命令的用处：</strong></p>
<p>scp在网络上不同的主机之间复制文件，它使用ssh安全协议传输数据，具有和ssh一样的验证机制，从而安全的远程拷贝文件。</p>
<p><strong>scp命令基本格式：</strong></p>
<p>scp [-1246BCpqrv] [-c cipher] [-F ssh_config] [-i identity_file]</p>
<p>[-l limit] [-o ssh_option] [-P port] [-S program]</p>
<p>[[user@]host1:]file1 [...] [[user@]host2:]file2</p>
<p>scp命令的参数说明：</p>
<p>-1 强制scp命令使用协议ssh1</p>
<p>-2 强制scp命令使用协议ssh2</p>
<p>-4 强制scp命令只使用IPv4寻址</p>
<p>-6 强制scp命令只使用IPv6寻址</p>
<p>-B 使用批处理模式（传输过程中不询问传输口令或短语）</p>
<p>-C 允许压缩。（将-C标志传递给ssh，从而打开压缩功能）</p>
<p>-p 保留原文件的修改时间，访问时间和访问权限。</p>
<p>-q 不显示传输进度条。</p>
<p>-r 递归复制整个目录。</p>
<p>-v 详细方式显示输出。scp和ssh(1)会显示出整个过程的调试信息。这些信息用于调试连接，验证和配置问题。</p>
<p>-c cipher 以cipher将数据传输进行加密，这个选项将直接传递给ssh。</p>
<p>-F ssh_config 指定一个替代的ssh配置文件，此参数直接传递给ssh。</p>
<p>-i identity_file 从指定文件中读取传输时使用的密钥文件，此参数直接传递给ssh。</p>
<p>-l limit 限定用户所能使用的带宽，以Kbit/s为单位。</p>
<p>-o ssh_option 如果习惯于使用ssh_config(5)中的参数传递方式，</p>
<p>-P port 注意是大写的P, port是指定数据传输用到的端口号</p>
<p>-S program 指定加密传输时所使用的程序。此程序必须能够理解ssh(1)的选项。</p>
<p><strong>scp命令的实际应用</strong></p>
<p><strong>1）从本地服务器复制到远程服务器</strong></p>
<p><strong>(1) 复制文件：</strong></p>
<p>命令格式：</p>
<p>scp local_file remote_username@remote_ip:remote_folder</p>
<p>或者</p>
<p>scp local_file remote_username@remote_ip:remote_file</p>
<p>或者</p>
<p>scp local_file remote_ip:remote_folder</p>
<p>或者</p>
<p>scp local_file remote_ip:remote_file</p>
<p>第1,2个指定了用户名，命令执行后需要输入用户密码，第1个仅指定了远程的目录，文件名字不变，第2个指定了文件名</p>
<p>第3,4个没有指定用户名，命令执行后需要输入用户名和密码，第3个仅指定了远程的目录，文件名字不变，第4个指定了文件名</p>
<p><strong>实例：</strong></p>
<p>scp /home/linux/soft/scp.zip root@www.mydomain.com:/home/linux/others/soft</p>
<p>scp /home/linux/soft/scp.zip root@www.mydomain.com:/home/linux/others/soft/scp2.zip</p>
<p>scp /home/linux/soft/scp.zip www.mydomain.com:/home/linux/others/soft</p>
<p>scp /home/linux/soft/scp.zip www.mydomain.com:/home/linux/others/soft/scp2.zip</p>
<p><strong>(2) 复制目录：</strong></p>
<p>命令格式：</p>
<p>scp -r local_folder remote_username@remote_ip:remote_folder</p>
<p>或者</p>
<p>scp -r local_folder remote_ip:remote_folder</p>
<p>第1个指定了用户名，命令执行后需要输入用户密码；</p>
<p>第2个没有指定用户名，命令执行后需要输入用户名和密码；</p>
<p><strong>例子：</strong></p>
<p>scp -r /home/linux/soft/ root@www.mydomain.com:/home/linux/others/</p>
<p>scp -r /home/linux/soft/ www.mydomain.com:/home/linux/others/</p>
<p>上面 命令 将 本地 soft 目录 复制 到 远程 others 目录下，即复制后远程服务器上会有/home/linux/others/soft/ 目录。</p>
<p><strong>2）从远程服务器复制到本地服务器</strong></p>
<p>从远程复制到本地的scp命令与上面的命令雷同，只要将从本地复制到远程的命令后面2个参数互换顺序就行了。</p>
<p><strong>例如：</strong></p>
<p>scp root@www.mydomain.com:/home/linux/soft/scp.zip /home/linux/others/scp.zip</p>
<p>scp www.mydomain.com:/home/linux/soft/ -r /home/linux/others/</p>
<p>linux系统下scp命令中很多参数都和ssh1有关，还需要看到更原汁原味的参数信息，可以运行man scp 看到更细致的英文说明。</p>
<p>文章下载地址：<a href="http://files.cnblogs.com/xia520pi/HadoopCluster_Vol.5.rar" target="_blank"><a href="http://files.cnblogs.com/xia520pi/HadoopCluster_Vol.5.rar">http://files.cnblogs.com/xia520pi/HadoopCluster_Vol.5.rar</a></a>
来源： &lt;<a href="http://www.cnblogs.com/xia520pi/archive/2012/05/16/2503949.html" target="_blank">Hadoop集群（第5期）_Hadoop安装配置 - 虾皮 - 博客园</a>&gt; </p>

      
    </div>
    
    
      
    
    
        <footer id="post-meta">
        <span class="categories">Posted in<span class="breadcrumb fa fa-folder"><li><a href="/categories/hadoop/">hadoop</a></li></span></span> | <span class="tags">Tagged <a href="/tags/hadoop/" class="label label-primary">hadoop</a></span> | <span class="time">recent updated:<time title="2014-03-07 09:54:36"datetime="2014-03-07 09:54:36"> mar. 7 2014</time></span> | <span class="comment-link">
<a href="http://itsolife.com/2014/02/02/2014-02-02-hadoop--Hadoop集群_Hadoop安装配置/#comments" class="ds-thread-count comment-link" data-thread-key="2014-02-02-hadoop--Hadoop集群_Hadoop安装配置" data-count-type="comments">暂无评论</a></span>
        </footer>
    
    <div class="clearfix"></div>
  </div>
</div>
</article>



  <article>
<div id="post" class="post well">
  <div class="post-content">
    <header class="well-sm">
      <i class="fa icon fa-5x pull-left"></i>
      <h1 class="title"><a href="/2014/02/02/2014-02-02-hadoop--mapreduce-osdi04/">mapreduce</a></h1>
      
        <span>Posted on<time datetime="2014-02-02T01:54:36.000Z"> <a href="/2014/02/02/2014-02-02-hadoop--mapreduce-osdi04/">feb. 2 2014</a></time></span>      
    </header>
    
    
    <div class="entry">
      
        <h1 id="mapreduce-osdi04">mapreduce-osdi04</h1>
<p>MapReduce: Simpliﬁed Data Processing on Large Clusters
Jeffrey Dean and Sanjay Ghemawat
jeff@google.com, sanjay@google.com
Google, Inc.
Abstract
MapReduce is a programming model and an associated implementation for processing and generating large data sets. Users specify a map function that processes a key/value pair to generate a set of intermediate key/value pairs, and a reduce function that merges all intermediate values associated with the same intermediate key. Many real world tasks are expressible in this model, as shown in the paper. Programs written in this functional style are automatically parallelized and executed on a large cluster of commodity machines. The run-time system takes care of the details of partitioning the input data, scheduling the program’s execution across a set of machines, handling machine failures, and managing the required inter-machine communication. This allows programmers without any experience with parallel and distributed systems to easily utilize the resources of a large distributed system. Our implementation of MapReduce runs on a large cluster of commodity machines and is highly scalable: a typical MapReduce computation processes many terabytes of data on thousands of machines. Programmers ﬁnd the system easy to use: hundreds of MapReduce programs have been implemented and upwards of one thousand MapReduce jobs are executed on Google’s clusters every day.
1 Introduction
Over the past ﬁve years, the authors and many others at Google have implemented hundreds of special-purpose computations that process large amounts of raw data, such as crawled documents, web request logs, etc., to compute various kinds of derived data, such as inverted indices, various representations of the graph structure of web documents, summaries of the number of pages crawled per host, the set of most frequent queries in a To appear in OSDI 2004
given day, etc. Most such computations are conceptually straightforward. However, the input data is usually large and the computations have to be distributed across hundreds or thousands of machines in order to ﬁnish in a reasonable amount of time. The issues of how to parallelize the computation, distribute the data, and handle failures conspire to obscure the original simple computation with large amounts of complex code to deal with these issues. As a reaction to this complexity, we designed a new abstraction that allows us to express the simple computations we were trying to perform but hides the messy details of parallelization, fault-tolerance, data distribution and load balancing in a library. Our abstraction is inspired by the map and reduce primitives present in Lisp and many other functional languages. We realized that most of our computations involved applying a map operation to each logical “record” in our input in order to compute a set of intermediate key/value pairs, and then applying a reduce operation to all the values that shared the same key, in order to combine the derived data appropriately. Our use of a functional model with userspeciﬁed map and reduce operations allows us to parallelize large computations easily and to use re-execution as the primary mechanism for fault tolerance. The major contributions of this work are a simple and powerful interface that enables automatic parallelization and distribution of large-scale computations, combined with an implementation of this interface that achieves high performance on large clusters of commodity PCs. Section 2 describes the basic programming model and gives several examples. Section 3 describes an implementation of the MapReduce interface tailored towards our cluster-based computing environment. Section 4 describes several reﬁnements of the programming model that we have found useful. Section 5 has performance measurements of our implementation for a variety of tasks. Section 6 explores the use of MapReduce within Google including our experiences in using it as the basis 1
for a rewrite of our production indexing system. Section 7 discusses related and future work.
2.2 Types
Even though the previous pseudo-code is written in terms of string inputs and outputs, conceptually the map and reduce functions supplied by the user have associated types: map reduce (k1,v1) (k2,list(v2)) → list(k2,v2) → list(v2)
2 Programming Model
The computation takes a set of input key/value pairs, and produces a set of output key/value pairs. The user of the MapReduce library expresses the computation as two functions: Map and Reduce. Map, written by the user, takes an input pair and produces a set of intermediate key/value pairs. The MapReduce library groups together all intermediate values associated with the same intermediate key I and passes them to the Reduce function. The Reduce function, also written by the user, accepts an intermediate key I and a set of values for that key. It merges together these values to form a possibly smaller set of values. Typically just zero or one output value is produced per Reduce invocation. The intermediate values are supplied to the user’s reduce function via an iterator. This allows us to handle lists of values that are too large to ﬁt in memory.
I.e., the input keys and values are drawn from a different domain than the output keys and values. Furthermore, the intermediate keys and values are from the same domain as the output keys and values. Our C++ implementation passes strings to and from the user-deﬁned functions and leaves it to the user code to convert between strings and appropriate types.
2.3 More Examples
Here are a few simple examples of interesting programs that can be easily expressed as MapReduce computations. Distributed Grep: The map function emits a line if it matches a supplied pattern. The reduce function is an identity function that just copies the supplied intermediate data to the output. Count of URL Access Frequency: The map function processes logs of web page requests and outputs URL, 1 . The reduce function adds together all values for the same URL and emits a URL, total count pair. Reverse Web-Link Graph: The map function outputs target, source pairs for each link to a target URL found in a page named source. The reduce function concatenates the list of all source URLs associated with a given target URL and emits the pair: target, list(source) Term-Vector per Host: A term vector summarizes the most important words that occur in a document or a set of documents as a list of word, f requency pairs. The map function emits a hostname, term vector pair for each input document (where the hostname is extracted from the URL of the document). The reduce function is passed all per-document term vectors for a given host. It adds these term vectors together, throwing away infrequent terms, and then emits a ﬁnal hostname, term vector pair. 2
2.1 Example
Consider the problem of counting the number of occurrences of each word in a large collection of documents. The user would write code similar to the following pseudo-code:
map(String key, String value): // key: document name // value: document contents for each word w in value: EmitIntermediate(w, &quot;1&quot;); reduce(String key, Iterator values): // key: a word // values: a list of counts int result = 0; for each v in values: result += ParseInt(v); Emit(AsString(result));
The map function emits each word plus an associated count of occurrences (just ‘1’ in this simple example). The reduce function sums together all counts emitted for a particular word. In addition, the user writes code to ﬁll in a mapreduce speciﬁcation object with the names of the input and output ﬁles, and optional tuning parameters. The user then invokes the MapReduce function, passing it the speciﬁcation object. The user’s code is linked together with the MapReduce library (implemented in C++). Appendix A contains the full program text for this example. To appear in OSDI 2004
User Program
(1) fork (1) fork (1) fork
Master
(2) assign map
(2) assign reduce
worker split 0 split 1 split 2 split 3 split 4 worker Input files Map phase Intermediate files (on local disks) Reduce phase Output files
(3) read (5) remote read
worker worker
(6) write
output file 0 output file 1
worker
(4) local write
Figure 1: Execution overview Inverted Index: The map function parses each document, and emits a sequence of word, document ID pairs. The reduce function accepts all pairs for a given word, sorts the corresponding document IDs and emits a word, list(document ID) pair. The set of all output pairs forms a simple inverted index. It is easy to augment this computation to keep track of word positions. Distributed Sort: The map function extracts the key from each record, and emits a key, record pair. The reduce function emits all pairs unchanged. This computation depends on the partitioning facilities described in Section 4.1 and the ordering properties described in Section 4.2. large clusters of commodity PCs connected together with switched Ethernet [4]. In our environment: (1) Machines are typically dual-processor x86 processors running Linux, with 2-4 GB of memory per machine. (2) Commodity networking hardware is used – typically either 100 megabits/second or 1 gigabit/second at the machine level, but averaging considerably less in overall bisection bandwidth. (3) A cluster consists of hundreds or thousands of machines, and therefore machine failures are common. (4) Storage is provided by inexpensive IDE disks attached directly to individual machines. A distributed ﬁle system [8] developed in-house is used to manage the data stored on these disks. The ﬁle system uses replication to provide availability and reliability on top of unreliable hardware. (5) Users submit jobs to a scheduling system. Each job consists of a set of tasks, and is mapped by the scheduler to a set of available machines within a cluster.
3 Implementation
Many different implementations of the MapReduce interface are possible. The right choice depends on the environment. For example, one implementation may be suitable for a small shared-memory machine, another for a large NUMA multi-processor, and yet another for an even larger collection of networked machines. This section describes an implementation targeted to the computing environment in wide use at Google: To appear in OSDI 2004
3.1 Execution Overview
The Map invocations are distributed across multiple machines by automatically partitioning the input data 3
into a set of M splits. The input splits can be processed in parallel by different machines. Reduce invocations are distributed by partitioning the intermediate key space into R pieces using a partitioning function (e.g., hash(key) mod R). The number of partitions (R) and the partitioning function are speciﬁed by the user. Figure 1 shows the overall ﬂow of a MapReduce operation in our implementation. When the user program calls the MapReduce function, the following sequence of actions occurs (the numbered labels in Figure 1 correspond to the numbers in the list below): 1. The MapReduce library in the user program ﬁrst splits the input ﬁles into M pieces of typically 16 megabytes to 64 megabytes (MB) per piece (controllable by the user via an optional parameter). It then starts up many copies of the program on a cluster of machines. 2. One of the copies of the program is special – the master. The rest are workers that are assigned work by the master. There are M map tasks and R reduce tasks to assign. The master picks idle workers and assigns each one a map task or a reduce task. 3. A worker who is assigned a map task reads the contents of the corresponding input split. It parses key/value pairs out of the input data and passes each pair to the user-deﬁned Map function. The intermediate key/value pairs produced by the Map function are buffered in memory. 4. Periodically, the buffered pairs are written to local disk, partitioned into R regions by the partitioning function. The locations of these buffered pairs on the local disk are passed back to the master, who is responsible for forwarding these locations to the reduce workers. 5. When a reduce worker is notiﬁed by the master about these locations, it uses remote procedure calls to read the buffered data from the local disks of the map workers. When a reduce worker has read all intermediate data, it sorts it by the intermediate keys so that all occurrences of the same key are grouped together. The sorting is needed because typically many different keys map to the same reduce task. If the amount of intermediate data is too large to ﬁt in memory, an external sort is used. 6. The reduce worker iterates over the sorted intermediate data and for each unique intermediate key encountered, it passes the key and the corresponding set of intermediate values to the user’s Reduce function. The output of the Reduce function is appended to a ﬁnal output ﬁle for this reduce partition. To appear in OSDI 2004</p>
<ol>
<li>When all map tasks and reduce tasks have been completed, the master wakes up the user program. At this point, the MapReduce call in the user program returns back to the user code. After successful completion, the output of the mapreduce execution is available in the R output ﬁles (one per reduce task, with ﬁle names as speciﬁed by the user). Typically, users do not need to combine these R output ﬁles into one ﬁle – they often pass these ﬁles as input to another MapReduce call, or use them from another distributed application that is able to deal with input that is partitioned into multiple ﬁles.
3.2 Master Data Structures
The master keeps several data structures. For each map task and reduce task, it stores the state (idle, in-progress, or completed), and the identity of the worker machine (for non-idle tasks). The master is the conduit through which the location of intermediate ﬁle regions is propagated from map tasks to reduce tasks. Therefore, for each completed map task, the master stores the locations and sizes of the R intermediate ﬁle regions produced by the map task. Updates to this location and size information are received as map tasks are completed. The information is pushed incrementally to workers that have in-progress reduce tasks.
3.3 Fault Tolerance
Since the MapReduce library is designed to help process very large amounts of data using hundreds or thousands of machines, the library must tolerate machine failures gracefully. Worker Failure The master pings every worker periodically. If no response is received from a worker in a certain amount of time, the master marks the worker as failed. Any map tasks completed by the worker are reset back to their initial idle state, and therefore become eligible for scheduling on other workers. Similarly, any map task or reduce task in progress on a failed worker is also reset to idle and becomes eligible for rescheduling. Completed map tasks are re-executed on a failure because their output is stored on the local disk(s) of the failed machine and is therefore inaccessible. Completed reduce tasks do not need to be re-executed since their output is stored in a global ﬁle system. When a map task is executed ﬁrst by worker A and then later executed by worker B (because A failed), all 4
workers executing reduce tasks are notiﬁed of the reexecution. Any reduce task that has not already read the data from worker A will read the data from worker B. MapReduce is resilient to large-scale worker failures. For example, during one MapReduce operation, network maintenance on a running cluster was causing groups of 80 machines at a time to become unreachable for several minutes. The MapReduce master simply re-executed the work done by the unreachable worker machines, and continued to make forward progress, eventually completing the MapReduce operation. Master Failure It is easy to make the master write periodic checkpoints of the master data structures described above. If the master task dies, a new copy can be started from the last checkpointed state. However, given that there is only a single master, its failure is unlikely; therefore our current implementation aborts the MapReduce computation if the master fails. Clients can check for this condition and retry the MapReduce operation if they desire. Semantics in the Presence of Failures When the user-supplied map and reduce operators are deterministic functions of their input values, our distributed implementation produces the same output as would have been produced by a non-faulting sequential execution of the entire program. We rely on atomic commits of map and reduce task outputs to achieve this property. Each in-progress task writes its output to private temporary ﬁles. A reduce task produces one such ﬁle, and a map task produces R such ﬁles (one per reduce task). When a map task completes, the worker sends a message to the master and includes the names of the R temporary ﬁles in the message. If the master receives a completion message for an already completed map task, it ignores the message. Otherwise, it records the names of R ﬁles in a master data structure. When a reduce task completes, the reduce worker atomically renames its temporary output ﬁle to the ﬁnal output ﬁle. If the same reduce task is executed on multiple machines, multiple rename calls will be executed for the same ﬁnal output ﬁle. We rely on the atomic rename operation provided by the underlying ﬁle system to guarantee that the ﬁnal ﬁle system state contains just the data produced by one execution of the reduce task. The vast majority of our map and reduce operators are deterministic, and the fact that our semantics are equivalent to a sequential execution in this case makes it very To appear in OSDI 2004
easy for programmers to reason about their program’s behavior. When the map and/or reduce operators are nondeterministic, we provide weaker but still reasonable semantics. In the presence of non-deterministic operators, the output of a particular reduce task R1 is equivalent to the output for R1 produced by a sequential execution of the non-deterministic program. However, the output for a different reduce task R2 may correspond to the output for R2 produced by a different sequential execution of the non-deterministic program. Consider map task M and reduce tasks R1 and R2 . Let e(Ri ) be the execution of Ri that committed (there is exactly one such execution). The weaker semantics arise because e(R1 ) may have read the output produced by one execution of M and e(R2 ) may have read the output produced by a different execution of M .
3.4 Locality
Network bandwidth is a relatively scarce resource in our computing environment. We conserve network bandwidth by taking advantage of the fact that the input data (managed by GFS [8]) is stored on the local disks of the machines that make up our cluster. GFS divides each ﬁle into 64 MB blocks, and stores several copies of each block (typically 3 copies) on different machines. The MapReduce master takes the location information of the input ﬁles into account and attempts to schedule a map task on a machine that contains a replica of the corresponding input data. Failing that, it attempts to schedule a map task near a replica of that task’s input data (e.g., on a worker machine that is on the same network switch as the machine containing the data). When running large MapReduce operations on a signiﬁcant fraction of the workers in a cluster, most input data is read locally and consumes no network bandwidth.
3.5 Task Granularity
We subdivide the map phase into M pieces and the reduce phase into R pieces, as described above. Ideally, M and R should be much larger than the number of worker machines. Having each worker perform many different tasks improves dynamic load balancing, and also speeds up recovery when a worker fails: the many map tasks it has completed can be spread out across all the other worker machines. There are practical bounds on how large M and R can be in our implementation, since the master must make O(M + R) scheduling decisions and keeps O(M ∗ R) state in memory as described above. (The constant factors for memory usage are small however: the O(M ∗ R) piece of the state consists of approximately one byte of data per map task/reduce task pair.) 5
Furthermore, R is often constrained by users because the output of each reduce task ends up in a separate output ﬁle. In practice, we tend to choose M so that each individual task is roughly 16 MB to 64 MB of input data (so that the locality optimization described above is most effective), and we make R a small multiple of the number of worker machines we expect to use. We often perform MapReduce computations with M = 200, 000 and R = 5, 000, using 2,000 worker machines.
3.6 Backup Tasks
One of the common causes that lengthens the total time taken for a MapReduce operation is a “straggler”: a machine that takes an unusually long time to complete one of the last few map or reduce tasks in the computation. Stragglers can arise for a whole host of reasons. For example, a machine with a bad disk may experience frequent correctable errors that slow its read performance from 30 MB/s to 1 MB/s. The cluster scheduling system may have scheduled other tasks on the machine, causing it to execute the MapReduce code more slowly due to competition for CPU, memory, local disk, or network bandwidth. A recent problem we experienced was a bug in machine initialization code that caused processor caches to be disabled: computations on affected machines slowed down by over a factor of one hundred. We have a general mechanism to alleviate the problem of stragglers. When a MapReduce operation is close to completion, the master schedules backup executions of the remaining in-progress tasks. The task is marked as completed whenever either the primary or the backup execution completes. We have tuned this mechanism so that it typically increases the computational resources used by the operation by no more than a few percent. We have found that this signiﬁcantly reduces the time to complete large MapReduce operations. As an example, the sort program described in Section 5.3 takes 44% longer to complete when the backup task mechanism is disabled.
the intermediate key. A default partitioning function is provided that uses hashing (e.g. “hash(key) mod R”). This tends to result in fairly well-balanced partitions. In some cases, however, it is useful to partition data by some other function of the key. For example, sometimes the output keys are URLs, and we want all entries for a single host to end up in the same output ﬁle. To support situations like this, the user of the MapReduce library can provide a special partitioning function. For example, using “hash(Hostname(urlkey)) mod R” as the partitioning function causes all URLs from the same host to end up in the same output ﬁle.
4.2 Ordering Guarantees
We guarantee that within a given partition, the intermediate key/value pairs are processed in increasing key order. This ordering guarantee makes it easy to generate a sorted output ﬁle per partition, which is useful when the output ﬁle format needs to support efﬁcient random access lookups by key, or users of the output ﬁnd it convenient to have the data sorted.
4.3 Combiner Function
In some cases, there is signiﬁcant repetition in the intermediate keys produced by each map task, and the userspeciﬁed Reduce function is commutative and associative. A good example of this is the word counting example in Section 2.1. Since word frequencies tend to follow a Zipf distribution, each map task will produce hundreds or thousands of records of the form <the, 1>. All of these counts will be sent over the network to a single reduce task and then added together by the Reduce function to produce one number. We allow the user to specify an optional Combiner function that does partial merging of this data before it is sent over the network. The Combiner function is executed on each machine that performs a map task. Typically the same code is used to implement both the combiner and the reduce functions. The only difference between a reduce function and a combiner function is how the MapReduce library handles the output of the function. The output of a reduce function is written to the ﬁnal output ﬁle. The output of a combiner function is written to an intermediate ﬁle that will be sent to a reduce task. Partial combining signiﬁcantly speeds up certain classes of MapReduce operations. Appendix A contains an example that uses a combiner.
4 Reﬁnements
Although the basic functionality provided by simply writing Map and Reduce functions is sufﬁcient for most needs, we have found a few extensions useful. These are described in this section.
4.1 Partitioning Function
The users of MapReduce specify the number of reduce tasks/output ﬁles that they desire (R). Data gets partitioned across these tasks using a partitioning function on To appear in OSDI 2004
4.4 Input and Output Types
The MapReduce library provides support for reading input data in several different formats. For example, “text” 6
mode input treats each line as a key/value pair: the key is the offset in the ﬁle and the value is the contents of the line. Another common supported format stores a sequence of key/value pairs sorted by key. Each input type implementation knows how to split itself into meaningful ranges for processing as separate map tasks (e.g. text mode’s range splitting ensures that range splits occur only at line boundaries). Users can add support for a new input type by providing an implementation of a simple reader interface, though most users just use one of a small number of predeﬁned input types. A reader does not necessarily need to provide data read from a ﬁle. For example, it is easy to deﬁne a reader that reads records from a database, or from data structures mapped in memory. In a similar fashion, we support a set of output types for producing data in different formats and it is easy for user code to add support for new output types.
the signal handler sends a “last gasp” UDP packet that contains the sequence number to the MapReduce master. When the master has seen more than one failure on a particular record, it indicates that the record should be skipped when it issues the next re-execution of the corresponding Map or Reduce task.
4.7 Local Execution
Debugging problems in Map or Reduce functions can be tricky, since the actual computation happens in a distributed system, often on several thousand machines, with work assignment decisions made dynamically by the master. To help facilitate debugging, proﬁling, and small-scale testing, we have developed an alternative implementation of the MapReduce library that sequentially executes all of the work for a MapReduce operation on the local machine. Controls are provided to the user so that the computation can be limited to particular map tasks. Users invoke their program with a special ﬂag and can then easily use any debugging or testing tools they ﬁnd useful (e.g. gdb).
4.5 Side-effects
In some cases, users of MapReduce have found it convenient to produce auxiliary ﬁles as additional outputs from their map and/or reduce operators. We rely on the application writer to make such side-effects atomic and idempotent. Typically the application writes to a temporary ﬁle and atomically renames this ﬁle once it has been fully generated. We do not provide support for atomic two-phase commits of multiple output ﬁles produced by a single task. Therefore, tasks that produce multiple output ﬁles with cross-ﬁle consistency requirements should be deterministic. This restriction has never been an issue in practice.
4.8 Status Information
The master runs an internal HTTP server and exports a set of status pages for human consumption. The status pages show the progress of the computation, such as how many tasks have been completed, how many are in progress, bytes of input, bytes of intermediate data, bytes of output, processing rates, etc. The pages also contain links to the standard error and standard output ﬁles generated by each task. The user can use this data to predict how long the computation will take, and whether or not more resources should be added to the computation. These pages can also be used to ﬁgure out when the computation is much slower than expected. In addition, the top-level status page shows which workers have failed, and which map and reduce tasks they were processing when they failed. This information is useful when attempting to diagnose bugs in the user code.
4.6 Skipping Bad Records
Sometimes there are bugs in user code that cause the Map or Reduce functions to crash deterministically on certain records. Such bugs prevent a MapReduce operation from completing. The usual course of action is to ﬁx the bug, but sometimes this is not feasible; perhaps the bug is in a third-party library for which source code is unavailable. Also, sometimes it is acceptable to ignore a few records, for example when doing statistical analysis on a large data set. We provide an optional mode of execution where the MapReduce library detects which records cause deterministic crashes and skips these records in order to make forward progress. Each worker process installs a signal handler that catches segmentation violations and bus errors. Before invoking a user Map or Reduce operation, the MapReduce library stores the sequence number of the argument in a global variable. If the user code generates a signal, To appear in OSDI 2004
4.9 Counters
The MapReduce library provides a counter facility to count occurrences of various events. For example, user code may want to count total number of words processed or the number of German documents indexed, etc. To use this facility, user code creates a named counter object and then increments the counter appropriately in the Map and/or Reduce function. For example: 7
Input (MB/s)
Counter/<em> uppercase; uppercase = GetCounter(&quot;uppercase&quot;); map(String name, String contents): for each word w in contents: if (IsCapitalized(w)): uppercase-&gt;Increment(); EmitIntermediate(w, &quot;1&quot;);
30000 20000 10000 0 20 40 60 80 100
The counter values from individual worker machines are periodically propagated to the master (piggybacked on the ping response). The master aggregates the counter values from successful map and reduce tasks and returns them to the user code when the MapReduce operation is completed. The current counter values are also displayed on the master status page so that a human can watch the progress of the live computation. When aggregating counter values, the master eliminates the effects of duplicate executions of the same map or reduce task to avoid double counting. (Duplicate executions can arise from our use of backup tasks and from re-execution of tasks due to failures.) Some counter values are automatically maintained by the MapReduce library, such as the number of input key/value pairs processed and the number of output key/value pairs produced. Users have found the counter facility useful for sanity checking the behavior of MapReduce operations. For example, in some MapReduce operations, the user code may want to ensure that the number of output pairs produced exactly equals the number of input pairs processed, or that the fraction of German documents processed is within some tolerable fraction of the total number of documents processed.
Seconds
Figure 2: Data transfer rate over time disks, and a gigabit Ethernet link. The machines were arranged in a two-level tree-shaped switched network with approximately 100-200 Gbps of aggregate bandwidth available at the root. All of the machines were in the same hosting facility and therefore the round-trip time between any pair of machines was less than a millisecond. Out of the 4GB of memory, approximately 1-1.5GB was reserved by other tasks running on the cluster. The programs were executed on a weekend afternoon, when the CPUs, disks, and network were mostly idle.
5.2 Grep
The grep program scans through 1010 100-byte records, searching for a relatively rare three-character pattern (the pattern occurs in 92,337 records). The input is split into approximately 64MB pieces (M = 15000), and the entire output is placed in one ﬁle (R = 1). Figure 2 shows the progress of the computation over time. The Y-axis shows the rate at which the input data is scanned. The rate gradually picks up as more machines are assigned to this MapReduce computation, and peaks at over 30 GB/s when 1764 workers have been assigned. As the map tasks ﬁnish, the rate starts dropping and hits zero about 80 seconds into the computation. The entire computation takes approximately 150 seconds from start to ﬁnish. This includes about a minute of startup overhead. The overhead is due to the propagation of the program to all worker machines, and delays interacting with GFS to open the set of 1000 input ﬁles and to get the information needed for the locality optimization.
5 Performance
In this section we measure the performance of MapReduce on two computations running on a large cluster of machines. One computation searches through approximately one terabyte of data looking for a particular pattern. The other computation sorts approximately one terabyte of data. These two programs are representative of a large subset of the real programs written by users of MapReduce – one class of programs shufﬂes data from one representation to another, and another class extracts a small amount of interesting data from a large data set.
5.3 Sort
The sort program sorts 1010 100-byte records (approximately 1 terabyte of data). This program is modeled after the TeraSort benchmark [10]. The sorting program consists of less than 50 lines of user code. A three-line Map function extracts a 10-byte sorting key from a text line and emits the key and the 8
5.1 Cluster Conﬁguration
All of the programs were executed on a cluster that consisted of approximately 1800 machines. Each machine had two 2GHz Intel Xeon processors with HyperThreading enabled, 4GB of memory, two 160GB IDE To appear in OSDI 2004
20000
Input (MB/s)
Input (MB/s)
15000 10000 5000 0 500 1000
Input (MB/s)
Done
20000 15000 10000 5000 0 500 1000
20000
Done
Done
15000 10000 5000 0 500 1000
Shuffle (MB/s)
15000 10000 5000 0 500 1000
Shuffle (MB/s)
500 1000
20000
Shuffle (MB/s)
20000 15000 10000 5000 0
20000 15000 10000 5000 0 500 1000
Output (MB/s)
Output (MB/s)
15000 10000 5000 0 500 1000
15000 10000 5000 0 500 1000
Output (MB/s)
20000
20000
20000 15000 10000 5000 0 500 1000
Seconds
Seconds
Seconds
(a) Normal execution
(b) No backup tasks
(c) 200 tasks killed
Figure 3: Data transfer rates over time for different executions of the sort program original text line as the intermediate key/value pair. We used a built-in Identity function as the Reduce operator. This functions passes the intermediate key/value pair unchanged as the output key/value pair. The ﬁnal sorted output is written to a set of 2-way replicated GFS ﬁles (i.e., 2 terabytes are written as the output of the program). As before, the input data is split into 64MB pieces (M = 15000). We partition the sorted output into 4000 ﬁles (R = 4000). The partitioning function uses the initial bytes of the key to segregate it into one of R pieces. Our partitioning function for this benchmark has builtin knowledge of the distribution of keys. In a general sorting program, we would add a pre-pass MapReduce operation that would collect a sample of the keys and use the distribution of the sampled keys to compute splitpoints for the ﬁnal sorting pass. Figure 3 (a) shows the progress of a normal execution of the sort program. The top-left graph shows the rate at which input is read. The rate peaks at about 13 GB/s and dies off fairly quickly since all map tasks ﬁnish before 200 seconds have elapsed. Note that the input rate is less than for grep. This is because the sort map tasks spend about half their time and I/O bandwidth writing intermediate output to their local disks. The corresponding intermediate output for grep had negligible size. The middle-left graph shows the rate at which data is sent over the network from the map tasks to the reduce tasks. This shufﬂing starts as soon as the ﬁrst map task completes. The ﬁrst hump in the graph is for To appear in OSDI 2004 the ﬁrst batch of approximately 1700 reduce tasks (the entire MapReduce was assigned about 1700 machines, and each machine executes at most one reduce task at a time). Roughly 300 seconds into the computation, some of these ﬁrst batch of reduce tasks ﬁnish and we start shufﬂing data for the remaining reduce tasks. All of the shufﬂing is done about 600 seconds into the computation. The bottom-left graph shows the rate at which sorted data is written to the ﬁnal output ﬁles by the reduce tasks. There is a delay between the end of the ﬁrst shufﬂing period and the start of the writing period because the machines are busy sorting the intermediate data. The writes continue at a rate of about 2-4 GB/s for a while. All of the writes ﬁnish about 850 seconds into the computation. Including startup overhead, the entire computation takes 891 seconds. This is similar to the current best reported result of 1057 seconds for the TeraSort benchmark [18]. A few things to note: the input rate is higher than the shufﬂe rate and the output rate because of our locality optimization – most data is read from a local disk and bypasses our relatively bandwidth constrained network. The shufﬂe rate is higher than the output rate because the output phase writes two copies of the sorted data (we make two replicas of the output for reliability and availability reasons). We write two replicas because that is the mechanism for reliability and availability provided by our underlying ﬁle system. Network bandwidth requirements for writing data would be reduced if the underlying ﬁle system used erasure coding [14] rather than replication. 9
In Figure 3 (b), we show an execution of the sort program with backup tasks disabled. The execution ﬂow is similar to that shown in Figure 3 (a), except that there is a very long tail where hardly any write activity occurs. After 960 seconds, all except 5 of the reduce tasks are completed. However these last few stragglers don’t ﬁnish until 300 seconds later. The entire computation takes 1283 seconds, an increase of 44% in elapsed time.
Number of instances in source tree
5.4 Effect of Backup Tasks
1000 800 600 400 200 0 2003/03 2003/06 2003/09 2003/12 2004/03 2004/06 2004/09
5.5 Machine Failures
In Figure 3 (c), we show an execution of the sort program where we intentionally killed 200 out of 1746 worker processes several minutes into the computation. The underlying cluster scheduler immediately restarted new worker processes on these machines (since only the processes were killed, the machines were still functioning properly). The worker deaths show up as a negative input rate since some previously completed map work disappears (since the corresponding map workers were killed) and needs to be redone. The re-execution of this map work happens relatively quickly. The entire computation ﬁnishes in 933 seconds including startup overhead (just an increase of 5% over the normal execution time).
Figure 4: MapReduce instances over time
Number of jobs Average job completion time Machine days used Input data read Intermediate data produced Output data written Average worker machines per job Average worker deaths per job Average map tasks per job Average reduce tasks per job Unique map implementations Unique reduce implementations Unique map/reduce combinations 29,423 634 secs 79,186 days 3,288 TB 758 TB 193 TB 157 1.2 3,351 55 395 269 426
6 Experience
We wrote the ﬁrst version of the MapReduce library in February of 2003, and made signiﬁcant enhancements to it in August of 2003, including the locality optimization, dynamic load balancing of task execution across worker machines, etc. Since that time, we have been pleasantly surprised at how broadly applicable the MapReduce library has been for the kinds of problems we work on. It has been used across a wide range of domains within Google, including: • large-scale machine learning problems, • clustering problems for the Google News and Froogle products, • extraction of data used to produce reports of popular queries (e.g. Google Zeitgeist), • extraction of properties of web pages for new experiments and products (e.g. extraction of geographical locations from a large corpus of web pages for localized search), and • large-scale graph computations. To appear in OSDI 2004
Table 1: MapReduce jobs run in August 2004 Figure 4 shows the signiﬁcant growth in the number of separate MapReduce programs checked into our primary source code management system over time, from 0 in early 2003 to almost 900 separate instances as of late September 2004. MapReduce has been so successful because it makes it possible to write a simple program and run it efﬁciently on a thousand machines in the course of half an hour, greatly speeding up the development and prototyping cycle. Furthermore, it allows programmers who have no experience with distributed and/or parallel systems to exploit large amounts of resources easily. At the end of each job, the MapReduce library logs statistics about the computational resources used by the job. In Table 1, we show some statistics for a subset of MapReduce jobs run at Google in August 2004.
6.1 Large-Scale Indexing
One of our most signiﬁcant uses of MapReduce to date has been a complete rewrite of the production index10
ing system that produces the data structures used for the Google web search service. The indexing system takes as input a large set of documents that have been retrieved by our crawling system, stored as a set of GFS ﬁles. The raw contents for these documents are more than 20 terabytes of data. The indexing process runs as a sequence of ﬁve to ten MapReduce operations. Using MapReduce (instead of the ad-hoc distributed passes in the prior version of the indexing system) has provided several beneﬁts: • The indexing code is simpler, smaller, and easier to understand, because the code that deals with fault tolerance, distribution and parallelization is hidden within the MapReduce library. For example, the size of one phase of the computation dropped from approximately 3800 lines of C++ code to approximately 700 lines when expressed using MapReduce. • The performance of the MapReduce library is good enough that we can keep conceptually unrelated computations separate, instead of mixing them together to avoid extra passes over the data. This makes it easy to change the indexing process. For example, one change that took a few months to make in our old indexing system took only a few days to implement in the new system. • The indexing process has become much easier to operate, because most of the problems caused by machine failures, slow machines, and networking hiccups are dealt with automatically by the MapReduce library without operator intervention. Furthermore, it is easy to improve the performance of the indexing process by adding new machines to the indexing cluster.
7 Related Work
Many systems have provided restricted programming models and used the restrictions to parallelize the computation automatically. For example, an associative function can be computed over all preﬁxes of an N element array in log N time on N processors using parallel preﬁx computations [6, 9, 13]. MapReduce can be considered a simpliﬁcation and distillation of some of these models based on our experience with large real-world computations. More signiﬁcantly, we provide a fault-tolerant implementation that scales to thousands of processors. In contrast, most of the parallel processing systems have only been implemented on smaller scales and leave the details of handling machine failures to the programmer. Bulk Synchronous Programming [17] and some MPI primitives [11] provide higher-level abstractions that To appear in OSDI 2004
make it easier for programmers to write parallel programs. A key difference between these systems and MapReduce is that MapReduce exploits a restricted programming model to parallelize the user program automatically and to provide transparent fault-tolerance. Our locality optimization draws its inspiration from techniques such as active disks [12, 15], where computation is pushed into processing elements that are close to local disks, to reduce the amount of data sent across I/O subsystems or the network. We run on commodity processors to which a small number of disks are directly connected instead of running directly on disk controller processors, but the general approach is similar. Our backup task mechanism is similar to the eager scheduling mechanism employed in the Charlotte System [3]. One of the shortcomings of simple eager scheduling is that if a given task causes repeated failures, the entire computation fails to complete. We ﬁx some instances of this problem with our mechanism for skipping bad records. The MapReduce implementation relies on an in-house cluster management system that is responsible for distributing and running user tasks on a large collection of shared machines. Though not the focus of this paper, the cluster management system is similar in spirit to other systems such as Condor [16]. The sorting facility that is a part of the MapReduce library is similar in operation to NOW-Sort [1]. Source machines (map workers) partition the data to be sorted and send it to one of R reduce workers. Each reduce worker sorts its data locally (in memory if possible). Of course NOW-Sort does not have the user-deﬁnable Map and Reduce functions that make our library widely applicable. River [2] provides a programming model where processes communicate with each other by sending data over distributed queues. Like MapReduce, the River system tries to provide good average case performance even in the presence of non-uniformities introduced by heterogeneous hardware or system perturbations. River achieves this by careful scheduling of disk and network transfers to achieve balanced completion times. MapReduce has a different approach. By restricting the programming model, the MapReduce framework is able to partition the problem into a large number of ﬁnegrained tasks. These tasks are dynamically scheduled on available workers so that faster workers process more tasks. The restricted programming model also allows us to schedule redundant executions of tasks near the end of the job which greatly reduces completion time in the presence of non-uniformities (such as slow or stuck workers). BAD-FS [5] has a very different programming model from MapReduce, and unlike MapReduce, is targeted to 11
the execution of jobs across a wide-area network. However, there are two fundamental similarities. (1) Both systems use redundant execution to recover from data loss caused by failures. (2) Both use locality-aware scheduling to reduce the amount of data sent across congested network links. TACC [7] is a system designed to simplify construction of highly-available networked services. Like MapReduce, it relies on re-execution as a mechanism for implementing fault-tolerance.
8 Conclusions
The MapReduce programming model has been successfully used at Google for many different purposes. We attribute this success to several reasons. First, the model is easy to use, even for programmers without experience with parallel and distributed systems, since it hides the details of parallelization, fault-tolerance, locality optimization, and load balancing. Second, a large variety of problems are easily expressible as MapReduce computations. For example, MapReduce is used for the generation of data for Google’s production web search service, for sorting, for data mining, for machine learning, and many other systems. Third, we have developed an implementation of MapReduce that scales to large clusters of machines comprising thousands of machines. The implementation makes efﬁcient use of these machine resources and therefore is suitable for use on many of the large computational problems encountered at Google. We have learned several things from this work. First, restricting the programming model makes it easy to parallelize and distribute computations and to make such computations fault-tolerant. Second, network bandwidth is a scarce resource. A number of optimizations in our system are therefore targeted at reducing the amount of data sent across the network: the locality optimization allows us to read data from local disks, and writing a single copy of the intermediate data to local disk saves network bandwidth. Third, redundant execution can be used to reduce the impact of slow machines, and to handle machine failures and data loss.
David Kramer, Shun-Tak Leung, and Josh Redstone for their work in developing GFS. We would also like to thank Percy Liang and Olcan Sercinoglu for their work in developing the cluster management system used by MapReduce. Mike Burrows, Wilson Hsieh, Josh Levenberg, Sharon Perl, Rob Pike, and Debby Wallach provided helpful comments on earlier drafts of this paper. The anonymous OSDI reviewers, and our shepherd, Eric Brewer, provided many useful suggestions of areas where the paper could be improved. Finally, we thank all the users of MapReduce within Google’s engineering organization for providing helpful feedback, suggestions, and bug reports.
References
[1] Andrea C. Arpaci-Dusseau, Remzi H. Arpaci-Dusseau, David E. Culler, Joseph M. Hellerstein, and David A. Patterson. High-performance sorting on networks of workstations. In Proceedings of the 1997 ACM SIGMOD International Conference on Management of Data, Tucson, Arizona, May 1997. [2] Remzi H. Arpaci-Dusseau, Eric Anderson, Noah Treuhaft, David E. Culler, Joseph M. Hellerstein, David Patterson, and Kathy Yelick. Cluster I/O with River: Making the fast case common. In Proceedings of the Sixth Workshop on Input/Output in Parallel and Distributed Systems (IOPADS ’99), pages 10–22, Atlanta, Georgia, May 1999. [3] Arash Baratloo, Mehmet Karaul, Zvi Kedem, and Peter Wyckoff. Charlotte: Metacomputing on the web. In Proceedings of the 9th International Conference on Parallel and Distributed Computing Systems, 1996. [4] Luiz A. Barroso, Jeffrey Dean, and Urs H¨ lzle. Web o search for a planet: The Google cluster architecture. IEEE Micro, 23(2):22–28, April 2003. [5] John Bent, Douglas Thain, Andrea C.Arpaci-Dusseau, Remzi H. Arpaci-Dusseau, and Miron Livny. Explicit control in a batch-aware distributed ﬁle system. In Proceedings of the 1st USENIX Symposium on Networked Systems Design and Implementation NSDI, March 2004. [6] Guy E. Blelloch. Scans as primitive parallel operations. IEEE Transactions on Computers, C-38(11), November 1989. [7] Armando Fox, Steven D. Gribble, Yatin Chawathe, Eric A. Brewer, and Paul Gauthier. Cluster-based scalable network services. In Proceedings of the 16th ACM Symposium on Operating System Principles, pages 78– 91, Saint-Malo, France, 1997. [8] Sanjay Ghemawat, Howard Gobioff, and Shun-Tak Leung. The Google ﬁle system. In 19th Symposium on Operating Systems Principles, pages 29–43, Lake George, New York, 2003.
Acknowledgements
Josh Levenberg has been instrumental in revising and extending the user-level MapReduce API with a number of new features based on his experience with using MapReduce and other people’s suggestions for enhancements. MapReduce reads its input from and writes its output to the Google File System [8]. We would like to thank Mohit Aron, Howard Gobioff, Markus Gutschke, To appear in OSDI 2004
12
[9] S. Gorlatch. Systematic efﬁcient parallelization of scan and other list homomorphisms. In L. Bouge, P. Fraigniaud, A. Mignotte, and Y. Robert, editors, Euro-Par’96. Parallel Processing, Lecture Notes in Computer Science 1124, pages 401–408. Springer-Verlag, 1996. [10] Jim Gray. Sort benchmark home page. <a href="http://research.microsoft.com/barc/SortBenchmark/" target="_blank">http://research.microsoft.com/barc/SortBenchmark/</a>. [11] William Gropp, Ewing Lusk, and Anthony Skjellum. Using MPI: Portable Parallel Programming with the Message-Passing Interface. MIT Press, Cambridge, MA, 1999. [12] L. Huston, R. Sukthankar, R. Wickremesinghe, M. Satyanarayanan, G. R. Ganger, E. Riedel, and A. Ailamaki. Diamond: A storage architecture for early discard in interactive search. In Proceedings of the 2004 USENIX File and Storage Technologies FAST Conference, April 2004. [13] Richard E. Ladner and Michael J. Fischer. Parallel preﬁx computation. Journal of the ACM, 27(4):831–838, 1980. [14] Michael O. Rabin. Efﬁcient dispersal of information for security, load balancing and fault tolerance. Journal of the ACM, 36(2):335–348, 1989. [15] Erik Riedel, Christos Faloutsos, Garth A. Gibson, and David Nagle. Active disks for large-scale data processing. IEEE Computer, pages 68–74, June 2001. [16] Douglas Thain, Todd Tannenbaum, and Miron Livny. Distributed computing in practice: The Condor experience. Concurrency and Computation: Practice and Experience, 2004. [17] L. G. Valiant. A bridging model for parallel computation. Communications of the ACM, 33(8):103–111, 1997. [18] Jim Wyllie. Spsort: How to sort a terabyte quickly. <a href="http://alme1.almaden.ibm.com/cs/spsort.pdf" target="_blank">http://alme1.almaden.ibm.com/cs/spsort.pdf</a>.
if (start &lt; i) Emit(text.substr(start,i-start),&quot;1&quot;); } } }; REGISTER_MAPPER(WordCounter); // User’s reduce function class Adder : public Reducer { virtual void Reduce(ReduceInput/</em> input) { // Iterate over all entries with the // same key and add the values int64 value = 0; while (!input-&gt;done()) { value += StringToInt(input-&gt;value()); input-&gt;NextValue(); } // Emit sum for input-&gt;key() Emit(IntToString(value)); } }; REGISTER_REDUCER(Adder); int main(int argc, char/<em>/</em> argv) { ParseCommandLineFlags(argc, argv); MapReduceSpecification spec; // Store list of input files into &quot;spec&quot; for (int i = 1; i &lt; argc; i++) { MapReduceInput/<em> input = spec.add_input(); input-&gt;set_format(&quot;text&quot;); input-&gt;set_filepattern(argv[i]); input-&gt;set_mapper_class(&quot;WordCounter&quot;); } // Specify the output files: // /gfs/test/freq-00000-of-00100 // /gfs/test/freq-00001-of-00100 // ... MapReduceOutput/</em> out = spec.output(); out-&gt;set_filebase(&quot;/gfs/test/freq&quot;); out-&gt;set_num_tasks(100); out-&gt;set_format(&quot;text&quot;); out-&gt;set_reducer_class(&quot;Adder&quot;); // Optional: do partial sums within map // tasks to save network bandwidth out-&gt;set_combiner_class(&quot;Adder&quot;); // Tuning parameters: use at most 2000 // machines and 100 MB of memory per task spec.set_machines(2000); spec.set_map_megabytes(100); spec.set_reduce_megabytes(100); // Now run it MapReduceResult result; if (!MapReduce(spec, &amp;result)) abort(); // Done: ’result’ structure contains info // about counters, time taken, number of // machines used, etc. return 0; }
A
Word Frequency
This section contains a program that counts the number of occurrences of each unique word in a set of input ﬁles speciﬁed on the command line.
/#include &quot;mapreduce/mapreduce.h&quot; // User’s map function class WordCounter : public Mapper { public: virtual void Map(const MapInput&amp; input) { const string&amp; text = input.value(); const int n = text.size(); for (int i = 0; i &lt; n; ) { // Skip past leading whitespace while ((i &lt; n) &amp;&amp; isspace(text[i])) i++; // Find word end int start = i; while ((i &lt; n) &amp;&amp; !isspace(text[i])) i++;
To appear in OSDI 2004
13</li>
</ol>

      
    </div>
    
    
      
    
    
        <footer id="post-meta">
        <span class="categories">Posted in<span class="breadcrumb fa fa-folder"><li><a href="/categories/hadoop/">hadoop</a></li></span></span> | <span class="tags">Tagged <a href="/tags/hadoop/" class="label label-primary">hadoop</a></span> | <span class="time">recent updated:<time title="2014-03-07 09:54:36"datetime="2014-03-07 09:54:36"> mar. 7 2014</time></span> | <span class="comment-link">
<a href="http://itsolife.com/2014/02/02/2014-02-02-hadoop--mapreduce-osdi04/#comments" class="ds-thread-count comment-link" data-thread-key="2014-02-02-hadoop--mapreduce-osdi04" data-count-type="comments">暂无评论</a></span>
        </footer>
    
    <div class="clearfix"></div>
  </div>
</div>
</article>





<ul id="pagination" class="pagination pagination-lg">
  <li><a class="extend prev" href="/page/113/">&laquo;</a></li><li><a class="page-number" href="/">1</a></li><li><a class="page-number" href="/page/2/">2</a></li><li><span class="space">&hellip;</span></li><li><a class="page-number" href="/page/111/">111</a></li><li><a class="page-number" href="/page/112/">112</a></li><li><a class="page-number" href="/page/113/">113</a></li><li class="active"><li><span class="page-number current">114</span></li><li><a class="page-number" href="/page/115/">115</a></li><li><a class="page-number" href="/page/116/">116</a></li><li><a class="page-number" href="/page/117/">117</a></li><li><span class="space">&hellip;</span></li></li><li><a class="page-number" href="/page/161/">161</a></li><li><a class="page-number" href="/page/162/">162</a></li><li><a class="extend next" href="/page/115/">&raquo;</a></li>
  <div class="clearfix"></div>
</ul></div><!--wapper-->
       </div><!-- ID main-col END -->
       <aside id="sidebar" class="alignright col-sx-6 col-sm-4 col-md-3 col-lg-3">
<div id="widget_search" class="widget panel panel-primary">
    <form action="//google.com/search" method="get" accept-charset="utf-8">
  <div class="input-group">
    <input class="form-control" id="searchbox" type="search" name="q" results="0" placeholder="search">
    <span class="input-group-btn">
      <button class="btn btn-default" type="submit">Go!</button>
    </span>
    <input type="hidden" name="q" value="site:itsolife.com">
  </div>
</form>
</div>

<div id="widget_category" class="widget panel panel-primary">
  <div class="panel-heading">category</div>  <div data-src='category' class='ajax_widgets'>正在加载...</div>
</div>

<div id="widget_recent_posts" class="widget panel panel-primary">
  <div class="panel-heading">recent posts</div>  <div data-src='recent_posts' class='ajax_widgets'>正在加载...</div></div>

<div id="widget_tagcloud" class="widget panel panel-primary">
  <div class="panel-heading">tagcloud</div>  <div data-src='tagcloud' class='ajax_widgets'>正在加载...</div></div>

<div id="widget_latest_update_posts" class="widget panel panel-primary">
  <div class="panel-heading">最近更新</div>  <div data-src='latest_update_posts' class='ajax_widgets'>正在加载...</div></div>

<div id="widget_recent_comments" class="widget panel panel-primary">
  <div class="panel-heading">recent comments</div>  

<div class="list-group-item ds-recent-comments" data-num-items="6" data-show-avatars="1" data-show-time="1" data-show-title="1" data-show-admin="1" data-excerpt-length="50"></div>



</div>

</aside>
       <div class="clearfix"></div>
     </div><!-- row END -->
  </div>
  <footer id="footer" class="container">
  <div class="panel panel-info">
  <section id='footer_widget'></section>  <div class="panel-footer">
  <div id="site-info">
    <span class='author'>
  
  &copy; 2014 RobinChia
  
    &nbsp;&nbsp;</span>
  
  <span id='analytics-51la'></span><span id='analytics-google'>
<script type="text/javascript">
  var _gaq = _gaq || [];
  _gaq.push(['_setAccount', 'UA-48559895-1']);
  _gaq.push(['_trackPageview']);
  _js2load.push({src:('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js'});
</script></span><span id='analytics-cnzz'>
<script type="text/javascript">var cnzz_protocol = (("https:" == document.location.protocol) ? " https://" : " http://");document.write(unescape("%3Cspan id='cnzz_stat_icon_5774006'%3E%3C/span%3E%3Cscript src='" + cnzz_protocol + "s17.cnzz.com/stat.php%3Fid%3D5774006%26show%3Dpic' type='text/javascript'%3E%3C/script%3E"));</script>
</span><span id='analytics-baidu'>
<script>
var _hmt = _hmt || [];
_js2load.push({src:"//hm.baidu.com/hm.js?1442f50724afc42380b51f097c43082c"});
</script>
</span>  </div>
  <div id="copyright">Site powered by <a href='http://zespia.tw/hexo/'><strong>hexo</strong></a>  update time: <em>2014-04-07 19:25:39</em></span></div>
</div>
<div class="clearfix"></div>


  </div>
  </footer>
  
        <script src="http://cdn.bootcss.com/jquery/1.10.2/jquery.min.js"></script>        
        <script src="http://cdn.staticfile.org/twitter-bootstrap/3.1.0/js/bootstrap.min.js"></script>        
                
        <script src="http://cdn.bootcss.com/prettify/r298/prettify.min.js"></script>    <script type="text/javascript">
   var lang=["bsh", "c", "cc", "cpp", "cs", "csh", "cyc", "cv", "htm", "html",
    "java", "js", "m", "mxml", "perl", "pl", "pm", "py", "rb", "sh",
    "xhtml", "xml", "xsl"];
   var pretty_base='';
   $('script').each(function(){
	var c = $(this).attr('src');
	if (!c)
	    return;
	if (c.match(/(\/)?prettify(\.min)?\.js/i))
	{
	    var index = c.lastIndexOf('/');
	    if (index != -1)
		pretty_base = c.substr(0,index + 1);
	    return false;
	}
   })
   $('pre code').each(function(){
	var c = $(this).attr('class')
	if (!c)
	    return;
	c = c.match(/\s?(lang\-\w+)/i);
	if (c && lang.indexOf(c[1]) == -1)
	{
	    lang.push(c[1]);
	    $.getScript(pretty_base + c[1] + '.min.js');
	}
   })

    $(window).load(function(){
       $("pre").addClass("prettyprint");
       prettyPrint();
    })
</script>    
            <script type="text/javascript">
var duoshuoQuery = {short_name:"robinchia"};
_js2load.push({src:'http://static.duoshuo.com/embed.js',charset:'UTF-8'});
</script>
    
            <!--wumii_relatedItems-->
    
        <script src="http://cdn.bootcss.com/fancybox/2.1.5/jquery.fancybox.min.js"></script>    <script type="text/javascript">
(function($){
  $('.entry').each(function(i){
    $(this).find('img').each(function(){
      var alt = this.alt;

      if (alt){
        $(this).before('<span class="caption">' + alt + '</span>');
      }

      $(this).wrap('<a href="' + this.src + '" title="' + alt + '" class="fancybox" rel="fancybox' + i + '" />');
    });
  });

  $('.fancybox').fancybox();
})(jQuery);
</script>    
        <script src="http://cdn.bootcss.com/mathjax/2.3/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>    <script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      processEscapes: true
    }
  });
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>
    
<script type="text/javascript">
$('.ajax_widgets').each(function(){var src=$(this).attr('data-src');if(src)$(this).load('/widgets/'+src+'.html');});
$.each(_js2load,function(index,obj){loadjs(obj.src,obj.charset)});
</script>

<div id="scroll2top">
<img src="/scroll2top/arrow.png"/>
</div>
<script src="/scroll2top/scroll2top.min.js"></script>
<div id="winterland">
  <canvas></canvas>
</div>
<script src="/js/winterland.min.js"></script>

  </body>
</html>
