
<!DOCTYPE HTML>
<html>
<head>
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<title>The 115 Page | It so life</title>
<meta name="author" content="RobinChia">

<meta name="description" content="It so life">


<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">


<meta property="og:site_name" content="It so life"/>

<!--[if IE]><style>.testIE.IE{display:inline;}</style><![endif]-->
<!--[if lte IE 7]><link rel="stylesheet" href="/css/ie7.css" type="text/css"><![endif]-->
<!--[if (lt IE 9)&(gt IE 7)]><style>.testIE.IE8{display:inline;}</style><![endif]-->
<!--[if gt IE 8]><style>.testIE.IE9{display:inline;}</style><![endif]-->

<link href="/favicon.png" rel="icon">
<link rel="alternate" href="/atom.xml" title="It so life Feed" type="application/atom+xml">

<link rel="stylesheet" href="http://cdn.staticfile.org/twitter-bootstrap/3.1.0/css/bootstrap.min.css" type="text/css">
<link rel="stylesheet" href="http://cdn.bootcss.com/font-awesome/4.0.3/css/font-awesome.min.css" type="text/css">
<link rel="stylesheet" href="http://cdn.bootcss.com/prettify/r298/prettify.min.css" type="text/css">
<link rel="stylesheet" href="http://cdn.bootcss.com/fancybox/2.1.5/jquery.fancybox.min.css" type="text/css">
<link rel="stylesheet" href="/css/style.css" type="text/css">
<!--[if lt IE 9]>
   <style>article,aside,dialog,footer,header,section,footer,nav,figure,menu{display:block}</style>
   <script src="http://cdn.staticfile.org/html5shiv/r29/html5.min.js"></script>
   <script src="http://cdn.staticfile.org/respond.js/1.4.2/respond.min.js"></script>
<link href="http://cdn.staticfile.org/respond.js/1.4.2/respond-proxy.html" id="respond-proxy" rel="respond-proxy" />
<link href="/js/respond.proxy.gif" id="respond-redirect" rel="respond-redirect" />
<script src="/js/respond.proxy.js"></script>
   <script src="http://cdn.bootcss.com/selectivizr/1.0.2/selectivizr-min.js"></script>
<![endif]-->
<script type="text/javascript">
function loadjs(c,d){var a=document.createElement("script");a.async=!0;a.type="text/javascript";a.src=c;a.charset=d||"gbk";(document.getElementsByTagName("HEAD")[0]||document.getElementsByTagName("BODY")[0]).appendChild(a)};
var _protocol = (("https:" == document.location.protocol) ? " https://" : " http://");
var _js2load = [];
</script>

</head>
<body>
      <header id="header" class="container"><nav id="main-nav" class="navbar navbar-default navbar-fixed-top " role="navigation">
  <div class="container">
    <div class="navbar-header">
    <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".navbar-collapse">
      <span class="sr-only">Toggle navigation</span>
      <span class="icon-bar"></span>
      <span class="icon-bar"></span>
      <span class="icon-bar"></span>
    </button>
    <a class="navbar-brand" href="/">It so life</a>
    </div>
    <div  class="collapse navbar-collapse">
      <ul  class="nav navbar-nav">
  
        <li><a href="/" title="Home">Home</a></li>      
        <li><a href="/about/" title="About">About</a></li>      
        <li><a href="/archives/" title="Archives">Archives</a></li>      
      <li class='dropdown'>
        <a class='dropdown-toggle' data-toggle='dropdown' href='#'>website<b class='caret'></b></a>
        <ul class='dropdown-menu pure-menu-selected'>
    
          <li><a href="//groups.google.com/forum/#!forum/pongba" title="TopLanguage">TopLanguage</a></li>    
          <li><a href="//itpub.net/" title="ITPub">ITPub</a></li>    
          <li><a href="//blog.jobbole.com/" title="Bole">Bole</a></li>    
          <li><a href="//nosql-database.org/" title="nosql">nosql</a></li>    
          <li><a href="//gitimmersion.googol.im/" title="Git">Git</a></li>    
        </ul>
      </li>
    
      </ul>
      <ul class="nav navbar-nav navbar-right">
      
        <li><a href="/atom.xml">RSS</a></li>
      
      
        <li><a href="https://twitter.com/robinchia">twitter</a></li>
      
      
      
      
        <li><a href="https://github.com/robinchia">github</a></li>
      
      </ul>
    </div>
  </div>
</nav>
<div class="clearfix"></div>
</header>
  <div id='content' class="container">
     <div class="page-header-wrapper">
      <!--[if lt IE 9]><div class="alert alert-warning alert-dismissable"><button type="button" class="close" data-dismiss="alert" aria-hidden="true">&times;</button><strong>提示:</strong>您的浏览器版本太低了,建议升级到 <strong><a href="http://windows.microsoft.com/zh-cn/internet-explorer/download-ie" title="IE9">IE9</a></strong> 以上,本站使用<a href="https://www.google.com/intl/zh-CN/chrome/">Chrome浏览器</a>可以获得最好的显示效果.</div><![endif]-->
      <div class="page-header"><h1 align="center"><big>It so life</big> </h1>
        <h5 align="center"><big>love as life</big></h5>
      </div>
     </div>
     <div class="row">
       <div id="main-col" class="alignleft col-sx-12 col-sm-8 col-md-9 col-lg-9">
      <section id='header_widget'></section>
          <div id="wrapper"><article>
<div id="post" class="post well">
  <div class="post-content">
    <header class="well-sm">
      <i class="fa icon fa-5x pull-left"></i>
      <h1 class="title"><a href="/2014/02/02/2014-02-02-hadoop--hdfs_shell/">hdfs_shell</a></h1>
      
        <span>Posted on<time datetime="2014-02-02T01:54:36.000Z"> <a href="/2014/02/02/2014-02-02-hadoop--hdfs_shell/">feb. 2 2014</a></time></span>      
    </header>
    
    
    <div class="entry">
      
        <h1 id="hdfs_shell">hdfs_shell</h1>
<p>HDFS File System Shell Guide
Table of contents
1
Overview............................................................................................................................3
1.1 1.2 1.3 1.4 1.5 1.6 1.7 1.8 1.9
cat ................................................................................................................................. 3 chgrp ............................................................................................................................. 3 chmod ........................................................................................................................... 3 chown ........................................................................................................................... 4 copyFromLocal..............................................................................................................4 copyToLocal..................................................................................................................4 count ............................................................................................................................. 4 cp .................................................................................................................................. 4 du................................................................................................................................... 5 dus ...............................................................................................................................5 expunge .......................................................................................................................5 get ................................................................................................................................5 getmerge ......................................................................................................................6 ls...................................................................................................................................6 lsr................................................................................................................................. 6 mkdir ...........................................................................................................................7 moveFromLocal ..........................................................................................................7 moveToLocal............................................................................................................... 7 mv ............................................................................................................................... 7 put ............................................................................................................................... 8 rm ................................................................................................................................ 8 rmr ...............................................................................................................................8 setrep ...........................................................................................................................9
Copyright © 2008 The Apache Software Foundation. All rights reserved.
1.10 1.11 1.12 1.13 1.14 1.15 1.16 1.17 1.18 1.19 1.20 1.21 1.22 1.23
HDFS File System Shell Guide
1.24 1.25 1.26 1.27 1.28
stat ...............................................................................................................................9 tail ............................................................................................................................... 9 test .............................................................................................................................10 text ............................................................................................................................ 10 touchz ........................................................................................................................10
Page 2
Copyright © 2008 The Apache Software Foundation. All rights reserved.
HDFS File System Shell Guide</p>
<ol>
<li>Overview
The FileSystem (FS) shell is invoked by bin/hadoop fs <args>. All FS shell commands take path URIs as arguments. The URI format is scheme://autority/path. For HDFS the scheme is hdfs, and for the local filesystem the scheme is file. The scheme and authority are optional. If not specified, the default scheme specified in the configuration is used. An HDFS file or directory such as /parent/child can be specified as hdfs://namenodehost/parent/child or simply as /parent/child (given that your configuration is set to point to hdfs://namenodehost). Most of the commands in FS shell behave like corresponding Unix commands. Differences are described with each of the commands. Error information is sent to stderr and the output is sent to stdout.
1.1. cat
Usage: hadoop fs -cat URI [URI …] Copies source paths to stdout. Example: • hadoop fs -cat hdfs://nn1.example.com/file1 hdfs://nn2.example.com/file2 • hadoop fs -cat file:///file3 /user/hadoop/file4 Exit Code: Returns 0 on success and -1 on error.
1.2. chgrp
Usage: hadoop fs -chgrp [-R] GROUP URI [URI …] Change group association of files. With -R, make the change recursively through the directory structure. The user must be the owner of files, or else a super-user. Additional information is in the HDFS Admin Guide: Permissions.
1.3. chmod
Usage: hadoop fs -chmod [-R] <MODE[,MODE]... | OCTALMODE> URI [URI …] Change the permissions of files. With -R, make the change recursively through the directory structure. The user must be the owner of the file, or else a super-user. Additional information is in the HDFS Admin Guide: Permissions.
Page 3
Copyright © 2008 The Apache Software Foundation. All rights reserved.
HDFS File System Shell Guide
1.4. chown
Usage: hadoop fs -chown [-R] [OWNER][:[GROUP]] URI [URI ] Change the owner of files. With -R, make the change recursively through the directory structure. The user must be a super-user. Additional information is in the HDFS Admin Guide: Permissions.
1.5. copyFromLocal
Usage: hadoop fs -copyFromLocal <localsrc> URI Similar to put command, except that the source is restricted to a local file reference.
1.6. copyToLocal
Usage: hadoop fs -copyToLocal [-ignorecrc] [-crc] URI <localdst> Similar to get command, except that the destination is restricted to a local file reference.
1.7. count
Usage: hadoop fs -count [-q] <paths> Count the number of directories, files and bytes under the paths that match the specified file pattern. The output columns are: DIR_COUNT, FILE_COUNT, CONTENT_SIZE FILE_NAME. The output columns with -q are: QUOTA, REMAINING_QUATA, SPACE_QUOTA, REMAINING_SPACE_QUOTA, DIR_COUNT, FILE_COUNT, CONTENT_SIZE, FILE_NAME. Example: • hadoop fs -count hdfs://nn1.example.com/file1 hdfs://nn2.example.com/file2 • hadoop fs -count -q hdfs://nn1.example.com/file1 Exit Code: Returns 0 on success and -1 on error.
1.8. cp
Page 4
Copyright © 2008 The Apache Software Foundation. All rights reserved.
HDFS File System Shell Guide
Usage: hadoop fs -cp URI [URI …] <dest> Copy files from source to destination. This command allows multiple sources as well in which case the destination must be a directory. Example: • hadoop fs -cp /user/hadoop/file1 /user/hadoop/file2 • hadoop fs -cp /user/hadoop/file1 /user/hadoop/file2 /user/hadoop/dir Exit Code: Returns 0 on success and -1 on error.
1.9. du
Usage: hadoop fs -du URI [URI …] Displays aggregate length of files contained in the directory or the length of a file in case its just a file. Example: hadoop fs -du /user/hadoop/dir1 /user/hadoop/file1 hdfs://nn.example.com/user/hadoop/dir1 Exit Code: Returns 0 on success and -1 on error.
1.10. dus
Usage: hadoop fs -dus <args> Displays a summary of file lengths.
1.11. expunge
Usage: hadoop fs -expunge Empty the Trash. Refer to HDFS Architecture for more information on Trash feature.
1.12. get
Usage: hadoop fs -get [-ignorecrc] [-crc] <src> <localdst> Copy files to the local file system. Files that fail the CRC check may be copied with the -ignorecrc option. Files and CRCs may be copied using the -crc option.
Page 5
Copyright © 2008 The Apache Software Foundation. All rights reserved.
HDFS File System Shell Guide
Example: • hadoop fs -get /user/hadoop/file localfile • hadoop fs -get hdfs://nn.example.com/user/hadoop/file localfile Exit Code: Returns 0 on success and -1 on error.
1.13. getmerge
Usage: hadoop fs -getmerge <src> <localdst> [addnl] Takes a source directory and a destination file as input and concatenates files in src into the destination local file. Optionally addnl can be set to enable adding a newline character at the end of each file.
1.14. ls
Usage: hadoop fs -ls <args> For a file returns stat on the file with the following format: permissions number_of_replicas userid groupid filesize modification_date modification_time filename For a directory it returns list of its direct children as in unix.A directory is listed as: permissions userid groupid modification_date modification_time dirname Example: hadoop fs -ls /user/hadoop/file1 Exit Code: Returns 0 on success and -1 on error.
1.15. lsr
Usage: hadoop fs -lsr <args> Recursive version of ls. Similar to Unix ls -R.
Page 6
Copyright © 2008 The Apache Software Foundation. All rights reserved.
HDFS File System Shell Guide
1.16. mkdir
Usage: hadoop fs -mkdir <paths> Takes path uri&#39;s as argument and creates directories. The behavior is much like unix mkdir -p creating parent directories along the path. Example: • hadoop fs -mkdir /user/hadoop/dir1 /user/hadoop/dir2 • hadoop fs -mkdir hdfs://nn1.example.com/user/hadoop/dir hdfs://nn2.example.com/user/hadoop/dir Exit Code: Returns 0 on success and -1 on error.
1.17. moveFromLocal
Usage: dfs -moveFromLocal <localsrc> <dst> Similar to put command, except that the source localsrc is deleted after it&#39;s copied.
1.18. moveToLocal
Usage: hadoop fs -moveToLocal [-crc] <src> <dst> Displays a &quot;Not implemented yet&quot; message.
1.19. mv
Usage: hadoop fs -mv URI [URI …] <dest> Moves files from source to destination. This command allows multiple sources as well in which case the destination needs to be a directory. Moving files across filesystems is not permitted. Example: • hadoop fs -mv /user/hadoop/file1 /user/hadoop/file2 • hadoop fs -mv hdfs://nn.example.com/file1 hdfs://nn.example.com/file2 hdfs://nn.example.com/file3 hdfs://nn.example.com/dir1 Exit Code:
Page 7
Copyright © 2008 The Apache Software Foundation. All rights reserved.
HDFS File System Shell Guide
Returns 0 on success and -1 on error.
1.20. put
Usage: hadoop fs -put <localsrc> ... <dst> Copy single src, or multiple srcs from local file system to the destination filesystem. Also reads input from stdin and writes to destination filesystem. • hadoop fs -put localfile /user/hadoop/hadoopfile • hadoop fs -put localfile1 localfile2 /user/hadoop/hadoopdir • hadoop fs -put localfile hdfs://nn.example.com/hadoop/hadoopfile • hadoop fs -put - hdfs://nn.example.com/hadoop/hadoopfile Reads the input from stdin. Exit Code: Returns 0 on success and -1 on error.
1.21. rm
Usage: hadoop fs -rm [-skipTrash] URI [URI …] Delete files specified as args. Only deletes non empty directory and files. If the -skipTrash option is specified, the trash, if enabled, will be bypassed and the specified file(s) deleted immediately. This can be useful when it is necessary to delete files from an over-quota directory. Refer to rmr for recursive deletes. Example: • hadoop fs -rm hdfs://nn.example.com/file /user/hadoop/emptydir Exit Code: Returns 0 on success and -1 on error.
1.22. rmr
Usage: hadoop fs -rmr [-skipTrash] URI [URI …] Recursive version of delete. If the -skipTrash option is specified, the trash, if enabled, will be bypassed and the specified file(s) deleted immediately. This can be useful when it is necessary to delete files from an over-quota directory.
Page 8
Copyright © 2008 The Apache Software Foundation. All rights reserved.
HDFS File System Shell Guide
Example: • hadoop fs -rmr /user/hadoop/dir • hadoop fs -rmr hdfs://nn.example.com/user/hadoop/dir Exit Code: Returns 0 on success and -1 on error.
1.23. setrep
Usage: hadoop fs -setrep [-R] <path> Changes the replication factor of a file. -R option is for recursively increasing the replication factor of files within a directory. Example: • hadoop fs -setrep -w 3 -R /user/hadoop/dir1 Exit Code: Returns 0 on success and -1 on error.
1.24. stat
Usage: hadoop fs -stat URI [URI …] Returns the stat information on the path. Example: • hadoop fs -stat path Exit Code: Returns 0 on success and -1 on error.
1.25. tail
Usage: hadoop fs -tail [-f] URI Displays last kilobyte of the file to stdout. -f option can be used as in Unix. Example: • hadoop fs -tail pathname Exit Code:
Page 9
Copyright © 2008 The Apache Software Foundation. All rights reserved.
HDFS File System Shell Guide
Returns 0 on success and -1 on error.
1.26. test
Usage: hadoop fs -test -[ezd] URI Options: -e check to see if the file exists. Return 0 if true. -z check to see if the file is zero length. Return 0 if true. -d check to see if the path is directory. Return 0 if true. Example: • hadoop fs -test -e filename
1.27. text
Usage: hadoop fs -text <src> Takes a source file and outputs the file in text format. The allowed formats are zip and TextRecordInputStream.
1.28. touchz
Usage: hadoop fs -touchz URI [URI …] Create a file of zero length. Example: • hadoop -touchz pathname Exit Code: Returns 0 on success and -1 on error.
Page 10
Copyright © 2008 The Apache Software Foundation. All rights reserved.</li>
</ol>

      
    </div>
    
    
      
    
    
        <footer id="post-meta">
        <span class="categories">Posted in<span class="breadcrumb fa fa-folder"><li><a href="/categories/hadoop/">hadoop</a></li></span></span> | <span class="tags">Tagged <a href="/tags/hadoop/" class="label label-primary">hadoop</a></span> | <span class="time">recent updated:<time title="2014-03-07 09:54:36"datetime="2014-03-07 09:54:36"> mar. 7 2014</time></span> | <span class="comment-link">
<a href="http://itsolife.com/2014/02/02/2014-02-02-hadoop--hdfs_shell/#comments" class="ds-thread-count comment-link" data-thread-key="2014-02-02-hadoop--hdfs_shell" data-count-type="comments">暂无评论</a></span>
        </footer>
    
    <div class="clearfix"></div>
  </div>
</div>
</article>



  <article>
<div id="post" class="post well">
  <div class="post-content">
    <header class="well-sm">
      <i class="fa icon fa-5x pull-left"></i>
      <h1 class="title"><a href="/2014/02/02/2014-02-02-hadoop--NotesforHadoopthedefinitiveguide/">Notes for Hadoop the definitive guide</a></h1>
      
        <span>Posted on<time datetime="2014-02-02T01:54:36.000Z"> <a href="/2014/02/02/2014-02-02-hadoop--NotesforHadoopthedefinitiveguide/">feb. 2 2014</a></time></span>      
    </header>
    
    
    <div class="entry">
      
        <h1 id="notes-for-hadoop-the-definitive-guide">Notes for Hadoop the definitive guide</h1>
<h1 id="1-introduction-to-hdfs">1. Introduction to HDFS</h1>
<h2 id="1-1-hdfs-concepts">1.1. HDFS Concepts</h2>
<h3 id="1-1-1-blocks">1.1.1. Blocks</h3>
<p>l HDFS too has the concept of a block, but it is a much larger unit 64 MB by default.</p>
<p>l Like in a filesystem for a single disk, files in HDFS are broken into block-sized chunks, which are stored as independent units.</p>
<p>l Unlike a filesystem for a single disk, a file in HDFS that is smaller than a single block does not occupy a full block’s worth of underlying storage.</p>
<h3 id="1-1-2-namenodes-and-datanodes">1.1.2. Namenodes and Datanodes</h3>
<p>l The namenode manages the filesystem namespace.</p>
<p>n It maintains the filesystem tree and the metadata for all the files and directories in the tree.</p>
<p>n This information is stored persistently on the local disk in the form of two files: the namespace image and the edit log.</p>
<p>n The namenode also knows the datanodes on which all the blocks for a given file are located, however, it does not store block locations persistently, since this information is reconstructed from datanodes when the system starts.</p>
<p>l Datanodes are the work horses of the filesystem.</p>
<p>n They store and retrieve blocks when they are told to (by clients or the namenode)</p>
<p>n They report back to the namenode periodically with lists of blocks that they are storing.</p>
<p>l secondary namenode</p>
<p>n It does not act as a namenode.</p>
<p>n Its main role is to periodically merge the namespace image with the edit log to prevent the edit log from becoming too large.</p>
<p>n It keeps a copy of the merged name space image, which can be used in the event of the namenode failing.</p>
<h3 id="namenode-directory-structure">Namenode directory structure</h3>
<p><a href="http://images.cnblogs.com/cnblogs_com/forfuture1978/WindowsLiveWriter/NotesforHadoopthedefinitiveguide_14109/clip_image002_2.jpg" target="_blank"><img src="&quot;clip_image002&quot;" alt="clip_image002"></a></p>
<p>l The VERSION file is a Java properties file that contains information about the version of HDFS that is running</p>
<p>n The layoutVersion is a negative integer that defines the version of HDFS’s persistent data structures.</p>
<p>n The namespaceID is a unique identifier for the filesystem, which is created when the filesystem is first formatted.</p>
<p>n The cTime property marks the creation time of the namenode’s storage.</p>
<p>n The storageType indicates that this storage directory contains data structures for a namenode.</p>
<p><a href="http://images.cnblogs.com/cnblogs_com/forfuture1978/WindowsLiveWriter/NotesforHadoopthedefinitiveguide_14109/clip_image004_2.jpg" target="_blank"><img src="&quot;clip_image004&quot;" alt="clip_image004"></a></p>
<h3 id="the-filesystem-image-and-edit-log">The filesystem image and edit log</h3>
<p>l When a filesystem client performs a write operation, it is first recorded in the edit log.</p>
<p>l The namenode also has an in-memory representation of the filesystem metadata, which it updates after the edit log has been modified.</p>
<p>l The edit log is flushed and synced after every write before a success code is returned to the client.</p>
<p>l The fsimage file is a persistent checkpoint of the filesystem metadata. it is not updated for every filesystem write operation.</p>
<p>l If the namenode fails, then the latest state of its metadata can be reconstructed by loading the fsimage from disk into memory, then applying each of the operations in the edit log.</p>
<p>l This is precisely what the namenode does when it starts up.</p>
<p>l The fsimage file contains a serialized form of all the directory and file inodes in the filesystem.</p>
<p>l The secondary namenode is to produce checkpoints of the primary’s in-memory filesystem metadata.</p>
<p>l The checkpointing process proceeds as follows :</p>
<p>n The secondary asks the primary to roll its edits file, so new edits go to a new file.</p>
<p>n The secondary retrieves fsimage and edits from the primary (using HTTP GET).</p>
<p>n The secondary loads fsimage into memory, applies each operation from edits, then creates a new consolidated fsimage file.</p>
<p>n The secondary sends the new fsimage back to the primary (using HTTP POST).</p>
<p>n The primary replaces the old fsimage with the new one from the secondary, and the old edits file with the new one it started in step 1. It also updates the fstime file to record the time that the checkpoint was taken.</p>
<p>n At the end of the process, the primary has an up-to-date fsimage file, and a shorter edits file.</p>
<p><a href="http://images.cnblogs.com/cnblogs_com/forfuture1978/WindowsLiveWriter/NotesforHadoopthedefinitiveguide_14109/clip_image006_2.jpg" target="_blank"><img src="&quot;clip_image006&quot;" alt="clip_image006"></a></p>
<h3 id="secondary-namenode-directory-structure">Secondary namenode directory structure</h3>
<p><a href="http://images.cnblogs.com/cnblogs_com/forfuture1978/WindowsLiveWriter/NotesforHadoopthedefinitiveguide_14109/clip_image008_2.jpg" target="_blank"><img src="&quot;clip_image008&quot;" alt="clip_image008"></a></p>
<h3 id="datanode-directory-structure">Datanode directory structure</h3>
<p><a href="http://images.cnblogs.com/cnblogs_com/forfuture1978/WindowsLiveWriter/NotesforHadoopthedefinitiveguide_14109/clip_image010_2.jpg" target="_blank"><img src="&quot;clip_image010&quot;" alt="clip_image010"></a></p>
<p>l A datanode’s VERSION file</p>
<p><a href="http://images.cnblogs.com/cnblogs_com/forfuture1978/WindowsLiveWriter/NotesforHadoopthedefinitiveguide_14109/clip_image012_2.jpg" target="_blank"><img src="&quot;clip_image012&quot;" alt="clip_image012"></a></p>
<p>l The other files in the datanode’s current storage directory are the files with the blk_ prefix.</p>
<p>n There are two types: the HDFS blocks themselves (which just consist of the file’s raw bytes) and the metadata for a block (with a .meta suffix).</p>
<p>n A block file just consists of the raw bytes of a portion of the file being stored;</p>
<p>n the metadata file is made up of a header with version and type information, followed by a series of checksums for sections of the block.</p>
<p>l When the number of blocks in a directory grows to a certain size, the datanode creates a new subdirectory in which to place new blocks and their accompanying metadata.</p>
<h2 id="1-2-data-flow">1.2. Data Flow</h2>
<h3 id="1-2-1-anatomy-of-a-file-read">1.2.1. Anatomy of a File Read</h3>
<p><a href="http://images.cnblogs.com/cnblogs_com/forfuture1978/WindowsLiveWriter/NotesforHadoopthedefinitiveguide_14109/clip_image014_2.jpg" target="_blank"><img src="&quot;clip_image014&quot;" alt="clip_image014"></a></p>
<p>l The client opens the file it wishes to read by calling open() on the FileSystem object (step 1).</p>
<p>l DistributedFileSystem calls the namenode, using RPC, to determine the locations of the blocks for the first few blocks in the file (step 2).</p>
<p>l For each block, the namenode returns the addresses of the datanodes that have a copy of that block.</p>
<p>l The datanodes are sorted according to their proximity to the client.</p>
<p>l The DistributedFileSystem returns a FSDataInputStream to the client for it to read data from.</p>
<p>l The client then calls read() on the stream (step 3).</p>
<p>l DFSInputStream connects to the first (closest) datanode for the first block in the file.</p>
<p>l Data is streamed from the datanode back to the client (step 4).</p>
<p>l When the end of the block is reached, DFSInputStream will close the connection to the datanode, then find the best datanode for the next block (step 5).</p>
<p>l When the client has finished reading, it calls close() on the FSDataInputStream (step 6).</p>
<p>l During reading, if the client encounters an error while communicating with a datanode, then it will try the next closest one for that block.</p>
<p>l It will also remember datanodes that have failed so that it doesn’t needlessly retry them for later blocks.</p>
<p>l The client also verifies checksums for the data transferred to it from the datanode. If a corrupted block is found, it is reported to the namenode.</p>
<h3 id="1-2-2-anatomy-of-a-file-write">1.2.2. Anatomy of a File Write</h3>
<p><a href="http://images.cnblogs.com/cnblogs_com/forfuture1978/WindowsLiveWriter/NotesforHadoopthedefinitiveguide_14109/clip_image016_2.jpg" target="_blank"><img src="&quot;clip_image016&quot;" alt="clip_image016"></a></p>
<p>l The client creates the file by calling create() (step 1).</p>
<p>l DistributedFileSystem makes an RPC call to the namenode to create a new file in the filesystem’s namespace, with no blocks associated with it (step 2).</p>
<p>l The namenode performs various checks to make sure the file doesn’t already exist, and that the client has the right permissions to create the file. If these checks pass, the namenode makes a record of the new file; otherwise, file creation fails and the client is thrown an IOException.</p>
<p>l The DistributedFileSystem returns a FSDataOutputStream for the client to start writing data to.</p>
<p>l As the client writes data (step 3), DFSOutputStream splits it into packets, which it writes to an internal queue, called the data queue.</p>
<p>l The data queue is consumed by the Data Streamer, whose responsibility it is to ask the namenode to allocate new blocks by picking a list of suitable datanodes to store the replicas. The list of datanodes forms apipeline.</p>
<p>l The DataStreamer streams the packets to the first datanode in the pipeline, which stores the packet and forwards it to the second datanode in the pipeline. Similarly, the second datanode stores the packet and forwards it to the third (and last) datanode in the pipe line (step 4).</p>
<p>l DFSOutputStream also maintains an internal queue of packets that are waiting to be acknowledged by datanodes, called the ack queue. A packet is removed from the ack queue only when it has been acknowledged by all the datanodes in the pipeline (step 5).</p>
<p>l If a datanode fails while data is being written to it,</p>
<p>n First the pipeline is closed, and any packets in the ack queue are added to the front of the data queue.</p>
<p>n The current block on the good datanodes is given a new identity by the namenode, so that the partial block on the failed datanode will be deleted if the failed data node recovers later on.</p>
<p>n The failed datanode is removed from the pipeline and the remainder of the block’s data is written to the two good datanodes in the pipeline.</p>
<p>n The namenode notices that the block is under-replicated, and it arranges for a further replica to be created on another node.</p>
<p>l When the client has finished writing data it calls close() on the stream (step 6). This action flushes all the remaining packets to the datanode pipeline and waits for acknowledgments before contacting the namenode to signal that the file is complete (step7).</p>
<h1 id="2-meet-map-reduce">2. Meet Map/Reduce</h1>
<p>l MapReduce has two phases: the map phase and the reduce phase.</p>
<p>l Each phase has key-value pairs as input and output (the types can be specified).</p>
<p>n The input key-value types of the map phase is determined by the input format</p>
<p>n The output key-value types of the map phase should match the input key value types of the reduce phase</p>
<p>n The output key-value types of the reduce phase can be set in the JobConf interface.</p>
<p>l The programmer specifies two functions: the map function and the reduce function.</p>
<h2 id="2-1-mapreduce-logical-data-flow">2.1. MapReduce logical data flow</h2>
<p><a href="http://images.cnblogs.com/cnblogs_com/forfuture1978/WindowsLiveWriter/NotesforHadoopthedefinitiveguide_14109/clip_image018_2.jpg" target="_blank"><img src="&quot;clip_image018&quot;" alt="clip_image018"></a></p>
<h2 id="2-2-mapreduce-code">2.2. MapReduce Code</h2>
<h3 id="2-2-1-the-map-function-is-represented-by-an-implementation-of-the-mapper-interface-which-declares-a-map-method-">2.2.1. The map function is represented by an implementation of the Mapper interface, which declares a map() method.</h3>
<p><a href="http://images.cnblogs.com/cnblogs_com/forfuture1978/WindowsLiveWriter/NotesforHadoopthedefinitiveguide_14109/clip_image020_2.jpg" target="_blank"><img src="&quot;clip_image020&quot;" alt="clip_image020"></a></p>
<h3 id="2-2-2-the-reduce-function-is-defined-using-a-reducer">2.2.2. The reduce function is defined using a Reducer</h3>
<p>l The input types of the reduce function must match the output type of the map function.</p>
<p><a href="http://images.cnblogs.com/cnblogs_com/forfuture1978/WindowsLiveWriter/NotesforHadoopthedefinitiveguide_14109/clip_image022_2.jpg" target="_blank"><img src="&quot;clip_image022&quot;" alt="clip_image022"></a></p>
<h3 id="2-2-3-the-code-runs-the-mapreduce-job">2.2.3. The code runs the MapReduce job</h3>
<p>l An input path is specified by calling the static addInputPath() method on FileInputFormat</p>
<p>n It can be a single file, a directory, or a file pattern.</p>
<p>n addInputPath() can be called more than once to use input from multiple paths.</p>
<p>l The output path is specified by the static setOutputPath() method on FileOutputFormat.</p>
<p>n It specifies a directory where the output files from the reducer functions are written.</p>
<p>n The directory shouldn’t exist before running the job</p>
<p>l The map and reduce types can be specified via the setMapperClass() and setReducerClass() methods.</p>
<p>l The setOutputKeyClass() and setOutputValueClass() methods control the output types for the map and the reduce functions, which are often the same.</p>
<p>n If they are different, then the map output types can be set using the methods setMapOutputKeyClass() and setMapOutputValueClass().</p>
<p>l The input types are controlled via the input format, which we have not explicitly set since we are using the default TextInputFormat.</p>
<p><a href="http://images.cnblogs.com/cnblogs_com/forfuture1978/WindowsLiveWriter/NotesforHadoopthedefinitiveguide_14109/clip_image024_2.jpg" target="_blank"><img src="&quot;clip_image024&quot;" alt="clip_image024"></a></p>
<h2 id="2-3-scaling-out">2.3. Scaling Out</h2>
<h3 id="2-3-1-mapreduce-data-flow-with-a-single-reduce-task">2.3.1. MapReduce data flow with a single reduce task</h3>
<p>l A MapReduce job is a unit of work that the client wants to be performed: it consists of the input data, the MapReduce program, and configuration information.</p>
<p>l Hadoop runs the job by dividing it into tasks, of which there are two types: map tasks and reduce tasks.</p>
<p>l There are two types of nodes that control the job execution process: a jobtracker and a number of tasktrackers.</p>
<p>n The jobtracker coordinates all the jobs run on the system by scheduling tasks to run on tasktrackers.</p>
<p>n Tasktrackers run tasks and send progress reports to the jobtracker, which keeps a record of the overall progress of each job.</p>
<p>n If a tasks fails, the jobtracker can reschedule it on a different tasktracker.</p>
<p>l Hadoop divides the input to a MapReduce job into fixed-size input splits.</p>
<p>l Hadoop creates one map task for each split, which runs the user defined map function for each record in the split.</p>
<p>l Hadoop does its best to run the map task on a node where the input data resides in HDFS.</p>
<p>n This is called the data locality optimization.</p>
<p>n This is why the optimal split size is the same as the block size: it is the largest size of input that can be guaranteed to be stored on a single node.</p>
<p>l Reduce tasks don’t have the advantage of data locality</p>
<p>n The input to a single reduce task is normally the output from all mappers.</p>
<p>n The output of the reduce is normally stored in HDFS for reliability.</p>
<p><a href="http://images.cnblogs.com/cnblogs_com/forfuture1978/WindowsLiveWriter/NotesforHadoopthedefinitiveguide_14109/clip_image026_2.jpg" target="_blank"><img src="&quot;clip_image026&quot;" alt="clip_image026"></a></p>
<h3 id="2-3-2-mapreduce-data-flow-with-multiple-reduce-tasks">2.3.2. MapReduce data flow with multiple reduce tasks</h3>
<p>The number of reduce tasks is not governed by the size of the input, but is specified independently.</p>
<p>l When there are multiple reducers, the map tasks partition their output, each creating one partition for each reduce task.</p>
<p>l There can be many keys (and their associated values) in each partition, but the records for every key are all in a single partition.</p>
<p>l The partitioning can be controlled by a user-defined partitioning function</p>
<p>n Normally the default partitioner which buckets keys using a hash function.</p>
<p>n conf.setPartitionerClass(HashPartitioner.class);</p>
<p>n conf.setNumReduceTasks(1);</p>
<p>l The data flow between map and reduce tasks is “the shuffle,” as each reduce task is fed by many map tasks.</p>
<p><a href="http://images.cnblogs.com/cnblogs_com/forfuture1978/WindowsLiveWriter/NotesforHadoopthedefinitiveguide_14109/clip_image028_2.jpg" target="_blank"><img src="&quot;clip_image028&quot;" alt="clip_image028"></a></p>
<p>l It’s also possible to have zero reduce tasks. This can be appropriate when you don’t need the shuffle since the processing can be carried out entirely in parallel</p>
<h1 id="3-mapreduce-types-and-formats">3. MapReduce Types and Formats</h1>
<h2 id="3-1-mapreduce-types">3.1. MapReduce Types</h2>
<p>l The map and reduce functions in Hadoop MapReduce have the following general form:</p>
<p><a href="http://images.cnblogs.com/cnblogs_com/forfuture1978/WindowsLiveWriter/NotesforHadoopthedefinitiveguide_14109/clip_image030_2.jpg" target="_blank"><img src="&quot;clip_image030&quot;" alt="clip_image030"></a></p>
<p><a href="http://images.cnblogs.com/cnblogs_com/forfuture1978/WindowsLiveWriter/NotesforHadoopthedefinitiveguide_14109/clip_image032_2.jpg" target="_blank"><img src="&quot;clip_image032&quot;" alt="clip_image032"></a></p>
<p>l The partition function operates on the intermediate key and value types (K2 and V2), and returns the partition index.</p>
<p><a href="http://images.cnblogs.com/cnblogs_com/forfuture1978/WindowsLiveWriter/NotesforHadoopthedefinitiveguide_14109/clip_image034_2.jpg" target="_blank"><img src="&quot;clip_image034&quot;" alt="clip_image034"></a></p>
<p><a href="http://images.cnblogs.com/cnblogs_com/forfuture1978/WindowsLiveWriter/NotesforHadoopthedefinitiveguide_14109/clip_image036_2.jpg" target="_blank"><img src="&quot;clip_image036&quot;" alt="clip_image036"></a></p>
<h3 id="3-1-1-configuration-of-mapreduce-types">3.1.1. Configuration of MapReduce types</h3>
<p><a href="http://images.cnblogs.com/cnblogs_com/forfuture1978/WindowsLiveWriter/NotesforHadoopthedefinitiveguide_14109/clip_image038_2.jpg" target="_blank"><img src="&quot;clip_image038&quot;" alt="clip_image038"></a></p>
<p>l Input types are set by the input format.</p>
<p>n For instance, a TextInputFormat generates keys of type LongWritable and values of type Text.</p>
<p>l A minimal MapReduce driver, with the defaults explicitly set</p>
<p><a href="http://images.cnblogs.com/cnblogs_com/forfuture1978/WindowsLiveWriter/NotesforHadoopthedefinitiveguide_14109/clip_image040_2.jpg" target="_blank"><img src="&quot;clip_image040&quot;" alt="clip_image040"></a></p>
<p>l The default input format is TextInputFormat, which produces keys of type LongWritable (the offset of the beginning of the line in the file) and values of type Text (the line of text).</p>
<p>l The setNumMapTasks() call does not necessarily set the number of map tasks to one</p>
<p>n The actual number of map tasks depends on the size of the input</p>
<p>l The default mapper is IdentityMapper</p>
<p><a href="http://images.cnblogs.com/cnblogs_com/forfuture1978/WindowsLiveWriter/NotesforHadoopthedefinitiveguide_14109/clip_image042_2.jpg" target="_blank"><img src="&quot;clip_image042&quot;" alt="clip_image042"></a></p>
<p>l Map tasks are run by MapRunner, the default implementation of MapRunnable that calls the Mapper’s map() method sequentially with each record.</p>
<p>l The default partitioner is HashPartitioner, which hashes a record’s key to determine which partition the record belongs in.</p>
<p>n Each partition is processed by a reduce task, so the number of partitions is equal to the number of reduce tasks for the job</p>
<p><a href="http://images.cnblogs.com/cnblogs_com/forfuture1978/WindowsLiveWriter/NotesforHadoopthedefinitiveguide_14109/clip_image044_2.jpg" target="_blank"><img src="&quot;clip_image044&quot;" alt="clip_image044"></a></p>
<p>l The default reducer is IdentityReducer</p>
<p><a href="http://images.cnblogs.com/cnblogs_com/forfuture1978/WindowsLiveWriter/NotesforHadoopthedefinitiveguide_14109/clip_image046_2.jpg" target="_blank"><img src="&quot;clip_image046&quot;" alt="clip_image046"></a></p>
<p>l Records are sorted by the MapReduce system before being presented to the reducer.</p>
<p>l The default output format is TextOutputFormat, which writes out records, one per line, by converting keys and values to strings and separating them with a tab character.</p>
<h2 id="3-2-input-formats">3.2. Input Formats</h2>
<h3 id="3-2-1-input-splits-and-records">3.2.1. Input Splits and Records</h3>
<p>l An input split is a chunk of the input that is processed by a single map.</p>
<p>l Each split is divided into records, and the map processes each record—a key-value pair—in turn.</p>
<p><a href="http://images.cnblogs.com/cnblogs_com/forfuture1978/WindowsLiveWriter/NotesforHadoopthedefinitiveguide_14109/clip_image048_2.jpg" target="_blank"><img src="&quot;clip_image048&quot;" alt="clip_image048"></a></p>
<p>l An InputSplit has a length in bytes, and a set of storage locations, which are just hostname strings.</p>
<p>l A split doesn’t contain the input data; it is just a reference to the data.</p>
<p>l The storage locations are used by the MapReduce system to place map tasks as close to the split’s data as possible</p>
<p>l The size is used to order the splits so that the largest get processed first</p>
<p>l An InputFormat is responsible for creating the input splits, and dividing them into records.</p>
<p><a href="http://images.cnblogs.com/cnblogs_com/forfuture1978/WindowsLiveWriter/NotesforHadoopthedefinitiveguide_14109/clip_image050_2.jpg" target="_blank"><img src="&quot;clip_image050&quot;" alt="clip_image050"></a></p>
<p>l The JobClient calls the getSplits() method, passing the desired number of map tasks as the numSplits argument.</p>
<p>l Having calculated the splits, the client sends them to the jobtracker, which uses their storage locations to schedule map tasks to process them on the tasktrackers.</p>
<p>l On a tasktracker, the map task passes the split to the getRecordReader() method on InputFormat to obtain a RecordReader for that split.</p>
<p>l A RecordReader is little more than an iterator over records, and the map task uses one to generate record key-value pairs, which it passes to the map function.</p>
<p><a href="http://images.cnblogs.com/cnblogs_com/forfuture1978/WindowsLiveWriter/NotesforHadoopthedefinitiveguide_14109/clip_image052_2.jpg" target="_blank"><img src="&quot;clip_image052&quot;" alt="clip_image052"></a></p>
<p>l The same key and value objects are used on each invocation of the map() method—only their contents are changed. If you need to change the value out of map, make a copy of the object you want to hold on to.</p>
<h3 id="3-2-2-fileinputformat">3.2.2. FileInputFormat</h3>
<p>l FileInputFormat is the base class for all implementations of InputFormat that use files as their data source.</p>
<p>l It provides two things: a place to define which files are included as the input to a job, and an implementation for generating splits for the input files.</p>
<p><a href="http://images.cnblogs.com/cnblogs_com/forfuture1978/WindowsLiveWriter/NotesforHadoopthedefinitiveguide_14109/clip_image054_2.jpg" target="_blank"><img src="&quot;clip_image054&quot;" alt="clip_image054"></a></p>
<p>l FileInputFormat input paths may represent a file, a directory, or, by using a glob, a collection of files and directories.</p>
<p><a href="http://images.cnblogs.com/cnblogs_com/forfuture1978/WindowsLiveWriter/NotesforHadoopthedefinitiveguide_14109/clip_image056_2.jpg" target="_blank"><img src="&quot;clip_image056&quot;" alt="clip_image056"></a></p>
<p>l To exclude certain files from the input, you can set a filter using the setInputPathFilter() method on FileInputFormat</p>
<p><a href="http://images.cnblogs.com/cnblogs_com/forfuture1978/WindowsLiveWriter/NotesforHadoopthedefinitiveguide_14109/clip_image058_2.jpg" target="_blank"><img src="&quot;clip_image058&quot;" alt="clip_image058"></a></p>
<p>l FileInputFormat splits only large files. Here “large” means larger than an HDFS block.</p>
<p>l Properties for controlling split size</p>
<p>n The minimum split size is usually 1 byte, by setting this to a value larger than the block size, they can force splits to be larger than a block.</p>
<p>n The maximum split size defaults to the maximum value that can be represented by a Java long type. It has an effect only when it is less than the block size, forcing splits to be smaller than a block.</p>
<h3 id="small-files-and-combinefileinputformat">Small files and CombineFileInputFormat</h3>
<p>l Hadoop works better with a small number of large files than a large number of small files.</p>
<p>l Where FileInputFormat creates a split per file, CombineFileInputFormat packs many files into each split so that each mapper has more to process.</p>
<p>l One technique for avoiding the many small files case is to merge small files into larger files by using a SequenceFile: the keys can act as filenames and the values as file contents.</p>
<h3 id="3-2-3-text-input">3.2.3. Text Input</h3>
<p>l TextInputFormat is the default InputFormat.</p>
<p>n Each record is a line of input.</p>
<p>n The key, a LongWritable, is the byte offset within the file of the beginning of the line.</p>
<p>n The value is the contents of the line, excluding any line terminators, and is packaged as a Text object.</p>
<p>l The logical records that FileInputFormats define do not usually fit neatly into HDFS blocks.</p>
<p>l A single file is broken into lines, and the line boundaries do not correspond with the HDFS block boundaries.</p>
<p>l Splits honor logical record boundaries</p>
<p>n The first split contains line 5, even though it spans the first and second block.</p>
<p>n The second split starts at line 6.</p>
<p>l Data-local maps will perform some remote reads.</p>
<p><a href="http://images.cnblogs.com/cnblogs_com/forfuture1978/WindowsLiveWriter/NotesforHadoopthedefinitiveguide_14109/clip_image060_2.jpg" target="_blank"><img src="&quot;clip_image060&quot;" alt="clip_image060"></a></p>
<h3 id="keyvaluetextinputformat">KeyValueTextInputFormat</h3>
<p>l It is common for each line in a file to be a key-value pair, separated by a delimiter such as a tab character.</p>
<p>l You can specify the separator via the key.value.separator.in.input.line property.</p>
<h3 id="nlineinputformat">NLineInputFormat</h3>
<p>l If you want your mappers to receive a fixed number of lines of input, then NLineInputFormat is the InputFormat to use.</p>
<p>l Like TextInputFormat, the keys are the byte offsets within the file and the values are the lines themselves.</p>
<p>l N refers to the number of lines of input that each mapper receives.</p>
<h3 id="3-2-4-binary-input">3.2.4. Binary Input</h3>
<h3 id="sequencefileinputformat">SequenceFileInputFormat</h3>
<p>l Hadoop’s sequence file format stores sequences of binary key-value pairs.</p>
<p>l To use data from sequence files as the input to MapReduce, you use SequenceFileInputFormat.</p>
<p>l The keys and values are determined by the sequence file, and you need to make sure that your map input types correspond.</p>
<p>l For example, if your sequence file has IntWritable keys and Text values, then the map signature would be Mapper<IntWritable, Text, K, V>.</p>
<h3 id="sequencefileastextinputformat">SequenceFileAsTextInputFormat</h3>
<p>l SequenceFileAsTextInputFormat is a variant of SequenceFileInputFormat that converts the sequence file’s keys and values to Text objects.</p>
<h3 id="sequencefileasbinaryinputformat">SequenceFileAsBinaryInputFormat</h3>
<p>l SequenceFileAsBinaryInputFormat is a variant of SequenceFileInputFormat that retrieves the sequence file’s keys and values as opaque binary objects.</p>
<p>l They are encapsulated as BytesWritable objects</p>
<h3 id="sequencefile">SequenceFile</h3>
<p>l Writing a SequenceFile</p>
<p>n To create a SequenceFile, use one of its createWriter() static methods, which returns a SequenceFile.Writer instance.</p>
<p>n specify a stream to write to (either a FSDataOutputStream or a FileSystem and Path pairing), a Configuration object, and the key and value types.</p>
<p>n Once you have a SequenceFile.Writer, you then write key-value pairs, using the append() method.</p>
<p>n Then when you’ve finished you call the close() method</p>
<p><a href="http://images.cnblogs.com/cnblogs_com/forfuture1978/WindowsLiveWriter/NotesforHadoopthedefinitiveguide_14109/clip_image062_2.jpg" target="_blank"><img src="&quot;clip_image062&quot;" alt="clip_image062"></a></p>
<p>l Reading a SequenceFile</p>
<p>n Reading sequence files from beginning to end is a matter of creating an instance of SequenceFile.Reader, and iterating over records by repeatedly invoking one of the next() methods.</p>
<p><a href="http://images.cnblogs.com/cnblogs_com/forfuture1978/WindowsLiveWriter/NotesforHadoopthedefinitiveguide_14109/clip_image064_2.jpg" target="_blank"><img src="&quot;clip_image064&quot;" alt="clip_image064"></a></p>
<p>l The SequenceFile Format</p>
<p>n A sequence file consists of a header followed by one or more records.</p>
<p>n The first three bytes of a sequence file are the bytes SEQ, which acts a magic number, followed by a single byte representing the version number.</p>
<p>n The header contains other fields including the names of the key and value classes, compression details, user-defined metadata, and the sync marker.</p>
<p>n The sync marker is used to allow a reader to synchronize to a record boundary from any position in the file.</p>
<p><a href="http://images.cnblogs.com/cnblogs_com/forfuture1978/WindowsLiveWriter/NotesforHadoopthedefinitiveguide_14109/clip_image066_2.jpg" target="_blank"><img src="&quot;clip_image066&quot;" alt="clip_image066"></a></p>
<h3 id="3-2-5-multiple-inputs">3.2.5. Multiple Inputs</h3>
<p>l The MultipleInputs class allows you to specify the InputFormat and Mapper to use on a per-path basis.</p>
<p><a href="http://images.cnblogs.com/cnblogs_com/forfuture1978/WindowsLiveWriter/NotesforHadoopthedefinitiveguide_14109/clip_image068_2.jpg" target="_blank"><img src="&quot;clip_image068&quot;" alt="clip_image068"></a></p>
<h2 id="3-3-output-formats">3.3. Output Formats</h2>
<h3 id="3-3-1-text-output">3.3.1. Text Output</h3>
<p>l The default output format, TextOutputFormat, writes records as lines of text.</p>
<p>l Its keys and values may be of any type, since TextOutputFormat turns them to strings by calling toString() on them.</p>
<p>l Each key-value pair is separated by a tab character, although that may be changed using the mapred.textoutputformat.separator property.</p>
<h3 id="3-3-2-binary-output">3.3.2. Binary Output</h3>
<p>l SequenceFileOutputFormat</p>
<p>l SequenceFileAsBinaryOutputFormat</p>
<p>l MapFileOutputFormat</p>
<h3 id="writing-a-mapfile">Writing a MapFile</h3>
<p>l You create an instance of MapFile.Writer, then call the append() method to add entries in order.</p>
<p>l Keys must be instances of WritableComparable, and values must be Writable</p>
<p><a href="http://images.cnblogs.com/cnblogs_com/forfuture1978/WindowsLiveWriter/NotesforHadoopthedefinitiveguide_14109/clip_image070_2.jpg" target="_blank"><img src="&quot;clip_image070&quot;" alt="clip_image070"></a></p>
<p>l If we look at the MapFile, we see it’s actually a directory containing two files called data and index:</p>
<p><a href="http://images.cnblogs.com/cnblogs_com/forfuture1978/WindowsLiveWriter/NotesforHadoopthedefinitiveguide_14109/clip_image072_2.jpg" target="_blank"><img src="&quot;clip_image072&quot;" alt="clip_image072"></a></p>
<p>l Both files are SequenceFiles. The data file contains all of the entries, in order:</p>
<p><a href="http://images.cnblogs.com/cnblogs_com/forfuture1978/WindowsLiveWriter/NotesforHadoopthedefinitiveguide_14109/clip_image074_2.jpg" target="_blank"><img src="&quot;clip_image074&quot;" alt="clip_image074"></a></p>
<p>l The index file contains a fraction of the keys, and contains a mapping from the key to that key’s offset in the data file:</p>
<p><a href="http://images.cnblogs.com/cnblogs_com/forfuture1978/WindowsLiveWriter/NotesforHadoopthedefinitiveguide_14109/clip_image076_2.jpg" target="_blank"><img src="&quot;clip_image076&quot;" alt="clip_image076"></a></p>
<h3 id="reading-a-mapfile">Reading a MapFile</h3>
<p>l you create a MapFile.Reader, then call the next() method until it returns false</p>
<h3 id="3-3-3-multiple-outputs">3.3.3. Multiple Outputs</h3>
<h3 id="multipleoutputformat">MultipleOutputFormat</h3>
<p>l MultipleOutputFormat allows you to write data to multiple files whose names are derived from the output keys and values.</p>
<p>n conf.setOutputFormat(StationNameMultipleTextOutputFormat.class);</p>
<p><a href="http://images.cnblogs.com/cnblogs_com/forfuture1978/WindowsLiveWriter/NotesforHadoopthedefinitiveguide_14109/clip_image078_2.jpg" target="_blank"><img src="&quot;clip_image078&quot;" alt="clip_image078"></a></p>
<h3 id="multipleoutputs">MultipleOutputs</h3>
<p>l MultipleOutputs can emit different types for each output.</p>
<p><a href="http://images.cnblogs.com/cnblogs_com/forfuture1978/WindowsLiveWriter/NotesforHadoopthedefinitiveguide_14109/clip_image080_2.jpg" target="_blank"><img src="&quot;clip_image080&quot;" alt="clip_image080"></a></p>
<h1 id="4-developing-a-mapreduce-application">4. Developing a MapReduce Application</h1>
<h2 id="4-1-the-configuration-api">4.1. The Configuration API</h2>
<p>l An instance of the Configuration class (found in the org.apache.hadoop.conf package) represents a collection of configuration properties and their values.</p>
<p>l Configurations read their properties from resources—XML files</p>
<p><a href="http://images.cnblogs.com/cnblogs_com/forfuture1978/WindowsLiveWriter/NotesforHadoopthedefinitiveguide_14109/clip_image082_2.jpg" target="_blank"><img src="&quot;clip_image082&quot;" alt="clip_image082"></a></p>
<p>l we can access its properties using a piece of code like this:</p>
<p><a href="http://images.cnblogs.com/cnblogs_com/forfuture1978/WindowsLiveWriter/NotesforHadoopthedefinitiveguide_14109/clip_image084_2.jpg" target="_blank"><img src="&quot;clip_image084&quot;" alt="clip_image084"></a></p>
<h2 id="4-2-configuring-the-development-environment">4.2. Configuring the Development Environment</h2>
<h3 id="4-2-1-managing-configuration">4.2.1. Managing Configuration</h3>
<p>l When developing Hadoop applications, it is common to switch between running the application locally and running it on a cluster.</p>
<p>l hadoop-local.xml</p>
<p><a href="http://images.cnblogs.com/cnblogs_com/forfuture1978/WindowsLiveWriter/NotesforHadoopthedefinitiveguide_14109/clip_image086_2.jpg" target="_blank"><img src="&quot;clip_image086&quot;" alt="clip_image086"></a></p>
<p>l hadoop-localhost.xml</p>
<p><a href="http://images.cnblogs.com/cnblogs_com/forfuture1978/WindowsLiveWriter/NotesforHadoopthedefinitiveguide_14109/clip_image088_2.jpg" target="_blank"><img src="&quot;clip_image088&quot;" alt="clip_image088"></a></p>
<p>l hadoop-cluster.xml</p>
<p><a href="http://images.cnblogs.com/cnblogs_com/forfuture1978/WindowsLiveWriter/NotesforHadoopthedefinitiveguide_14109/clip_image090_2.jpg" target="_blank"><img src="&quot;clip_image090&quot;" alt="clip_image090"></a></p>
<p><a href="http://images.cnblogs.com/cnblogs_com/forfuture1978/WindowsLiveWriter/NotesforHadoopthedefinitiveguide_14109/clip_image092_2.jpg" target="_blank"><img src="&quot;clip_image092&quot;" alt="clip_image092"></a></p>
<p>l With this setup, it is easy to use any configuration with the -conf command-line switch.</p>
<p>l For example, the following command shows a directory listing on the HDFS server running in pseudo-distributed mode on localhost:</p>
<p><a href="http://images.cnblogs.com/cnblogs_com/forfuture1978/WindowsLiveWriter/NotesforHadoopthedefinitiveguide_14109/clip_image094_2.jpg" target="_blank"><img src="&quot;clip_image094&quot;" alt="clip_image094"></a></p>
<h3 id="4-2-2-genericoptionsparser-tool-and-toolrunner">4.2.2. GenericOptionsParser, Tool, and ToolRunner</h3>
<p><a href="http://images.cnblogs.com/cnblogs_com/forfuture1978/WindowsLiveWriter/NotesforHadoopthedefinitiveguide_14109/clip_image096_2.jpg" target="_blank"><img src="&quot;clip_image096&quot;" alt="clip_image096"></a></p>
<p><a href="http://images.cnblogs.com/cnblogs_com/forfuture1978/WindowsLiveWriter/NotesforHadoopthedefinitiveguide_14109/clip_image098_2.jpg" target="_blank"><img src="&quot;clip_image098&quot;" alt="clip_image098"></a></p>
<h1 id="5-how-mapreduce-works">5. How MapReduce Works</h1>
<h2 id="5-1-anatomy-of-a-mapreduce-job-run">5.1. Anatomy of a MapReduce Job Run</h2>
<p>l There are four independent entities:</p>
<p>n The client, which submits the MapReduce job.</p>
<p>n The jobtracker, which coordinates the job run. The jobtracker is a Java application whose main class is JobTracker.</p>
<p>n The tasktrackers, which run the tasks that the job has been split into. Tasktrackers are Java applications whose main class is TaskTracker.</p>
<p>n The distributed filesystem, which is used for sharing job files between the other entities.</p>
<p><a href="http://images.cnblogs.com/cnblogs_com/forfuture1978/WindowsLiveWriter/NotesforHadoopthedefinitiveguide_14109/clip_image100_2.jpg" target="_blank"><img src="&quot;clip_image100&quot;" alt="clip_image100"></a></p>
<h3 id="5-1-1-job-submission">5.1.1. Job Submission</h3>
<p>l The runJob() method on JobClient creates a new JobClient instance and calls submitJob() on it.</p>
<p>l Having submitted the job, runJob() polls the job’s progress once a second, and reports the progress to the console if it has changed since the last report.</p>
<p>l When the job is complete, if it was successful, the job counters are displayed. Otherwise, the error that caused the job to fail is logged to the console.</p>
<h3 id="the-job-submission-process">The job submission process</h3>
<p>l Asks the jobtracker for a new job ID (by calling getNewJobId() on JobTracker)</p>
<p>l Checks the output specification of the job.</p>
<p>l Computes the input splits for the job.</p>
<p>l Copies the resources needed to run the job, including the job JAR file, the configuration file and the computed input splits, to the jobtracker’s filesystem in a directory named after the job ID.</p>
<p>l Tells the jobtracker that the job is ready for execution (by calling submitJob() on JobTracker)</p>
<h3 id="5-1-2-job-initialization">5.1.2. Job Initialization</h3>
<p>l When the JobTracker receives a call to its submitJob() method, it puts it into an internal queue from where the job scheduler will pick it up and initialize it.</p>
<p>l Initialization involves creating an object to represent the job being run, which encapsulates its tasks, and bookkeeping information to keep track of the tasks’ status and progress.</p>
<p>l To create the list of tasks to run, the job scheduler first retrieves the input splits computed by the JobClient from the shared filesystem.</p>
<p>l It then creates one map task for each split.</p>
<p>l Tasks are given IDs at this point.</p>
<h3 id="5-1-3-task-assignment">5.1.3. Task Assignment</h3>
<p>l Tasktrackers run a simple loop that periodically sends heartbeat method calls to the jobtracker.</p>
<p>l As a part of the heartbeat, a tasktracker will indicate whether it is ready to run a new task, and if it is, the jobtracker will allocate it a task, which it communicates to the tasktracker using the heartbeat return value</p>
<p>l Before it can choose a task for the tasktracker, the jobtracker must choose a job to select the task from according to priority.(setJobPriority() and FIFO)</p>
<p>l Tasktrackers have a fixed number of slots for map tasks and for reduce tasks.</p>
<p>l The default scheduler fills empty map task slots before reduce task slots</p>
<p>l To choose a reduce task the jobtracker simply takes the next in its list of yet-to-be-run reduce tasks, since there are no data locality considerations.</p>
<h3 id="5-1-4-task-execution">5.1.4. Task Execution</h3>
<p>l Now the tasktracker has been assigned a task, the next step is for it to run the task.</p>
<p>l First, it localizes the job JAR by copying it from the shared filesystem to the tasktracker’s filesystem.</p>
<p>l It also copies any files needed from the distributed cache by the application to the local disk</p>
<p>l Second, it creates a local working directory for the task, and un-jars the contents of the JAR into this directory.</p>
<p>l Third, it creates an instance of TaskRunner to run the task.</p>
<p>l TaskRunner launches a new Java Virtual Machine to run each task in</p>
<p>l It is however possible to reuse the JVM between tasks;</p>
<p>l The child process communicates with its parent through the umbilical interface.</p>
<h3 id="5-1-5-job-completion">5.1.5. Job Completion</h3>
<p>l When the jobtracker receives a notification that the last task for a job is complete, it changes the status for the job to “successful.” T</p>
<p>l hen, when the JobClient polls for status, it learns that the job has completed successfully, so it prints a message to tell the user, and then returns from the runJob() method.</p>
<p><a href="http://images.cnblogs.com/cnblogs_com/forfuture1978/WindowsLiveWriter/NotesforHadoopthedefinitiveguide_14109/clip_image102_2.jpg" target="_blank"><img src="&quot;clip_image102&quot;" alt="clip_image102"></a></p>
<h2 id="5-2-failures">5.2. Failures</h2>
<h3 id="5-2-1-task-failure">5.2.1. Task Failure</h3>
<p>l The most common way is when user code in the map or reduce task throws a runtime exception.</p>
<p>n the child JVM reports the error back to its parent tasktracker, before it exits.</p>
<p>n The error ultimately makes it into the user logs.</p>
<p>n The tasktracker marks the task attempt as failed, freeing up a slot to run another task.</p>
<p>l Another failure mode is the sudden exit of the child JVM</p>
<p>n the tasktracker notices that the process has exited, and marks the attempt as failed.</p>
<p>l Hanging tasks are dealt with differently.</p>
<p>n The tasktracker notices that it hasn’t received a progress update for a while, and proceeds to mark the task as failed.</p>
<p>n The child JVM process will be automatically killed after this period</p>
<p>l When the jobtracker is notified of a task attempt that has failed (by the tasktracker’s heartbeat call) it will reschedule execution of the task.</p>
<p>n The jobtracker will try to avoid rescheduling the task on a tasktracker where it has previously failed.</p>
<p>n If a task fails more than four times, it will not be retried further.</p>
<h3 id="5-2-2-tasktracker-failure">5.2.2. Tasktracker Failure</h3>
<p>l If a tasktracker fails by crashing, or running very slowly, it will stop sending heartbeats to the jobtracker (or send them very infrequently).</p>
<p>l The jobtracker will notice a tasktracker that has stopped sending heartbeats and remove it from its pool of tasktrackers to schedule tasks on.</p>
<p>l The jobtracker arranges for map tasks that were run and completed successfully on that tasktracker to be rerun if they belong to incomplete jobs, since their intermediate output residing on the failed tasktracker’s local filesystem may not be accessible to the reduce task. Any tasks in progress are also rescheduled.</p>
<h3 id="5-2-3-jobtracker-failure">5.2.3. Jobtracker Failure</h3>
<h2 id="5-3-shuffle-and-sort">5.3. Shuffle and Sort</h2>
<p><a href="http://images.cnblogs.com/cnblogs_com/forfuture1978/WindowsLiveWriter/NotesforHadoopthedefinitiveguide_14109/clip_image104_2.jpg" target="_blank"><img src="&quot;clip_image104&quot;" alt="clip_image104"></a></p>
<h3 id="5-3-1-the-map-side">5.3.1. The Map Side</h3>
<p>l When the map function starts producing output, it is not simply written to disk.</p>
<p>l Each map task has a circular memory buffer that it writes the output to.</p>
<p>l When the contents of the buffer reach a certain threshold size, a background thread will start to spill the contents to disk.</p>
<p>l Spills are written in round-robin fashion to the directories specified by the mapred.local.dir property</p>
<p>l Before it writes to disk, the thread first divides the data into partitions corresponding to the reducers that they will ultimately be sent to.</p>
<p>l Within each partition, the background thread performs an in-memory sort by key.</p>
<p>l Each time the memory buffer reaches the spill threshold, a new spill file is created, so after the map task has written its last output record there could be several spill files.</p>
<p>l Before the task is finished, the spill files are merged into a single partitioned and sorted output file.</p>
<p>l The output file’s partitions are made available to the reducers over HTTP.</p>
<p>l The number of worker threads used to serve the file partitions is controlled by the task tracker.http.threads property</p>
<h3 id="5-3-2-the-reduce-side">5.3.2. The Reduce Side</h3>
<p>l As map tasks complete successfully, they notify their parent tasktracker of the status update, which in turn notifies the jobtracker.</p>
<p>l for a given job, the jobtracker knows the mapping between map outputs and tasktrackers.</p>
<p>l A thread in the reducer periodically asks the jobtracker for map output locations until it has retrieved them all.</p>
<p>l The reduce task needs the map output for its particular partition from several map tasks across the cluster.</p>
<p>l The map tasks may finish at different times, so the reduce task starts copying their outputs as soon as each completes. This is known as the copy phase of the reduce task.</p>
<p>l The reduce task has a small number of copier threads so that it can fetch map outputs in parallel.</p>
<p>l As the copies accumulate on disk, a background thread merges them into larger, sorted files.</p>
<p>l When all the map outputs have been copied, the reduce task moves into the sort phase (which should properly be called the merge phase, as the sorting was carried out on the map side), which merges the map outputs, maintaining their sort ordering.</p>
<p>l During the reduce phase the reduce function is invoked for each key in the sorted output. The output of this phase is written directly to the output filesystem, typically HDFS.
来源： &lt;<a href="http://www.cnblogs.com/forfuture1978/archive/2010/02/27/1674955.html" target="_blank">Notes for Hadoop the definitive guide - 觉先 - 博客园</a>&gt; </p>

      
    </div>
    
    
      
    
    
        <footer id="post-meta">
        <span class="categories">Posted in<span class="breadcrumb fa fa-folder"><li><a href="/categories/hadoop/">hadoop</a></li></span></span> | <span class="tags">Tagged <a href="/tags/hadoop/" class="label label-primary">hadoop</a></span> | <span class="time">recent updated:<time title="2014-03-07 09:54:36"datetime="2014-03-07 09:54:36"> mar. 7 2014</time></span> | <span class="comment-link">
<a href="http://itsolife.com/2014/02/02/2014-02-02-hadoop--NotesforHadoopthedefinitiveguide/#comments" class="ds-thread-count comment-link" data-thread-key="2014-02-02-hadoop--NotesforHadoopthedefinitiveguide" data-count-type="comments">暂无评论</a></span>
        </footer>
    
    <div class="clearfix"></div>
  </div>
</div>
</article>



  <article>
<div id="post" class="post well">
  <div class="post-content">
    <header class="well-sm">
      <i class="fa icon fa-5x pull-left"></i>
      <h1 class="title"><a href="/2014/02/02/2014-02-02-hadoop--hadoop/">hadoop</a></h1>
      
        <span>Posted on<time datetime="2014-02-02T01:54:36.000Z"> <a href="/2014/02/02/2014-02-02-hadoop--hadoop/">feb. 2 2014</a></time></span>      
    </header>
    
    
    <div class="entry">
      
        <h1 id="hadoop">hadoop</h1>
<p><img src="" alt=""></p>
<h1 id="hadoop">hadoop</h1>
<h2 id="table-of-contents">Table of Contents</h2>
<ul>
<li><p><a href="http://dirlt.com/hadoop.html#sec-1" target="_blank">1 hadoop</a></p>
</li>
<li><p><a href="http://dirlt.com/hadoop.html#sec-1-1" target="_blank">1.1 FAQ</a></p>
</li>
<li><p><a href="http://dirlt.com/hadoop.html#sec-1-1-1" target="_blank">1.1.1 Hadoop可以用来做什么</a></p>
</li>
<li><a href="http://dirlt.com/hadoop.html#sec-1-1-2" target="_blank">1.1.2 Hadoop包括哪些组件</a></li>
<li><a href="http://dirlt.com/hadoop.html#sec-1-1-3" target="_blank">1.1.3 CDH和Apache Hadoop的关系</a></li>
<li><a href="http://dirlt.com/hadoop.html#sec-1-1-4" target="_blank">1.1.4 CDH产品组件构成</a></li>
<li><p><a href="http://dirlt.com/hadoop.html#sec-1-1-5" target="_blank">1.1.5 CDH产品组件端口分布和配置</a></p>
</li>
<li><p><a href="http://dirlt.com/hadoop.html#sec-1-1-5-1" target="_blank">1.1.5.1 Hadoop HDFS</a></p>
</li>
<li><a href="http://dirlt.com/hadoop.html#sec-1-1-5-2" target="_blank">1.1.5.2 Hadoop MRv1</a></li>
<li><a href="http://dirlt.com/hadoop.html#sec-1-1-5-3" target="_blank">1.1.5.3 Hadoop YARN</a></li>
<li><a href="http://dirlt.com/hadoop.html#sec-1-1-5-4" target="_blank">1.1.5.4 HBase</a></li>
<li><a href="http://dirlt.com/hadoop.html#sec-1-1-5-5" target="_blank">1.1.5.5 Hive</a></li>
<li><a href="http://dirlt.com/hadoop.html#sec-1-1-5-6" target="_blank">1.1.5.6 Sqoop</a></li>
<li><a href="http://dirlt.com/hadoop.html#sec-1-1-5-7" target="_blank">1.1.5.7 Zookeeper</a></li>
<li><a href="http://dirlt.com/hadoop.html#sec-1-1-5-8" target="_blank">1.1.5.8 Hue</a></li>
<li><a href="http://dirlt.com/hadoop.html#sec-1-1-5-9" target="_blank">1.1.5.9 Ozzie</a></li>
<li><a href="http://dirlt.com/hadoop.html#sec-1-1-5-10" target="_blank">1.1.5.10 Ganglia</a></li>
<li><a href="http://dirlt.com/hadoop.html#sec-1-1-5-11" target="_blank">1.1.5.11 Kerberos</a></li>
<li><p><a href="http://dirlt.com/hadoop.html#sec-1-2" target="_blank">1.2 观点</a></p>
</li>
<li><p><a href="http://dirlt.com/hadoop.html#sec-1-2-1" target="_blank">1.2.1 Hadoop即将过时了吗？</a></p>
</li>
<li><a href="http://dirlt.com/hadoop.html#sec-1-2-2" target="_blank">1.2.2 Best Practices for Selecting Apache Hadoop Hardware</a></li>
<li><a href="http://dirlt.com/hadoop.html#sec-1-2-3" target="_blank">1.2.3 The dark side of Hadoop - BackType Technology</a></li>
<li><p><a href="http://dirlt.com/hadoop.html#sec-1-3" target="_blank">1.3 使用问题</a></p>
</li>
<li><p><a href="http://dirlt.com/hadoop.html#sec-1-3-1" target="_blank">1.3.1 CDH3u3搭建单节点集群</a></p>
</li>
<li><a href="http://dirlt.com/hadoop.html#sec-1-3-2" target="_blank">1.3.2 CDH4.2.0搭建单节点集群</a></li>
<li><a href="http://dirlt.com/hadoop.html#sec-1-3-3" target="_blank">1.3.3 CDH4.3.0</a></li>
<li><p><a href="http://dirlt.com/hadoop.html#sec-1-3-4" target="_blank">1.3.4 Configuration</a></p>
</li>
<li><p><a href="http://dirlt.com/hadoop.html#sec-1-3-4-1" target="_blank">1.3.4.1 .bash_profile</a></p>
</li>
<li><a href="http://dirlt.com/hadoop.html#sec-1-3-4-2" target="_blank">1.3.4.2 core-site.xml</a></li>
<li><a href="http://dirlt.com/hadoop.html#sec-1-3-4-3" target="_blank">1.3.4.3 hdfs-site.xml</a></li>
<li><a href="http://dirlt.com/hadoop.html#sec-1-3-4-4" target="_blank">1.3.4.4 mapred-site.xml</a></li>
<li><a href="http://dirlt.com/hadoop.html#sec-1-3-4-5" target="_blank">1.3.4.5 hadoop-env.sh</a></li>
<li><a href="http://dirlt.com/hadoop.html#sec-1-3-4-6" target="_blank">1.3.4.6 hbase-site.xml</a></li>
<li><a href="http://dirlt.com/hadoop.html#sec-1-3-4-7" target="_blank">1.3.4.7 hbase-env.sh</a></li>
<li><p><a href="http://dirlt.com/hadoop.html#sec-1-4" target="_blank">1.4 Hadoop权威指南</a></p>
</li>
<li><p><a href="http://dirlt.com/hadoop.html#sec-1-4-1" target="_blank">1.4.1 初识Hadoop</a></p>
</li>
<li><a href="http://dirlt.com/hadoop.html#sec-1-4-2" target="_blank">1.4.2 关于MapReduce</a></li>
<li><a href="http://dirlt.com/hadoop.html#sec-1-4-3" target="_blank">1.4.3 Hadoop分布式文件系统</a></li>
<li><a href="http://dirlt.com/hadoop.html#sec-1-4-4" target="_blank">1.4.4 Hadoop IO</a></li>
<li><a href="http://dirlt.com/hadoop.html#sec-1-4-5" target="_blank">1.4.5 MapReduce应用开发</a></li>
<li><a href="http://dirlt.com/hadoop.html#sec-1-4-6" target="_blank">1.4.6 MapReduce的工作机制</a></li>
<li><a href="http://dirlt.com/hadoop.html#sec-1-4-7" target="_blank">1.4.7 MapReduce的类型与格式</a></li>
<li><a href="http://dirlt.com/hadoop.html#sec-1-4-8" target="_blank">1.4.8 MapReduce的特性</a></li>
<li><a href="http://dirlt.com/hadoop.html#sec-1-4-9" target="_blank">1.4.9 构建Hadoop集群</a></li>
<li><a href="http://dirlt.com/hadoop.html#sec-1-4-10" target="_blank">1.4.10 管理Hadoop</a></li>
<li><p><a href="http://dirlt.com/hadoop.html#sec-1-5" target="_blank">1.5 Benchmark</a></p>
</li>
<li><p><a href="http://dirlt.com/hadoop.html#sec-1-5-1" target="_blank">1.5.1 TestDFSIO</a></p>
</li>
<li><a href="http://dirlt.com/hadoop.html#sec-1-5-2" target="_blank">1.5.2 TeraSort</a></li>
<li><a href="http://dirlt.com/hadoop.html#sec-1-5-3" target="_blank">1.5.3 nnbench</a></li>
<li><a href="http://dirlt.com/hadoop.html#sec-1-5-4" target="_blank">1.5.4 mrbench</a></li>
<li><a href="http://dirlt.com/hadoop.html#sec-1-5-5" target="_blank">1.5.5 hbase.PerformanceEvaluation</a></li>
</ul>
<h2 id="1-hadoop">1 hadoop</h2>
<p>参考资源</p>
<ul>
<li>Cloudera <a href="http://www.cloudera.com/" target="_blank"><a href="http://www.cloudera.com/">http://www.cloudera.com/</a></a></li>
<li>Apache Hadoop <a href="http://hadoop.apache.org/" target="_blank"><a href="http://hadoop.apache.org/">http://hadoop.apache.org/</a></a></li>
<li>Apache Hadoop r1.0.3 文档 <a href="http://hadoop.apache.org/common/docs/r1.0.3/" target="_blank"><a href="http://hadoop.apache.org/common/docs/r1.0.3/">http://hadoop.apache.org/common/docs/r1.0.3/</a></a></li>
<li>Apache Hadoop r1.0.3 中文文档 <a href="http://hadoop.apache.org/common/docs/r1.0.3/cn" target="_blank"><a href="http://hadoop.apache.org/common/docs/r1.0.3/cn">http://hadoop.apache.org/common/docs/r1.0.3/cn</a></a></li>
<li>CDH Downloads <a href="https://ccp.cloudera.com/display/SUPPORT/Downloads" target="_blank"><a href="https://ccp.cloudera.com/display/SUPPORT/Downloads">https://ccp.cloudera.com/display/SUPPORT/Downloads</a></a></li>
<li>CDH Documentation <a href="https://ccp.cloudera.com/display/DOC/Documentation" target="_blank"><a href="https://ccp.cloudera.com/display/DOC/Documentation">https://ccp.cloudera.com/display/DOC/Documentation</a></a></li>
<li>CDH Tutorial <a href="https://ccp.cloudera.com/display/SUPPORT/Hadoop+Tutorial" target="_blank"><a href="https://ccp.cloudera.com/display/SUPPORT/Hadoop+Tutorial">https://ccp.cloudera.com/display/SUPPORT/Hadoop+Tutorial</a></a></li>
</ul>
<h3 id="1-1-faq">1.1 FAQ</h3>
<h3 id="1-1-1-hadoop-">1.1.1 Hadoop可以用来做什么</h3>
<p>Why Hadoop? <a href="http://www.cloudera.com/why-hadoop/" target="_blank"><a href="http://www.cloudera.com/why-hadoop/">http://www.cloudera.com/why-hadoop/</a></a></p>
<p>TODO(dirlt):translate it!!!</p>
<p>Simply put, Hadoop can transform the way you store and process data throughout your enterprise. According to analysts, about 80% of the data in the world is unstructured, and until Hadoop, it was essentially unusable in any systematic way. With Hadoop, for the first time you can combine all your data and look at it as one.</p>
<ul>
<li>Make All Your Data Profitable. Hadoop enables you to gain insight from all the data you already have; to ingest the data flowing into your systems 24/7 and leverage it to make optimizations that were impossible before; to make decisions based on hard data, not hunches; to look at complete data, not samples; to look at years of transactions, not days or weeks. In short, Hadoop will change the way you run your organization.</li>
<li>Leverage All Types of Data, From All Types of Systems. Hadoop can handle all types of data from disparate systems: structured, unstructured, log files, pictures, audio files, communications records, email– just about anything you can think of. Even when different types of data have been stored in unrelated systems, you can dump it all into your Hadoop cluster before you even know how you might take advantage of it in the future.</li>
<li>Scale Beyond Anything You Have Today. The largest social network in the world is built on the same open-source technology as Hadoop, and now exceeds 100 petabytes. It’s unlikely your organization has that much data. As you need more capacity, you just add more commodity servers and Hadoop automatically incorporates the new storage and compute capacity.</li>
</ul>
<h3 id="1-1-2-hadoop-">1.1.2 Hadoop包括哪些组件</h3>
<p>TODO(dirlt):translate it!!!</p>
<p>Apache Hadoop包括了下面这些组件：</p>
<ul>
<li><a href="http://hadoop.apache.org/common/" target="_blank">Hadoop Common</a> The common utilities that support the other Hadoop subprojects.</li>
<li><a href="http://hadoop.apache.org/hdfs/" target="_blank">Hadoop Distributed File System(HDFS)</a> A distributed file system that provides high-throughput access to application data.</li>
<li><a href="http://hadoop.apache.org/mapreduce/" target="_blank">Hadoop MapReduce</a> A software framework for distributed processing of large data sets on compute clusters.</li>
</ul>
<p>和Apache Hadoop相关的组件有：</p>
<ul>
<li><a href="http://avro.apache.org/" target="_blank">Avro</a> A data serialization system.</li>
<li><a href="http://cassandra.apache.org/" target="_blank">Cassandra</a> A scalable multi-master database with no single points of failure.</li>
<li><a href="http://incubator.apache.org/chukwa/" target="_blank">Chukwa</a> A data collection system for managing large distributed systems.</li>
<li><a href="http://hbase.apache.org/" target="_blank">HBase</a> A scalable, distributed database that supports structured data storage for large tables.</li>
<li><a href="http://hive.apache.org/" target="_blank">Hive</a> A data warehouse infrastructure that provides data summarization and ad hoc querying.</li>
<li><a href="http://mahout.apache.org/" target="_blank">Mahout</a> A Scalable machine learning and data mining library.</li>
<li><a href="http://pig.apache.org/" target="_blank">Pig</a> A high-level data-flow language and execution framework for parallel computation.</li>
<li><a href="http://zookeeper.apache.org/" target="_blank">ZooKeeper</a> A high-performance coordination service for distributed applications.<h3 id="1-1-3-cdh-apache-hadoop-">1.1.3 CDH和Apache Hadoop的关系</h3>
</li>
</ul>
<p>CDH Hadoop FAQ <a href="https://ccp.cloudera.com/display/SUPPORT/Hadoop+FAQ" target="_blank"><a href="https://ccp.cloudera.com/display/SUPPORT/Hadoop+FAQ">https://ccp.cloudera.com/display/SUPPORT/Hadoop+FAQ</a></a></p>
<p>TODO(dirlt):translate it!!!</p>
<ul>
<li>What exactly is included in CDH? / Cloudera&#39;s Distribution Including Apache Hadoop (CDH) is a certified release of Apache Hadoop. We include some stable patches scheduled to be included in future releases, as well as some patches we have developed for our supported customers, and are in the process of contributing back to Apache.</li>
<li>What license is Cloudera&#39;s Distribution Including Apache Hadoop released under? / Just like Hadoop, Cloudera&#39;s Distribution Including Apache Hadoop is released under the Apache Public License version 2.</li>
<li>Is Cloudera forking Hadoop? / Absolutely not. Cloudera is committed to the Hadoop project and the principles of the Apache Software License and Foundation. We continue to work actively with current releases of Hadoop and deliver certified releases to the community as appropriate.</li>
<li>Does Cloudera contribute their changes back to Apache? / We do, and will continue to contribute all eligible changes back to Apache. We occasionally release code we know to be stable even if our contribution to Apache is still in progress. Some of our changes are not eligible for contribution, as they capture the Cloudera brand, or link to our tools and documentation, but these do not affect compatibility with core project.</li>
</ul>
<h3 id="1-1-4-cdh-">1.1.4 CDH产品组件构成</h3>
<p><a href="http://www.cloudera.com/content/cloudera/en/products/cdh.html" target="_blank"><a href="http://www.cloudera.com/content/cloudera/en/products/cdh.html">http://www.cloudera.com/content/cloudera/en/products/cdh.html</a></a></p>
<p>从这里可以下载CDH4组件 <a href="http://www.cloudera.com/content/cloudera-content/cloudera-docs/CDHTarballs/3.25.2013/CDH4-Downloadable-Tarballs/CDH4-Downloadable-Tarballs.html" target="_blank"><a href="http://www.cloudera.com/content/cloudera-content/cloudera-docs/CDHTarballs/3.25.2013/CDH4-Downloadable-Tarballs/CDH4-Downloadable-Tarballs.html">http://www.cloudera.com/content/cloudera-content/cloudera-docs/CDHTarballs/3.25.2013/CDH4-Downloadable-Tarballs/CDH4-Downloadable-Tarballs.html</a></a></p>
<p><img src="" alt="./images/cloudera-enterprise-diagram.png"></p>
<h3 id="1-1-5-cdh-">1.1.5 CDH产品组件端口分布和配置</h3>
<p>The CDH4 components, and third parties such as Kerberos, use the ports listed in the tables that follow. Before you deploy CDH4, make sure these ports are open on each system.</p>
<h3 id="1-1-5-1-hadoop-hdfs">1.1.5.1 Hadoop HDFS</h3>
<p>ServiceQualifierPortProtocolAccess RequirementConfigurationCommentDataNode50010TCPExternaldfs.datanode.addressDataNode HTTP server portDataNodeSecure1004TCPExternaldfs.datanode.addressDataNode50075TCPExternaldfs.datanode.http.addressDataNodeSecure1006TCPExternaldfs.datanode.http.addressDataNode50020TCPExternaldfs.datanode.ipc.addressNameNode8020TCPExternalfs.default.name or fs.defaultFSfs.default.name is deprecated (but still works)NameNode50070TCPExternaldfs.http.address or dfs.namenode.http-addressdfs.http.address is deprecated (but still works)NameNodeSecure50470TCPExternaldfs.https.address or dfs.namenode.https-addressdfs.https.address is deprecated (but still works)Sec NameNode50090TCPInternaldfs.secondary.http.address or dfs.namenode.secondary.http-addressdfs.secondary.http.address is deprecated (but still works)Sec NameNodeSecure50495TCPInternaldfs.secondary.https.addressJournalNode8485TCPInternaldfs.namenode.shared.edits.dirJournalNode8480TCPInternal</p>
<h3 id="1-1-5-2-hadoop-mrv1">1.1.5.2 Hadoop MRv1</h3>
<p>ServiceQualifierPortProtocolAccess RequirementConfigurationCommentJobTracker8021TCPExternalmapred.job.trackerJobTracker50030TCPExternalmapred.job.tracker.http.addressJobTrackerThrift Plugin9290TCPInternaljobtracker.thrift.addressRequired by Hue and Cloudera Manager Activity MonitorTaskTracker50060TCPExternalmapred.task.tracker.http.addressTaskTracker0TCPLocalhostmapred.task.tracker.report.addressCommunicating with child (umbilical)</p>
<h3 id="1-1-5-3-hadoop-yarn">1.1.5.3 Hadoop YARN</h3>
<p>ServiceQualifierPortProtocolAccess RequirementConfigurationCommentResourceManager8032TCPyarn.resourcemanager.addressResourceManager8030TCPyarn.resourcemanager.scheduler.addressResourceManager8031TCPyarn.resourcemanager.resource-tracker.addressResourceManager8033TCPyarn.resourcemanager.admin.addressResourceManager8088TCPyarn.resourcemanager.webapp.addressNodeManager8040TCPyarn.nodemanager.localizer.addressNodeManager8042TCPyarn.nodemanager.webapp.addressNodeManager8041TCPyarn.nodemanager.addressMapReduce JobHistory Server10020TCPmapreduce.jobhistory.addressMapReduce JobHistory Server19888TCPmapreduce.jobhistory.webapp.address</p>
<h3 id="1-1-5-4-hbase">1.1.5.4 HBase</h3>
<p>ServiceQualifierPortProtocolAccess RequirementConfigurationCommentMaster60000TCPExternalhbase.master.portIPCMaster60010TCPExternalhbase.master.info.portHTTPRegionServer60020TCPExternalhbase.regionserver.portIPCRegionServer60030TCPExternalhbase.regionserver.info.portHTTPHQuorumPeer2181TCPhbase.zookeeper.property.clientPortHBase-managed ZK modeHQuorumPeer2888TCPhbase.zookeeper.peerportHBase-managed ZK modeHQuorumPeer3888TCPhbase.zookeeper.leaderportHBase-managed ZK modeRESTREST Service8080TCPExternalhbase.rest.portThriftServerThrift Server9090TCPExternalPass -p <port> on CLIAvro server9090TCPExternalPass –port <port> on CLI</p>
<h3 id="1-1-5-5-hive">1.1.5.5 Hive</h3>
<p>ServiceQualifierPortProtocolAccess RequirementConfigurationCommentMetastore9083TCPExternalHiveServer10000TCPExternal</p>
<h3 id="1-1-5-6-sqoop">1.1.5.6 Sqoop</h3>
<p>ServiceQualifierPortProtocolAccess RequirementConfigurationCommentMetastore16000TCPExternalsqoop.metastore.server.portSqoop 2 server12000TCPExternal</p>
<h3 id="1-1-5-7-zookeeper">1.1.5.7 Zookeeper</h3>
<p>ServiceQualifierPortProtocolAccess RequirementConfigurationCommentServer (with CDH4 and/or Cloudera Manager 4)2181TCPExternalclientPortClient portServer (with CDH4 only)2888TCPInternalX in server.N=host:X:YPeerServer (with CDH4 only)3888TCPInternalY in server.N=host:X:YPeerServer (with CDH4 and Cloudera Manager 4)3181TCPInternalX in server.N=host:X:YPeerServer (with CDH4 and Cloudera Manager 4)4181TCPInternalY in server.N=host:X:YPeerZooKeeper FailoverController (ZKFC)8019TCPInternalUsed for HAZooKeeper JMX port9010TCPInternal</p>
<p>As JMX port, ZooKeeper will also use another randomly selected port for RMI. In order for Cloudera Manager to monitor ZooKeeper, you must open up all ports when the connection originates from the Cloudera Manager server.</p>
<h3 id="1-1-5-8-hue">1.1.5.8 Hue</h3>
<p>ServiceQualifierPortProtocolAccess RequirementConfigurationCommentServer8888TCPExternalBeeswax Server8002InternalBeeswax Metastore8003Internal</p>
<h3 id="1-1-5-9-ozzie">1.1.5.9 Ozzie</h3>
<p>ServiceQualifierPortProtocolAccess RequirementConfigurationCommentOozie Server11000TCPExternalOOZIE_HTTP_PORT in oozie-env.shHTTPOozie Server11001TCPlocalhostOOZIE_ADMIN_PORT in oozie-env.shShutdown port</p>
<h3 id="1-1-5-10-ganglia">1.1.5.10 Ganglia</h3>
<p>ServiceQualifierPortProtocolAccess RequirementConfigurationCommentganglia-gmond8649UDP/TCPInternalganglia-web80TCPExternalVia Apache httpd</p>
<h3 id="1-1-5-11-kerberos">1.1.5.11 Kerberos</h3>
<p>ServiceQualifierPortProtocolAccess RequirementConfigurationCommentKRB5 KDC ServerSecure88UDP/TCPExternalkdc_ports and kdc_tcp_ports in either the [kdcdefaults] or [realms] sections of kdc.confBy default only UDPKRB5 Admin ServerSecure749TCPInternalkadmind_port in the [realms] section of kdc.conf</p>
<h3 id="1-2-">1.2 观点</h3>
<h3 id="1-2-1-hadoop-">1.2.1 Hadoop即将过时了吗？</h3>
<p><a href="http://www.kuqin.com/database/20120715/322528.html" target="_blank"><a href="http://www.kuqin.com/database/20120715/322528.html">http://www.kuqin.com/database/20120715/322528.html</a></a></p>
<p>google提出的三个东西都是解决hadoop的软肋，最终目的还是需要解决大数据上面的实时性问题。</p>
<ul>
<li>增量索引过滤器（Percolator for incremental indexing）和频繁变化数据集分析。Hadoop是一台大型“机器”，当启动并全速运转时处理数据的性能惊人，你唯一需要操心的就是硬盘的传输速度跟不上。但是每次你准备启动分析数据时，都需要把所有的数据都过一遍，当数据集越来越庞大时，这个问题将导致分析时间无限延长。那么Google是如何解决让搜索结果返回速度越来越接近实时的呢？答案是用增量处理引擎Percolator代替GMR。通过只处理新增的、改动过的或删除的文档和使用二级指数来高效率建目录，返回查询结果。Percolator论文的作者写道：“将索引系统转换成增量系统…将文档处理延迟缩短了100倍。”这意味着索引web新内容的速度比用MapReduce快100倍！类似大型强子对撞机产生的数据将不断变大，Twitter也是如此。这也是为什么HBase中会新增触发流程，而Twitter Storm正在成为实时处理流数据的热门技术。</li>
<li>用于点对点分析的Dremel。Google和Hadoop生态系统都致力于让MapReduce成为可用的点对点分析工具。从Sawzall到Pig和Hive，创建了大量的界面层，但是尽管这让Hadoop看上去更像SQL系统，但是人们忘记了一个基本事实——MapReduce(以及Hadoop)是为组织数据处理任务开发的系统，诞生于工作流内核，而不是点对点分析。今天有大量的BI/分析查询都是点对点模式，属于互动和低延迟的分析。Hadoop的Map和Reduce工作流让很多分析师望而却步，而且工作启动和完成工作流运行的漫长周期对于很多互动性分析来说意味着糟糕的用户体验。于是，Google发明了Dremel（业界也称之为BigQuery产品）专用工具，可以让分析师数秒钟内就扫描成PB（Petabyte）的数据完成点到点查询，而且还能支持可视化。Google在Dremel的论文中声称：“Dremel能够在数秒内完成数万亿行数据的聚合查询，比MapReduce快上100倍！”</li>
<li>分析图数据的Pregel。Google MapReduce的设计初衷是分析世界上最大的数据图谱——互联网。但是在分析人际网络、电信设备、文档和其他一些图数据时就没有那么灵光了，例如MapReduce在计算单源最短路径（SSSP）时效率非常低下，已有的并行图算法库Parallel BGL或者CGMgraph又没有容错。于是Google开发了Pregel，一个可以在分布式通用服务器上处理PB级别图数据的大型同步处理应用。与Hadoop经常在处理图数据时产生指数级数据放大相比，Pregel能够自然高效地处理SSSP或PageRank等图算法，所用时间要短得多，代码也简洁得多。目前唯一能与Pregel媲美的开源选择是Giraph，这是一个早期的Apache孵化项目，调用了HDFS和Zookeeper。Githb上还有一个项目Golden Orb可用。</li>
</ul>
<h3 id="1-2-2-best-practices-for-selecting-apache-hadoop-hardware">1.2.2 Best Practices for Selecting Apache Hadoop Hardware</h3>
<p><a href="http://hortonworks.com/blog/best-practices-for-selecting-apache-hadoop-hardware/" target="_blank"><a href="http://hortonworks.com/blog/best-practices-for-selecting-apache-hadoop-hardware/">http://hortonworks.com/blog/best-practices-for-selecting-apache-hadoop-hardware/</a></a></p>
<p>RAID cards, redundant power supplies and other per-component reliability features are not needed. Buy error-correcting RAM and SATA drives with good MTBF numbers. Good RAM allows you to trust the quality of your computations. Hard drives are the largest source of failures, so buy decent ones.（不需要选购RAID，冗余电源或者是一些满足高可靠性组件，但是选择带有ECC的RAM以及good MTBF的SATA硬盘却是非常需要的。ECC RAM可以让你确保计算结果的正确性，而SATA故障是大部分故障的主要原因）</p>
<ul>
<li>On CPU: It helps to understand your workload, but for most systems I recommend sticking with medium clock speeds and no more than 2 sockets. Both your upfront costs and power costs rise quickly on the high-end. For many workloads, the extra performance per node is not cost-effective.（没有特别要求，普通频率，dual-socket？？？）</li>
<li>On Power: Power is a major concern when designing Hadoop clusters. It is worth understanding how much power the systems you are buying use and not buying the biggest and fastest nodes on the market.In years past we saw huge savings in pricing and significant power savings by avoiding the fastest CPUs, not buying redundant power supplies, etc. Nowadays, vendors are building machines for cloud data centers that are designed to reduce cost and power and that exclude a lot of the niceties that bulk up traditional servers. Spermicro, Dell and HP all have such product lines for cloud providers, so if you are buying in large volume, it is worth looking for stripped-down cloud servers. （根据自己的需要尽量减少能耗开销，撇去一些不需要的部件。而且现在很多厂商也在尽量减少不必要的部件）</li>
<li>On RAM: What you need to consider is the amount of RAM needed to keep the processors busy and where the knee in the cost curve resides. Right now 48GB seems like a pretty good number. You can get this much RAM at commodity prices on low-end server motherboards. This is enough to provide the Hadoop framework with lots of RAM (~4 GB) and still have plenty to run many processes. Don’t worry too much about RAM, you’ll find a use for it, often running more processes in parallel. If you don’t, the system will still use it to good effect, caching disk data and improving performance.（RAM方面的话越大越好，对于48GB的RAM来说普通的主板也是支持的。如果RAM用的上的话那么允许多个进程并行执行，如果暂时永不上的话可以做cache来提高速度）</li>
<li>On Disk: Look to buy high-capacity SATA drives, usually 7200RPM. Hadoop is storage hungry and seek efficient but it does not require fast, expensive hard drives. Keep in mind that with 12-drive systems you are generally getting 24 or 36 TB/node. Until recently, putting this much storage in a node was not practical because, in large clusters, disk failures are a regular occurrence and replicating 24+TB could swamp the network for long enough to really disrupt work and cause jobs to miss SLAs. The most recent release of Hadoop 0.20.204 is engineered to handle the failure of drives more elegantly, allowing machines to continue serving from their remaining drives. With these changes, we expect to see a lot of 12+ drive systems. In general, add disks for storage and not seeks. If your workload does not require huge amounts of storage, dropping disk count to 6 or 4 per box is a reasonable way to economize.（高容量SATA硬盘，最好是7.2KRPM，并且最好单机上面挂在12个硬盘。对于hadoop之前这种方式并不实际，因为磁盘非常容易损坏并且备份这24TB的数据非常耗时。而hadoop可以很好地解决这个问题。</li>
</ul>
<p>小集群来说的话，通常单个机器上面挂在4-6个disk即可）</p>
<ul>
<li>On Network: This is the hardest variable to nail down. Hadoop workloads vary a lot. The key is to buy enough network capacity to allow all nodes in your cluster to communicate with each other at reasonable speeds and for reasonable cost. For smaller clusters, I’d recommend at least 1GB all-to-all bandwidth, which is easily achieved by just connecting all of your nodes to a good switch. With larger clusters this is still a good target although based on workload you can probably go lower. In the very large data centers the Yahoo! built, they are seeing 2/<em>10GB per 20 node rack going up to a pair of central switches, with rack nodes connected with two 1GB links. As a rule of thumb, watch the ratio of network-to-computer cost and aim for network cost being somewhere around 20% of your total cost. Network costs should include your complete network, core switches, rack switches, any network cards needed, etc. We’ve been seeing InfiniBand and 10GB Ethernet networks to the node now. If you can build this cost effectively, that’s great. However, keep in mind that Hadoop grew up with commodity Ethernet, so understand your workload requirements before spending too much on the network.（这个主要还是看需求。通常来说网络整体开销占据所有开销的20%，包括核心交换机，机架之间的交换机以及网卡设备等。yahoo大集群的部署方式是rack之间使用2/</em>10GB的核心交换机工作，而20个节点的rack之间内部使用1GB链路）。<h3 id="1-2-3-the-dark-side-of-hadoop-backtype-technology">1.2.3 The dark side of Hadoop - BackType Technology</h3>
</li>
</ul>
<p><a href="http://web.archive.org/web/20110510125644/http://tech.backtype.com/the-dark-side-of-hadoop" target="_blank"><a href="http://web.archive.org/web/20110510125644/http://tech.backtype.com/the-dark-side-of-hadoop">http://web.archive.org/web/20110510125644/http://tech.backtype.com/the-dark-side-of-hadoop</a></a></p>
<p>谈到了一些在使用hadoop出现的一些问题，而这些问题是hadoop本身的。</p>
<ul>
<li>Critical configuration poorly documented 一些关键的参数和配置并没有很好地说明清楚。</li>
<li><p>Terrible with memory usage 内存使用上面存在问题。hadoop里面有一些非常sloppy的实现，比如chmod以及ln -s等操作，并没有调用fs API而是直接创建一个shell进程来完成。因为fork出一个shell进程需要申请同样大小的内存（虽然实现上是COW），但是这样造成jvm出现oom。解决的办法是开辟一定空间的swap The solution to these memory problems is to allocate a healthy amount of swap space for each machine to protect you from these memory glitches. We couldn&#39;t believe how much more stable everything became when we added swap space to our worker machines.</p>
</li>
<li><p>Thomas Jungblut&#39;s Blog: Dealing with &quot;OutOfMemoryError&quot; in Hadoop <a href="http://codingwiththomas.blogspot.jp/2011/07/dealing-with-outofmemoryerror-in-hadoop.html" target="_blank"><a href="http://codingwiththomas.blogspot.jp/2011/07/dealing-with-outofmemoryerror-in-hadoop.html">http://codingwiththomas.blogspot.jp/2011/07/dealing-with-outofmemoryerror-in-hadoop.html</a></a> 作者给出的解决办法就是修改hadoop的代码，通过调用Java API而不是使用ProcessBuilder来解决。</p>
</li>
<li><strong>NOTE(dirlt):出现OOM的话必须区分JVM还是Linux System本身的OOM。JVM出现OOM是抛出异常，而Linux出现OOM是会触发OOM killer</strong></li>
<li>Zombies hadoop集群出现一些zombie进程，而这些进程会一直持有内存直到大量zombie进程存在最后需要重启。造成这些zombie进程的原因通常是因为jvm oom（增加了swap之后就没有出现这个问题了），但是奇怪的是tasktracker作为这些process的parent，并不负责cleanup这些zombie进程而是依赖这些zombie进程的自己退出，这就是hadoop设计方面的问题。</li>
</ul>
<p>Making Hadoop easy to deploy, use, and operate should be the /#1 priority for the developers of Hadoop.</p>
<h3 id="1-3-">1.3 使用问题</h3>
<h3 id="1-3-1-cdh3u3-">1.3.1 CDH3u3搭建单节点集群</h3>
<p>搭建单节点集群允许我们在单机做一些模拟或者是测试，还是非常有意义的。如何操作的话可以参考链接 <a href="http://localhost/utils/hadoop-0.20.2-cdh3u3/docs/single_node_setup.html" target="_blank"><a href="http://localhost/utils/hadoop-0.20.2-cdh3u3/docs/single_node_setup.html">http://localhost/utils/hadoop-0.20.2-cdh3u3/docs/single_node_setup.html</a></a></p>
<p>这里稍微总结一下：</p>
<ul>
<li>首先安装ssh和rsync /# sudo apt-get install ssh &amp;&amp; sudo apt-get install rsync</li>
<li>本机建立好信任关系 /# cat ~/.ssh/id_dsa.pub &gt;&gt; ~/.ssh/authorized_keys</li>
<li>将{hadoop-package}/conf配置文件修改如下：</li>
<li><p>conf/core-site.xml</p>
<configuration>

   <property>
       <name>fs.default.name</name>

       <value>hdfs://localhost:9000</value>
   </property>

</li>
</ul>
<p></configuration></p>
<ul>
<li><p>conf/hdfs-site.xml</p>
<configuration>

   <property>
       <name>dfs.replication</name>

       <value>1</value>
   </property>

</li>
</ul>
<p></configuration></p>
<ul>
<li><p>conf/mapred-site.xml</p>
<configuration>

   <property>
       <name>mapred.job.tracker</name>

       <value>localhost:9001</value>
   </property>

</li>
</ul>
<p></configuration></p>
<ul>
<li>格式化namenode /# bin/hadoop namenode -format</li>
<li>启动hadoop集群 /# bin/start-all.sh</li>
<li>停止hadoop集群 /# bin/stop-all.sh</li>
<li><p>webconsole</p>
</li>
<li><p>NameNode - <a href="http://localhost:50070/" target="_blank"><a href="http://localhost:50070/">http://localhost:50070/</a></a></p>
</li>
<li>JobTracker - <a href="http://localhost:50030/" target="_blank"><a href="http://localhost:50030/">http://localhost:50030/</a></a></li>
</ul>
<h3 id="1-3-2-cdh4-2-0-">1.3.2 CDH4.2.0搭建单节点集群</h3>
<p>基本流程和CDH3u3是相同的，但是有一些差异我记录下来。</p>
<ul>
<li><p>配置文件</p>
</li>
<li><p>配置文件在etc/hadoop，包括环境配置脚本比如hadoop-env.sh</p>
</li>
<li>bin/sbin目录下面有hadoop集群启动停止工具 <strong>NOTE（dirlt）：不要使用它们</strong></li>
<li>libexec目录下面是公用的配置脚本</li>
<li>mapred-site.xml中jobtracker地址配置key修改为 mapred.jobtracker.address <strong>NOTE(dirlt):this for yarn.如果是mr1那么不用修改,依然是mapred.job.tracker</strong></li>
<li>hadoop-daemons.sh会使用/sbin/slaves.sh来在各个节点启动，但是 /<em>不知道什么原因，很多环境变量没有设置/</em> ，所以在slaves.sh执行ssh命令部分最开始增加了 source ~/.shrc; 来强制设置我的环境变量</li>
<li><strong>NOTE(dirlt):不要使用shell脚本来启动，而是直接使用类似hadoop namenode这种方式来启动单个机器上的实例</strong></li>
<li><p>公共组件</p>
</li>
<li><p>CDH4.2.0 native-library都放在了目录lib/native下面，而不是CDH3u3的lib/native/Linux-amd64-64下面，这点需要注意。</p>
</li>
<li>CDH4.2.0 没有自带libhadoop.so, 所以启动的时候都会出现 ”Unable to load native-hadoop library for your platform… using builtin-java classes where applicable“ 这个警告。需要自己编译放到lib/native目录下面。</li>
<li>CDH4.2.0 lib下面没有任何文件，所有的lib都在share/hadoop//*/lib下面，比如share/hadoop/common/lib. 这点和CDH3有差别，CDH3所有的jar都放在lib目录下面。使用 hadoop classpath 命令可以察看</li>
<li><p>环境变量</p>
</li>
<li><p>JAVA_LIBRARY_PATH用来设置native library path</p>
</li>
<li>HADOOP_CLASSPATH可以用来设置hadoop相关的classpath（比如使用hadoop-lzo等）</li>
<li><p>准备工作</p>
</li>
<li><p>使用hdfs namenode -format来做格式化 <strong>注意如果使用sudo apt-get来安装的话，是其他用户比如hdfs,impala,mapred,yarn来启动的，所以必须确保目录对于这些用户是可写的</strong></p>
</li>
<li>使用命令 hadoop org/apache/hadoop/examples/QuasiMonteCarlo 1 1 确定集群是否可以正常运行。<h3 id="1-3-3-cdh4-3-0">1.3.3 CDH4.3.0</h3>
</li>
</ul>
<p>基本流程和CDH4.2.0是相同的，但是存在一些差异我记录下来的。从4.3.0开始将mr1和mr2分开存放，还是一个比较大的区别的。这里我以使用mr1为例。</p>
<ul>
<li>在libexec/hadoop-config.sh添加source ~/.shrc 来强制设置环境变量。</li>
<li><p>mr1和mr2分开存放主要有</p>
</li>
<li><p>etc目录，hadoop and hadoop-mapreduce1</p>
</li>
<li>bin目录，bin and bin-mapreduce1</li>
<li><p>lib目录。如果需要使用mr1的话，那么将cp -r share/hadoop/mapreduce1/ .</p>
</li>
<li><p><strong>NOTE（dirlt）：似乎只需要最顶层的一些jar文件即可</strong></p>
</li>
<li>在bin/hadoop-config.sh添加source ~/.shrc 来强制设置环境变量。</li>
<li><strong>NOTE（dirlt）：不要使用start-dfs.sh这些脚本启动，似乎这些脚本会去读取master,slaves这些文件然后逐个上去ssh启动。直接使用hadoop namenode这种方式可以只启动单个机器上的实例</strong></li>
</ul>
<h3 id="1-3-4-configuration">1.3.4 Configuration</h3>
<h3 id="1-3-4-1-bash_profile">1.3.4.1 .bash_profile</h3>
<p>export HADOOP_HOME=$HOME/dirlt/hadoop-2.0.0-cdh4.3.0/</p>
<p>export HADOOP_CONF_DIR=$HADOOP_HOME/etc/hadoop
export HBASE_HOME=/home/alium_zhanyinan/dirlt/hbase-0.94.6-cdh4.3.0</p>
<p>export HBASE_CLASSPATH=$HBASE_HOME/hbase-0.94.6-cdh4.3.0-security.jar:$HBASE_HOME/conf
export ZK_HOME=/home/alium_zhanyinan/dirlt/zookeeper-3.4.5-cdh4.3.0</p>
<p>export ZK_CLASSPATH=$ZK_HOME/zookeeper-3.4.5-cdh4.3.0.jar
export HADOOP_CLASSPATH=$HADOOP_CLASSPATH:$HBASE_CLASSPATH:$ZK_CLASSPATH</p>
<p>export JAVA_HOME=/usr/java/default/</p>
<h3 id="1-3-4-2-core-site-xml">1.3.4.2 core-site.xml</h3>
<configuration>

  <property>
    <name>fs.default.name</name>

    <value>hdfs://umengds1.mob.cm3:8020</value>
  </property>


  <property>

    <name>fs.trash.interval</name>
    <value>1440</value>

  </property>
</configuration>


<h3 id="1-3-4-3-hdfs-site-xml">1.3.4.3 hdfs-site.xml</h3>
<configuration>

  <property>
    <name>dfs.name.dir</name>

    <value>/disk1/data/dfs/nn</value>
  </property>


  <property>

    <name>dfs.data.dir</name>
    <value>/disk1/data/dfs/dn</value>

  </property>


  <property>
    <name>fs.checkpoint.dir</name>

    <value>/disk1/data/dfs/snn</value>
  </property>


  <property>

    <name>dfs.replication</name>
    <value>3</value>

  </property>


  <property>
    <name>dfs.block.size</name>

    <value>134217728</value>
  </property>


  <property>

    <name>dfs.datanode.max.xcievers</name>
    <value>8192</value>

  </property>


  <property>
    <name>dfs.datanode.du.reserved</name>

    <value>21474836480</value>
  </property>


  <property>

    <name>dfs.namenode.handler.count</name>
    <value>64</value>

  </property>


  <property>
    <name>dfs.datanode.handler.count</name>

    <value>32</value>
  </property>


  <property>

    <name>dfs.client.read.shortcircuit</name>
    <value>true</value>

  </property>
</configuration>



<h3 id="1-3-4-4-mapred-site-xml">1.3.4.4 mapred-site.xml</h3>
<configuration>

  <property>
    <name>mapred.job.tracker</name>

    <value>umengds2.mob.cm3:8021</value>
  </property>


  <property>

    <name>mapred.system.dir</name>
    <value>/tmp/mapred/system</value>

  </property>


  <property>
    <name>mapreduce.jobtracker.staging.root.dir</name>

    <value>/user</value>
  </property>


  <property>

    <name>mapred.local.dir</name>
    <value>/disk1/data/mapred/local</value>

  </property>


  <property>
    <name>mapred.submit.replication</name>

    <value>3</value>
    <final>true</final>

  </property>


  <property>
    <name>mapred.tasktracker.map.tasks.maximum</name>

    <value>6</value>
  </property>

  <property>
    <name>mapred.tasktracker.reduce.tasks.maximum</name>

    <value>8</value>
  </property>


  <property>

    <name>mapred.child.java.opts</name>
    <value> -Xmx2048M -XX:-UseGCOverheadLimit</value>

  </property>


  <property>
    <name>mapred.job.tracker.handler.count</name>

    <value>64</value>
  </property>


  <property>

    <name>io.sort.mb</name>
    <value>256</value>

  </property>


  <property>
    <name>io.sort.factor</name>

    <value>64</value>
  </property>

</configuration>

<h3 id="1-3-4-5-hadoop-env-sh">1.3.4.5 hadoop-env.sh</h3>
<p>/# The maximum amount of heap to use, in MB. Default is 1000.</p>
<p>export HADOOP_HEAPSIZE=6000</p>
<p>/# Extra Java runtime options. Empty by default.
/# if [&quot;$HADOOP_OPTS&quot; == &quot;&quot; ]; then export HADOOP_OPTS=-server; else HADOOP_OPTS+=&quot; -server&quot;</p>
<p>; fi</p>
<p>/# Command specific options appended to HADOOP_OPTS when specified
export HADOOP_NAMENODE_OPTS=&quot;-Xmx12000m $HADOOP_NAMENODE_OPTS&quot;export HADOOP_SECONDARYNAMENODE_OPTS=&quot;-Xmx12000m $HADOOP_SECONDARYNAMENODE_OPTS&quot;export HADOOP_DATANODE_OPTS=&quot;-Xmx6000m $HADOOP_DATANODE_OPTS&quot;export HADOOP_BALANCER_OPTS=&quot;-Xmx3000m $HADOOP_BALANCER_OPTS&quot;export HADOOP_JOBTRACKER_OPTS=&quot;-Xmx12000m $HADOOP_JOBTRACKER_OPTS&quot;</p>
<h3 id="1-3-4-6-hbase-site-xml">1.3.4.6 hbase-site.xml</h3>
<configuration>

  <property>
    <name>hbase.cluster.distributed</name>

    <value>true</value>
  </property>


  <property>

    <name>hbase.rootdir</name>
    <value>hdfs://umengds1.mob.cm3:8020/hbase</value>

  </property>


  <property>
    <name>hbase.zookeeper.quorum</name>

    <value>umengds1.mob.cm3,umengds2.mob.cm3</value>
  </property>


  <property>

    <name>hbase.hregion.memstore.mslab.enabled</name>
    <value>true</value>

  </property>


  <property>
    <name>hbase.regionserver.handler.count</name>

    <value>128</value>
  </property>


  <property>

    <name>hbase.client.write.buffer</name>
    <value>4194304</value>

  </property>


  <property>
    <name>hbase.hregion.memstore.block.multiplier</name>

    <value>8</value>
  </property>


  <property>

    <name>hbase.server.thread.wakefrequency</name>
    <value>1000</value>

  </property>


  <property>
    <name>hbase.regionserver.lease.period</name>

    <value>600000</value>
  </property>


  <property>

    <name>hbase.hstore.blockingStoreFiles</name>
    <value>15</value>

  </property>


  <property>
    <name>hbase.hregion.max.filesize</name>

    <value>2147483648</value>
  </property>


  <property>

    <name>hbase.ipc.client.tcpnodelay</name>
    <value>true</value>

  </property>


  <property>
    <name>ipc.ping.interval</name>

    <value>10000</value>
  </property>


  <property>

    <name>hbase.hregion.majorcompaction</name>
    <value>0</value>

  </property>


  <property>
    <name>hbase.regionserver.checksum.verify</name>

    <value>true</value>
  </property>

</configuration>

<h3 id="1-3-4-7-hbase-env-sh">1.3.4.7 hbase-env.sh</h3>
<p>/# The maximum amount of heap to use, in MB. Default is 1000.</p>
<p>export HBASE_HEAPSIZE=14000</p>
<p>/# Extra Java runtime options.
/# Below are what we set by default. May only work with SUN JVM.</p>
<p>/# For more on why as well as other possible settings,
/# see <a href="http://wiki.apache.org/hadoop/PerformanceTuning" target="_blank">http://wiki.apache.org/hadoop/PerformanceTuning</a></p>
<p>/# export HBASE_OPTS=
&quot;-ea -XX:+UseConcMarkSweepGC -XX:+CMSIncrementalMode&quot;export HBASE_OPTS=&quot;-ea -XX:+UseParNewGC -XX:+UseConcMarkSweepGC -XX:+UseCMSCompactAtFullCollection -XX:CMSInitiatingOccupancyFraction=90&quot;</p>
<h3 id="1-4-hadoop-">1.4 Hadoop权威指南</h3>
<h3 id="1-4-1-hadoop">1.4.1 初识Hadoop</h3>
<p>古代，人们用牛来拉中午，当一头牛拉不动一根圆木的时候，他们不曾想过培育更大更壮的牛。同样，我们也不需要尝试开发超级计算机，而应试着结合使用更多计算机系统。</p>
<h3 id="1-4-2-mapreduce">1.4.2 关于MapReduce</h3>
<ul>
<li>设置HADOOP_CLASSPATH就可以直接使用hadoop CLASSNAME来在本地运行mapreduce程序。</li>
<li><p>hadoop jar $HADOOP_HOME/contrib/streaming/hadoop-streaming-0.20.2-cdh3u3.jar 可以用来启动streaming任务</p>
</li>
<li><p>使用stdin/stdout来作为输入和输出</p>
</li>
<li><p><strong>NOTE（dirlt）：倒是可以探索一下如何使用，但是觉得能力有限</strong></p>
</li>
<li><p>Input/Output Format</p>
</li>
<li>外围环境的访问比如访问hdfs以及hbase</li>
<li>程序打包。比如使用很多第三方库的话在其他机器上面没有部署。</li>
<li><p>hadoop pipes 可以用来启动pipes任务</p>
</li>
<li><p>Hadoop的Pipes是Hadoop MapReduce的C++接口代称</p>
</li>
<li>使用Unix Domain Socket来作为输入和输出</li>
<li><p><strong>NOTE（dirlt）：可能使用上面还是没有native mr或者是streaming方式方便</strong></p>
<h3 id="1-4-3-hadoop-">1.4.3 Hadoop分布式文件系统</h3>
</li>
<li><p>使用hadoop archive能够将大量小文档打包，存档文件之能够只读访问</p>
</li>
<li><p>使用hadoop archive -archiveName <file>.har -p <parent-path> src dst</p>
</li>
<li><p>存档过程使用mapreduce完成，输出结果为目录</p>
</li>
<li><p>part-0 表示存档内容文件，应该是使用一个reduce做聚合。</p>
</li>
<li>_index,_masterindex 是对存档内容文件的索引文件。</li>
<li><p>har(hadoop archive)文件系统是建立在其他文件系统上面的，比如hdfs或者是local fs.</p>
</li>
<li><p>hadoop fs -ls har:///file.har 那么访问的是默认的文件系统上面的file.har</p>
</li>
<li>如果想显示地访问hdfs文件系统的话，那么可以hadoop fs -ls har://hdfs-localhost:9000/file.har</li>
<li>如果想显示地访问本地文件系统的话，那么可以使用hadoop fs -ls har://file-localhost/file.har</li>
<li>hadoop fs -ls har://schema-<host>/<path> 是通用的访问方式</li>
</ul>
<h3 id="1-4-4-hadoop-io">1.4.4 Hadoop IO</h3>
<ul>
<li><p>文件系统</p>
</li>
<li><p>ChecksumFileSystem</p>
</li>
<li><p>使用decorator设计模式，底层filesystem称为RawFileSystem</p>
</li>
<li>对于每个文件filename都会创建.filename.crc文件存储校验和</li>
<li>计算crc的单位大小通过io.bytes.per.checksum来进行控制</li>
<li>读取文件如果出现错误的话，那么会抛出ChecksumException</li>
<li><p>考虑到存在多副本的情况，如果读取某个副本出错的话，期间那么会调用reportChecksumFailure方法</p>
</li>
<li><p><strong>NOTE（dirlt）：这个部分的代码不太好读，非常绕</strong></p>
</li>
<li><p>RawLocalFileSystem</p>
</li>
<li><p>本地文件系统</p>
</li>
<li><p>LocalFileSystem</p>
</li>
<li><p>RawLocalFileSystem + ChecksumFileSystem</p>
</li>
<li>reportChecksumFailure实现为将校验和存在问题的文件移动到bad_files边际文件夹（side directory）</li>
<li><p>DistributedFileSystem</p>
</li>
<li><p>分布式文件系统</p>
</li>
<li><p>ChecksumDistributedFileSystem</p>
</li>
<li><p>DistributedFileSystem + ChecksumFileSystem</p>
</li>
<li><p>压缩解压</p>
</li>
<li><p>DEFLATE org.apache.hadoop.io.compress.DefaultCodec 扩展名.defalte</p>
</li>
<li>Gzip org.apache.hadoop.io.compress.GzipCodec 扩展名.gz 使用DEFLATE算法但是增加了额外的文件头。</li>
<li>bzip2 org.apache.hadoop.io.compress.BZip2Codec 扩展名.bz2 自身支持文件切分，内置同步点。</li>
<li><p>LZO com.hadoop.compression.lzo.LzopCodec 扩展名.lzo 和lzop工具兼容，LZO算法增加了额外的文件头。</p>
</li>
<li><p>LzopCodec则是纯lzo格式的codec,使用.lzo_deflate作为文件扩展名</p>
</li>
<li>因为LZO代码库拥有GPL许可，因此没有办法包含在Apache的发行版本里面。</li>
<li><p>运行MapReduce时候可能需要针对不同压缩文件解压读取，就需要构造CompressionCodec对象，我们可以通过CompressionCodecFactory来构造这个对象</p>
</li>
<li><p>CompressionCodecFactory读取变量io.compression.codecs</p>
</li>
<li>然后根据输入文件的扩展名来选择使用何种codec.</li>
<li>getDefaultExtension</li>
<li><p>压缩和解压算法可能同时存在Java实现和原生实现</p>
</li>
<li><p>如果是原生实现的话通常是.so，那么需要设置java.library.path或者是在环境变量里面设置LD_LIBRARY_PATH</p>
</li>
<li>如果同时有原生实现和Java实现，我们想只是使用原生实现的话，那么可以设置hadoop.native.lib = false来禁用原生实现。</li>
<li><p>压缩算法涉及到对应的InputFormat,也就涉及到是否支持切分</p>
</li>
<li><p>对于一些不支持切分的文件，可能存在一些外部工具来建立索引，从而支持切分。</p>
</li>
<li><p>下面这些选项可以针对map结果以及mapreduce结果进行压缩</p>
</li>
<li><p>mapred.output.compress = true 将mapreduce结果做压缩</p>
</li>
<li>mapred.output.compression.codec mapreduce压缩格式</li>
<li>mapred.output.compress.type = BLOCK/RECORD 如果输出格式为SequenceFile的话，那么这个参数可以控制是块压缩还是记录压缩</li>
<li><strong>NOTE（dirlt）：我现在强烈感觉MR的中间结果存储格式为SequenceFile</strong></li>
<li><strong>NOTE（dirlt）：应该是IFile，但是是否共享了这个配置呢？</strong></li>
<li>mapred.compress.map.output = true 将map结果做压缩</li>
<li><p>mapred.map.output.compression.codec map压缩格式</p>
</li>
<li><p>序列化</p>
</li>
<li><p>Hadoop的序列化都是基于Writable实现的，WritableComparable则是同时继承Writable,Comparable<T>.</p>
</li>
<li><p>序列化对象需要实现RawComparator，接口为public int compare(byte[] b1, int s1, int l1, byte[] b2, int s2, int l2)进行二进制比较。</p>
</li>
<li><p>WritableComparator简化了这个实现，继承WritableComparator就实现了这个接口</p>
</li>
<li>但是这个接口实现起来非常naive，就是将两个byte stream反序列化然后调用对象的compareTo实现</li>
<li>如果想要提高效率的话，可以考虑通过直接比较两个byte stream来做优化。</li>
<li><p>基于文件的数据结构</p>
</li>
<li><p>SequenceFile 主要用来存储KV数据结构，多条记录之间会穿插一些同步标记，因此允许进行切分。</p>
</li>
<li><p>使用SequenceFileInputFormat和SequenceFileOutputFormat来读取和输出SequenceFile</p>
</li>
<li>hadoop fs -text 可以用来读取文件</li>
<li><p>mapred.output.compress.type = BLOCK/RECORD 可以用来控制压缩方式</p>
</li>
<li><p>如果没有使用压缩的话，那么格式为 recordLength(4byte) + keyLength(4byte) + key + value</p>
</li>
<li>如果使用记录压缩的话，那么格式为 recordLnegth(4byte) + keyLength(4byte) + key + compressedValue</li>
<li>如果使用块压缩的话，那么格式为 numberRecord(1-5byte) + keyLength(4byte) + compressedKeys + valueLength(4byte) + compressedValues.每个block之间会插入sync标记</li>
<li>块压缩大小可以使用io.seqfile.compress.blocksize来控制，默认1MB</li>
<li><p>MapFile 也是用来存储KV数据结构，但是可以认为已经按照了Key进行排序 <strong>NOTE（dirlt）：要求添加顺序就按照Key排序</strong></p>
</li>
<li><p>存储格式实际上也是SequenceFile，data，index都是。</p>
</li>
<li>底层会建立index，index在搜索的时候会加载到内存里面，这样可以减少data上的随机查询次数。</li>
<li>使用io.map.index.interval可以控制多少个item在index里面创建一个条目</li>
<li>使用io.map.index.skip = 0/1/2/n 可以控制skip几个index的item，如果为1的话那么表示只是使用1/2的索引。</li>
<li><p>从SequenceFile创建MapFile非常简单</p>
</li>
<li><p>首先使用sort将SequenceFile进行排序(可以使用hadoop example的sort）</p>
</li>
<li><p>然后调用hadoop MapFileFixer来建立索引</p>
<h3 id="1-4-5-mapreduce-">1.4.5 MapReduce应用开发</h3>
</li>
<li><p>Configuration用来读取配置文件，功能还是比较强大的，有变量替换的功能</p>
</li>
<li><property><name>…</name><value>…</value></property></li>
<li>如果使用<final>true</final>标记的话那么这个变量不允许被重置</li>
<li>变量替换可以使用${variable}</li>
<li><p>通过addResource来添加读取的配置文件</p>
</li>
<li><p>Hadoop集群有三种工作方式，分别为</p>
</li>
<li><p>standalone 使用单个JVM进程来模拟</p>
</li>
<li><p>如果不进行任何配置的话默认使用这个模式 <strong>NOTE（dirlt）：这个模式确实不错</strong></p>
</li>
<li>fs.default.name = file 本地文件系统</li>
<li>mapred.job.tracker = local</li>
<li><p>pseudo-distributed 本地启动单节点集群</p>
</li>
<li><p>fs.default.name = hdfs://localhost</p>
</li>
<li>mapred.job.tracker = localhost:8021</li>
<li><p>fully-distributed 完全分布式环境</p>
</li>
<li><p>fs.default.name = hdfs://<namenode></p>
</li>
<li><p>mapred.job.tracer = <jobtracker>:8021</p>
</li>
<li><p>使用hadoop启动MapReduce任务的常用参数</p>
</li>
</ul>
<ol>
<li>-D property=value 覆盖默认配置属性</li>
<li>-conf filename 添加配置文件</li>
<li>-fs uri 设置默认文件系统</li>
<li>-jt host:port 设置jobtracker</li>
<li>-files file,file2 这些文件可以在tasktracker工作目录下面访问</li>
<li>-archives archive,archive2 和files类似，但是是存档文件</li>
</ol>
<ul>
<li>突然觉得这个差别在files只能是平级结构，而archive可以是层级结构。</li>
<li>-libjars jar1,jar2 和files类似，通常这些JAR文件是MapReduce所需要的。</li>
</ul>
<p>如果希望运行时候动态创建集群的话，可以通过这几个类来创建</p>
<ul>
<li>MiniDFSCluster</li>
<li>MiniMRCluster</li>
<li>MiniHBaseCluster</li>
<li>MiniZooKeeperClutser</li>
<li><strong>NOTE(dirlt):都称为Mini???Cluster？</strong></li>
</ul>
<p>另外还有自带的ClusterMapReduceTestCase以及HBaseTestingUtility来帮助进行mapreduce的testcase. 这些类散步在hadoop,hbase,hadoop-test以及hbase-test里面。</p>
<p><strong>NOTE（dirlt）：但是个人觉得可能还是没有本地测试方便，不过倒是可以试试</strong></p>
<p>job，task and attempt</p>
<ul>
<li><p>jobID常见格式为 job_200904110811_0002</p>
</li>
<li><p>其中200904110811表示jobtracker从2009.04.11的08:11启动的</p>
</li>
<li>0002 表示第三个job,从0000开始计数。超过10000的话就不能够很好地排序</li>
<li><p>taskID常见格式为 task_200904110811_0002_m_000003</p>
</li>
<li><p>前面一串数字和jobID匹配，表示从属于这个job</p>
</li>
<li>m表示map任务，r表示reduce任务</li>
<li>000003表示这是第4个map任务。顺序是在初始化时候指定的，并不反应具体的执行顺序。</li>
<li><p>attemptID常见格式为 attempt_200904110811_0002_m_000003_0</p>
</li>
<li><p>前面一串数字和taskID匹配，表示从属与这个task</p>
</li>
<li>attempt出现的原因是因为一个task可能会因为失败重启或者是预测执行而执行多次</li>
<li>如果jobtracker重启而导致作业重启的话，那么做后面id从1000开始避免和原来的attempt冲突。</li>
</ul>
<p>作业调试</p>
<ul>
<li><p>相关配置</p>
</li>
<li><p>mapred.jobtracker.completeuserjobs.maximum 表示web页面下面展示completed jobs的个数，默认是100，超过的部分放到历史信息页。</p>
</li>
<li>mapred.jobtracker.restart.recover = true jobtracker重启之后自动恢复作业</li>
<li>hadoop.job.history.location 历史作业信息存放位置，超过30天删除，默认在_logs/history</li>
<li>hadoop.job.history.user.location 如果不为none那么历史作业信息在这里也会存在一份，不会删除。</li>
<li><p>相关命令</p>
</li>
<li><p>hadoop fs -getmerge <src> <dst> 能够将hdfs的src下面所有的文件merge合并成为一份文件并且copy到本地</p>
</li>
<li>hadoop job -history 察看作业历史</li>
<li>hadoop job -counter 察看作业计数器</li>
<li><p>相关日志</p>
</li>
<li><p>系统守护进程日志 写入HADOOP_LOG_DIR里面，可以用来监控namenode以及datanode的运行情况</p>
</li>
<li>MapReduce作业历史日志 _logs/history</li>
<li>MapReduce任务日志 写入HADOOP_LOG_DIR/userlogs里面，可以用来监控每个job的运行情况</li>
<li><p>分析任务</p>
</li>
<li><p>JobConf允许设置profile参数 <strong>NOTE（dirlt）：新的接口里面JobConf-&gt;JobContext-&gt;Job，Job没有这些接口，但是可以通过Configuration来设置</strong></p>
</li>
<li><p>setProfileEnabled 打开profile功能，默认false，属性 mapred.task.profile</p>
</li>
<li><p>setProfileParams 设置profile参数</p>
</li>
<li><p>属性 mapred.task.profile.params</p>
</li>
<li>默认使用hprof -agentlib:hprof=cpu=samples,heap=sites,force=n,thread=y,verbose=n,file=%s&quot;</li>
<li>其中%s会替换成为profile输出文件</li>
<li><strong>NOTE（dirlt）：其实这里似乎也可以设置成为jmxremote来通过jvisualvm来调试</strong></li>
<li><p>setProfileTaskRange(boolean,String)</p>
</li>
<li><p>参数1表示针对map还是reduce task做profile, true表示map, false表示reduce</p>
</li>
<li>参数2表示针对哪些tasks做优化，&quot;0-2&quot;表示针对0，1，2三个任务，默认也是&quot;0-2&quot;</li>
<li>map task对应属性mapred.task.profile.maps，reduce task对应属性mapred.task.profile.reduces</li>
<li><p>任务重现</p>
</li>
<li><p>首先将keep.failed.task.files设置为true,这样如果任务失败的话，那么这个任务的输入和输出都会保留下来</p>
</li>
<li><p>如果是map任务的话，那么输入分别会在本地保留</p>
</li>
<li>如果是reduce任务的话，那么对应的map任务输出会在本地保留</li>
<li>然后我们使用hadoop IsolationRunner job.xml来重新运行这个任务</li>
<li>可以修改HADOOP_OPTS添加远程调试选项来启动这个任务。</li>
<li>如果希望任务都保留而不仅仅是失败任务保留的话，那么可以设置 keep.task.files.pattern 为正则表达式（与保留的任务ID匹配）</li>
</ul>
<h3 id="1-4-6-mapreduce-">1.4.6 MapReduce的工作机制</h3>
<p>Hadoop运行MapReduce作业的工作原理</p>
<p><img src="" alt="./images/mapreduce-workflow-architecture.png"></p>
<p>其中有几点需要注意的：</p>
<ul>
<li>计算分片信息是在本地完成的，分片信息和其他resouce(包括jars,files,archives等）一起copy到HDFS上面，然后jobtracker直接读取分片信息。</li>
<li>提交的资源可以设置replication数目，高副本数目可以缓解tasktracker获取resource的压力。参数是mapred.submit.replication.</li>
<li>对于streaming以及pipes的实现，无非就是task并不直接执行任务，而是开辟另外一个子进程来运行streaming或者是pipes的程序。</li>
</ul>
<p><img src="" alt="./images/mapreduce-streamming-pipes.jpg"></p>
<p>进度和状态的更新</p>
<ul>
<li>map任务进度是已经处理输入的比例</li>
<li><p>reduce任务进度分为三个部分</p>
</li>
<li><p>shuffle 1/3</p>
</li>
<li>sort 1/3</li>
<li>reduce 1/3</li>
<li>也就是说如果刚运行完成sort的话，那么进度是2/3</li>
<li><p>状态的更新</p>
</li>
<li><p>触发事件</p>
</li>
<li><p>读取记录</p>
</li>
<li>输出记录</li>
<li>修改状态 reporter的setStatus</li>
<li>计数器修改</li>
<li>reporter的progress</li>
<li><p>子进程有单独线程每隔3秒检查progress位是否设置，如果设置的话那么和tasktracker发起心跳</p>
</li>
<li><p>通过mapred.task.timeout控制</p>
</li>
<li><p>tasktracker每隔5秒和jobtracker做心跳</p>
</li>
<li><p>心跳时间通过 mapred.tasktracker.expircy.interval 设置</p>
</li>
<li><p>jobClient定期会去jobtracker询问job是否完成</p>
</li>
<li><p>jobClient也可以设置属性job.end.notification.url,任务完成jobtracker会调用这个url</p>
</li>
<li>可以认为就是推拉方式的结合。</li>
</ul>
<p>失败检测和处理</p>
<ul>
<li><p>任务失败</p>
</li>
<li><p>子进程抛出异常的话，tasktracker将异常信息记录到日志文件然后标记失败</p>
</li>
<li>对于streaming任务的话非0退出表示出现问题，也可以使用stream.non.zero.exit.is.failure = false来规避（ <strong>这样是否就没有办法判断是否正常退出了？</strong> ）</li>
<li>如果长时间没有响应的话，没有和tasktracker有交互，那么也会认为失败。这个时间使用mapred.task.timeout控制，默认10min</li>
<li><p>如果任务失败的话，jobtracker会尝试进行多次重试</p>
</li>
<li><p>map重试次数通过 mapred.map.max.attempts 配置</p>
</li>
<li>reduce重试次数通过 mapre.reduce.max.attempts 配置</li>
<li><strong>任何任务重试超过4次的话那么会认为整个job失败</strong></li>
<li>另外需要区分KILLED状态和FAILED状态，对于KILLED状态可能是因为推测执行造成的，不会记录到failed attempts里面</li>
<li><p>如果我们希望允许少量任务失败的话，那么可以配置</p>
</li>
<li><p>mapred.max.map.failures.percent 允许map失败的最大比率</p>
</li>
<li>mapred.max.reduce.failures.percent 允许reduce失败的最大比率</li>
<li>如果一个job超过一定的task在某个tt上面运行失败的话，那么就会将这个tt加入到这个job的blacklist. mapred.max.tracker.failures = 4</li>
<li>如果job成功的话，检查运行task失败的tt并且标记，如果超过一定阈值的话，那么会将tt加入到全局的blacklist. mapred.max.tracker.blacklists = 4</li>
</ul>
<p>作业的调度</p>
<ul>
<li><p>fifo scheduler</p>
</li>
<li><p>可以通过mapred.job.priority或者是setJobPriority设置</p>
</li>
<li>当队列中有空闲的槽位需要执行任务时，从等待队列中选择优先级最高的作业</li>
<li>fair scheduler</li>
<li>capacity scheduler</li>
</ul>
<p>shuffle和排序</p>
<p><img src="" alt="./images/mapreduce-shuffle-sort.jpg"></p>
<p><img src="" alt="./images/mapreduce-shuffle-sort-2.png"></p>
<p>有下面这些参数控制shuffle和sort的过程 <strong>NOTE（dirlt）：书上倒是有很多参数，但是好多还是不太理解</strong></p>
<ul>
<li>io.sort.mb map输出缓存空间大小，默认是100MB. 建议设置10/* io.sort.factor.</li>
<li><p>io.sort.spill.percent 如果map输出超过了缓存空间大小的这个阈值的话，那么就会spill,默认是0.8</p>
</li>
<li><p>每次spill之前先会对这个文件进行排序，如果有combiner的话那么会在上面调用combiner</p>
</li>
<li>写磁盘是按照轮询的方式写到mapred.local.dir属性指定的目录下面</li>
<li>如果spill速度太慢的话，那么往缓存空间写入进程就会阻塞，直到spill腾出空间。</li>
<li><p>io.sort.factor 多路归并的数量，默认是10. 建议设置在25-32.</p>
</li>
<li><p>在map阶段，因为最终会存在多个spill文件，所以需要做多路归并。 <strong>TODO（dirlt）：如果归并数量少的话是否可能会多次merge？</strong></p>
</li>
<li>在reduce阶段的话，因为可能存在多路map输出的结果，所以需要做多路归并。</li>
<li>min.num.spill.for.combine 如果指定combiner并且spill次数超过这个值的话就会调用combine,默认为3</li>
<li>tasktracker.http.threads reduce通过HTTP接口来发起数据请求，这个就是HTTP接口相应线程数目，默认为40。 <strong>mapper as server</strong></li>
<li><p>mapred.reduce.parallel.copies reduce启动多少个线程去请求map输出，默认为5。 <strong>reducer as client</strong></p>
</li>
<li><p><strong>NOTE(dirlt):如果reduce和每个map都使用一个线程去请求输出结果的话，只要shuffle阶段没有出现network congestion，那么提高线程数量是有效果的</strong></p>
</li>
<li><strong>NOTE（dirlt）：可以设置到15-50</strong></li>
<li>mapred.reduce.copy.backoff = 300(s) reduce下载线程最大等待时间</li>
<li>mapred.job.shuffle.input.buffer.percent = 0.7 用来缓存shuffle数据的reduce task heap百分比</li>
<li>mapred.job.shuffle.merge.percent = 0.66 缓存的内存中多少百分比后开始做merge操作</li>
<li>mapred.job.reduce.input.buffer.percent = 0.0 sort完成后reduce计算阶段用来缓存数据的百分比. 默认来说不会使用任何内存来缓存，因此完全从磁盘上进行读取。</li>
</ul>
<p>任务的执行</p>
<ul>
<li><p>推测执行参数</p>
</li>
<li><p>如果某个任务执行缓慢的话会执行另外一个备份任务</p>
</li>
<li>mapred.map.tasks.speculative.execution true</li>
<li>mapred.reduce.tasks.speculative.execution true</li>
<li><p>JVM重用</p>
</li>
<li><p>一个JVM实例可以用来执行多个task.</p>
</li>
<li>mapred.job.reuse.jvm.num.tasks/setNumTasksToExecutePerJvm 单个JVM运行任务的最大数目</li>
<li>-1表示没有限制</li>
<li><p>任务执行环境</p>
</li>
<li><p>程序自身可以知道执行环境对于开发还是比较有帮助的</p>
</li>
<li><p>这些属性对于streaming可以通过环境变量获得</p>
</li>
<li><p><strong>对于streaming来说.替换成为_</strong></p>
</li>
<li>mapred.job.id string jobID</li>
<li>mapred.tip.id string taskID</li>
<li>mapred.task.id string attemptID</li>
<li>mapred.task.partition int 作业中任务编号</li>
<li>mapred.task.is.map boolean 是否为map</li>
<li>mapred.work.output.dir / FileOutputFormat.getWorkOutputPath 当前工作目录</li>
<li><p>杂项 <strong>NOTE（dirlt）：from misc articles</strong></p>
</li>
<li><p>mapred.job.map.capacity /# 最大同时运行map数量</p>
</li>
<li>mapred.job.reduce.capacity /# 最大同时运行reduce数量</li>
<li>mapred.job.queue.name /# 选择执行queue<h3 id="1-4-7-mapreduce-">1.4.7 MapReduce的类型与格式</h3>
</li>
</ul>
<p>MapReduce的类型</p>
<p>老API里面还有MapRunner这个类，这个类主要的作用是可以用来控制Mapper运行的方法，比如可以多线程来控制Mapper的运行。 但是在新API里面已经完全集成到Mapper实现里面来了，用户可以重写两个方法来完全控制mapper的运行</p>
<ul>
<li>map 如何处理kv</li>
<li><p>run 如何从context里面读取kv
protected void map(KEYIN key, VALUEIN value,</p>
<pre><code>             Context context) throws IOException, InterruptedException {
</code></pre><p>context.write((KEYOUT) key, (VALUEOUT) value);</p>
</li>
</ul>
<p>}
public void run(Context context) throws IOException, InterruptedException {</p>
<p>  setup(context);
  while (context.nextKeyValue()) {</p>
<pre><code>map(context.getCurrentKey(), context.getCurrentValue(), context);
</code></pre><p>  }</p>
<p>  cleanup(context);
}</p>
<p><strong>NOTE（dirlt）：觉得这个特性不是特别有用</strong></p>
<ul>
<li>mapred.input.format.class setInputFormat</li>
<li>mapred.mapoutput.key.class setMapOutputKeyClass</li>
<li>mapred.mapoutput.value.class setMapOutputValueClass</li>
<li>mapred.output.key.class setOutputKeyClass</li>
<li>mapred.output.value.class setOutputValueClass</li>
<li>mapred.mapper.class setMapperClass</li>
<li>mapred.map.runner.class setMapRunnerClass</li>
<li>mapred.combiner.class setCombinerClass</li>
<li>mapred.partitioner.class setPartitionerClass</li>
<li>mapred.output.key.comparator.class setOutputKeyComparatorClass</li>
<li>mapred.output.value.groupfn.class setOutputValueGroupingComparator</li>
<li>mapred.reducer.class setReducerClass</li>
<li>mapred.output.format.class setOutputFormat</li>
</ul>
<p>输入格式</p>
<p>对于InputFormat来说包含两个任务</p>
<ul>
<li>根据job描述来对输入进行切片（InputSplit）</li>
<li><p>根据切片信息来读取记录（RecordReader）
public abstract class InputFormat<K, V> {</p>
<p>public abstract
  List<InputSplit> getSplits(JobContext context</p>
<pre><code>                         ) throws IOException, InterruptedException;
</code></pre></li>
</ul>
<p>   public abstract
    RecordReader<K,V> createRecordReader(InputSplit split,</p>
<pre><code>                                     TaskAttemptContext context
                                    ) throws IOException,

                                             InterruptedException;
</code></pre><p>}</p>
<p>public abstract class InputSplit {
  public abstract long getLength() throws IOException, InterruptedException;</p>
<p>  public abstract</p>
<pre><code>String[] getLocations() throws IOException, InterruptedException;
</code></pre><p>}</p>
<p>public abstract class RecordReader<KEYIN, VALUEIN> implements Closeable {</p>
<p>  public abstract void initialize(InputSplit split,
                                  TaskAttemptContext context</p>
<pre><code>                              ) throws IOException, InterruptedException;
</code></pre><p>  public abstract
  boolean nextKeyValue() throws IOException, InterruptedException;</p>
<p>  public abstract</p>
<p>  KEYIN getCurrentKey() throws IOException, InterruptedException;</p>
<p>  public abstract
  VALUEIN getCurrentValue() throws IOException, InterruptedException;</p>
<p>  public abstract float getProgress() throws IOException, InterruptedException;</p>
<p>  public abstract void close() throws IOException;</p>
<p>}</p>
<p>下面是一些常见的InputFormat实现</p>
<ul>
<li><p>FileInputFormat</p>
</li>
<li><p>addInputPath或者是setInputPaths修改输入路径 mapred.input.dir</p>
</li>
<li><p>setInputPathFilter可以修改过滤器 mapred.input.path.Filter.class</p>
</li>
<li><p>基本实现会排除隐藏.或者是_开头文件。</p>
</li>
<li>自定义的过滤器是建立在默认过滤器的基础上的。</li>
<li><p>分片大小由下面三个参数控制</p>
</li>
<li><p>mapred.min.split.size 1</p>
</li>
<li>mapred.max.split.size MAX</li>
<li>dfs.block.size 64MB</li>
<li>算法是max(minSplitSize,min(maxSplitSize,blockSize))</li>
<li>isSplitable可以控制输入文件是否需要分片</li>
<li>CombineFileInputFormat 可以处理多个小文件输入，抽象类需要继承实现。</li>
<li><p>TextInputFormat</p>
</li>
<li><p>输入单位是行，key是LongWritable表示行偏移，value是Text表示行内容</p>
</li>
<li><p>KeyValueTextInputFormat</p>
</li>
<li><p>输入单位是行，按照key.value.seperator.in.input.line来进行分隔默认是\t</p>
</li>
<li>key和value的格式都是Text</li>
<li><p>NLineInputFormat</p>
</li>
<li><p>和TextInputFormat非常类似，大师使用多行输入默认为1行</p>
</li>
<li>通过mapred.line.input.format.linespermap来控制行数</li>
<li><p>XML</p>
</li>
<li><p>InputFormat使用StreamInputFormat,</p>
</li>
<li>设置RecordReader使用stream.recordreader.class来设置</li>
<li>RecordReader使用org.apache.hadoop.streaming.StreamXmlRecordReader</li>
<li><strong>NOTE（dirlt）：也有现成的XmlInputFormat的实现</strong></li>
<li>SequenceFileInputFormat</li>
<li><p>SequenceFileAsTextInputFormat</p>
</li>
<li><p>将输入的kv转换成为text对象适合streaming处理方式</p>
</li>
<li>SequenceFileAsBinaryInputFormat <strong>NOTE（dirlt）：似乎没有什么用！</strong></li>
<li>MultipleInputs</li>
<li>DBInputFormat/DBOutputFormat JDBC数据库输入输出</li>
<li>TableInputFormat/TableOutputFormat HBase输入输出</li>
</ul>
<p>输出格式</p>
<ul>
<li><p>TextOutputFormat</p>
</li>
<li><p>使用mpared.textoutputformat.seperator来控制kv的分隔，默认是\t</p>
</li>
<li>对应的输入格式为KeyValueTextInputFormat</li>
<li>可以使用NullWritable来忽略输出的k或者是v</li>
<li>SequenceFileOutputFormat</li>
<li>SequenceFileAsBinaryOutpuFormat <strong>NOTE（dirlt）：似乎没有什么用！</strong></li>
<li>MapFileOutputFormat</li>
<li>MultipleOutputFormat</li>
<li><p>MultipleOutputs</p>
</li>
<li><p>如果不像生成那写part-r-00000这些空文件的话，那么可以将OutputFormat设置成为NullOutputFormat</p>
</li>
<li>但是使用NullOutputFormat的话会没有输出目录，如果想保留目录的话那么可以使用LazyOutputFormat</li>
</ul>
<h3 id="1-4-8-mapreduce-">1.4.8 MapReduce的特性</h3>
<ul>
<li><p>计数器</p>
</li>
<li><p>streaming计数器和可以通过写stderr来提交</p>
</li>
<li><p>reporter:counter:<group>,<counter>,<amount></p>
</li>
<li>reporter:status:<message></li>
<li><p>连接</p>
</li>
<li><p>map端连接</p>
</li>
<li><p>必须确保多路输入文件的reduce数量相同以及键相同。</p>
</li>
<li>使用CompositeInputFormat来运行map端连接。</li>
<li><strong>NOTE（dirlt)；不过我稍微看了一下代码，实现上其实也是针对输入文件对每条记录读取，然后进行join包括inner或者是outer。感觉场景会有限，而且效率不会太高</strong></li>
<li><p>分布式缓存</p>
</li>
<li><p>使用-files以及-archives来添加缓存文件</p>
</li>
<li><p>也可以使用DistributedAPI来完成之间事情</p>
</li>
<li><p>addCacheFile/addCacheArchive</p>
</li>
<li>然后在task里面通过configuration的getLocalCacheFiles以及getLocalCacheArchives来获得这些缓存文件</li>
<li><p>工作原理</p>
</li>
<li><p>缓存文件首先被放到hdfs上面</p>
</li>
<li><p>task需要的话那么会尝试下载，之后会对这个缓存文件进行引用计数，如果为0那么删除</p>
</li>
<li><p>这也就意味着缓存文件可能会被多次下载</p>
</li>
<li>但是运气好的话多个task在一个node上面的话那么就不用重复下载</li>
<li>缓存文件存放在${mapred.local.dir}/taskTracker/archive下面，但是通过软连接指向工作目录</li>
<li>缓存大小通过local.cache.size来配置</li>
<li><p>MapReduce库类</p>
</li>
<li><p>ChainMapper/ChainReducer 能够在一个mapper以及reducer里面运行多次mapper以及reducer</p>
</li>
<li><p>ChainMapper 允许在Map阶段，多个mapper组成一个chain,然后连续进行调用</p>
</li>
<li>ChainReducer 允许在Reuduce阶段，reducer完成之后执行一个mapper chain.</li>
<li>最终达到的效果就是 M+ -&gt; R -&gt; M/* （1个或者是多个mapper, 一个reducer，然后0个或者是多个mapper)</li>
<li><p><strong>TODO(dirlt):这样做倒是可以将各个mapper组合起来用作adapter.</strong></p>
<h3 id="1-4-9-hadoop-">1.4.9 构建Hadoop集群</h3>
</li>
<li><p>很多教程说hadoop集群需要配置ssh,但是配置这个前提是你希望使用start-all.sh这个脚本来启动集群</p>
</li>
<li><p>我现在的公司使用apt-get来安装，使用cssh来登陆到所有的节点上面进行配置，因此没有配置这个信任关系</p>
</li>
<li><p>Hadoop配置</p>
</li>
<li><p>配置文件</p>
</li>
<li><p>hadoop-env.sh 环境变量脚本</p>
</li>
<li>core-site.xml core配置，包括hdfs以及mapred的IO配置等</li>
<li>hdfs-site.xml hadoop进程配置比如namenode以及datanode以及secondary namenode</li>
<li>mapred-site.xml mapred进程配置比如jobtracker以及tasktracker</li>
<li><p>masters 运行namenode（secondary namenode)的机器列表，每行一个, <strong>无需分发到各个节点</strong></p>
</li>
<li><p><strong>在本地启动primary namenode</strong></p>
</li>
<li><p>slaves 运行datanode以及tasktracker的机器列表，每行一个 <strong>无需分发到各个节点</strong></p>
</li>
<li><p><strong>在本地启动jobtracker</strong></p>
</li>
<li>hadoop-metrics.properties 对hadoop做监控的配置文件</li>
<li>log4j.properties 日志配置文件</li>
<li>这些文件在conf目录下面有，如果想使用不同的文件也可以使用-config来另行指定</li>
<li><strong>NOTE(dirlt):所以从上面这个脚本来看，还是具有一定的局限性的</strong></li>
<li><p>hadoop-env.sh</p>
</li>
<li><p>HADOOP_HEAPSIZE = 1000MB 守护进程大小</p>
</li>
<li>HADOOP_NAMENODE_OPTS</li>
<li>HADOOP_SECONDARYNAMENODE_OPTS</li>
<li>HADOOP_IDENT_STRING 用户名称标记，默认为${USER}</li>
<li>HADOOP_LOG_DIR hadoop日志文件，默认是HADOOP_INSTALL/logs</li>
<li><p>core-site.xml</p>
</li>
<li><p>io.file.buffer.size IO操作缓冲区大小，默认是4KB <strong>这个需要提高</strong></p>
</li>
<li><p>hdfs-site.xml</p>
</li>
<li><p>fs.default.name</p>
</li>
<li>hadoop.tmp.dir hadoop临时目录，默认是在/tmp/hadoop-${user.name}</li>
<li>dfs.name.dir namenode数据目录，一系列的目录，namenode内容会同时备份在所有指定的目录中。默认为${hadoop.tmp.dir}/dfs/name</li>
<li>dfs.data.dir datanode数据目录，一系列的目录，循环将数据写在各个目录里面。默认是${hadoop.tmp.dir}/dfs/data</li>
<li>fs.checkpoint.dir secondarynamenode数据目录，一系列目录，所有目录都会写一份。默认为${hadoop.tmp.dir}/dfs/namesecondary</li>
<li>dfs.namenode.handler.count namenode上用来处理请求的线程数目</li>
<li>dfs.datanode.ipc.address 0.0.0.0:50020 datanode的RPC接口，主要和namenode交互</li>
<li>dfs.datanode.address 0.0.0.0:50010 datanode的data block传输接口，主要和client交互</li>
<li>dfs.datanode.http.address 0.0.0.0:50075 datanode的HTTP接口，和user交互</li>
<li>dfs.datanode.handler.count datanode上用来处理请求的线程数目</li>
<li>dfs.datanode.max.xcievers datanode允许最多同时打开的文件数量</li>
<li>dfs.http.address 0.0.0.0:50070 namenode的HTTP接口</li>
<li>dfs.secondary.http.address 0.0.0.0:50090 secondard namenode的HTTP接口</li>
<li>dfs.datanode.dns.interface default 绑定的NIC，默认是绑定默认的NIC比如eth0</li>
<li>dfs.hosts / dfs.hosts.exclude 加入的datanode以及排除的datanode</li>
<li>dfs.replication = 3 副本数目</li>
<li>dfs.block.size = 64MB</li>
<li>dfs.datanode.du.reserved 默认datanode会使用目录所在磁盘所有空间，这个值可以保证有多少空间被reserved的</li>
<li><p>fs.trash.interval 单位分钟，如果不为0的话，那么删除文件会移动到回收站，超过这个单位时间的文件才会完全删除。</p>
</li>
<li><p>回收站位置/home/${user]/.Trash <strong>NOTE(dirlt):回收站这个功能只是对fs shell有效。fs shell remove时候会构造Trash这个类来处理删除文件的请求。如果调用Java API的话那么会直接删除文件</strong></p>
</li>
<li>haddop fs -expunge 强制删除</li>
<li><strong>NOTE（dirlt）：grep代码发现只有NameNode在TrashEmptier里面构造了Trash这个类，因此这个配置之需要在nn上配置即可，决定多久定期删除垃圾文件</strong></li>
<li><p>fs.trash.checkpoint.interval 单位分钟，namenode多久检查一次文件是否需要删除。</p>
</li>
<li><p><strong>NOTE（dirlt）：似乎没有这个参数。如果没有这个参数的话，那么两次检查时长应该是由参数fs.trasn.interval来决定</strong></p>
</li>
<li><p>mapred-site.xml</p>
</li>
<li><p>mapred.job.tracker</p>
</li>
<li>mapred.local.dir MR中间数据存储，一系列目录，分散写到各个目录下面，默认为${hadoop.tmp.dir}/mapred/local</li>
<li>mapred.system.dir MR运行期间存储，比如存放jar或者是缓存文件等。默认${hadoop.tmp.dir}/mapred/system</li>
<li>mapred.tasktracker.map.tasks.maximum = 2 单个tasktracker最多多少map任务</li>
<li>mapred.tasktracker.reduce.tasks.maximum = 2 单个tasktracker最多多少个reduce任务</li>
<li>mapred.tasktracker.dns.interface default 绑定的NIC，默认是绑定默认的NIC比如eth0</li>
<li>mapred.child.ulimit 单个tasktracker允许子进程占用的最大内存空间。通常为2-3/* mapred.child.java.opts.</li>
<li><p>mapred.child.java.opts = -Xmx200m 每个子JVM进程200M. <strong>NOTE（dirlt）：这个是在提交机器上面设置的，而不是每个tasktracker上面设置的，每个job可以不同</strong></p>
</li>
<li><p>不一定支持将map/reduce的jvm参数分开设置 <a href="http://hadoop-common.472056.n3.nabble.com/separate-JVM-flags-for-map-and-reduce-tasks-td743351.html" target="_blank"><a href="http://hadoop-common.472056.n3.nabble.com/separate-JVM-flags-for-map-and-reduce-tasks-td743351.html">http://hadoop-common.472056.n3.nabble.com/separate-JVM-flags-for-map-and-reduce-tasks-td743351.html</a></a></p>
</li>
<li><strong>NOTE（dirlt）：个人折中思路是限制内存大小为1G，然后大内存机器允许同时执行map/reduce数量上限提高，通过增加job的map/reduce数量来提高并发增加性能</strong></li>
<li><p><strong>NOTE（dirlt）：我grep了一下cdh3u3的代码，应该是将map/reduce的jvm参数分开进行了设置</strong></p>
</li>
<li><p>mapred.map.child.java.opts</p>
</li>
<li>mapred.reduce.child.java.opts</li>
<li>mapred.task.tracker.report.address 127.0.0.1:0 tasktracker启动子进程通信的端口，0表示使用任意端口</li>
<li>mapred.task.tracker.expiry.interval 600(sec) tt和jt之间的心跳间隔</li>
<li>mapred.job.tracker.handler.count. jobtracker用来处理请求的线程数目。</li>
<li>mapred.job.tracker.http.address 0.0.0.0:50030 jobtracker的HTTP接口</li>
<li>mapred.task.tracker.http.address 0.0.0.0:50060 tasktrackder的HTTP接口</li>
<li>mapred.hosts / mapred.hosts.exclude 加入的tasktracker以及排除的tasktracker.</li>
<li><p>Hadoop Benchmark <strong>NOTE（dirlt）：try it out</strong></p>
</li>
<li><p>在hadoop安装目录下面有jar可以来做基准测试</p>
</li>
<li>TestDFSIO测试HDFS的IO性能</li>
<li>Sort测试MapReduce性能</li>
<li>MRBench多次运行一个小作业来检验小作业能否快速相应</li>
<li>NNBench测试namenode硬件的负载</li>
</ul>
<h3 id="1-4-10-hadoop">1.4.10 管理Hadoop</h3>
<ul>
<li><p>永久性数据结构</p>
</li>
<li><p>namenode的目录结构</p>
</li>
<li><p>current表示当前的namenode数据（对于辅助节点上这个数据并不是最新的）</p>
</li>
<li><p>previous.checkpoint表示secondarynamenode完成checkpoint的数据（和current可能存在一些编辑差距）</p>
</li>
<li><p>hadoop dfsadmin -saveNamespace 可以强制创建检查点,仅仅在安全模式下面运行</p>
</li>
<li><p>辅助namenode每隔5分钟会检查</p>
</li>
<li><p>如果超过fs.checkpoint.period = 3600（sec），那么会创建检查点</p>
</li>
<li>如果编辑日志大小超过fs.checkpoint.size = 64MB,同样也会创建检查点</li>
<li>除了将文件copy到namenode之外，在辅助节点上面可以使用选项-importCheckpoint来载入</li>
<li><p>VERSION Java属性文件</p>
</li>
<li><p>namespaceID 每次格式化都会重新生成一个ID，这样可以防止错误的datanode加入</p>
</li>
<li>cTime namenode存储系统创建时间，对于刚格式化的存储系统为0.对于升级的话会更新到最新的时间戳</li>
<li>storageType NAME_NODE or DATA_NODE</li>
<li>layoutVersion 负整数表示hdfs文件系统布局版本号，对于hadoop升级的话这个版本号可能不会变化</li>
<li>edits 编辑日志文件</li>
<li>fsimage 镜像文件</li>
<li>fstime ???</li>
<li><p>datanode的目录结构</p>
</li>
<li><p>blk<em><id>以及blk</em><id>.meta 表示块数据以及对应的元信息，元数据主要包括校验和等内容</p>
</li>
<li>如果datanode文件非常多的话，超过dfs.datanode.numblocks = 64的话，那么会创建一个目录单独存放，最终结果就是形成树存储结构。</li>
<li>dfs.data.dir目录是按照round-robin的算法选择的。</li>
<li><p>安全模式</p>
</li>
<li><p>namenode启动的时候会尝试合并edit数据并且新建一个checkpoint，然后进入安全模式，在这个模式内文件系统是只读的</p>
</li>
<li>可以通过hadoop dfsadmin -safemode来操作安全模式</li>
<li><p>当达到下面几个条件的时候会离开安全模式</p>
</li>
<li><p>整个系统的副本数目大于某个阈值的副本数目比率超过一个阈值之后，然后继续等待一段时间就会离开安全模式</p>
</li>
<li>dfs.replication.min = 1 副本数目阈值</li>
<li>dfs.safemode.threshold.pct = 0.999 比率阈值</li>
<li>dfs.safemode.extension = 30000(ms) 等待时间</li>
<li><p>工具</p>
</li>
<li><p>dfsadmin</p>
</li>
<li>fsck</li>
<li><p>scanner</p>
</li>
<li><p>DataBlockScanner每隔一段时间会扫描本地的data block检查是否出现校验和问题</p>
</li>
<li>时间间隔是dfs.datanode.scan.period.hours = 504默认三周</li>
<li>可以通过页面访问每个datanode的block情况 <a href="http://localhost:50075/blockScannerReport" target="_blank"><a href="http://localhost:50075/blockScannerReport">http://localhost:50075/blockScannerReport</a></a></li>
<li>加上listblocks参数可以看每个block情况 <a href="http://localhost:50075/blockScannerReport?listblocks" target="_blank"><a href="http://localhost:50075/blockScannerReport?listblocks">http://localhost:50075/blockScannerReport?listblocks</a></a> <strong>NOTE（dirlt）：可能会很大</strong></li>
<li><p>balancer</p>
</li>
<li><p>通过start-balancer.sh来启动,集群中只允许存在一个均衡器</p>
</li>
<li>均衡的标准是datanode的利用率和集群平均利用率的插值，如果超过某个阈值就会进行block movement</li>
<li>-threshold可以执行阈值，默认为10%</li>
<li>dfs.balance.bandwidthPerSec = 1024 /* 1024 用于balance的带宽上限。</li>
<li><p>监控</p>
</li>
<li><p>日志</p>
</li>
<li><p>jobtracker的stack信息（thread-dump）<a href="http://localhost:50030/stacks" target="_blank"><a href="http://localhost:50030/stacks">http://localhost:50030/stacks</a></a></p>
</li>
<li><p>度量</p>
</li>
<li><p>度量从属于特性的上下文(context),包括下面几个</p>
</li>
<li><p>dfs</p>
</li>
<li>mapred</li>
<li>rpc</li>
<li>jvm</li>
<li><p>下面是几种常见的context</p>
</li>
<li><p>FileContext 度量写到文件</p>
</li>
<li>GangliaContext 度量写到ganglia <strong>(这个似乎比较靠谱）</strong></li>
<li>CompositeContext 组合context</li>
<li>度量可以从hadoop-metrics.properties进行配置</li>
</ul>
<h3 id="1-5-benchmark">1.5 Benchmark</h3>
<ul>
<li>Benchmarking and Stress Testing an Hadoop Cluster with TeraSort, TestDFSIO &amp; Co. - Michael G. Noll <a href="http://www.michael-noll.com/blog/2011/04/09/benchmarking-and-stress-testing-an-hadoop-cluster-with-terasort-testdfsio-nnbench-mrbench/" target="_blank"><a href="http://www.michael-noll.com/blog/2011/04/09/benchmarking-and-stress-testing-an-hadoop-cluster-with-terasort-testdfsio-nnbench-mrbench/">http://www.michael-noll.com/blog/2011/04/09/benchmarking-and-stress-testing-an-hadoop-cluster-with-terasort-testdfsio-nnbench-mrbench/</a></a></li>
<li>intel-hadoop/HiBench · GitHub <a href="https://github.com/intel-hadoop/HiBench" target="_blank"><a href="https://github.com/intel-hadoop/HiBench">https://github.com/intel-hadoop/HiBench</a></a></li>
<li>HBase Performance Testing at hstack <a href="http://hstack.org/hbase-performance-testing/" target="_blank"><a href="http://hstack.org/hbase-performance-testing/">http://hstack.org/hbase-performance-testing/</a></a></li>
<li>Performance testing / Benchmarking a HBase cluster – Sujee Maniyam <a href="http://sujee.net/tech/articles/hadoop/hbase-performance-testing/" target="_blank"><a href="http://sujee.net/tech/articles/hadoop/hbase-performance-testing/">http://sujee.net/tech/articles/hadoop/hbase-performance-testing/</a></a></li>
<li>new Put(&quot;lars&quot;.toBytes(&quot;UTF-8&quot;)) : Performance testing HBase using YCSB <a href="http://blog.lars-francke.de/2010/08/16/performance-testing-hbase-using-ycsb/" target="_blank"><a href="http://blog.lars-francke.de/2010/08/16/performance-testing-hbase-using-ycsb/">http://blog.lars-francke.de/2010/08/16/performance-testing-hbase-using-ycsb/</a></a></li>
<li>Hbase/PerformanceEvaluation - Hadoop Wiki <a href="http://wiki.apache.org/hadoop/Hbase/PerformanceEvaluation" target="_blank"><a href="http://wiki.apache.org/hadoop/Hbase/PerformanceEvaluation">http://wiki.apache.org/hadoop/Hbase/PerformanceEvaluation</a></a></li>
</ul>
<h3 id="1-5-1-testdfsio">1.5.1 TestDFSIO</h3>
<p>测试hdfs吞吐
hdfs@hadoop1:~$ hadoop jar /usr/lib/hadoop/hadoop-test-0.20.2-cdh3u3.jar TestDFSIO</p>
<p>Usage: TestDFSIO [genericOptions] -read | -write | -append | -clean [-nrFiles N] [-fileSize Size[B|KB|MB|GB|TB]] [-resFile resultFileName] [-bufferSize Bytes] [-rootDir]%</p>
<ul>
<li>read / write / append / clean 操作类型 <strong>append和write执行效率差别不大，但是write会创建新文件所以使用比较方便</strong> (default read)</li>
<li>nrFiles 文件数目(default 1) <strong>启动相同数量的map</strong></li>
<li>fileSize 每个文件大小(1MB)</li>
<li>resFile 结果报告文件(TestDFSIO_results.log)</li>
<li>bufferSize write buffer size(单次write写入大小）（1000000 bytes)</li>
<li><p>rootDir 操作文件根目录（/benchmarks/TestDFSIO/）
----- TestDFSIO ----- : write</p>
<pre><code>     Date &amp; time: Thu Apr 25 19:14:21 CST 2013
 Number of files: 2
</code></pre></li>
</ul>
<p>Total MBytes processed: 2.0
     Throughput mb/sec: 7.575757575757576</p>
<p>Average IO rate mb/sec: 7.61113977432251
IO rate std deviation: 0.5189420757292891</p>
<pre><code>Test exec time sec: 14.565
</code></pre><p>----- TestDFSIO ----- : read
           Date &amp; time: Thu Apr 25 19:15:13 CST 2013</p>
<pre><code>   Number of files: 2
</code></pre><p>Total MBytes processed: 2.0</p>
<pre><code> Throughput mb/sec: 27.77777777777778
</code></pre><p>Average IO rate mb/sec: 28.125</p>
<p>IO rate std deviation: 3.125
    Test exec time sec: 14.664</p>
<ul>
<li>throughtput = sum(filesize) / sum(time)</li>
<li>avaerage io rate = sum(filesize/time) / n</li>
<li>io rate std deviation<h3 id="1-5-2-terasort">1.5.2 TeraSort</h3>
</li>
</ul>
<p>通过排序测试MR执行效率 <strong>我看了一下代码map/reduce都有CPU操作，并且这个也非常依靠shuffle/copy.因此这个测试应该会是比较全面的</strong>
hdfs@hadoop1:~$ hadoop jar /usr/lib/hadoop/hadoop-examples-0.20.2-cdh3u3.jar <command></p>
<ul>
<li><p>teragen 产生排序数据</p>
</li>
<li><number of 100-byte rows>
</li>
<li><p>10 bytes key(random characters)</p>
</li>
<li>10 bytes rowid(right justified row id as a int)</li>
<li>78 bytes filler</li>
<li>\r\n</li>
<li><output dir></li>
<li><p>terasort 对数据排序</p>
</li>
<li><input dir></li>
<li><output dir></li>
<li>teravalidate 对排序数据做验证</li>
</ul>
<p>可以使用hadoop job -history all <job-output-dir>来观察程序运行数据，也可以通过web page来分析。</p>
<h3 id="1-5-3-nnbench">1.5.3 nnbench</h3>
<p>测试nn负载能力
➜  ~HADOOP_HOME  hadoop jar hadoop-test-0.20.2-cdh3u3.jar nnbench</p>
<p>NameNode Benchmark 0.4
Usage: nnbench <options></p>
<p>Options:
        -operation <Available operations are create_write open_read rename delete. This option is mandatory></p>
<pre><code>     /* NOTE: The open_read, rename and delete operations assume that the files they operate on, are already available. The create_write operation must be run before running the other operations.
    -maps &lt;number of maps. default is 1. This is not mandatory&gt;

    -reduces &lt;number of reduces. default is 1. This is not mandatory&gt;
    -startTime &lt;time to start, given in seconds from the epoch. Make sure this is far enough into the future, so all maps (operations) will start at the same time&gt;. default is launch time + 2 mins. This is not mandatory

    -blockSize &lt;Block size in bytes. default is 1. This is not mandatory&gt;
    -bytesToWrite &lt;Bytes to write. default is 0. This is not mandatory&gt;

    -bytesPerChecksum &lt;Bytes per checksum for the files. default is 1. This is not mandatory&gt;
    -numberOfFiles &lt;number of files to create. default is 1. This is not mandatory&gt;

    -replicationFactorPerFile &lt;Replication factor for the files. default is 1. This is not mandatory&gt;
    -baseDir &lt;base DFS path. default is /becnhmarks/NNBench. This is not mandatory&gt;

    -readFileAfterOpen &lt;true or false. if true, it reads the file and reports the average time to read. This is valid with the open_read operation. default is false. This is not mandatory&gt;
    -help: Display the help statement
</code></pre><ul>
<li>startTime 作用是为了能够让所有的map同时启动以便对nn造成压力
➜  ~HADOOP_HOME  hadoop jar hadoop-test-0.20.2-cdh3u3.jar nnbench -operation create_write -bytesToWrite 0 -numberOfFiles 1200</li>
</ul>
<p>➜  ~HADOOP_HOME  hadoop jar hadoop-test-0.20.2-cdh3u3.jar nnbench -operation open_read</p>
<p>结果报告文件是 NNBench_results.log</p>
<p>-------------- NNBench -------------- :</p>
<pre><code>                           Version: NameNode Benchmark 0.4
                       Date &amp; time: 2013-04-25 19:41:02,873


                    Test Operation: create_write

                        Start time: 2013-04-25 19:40:21,70
                       Maps to run: 1

                    Reduces to run: 1
                Block Size (bytes): 1

                    Bytes to write: 0
                Bytes per checksum: 1

                   Number of files: 1200
                Replication factor: 1

        Successful file operations: 1200


    /# maps that missed the barrier: 0
                      /# exceptions: 0


           TPS: Create/Write/Close: 75
</code></pre><p>Avg exec time (ms): Create/Write/Close: 26.526666666666667
            Avg Lat (ms): Create/Write: 13.236666666666666</p>
<pre><code>               Avg Lat (ms): Close: 13.164166666666667


             RAW DATA: AL Total /#1: 15884
             RAW DATA: AL Total /#2: 15797

          RAW DATA: TPS Total (ms): 31832
   RAW DATA: Longest Map Time (ms): 31832.0

               RAW DATA: Late maps: 0
         RAW DATA: /# of exceptions: 0
</code></pre><p>-------------- NNBench -------------- :</p>
<pre><code>                           Version: NameNode Benchmark 0.4
                       Date &amp; time: 2013-04-25 19:44:42,354


                    Test Operation: open_read

                        Start time: 2013-04-25 19:44:31,921
                       Maps to run: 1

                    Reduces to run: 1
                Block Size (bytes): 1

                    Bytes to write: 0
                Bytes per checksum: 1

                   Number of files: 1
                Replication factor: 1

        Successful file operations: 1


    /# maps that missed the barrier: 0
                      /# exceptions: 0


                    TPS: Open/Read: 500

     Avg Exec time (ms): Open/Read: 2.0
                Avg Lat (ms): Open: 2.0


             RAW DATA: AL Total /#1: 2

             RAW DATA: AL Total /#2: 0
          RAW DATA: TPS Total (ms): 2

   RAW DATA: Longest Map Time (ms): 2.0
               RAW DATA: Late maps: 0

         RAW DATA: /# of exceptions: 0
</code></pre><ul>
<li>maps that missed the barrier 从代码上分析是，在等待到start time期间中,如果sleep出现异常的话。</li>
<li>exceptions 表示在操作文件系统时候的exception数量</li>
<li>TPS transactions per second</li>
<li>exec（execution） 执行时间</li>
<li>lat（latency） 延迟时间</li>
<li>late maps 和 maps missed the barrier是一个概念。</li>
</ul>
<p>对于后面RAW DATA部分的话，从代码上看，就是为了计算出上面那些指标的，所以没有必要关注。</p>
<h3 id="1-5-4-mrbench">1.5.4 mrbench</h3>
<p>测试运行small mr jobs执行效率，主要关注响应时间。
MRBenchmark.0.0.2</p>
<p>Usage: mrbench [-baseDir <base DFS path for output/input, default is /benchmarks/MRBench>] [-jar <local path to job jar file containing Mapper and Reducer implementations, default is current jar file>] [-numRuns <number of times to run the job, default is 1>] [-maps <number of maps for each run, default is 2>] [-reduces <number of reduces for each run, default is 1>] [-inputLines <number of input lines to generate, default is 1>] [-inputType <type of input to generate, one of ascending (default), descending, random>] [-verbose]</p>
<ul>
<li>baseDir 输入输出目录</li>
<li>jar 通常不需要指定，用默认即可。</li>
<li>inputLines 输入条数</li>
<li>inputType 输入是否有序
hdfs@hadoop1:~$ hadoop jar /usr/lib/hadoop/hadoop-test-0.20.2-cdh3u3.jar mrbench -verbose</li>
</ul>
<p>结果直接输出在终端上面，</p>
<p>Total MapReduce jobs executed: 1</p>
<p>Total lines of data per job: 1
Maps per job: 2</p>
<p>Reduces per job: 1
Total milliseconds for task: 1 = 16452</p>
<p>DataLines       Maps    Reduces AvgTime (milliseconds)
1               2       1       16452</p>
<p>可以看到每个任务平均执行时间在16.452s.</p>
<h3 id="1-5-5-hbase-performanceevaluation">1.5.5 hbase.PerformanceEvaluation</h3>
<p>hdfs@hadoop1:~$ hbase org.apache.hadoop.hbase.PerformanceEvaluation</p>
<p>Usage: java org.apache.hadoop.hbase.PerformanceEvaluation \
  [--miniCluster] [--nomapred] [--rows=ROWS] <command> <nclients></p>
<p>Options:</p>
<p> miniCluster     Run the test on an HBaseMiniCluster
 nomapred        Run multiple clients using threads (rather than use mapreduce)</p>
<p> rows            Rows each client runs. Default: One million
 flushCommits    Used to determine if the test should flush the table.  Default: false</p>
<p> writeToWAL      Set writeToWAL on puts. Default: True</p>
<p>Command:
 filterScan      Run scan test using a filter to find a specific row based on it&#39;s value (make sure to use --rows=20)</p>
<p> randomRead      Run random read test
 randomSeekScan  Run random seek and scan 100 test</p>
<p> randomWrite     Run random write test
 scan            Run scan test (read every row)</p>
<p> scanRange10     Run random seek scan with both start and stop row (max 10 rows)
 scanRange100    Run random seek scan with both start and stop row (max 100 rows)</p>
<p> scanRange1000   Run random seek scan with both start and stop row (max 1000 rows)
 scanRange10000  Run random seek scan with both start and stop row (max 10000 rows)</p>
<p> sequentialRead  Run sequential read test
sequentialWrite Run sequential write test</p>
<p>Args:</p>
<p> nclients        Integer. Required. Total number of clients (and HRegionServers)
                 running: 1 &lt;= value &lt;= 500</p>
<p>Examples:
To run a single evaluation client:</p>
<p>$ bin/hbase org.apache.hadoop.hbase.PerformanceEvaluation sequentialWrite 1</p>
<p>从参数上看还是比较直接的。benchmark每个client通常对应10个mapper, 每个client操作<rows>个row,因此每个mapper操作<rows>/10个row,每个row大约1000bytes.</p>
<ul>
<li>filterScan 随机生成value，然后从头开始scan直到equal</li>
<li>randomRead 随机选取key读取</li>
<li>randomSeekScan 从某个随机位置开始scan最多100个</li>
<li>randomWrite 随即生成key写入</li>
<li>scan 每次scan 1个row，start随机</li>
<li>scan<num> 每次scan num个row，start随机</li>
<li>seqRead 顺序地读取每个key</li>
<li>seqWrite 顺序地写入每个key</li>
<li><strong>NOTE(dirlt):这里的key都非常简单，10个字符的数字，printf(&quot;%010d&quot;,row)</strong>
hdfs@hadoop1:~$ time hbase org.apache.hadoop.hbase.PerformanceEvaluation --rows=1000 sequentialWrite 2</li>
</ul>
<p>13/04/25 23:47:56 INFO mapred.JobClient:   HBase Performance Evaluation
13/04/25 23:47:56 INFO mapred.JobClient:     Row count=2000</p>
<p>13/04/25 23:47:56 INFO mapred.JobClient:     Elapsed time in milliseconds=258</p>
<p>输出结果是在counter里面，这里面row count = 2000, 占用时间为258 ms.
Date: 2013-12-15T10:28+0800</p>
<p><a href="http://orgmode.org/" target="_blank">Org</a> version 7.9.2 with <a href="http://www.gnu.org/software/emacs/" target="_blank">Emacs</a> version 24
<a href="http://validator.w3.org/check?uri=referer" target="_blank">Validate XHTML 1.0</a>
来源： <a href="[http://dirlt.com/hadoop.html/#sec-1-1-4](http://dirlt.com/hadoop.html#sec-1-1-4)">[http://dirlt.com/hadoop.html/#sec-1-1-4](http://dirlt.com/hadoop.html#sec-1-1-4)</a> </p>

      
    </div>
    
    
      
    
    
        <footer id="post-meta">
        <span class="categories">Posted in<span class="breadcrumb fa fa-folder"><li><a href="/categories/hadoop/">hadoop</a></li></span></span> | <span class="tags">Tagged <a href="/tags/hadoop/" class="label label-primary">hadoop</a></span> | <span class="time">recent updated:<time title="2014-03-07 09:54:36"datetime="2014-03-07 09:54:36"> mar. 7 2014</time></span> | <span class="comment-link">
<a href="http://itsolife.com/2014/02/02/2014-02-02-hadoop--hadoop/#comments" class="ds-thread-count comment-link" data-thread-key="2014-02-02-hadoop--hadoop" data-count-type="comments">暂无评论</a></span>
        </footer>
    
    <div class="clearfix"></div>
  </div>
</div>
</article>



  <article>
<div id="post" class="post well">
  <div class="post-content">
    <header class="well-sm">
      <i class="fa icon fa-5x pull-left"></i>
      <h1 class="title"><a href="/2014/02/02/2014-02-02-hadoop--OpenStack_Hadoop/">OpenStack_Hadoop</a></h1>
      
        <span>Posted on<time datetime="2014-02-02T01:54:36.000Z"> <a href="/2014/02/02/2014-02-02-hadoop--OpenStack_Hadoop/">feb. 2 2014</a></time></span>      
    </header>
    
    
    <div class="entry">
      
        <h1 id="openstack_hadoop">OpenStack_Hadoop</h1>
<p>针对OpenStack、Hadoop种不同领域软件的分析</p>
<p>1 <img src="" alt=""> OpenStack</p>
<p>1.1 简介</p>
<p>一款管理分布在多台物理机器上的多台虚拟机的开源虚拟机管理软件。</p>
<p>虚拟化是指在同一台物理机器上提供多台虚拟机器的技术。</p>
<p>其可以在多台计算机（PC或者小型机）组成的网络集群上不跨物理机器的前提下自由调配单机资源以虚拟化成一台或者多台个人PC机器、局域网供用户使用和服务。</p>
<p>优点：可自由调配、定制以几乎单机全部的有限资源合理应对模拟几乎无限种可能的低运算量业务处理环境。</p>
<p>缺点：当全部资源都被使用后，实际资源有效利用率低（考虑多个虚拟机重复的操作系统环境、分布式虚拟机镜像文件存储的冗余备份、虚拟机技术本身的VM运行机制）。</p>
<p>其更适用于办公、开发、测试环境使用。</p>
<p>1.2 网文：虚拟化的误区</p>
<p>服务器虚拟化技术之十大误区</p>
<p>2008-09-08 10:20 摘自<a href="http://datacenter.chinabyte.com/280/8297280.shtml" target="_blank">http://datacenter.chinabyte.com/280/8297280.shtml</a></p>
<p>[导读]服务器虚拟化技术之十大误区</p>
<p>误区1：虚拟化技术可以实现多台物理服务器资源整合，从而实现单个应用通过虚拟化技术而运行在多台物理硬件上</p>
<p>实际上，虚拟化技术不能将一个应用分布运行在多台物理硬件上，那是分布式计算要去解决的问题。分布式计算环境和虚拟化环境是两种不同的资源整合方式。当然，如果想通过虚拟化技术实现一个应用跨物理平台运行技术上来说是可行的，只是为了解决不同硬件之间的CPU和内存级指令、数据的同步，需要使用一些特别的技术，比如Infiniband等，这会极大地增加系统的复杂性和成本。实际上，基于这种理念的虚拟化产品曾在实验室实现，但是由于成本等因素无法投入市场。今天能看到的所有服务器虚拟化技术解决方案都不提供一个应用跨物理服务器运行，也就是说，虚拟化环境下一个应用能使用的最大资源就是一台独立的物理服务器。</p>
<p>误区2：服务器虚拟化技术就会陷入将多个鸡蛋放到一个篮子的尴尬</p>
<p>通过虚拟化技术，提高了服务器的利用效率和灵活性。但同时也使得单台服务器上运行了多个独立的虚拟机，也就是多个不同的应用。我们原来在一台服务器上只运行一个应用，服务器维护和升级时只会影响单个应用。通过运行虚拟化技术，我们在维护和升级服务器时会影响该服务器上运行的所有虚拟机和应用。这导致很多人认为的问题：多个虚拟机放置在一台服务器上的“鸡蛋和篮子”问题。</p>
<p>实际上，VMware很早就意识到了这个问题，这个问题可以通过两个方面的能力去解决。一是怎么保证虚拟化后的服务器物理硬件维护和升级的问题。二是物理服务器故障时如何保护这些虚拟机的安全。</p>
<p>首先，VMware创造性的发明了VMotion的技术，解决了虚拟化后物理服务器的升级和维护问题。通过VMotion，VMware可以在服务器需要维护升级时动态将虚拟机迁移到其他的物理服务器，通过内存复制技术，确保每台虚拟机任何对外的服务都不发生中断，从而实现了：停物理硬件、不停应用。下图是VMotion的具体实现，已经有超过50%的VMware客户部署了VMotion技术。</p>
<p><img src="" alt=""></p>
<p>其次，VMware推出了VMware HA的功能来保护物理服务器的安全。一旦发生物理服务器故障，VMware HA可以智能检测到这一事件，及时快速地在其他物理服务器上重新启用这些虚拟机，从而保证虚拟机的安全性和可靠性。</p>
<p><img src="" alt=""></p>
<p>误区3：动态在线虚拟机迁移可以跨越任何硬件进行</p>
<p>目前VMware在业界推出了标志性的创新产品功能VMotion，可以实现虚拟机动态在线跨越硬件服务器进行迁移。但是这是有一个兼容前提，也就是两台物理服务器要达到CPU指令级的兼容，或者是完全一样的CPU，或者是同一家族的CPU。如果CPU指令不兼容，进行内存复制后新机器CPU不能识别这些指令就会导致系统崩溃。当然，具体CPU指令级是否兼容，VMotion会自动进行判定。</p>
<p>当然，如果您可以离线进行虚拟机的迁移，就可以跨越任何ESX兼容的硬件进行迁移，就没有CPU型号等的制约。</p>
<p>误区4：数据中心虚拟化后可以节约虚拟机里运行软件许可证的成本</p>
<p>虚拟化技术并未改变软件许可证的发放方式，因此虚拟化技术并不意味着操作系统或应用软件许可证成本的节约，除非操作系统、应用软件厂商重新调整了软件许可证策略。因此，想通过使用虚拟化技术来减少应用软件许可成本的想法是错误的。当然，实施虚拟化技术也不会增加操作系统或应用软件的许可证成本。</p>
<p>误区5：数据中心虚拟化只使用于边缘应用，对关键应用或资源消耗较大的应用目前还不能虚拟化</p>
<p>PC服务器的虚拟化技术已经相当成熟，在美国和欧洲已经获得了广泛应用。实际上，很多关键的业务应用已经运行在虚拟化的平台上。对于资源消耗比较高的应用，需要进行合理的规划才能迁移到虚拟化上来，即使某个机器的资源消耗特别巨大，仍然可以通过升级服务器的内存、CPU来使它顺利迁移到高端PC服务器上来。当然，某个虚拟机能够支持的最大资源仍然是有限制的，比如运行在VMware的ESX Server 3.0上的虚拟机，最多可以支持16GB内存和4颗虚拟CPU。如果这些资源仍然无法满足某个应用的需求，该应用还是不能运行在虚拟化的平台上。基于一般考虑，大多数资源消耗较大的应用仍然能够安全运行到虚拟化平台上。</p>
<p>误区6：英特尔和AMD都开始在CPU级支持虚拟化技术，已不需要再购买虚拟化软件了</p>
<p>CPU的厂商英特尔和AMD都在推行基于CPU的虚拟化，实际上CPU级的虚拟化就是在CPU指令级增加了许多虚拟化的指令而已，这并非说用户可以不需要购买虚拟化软件了，CPU级的虚拟化需要虚拟化软件才能使用起来。目前所有的常用操作系统都不支持CPU级的虚拟化。而VMware提供的虚拟化平台正是通过利用英特尔和AMD提供的CPU指令的虚拟化，进而提高了虚拟化的效率，有效提高了虚拟机的性能，降低了虚拟化带来的损耗，大大加速数据中心虚拟化的进程。所以说，CPU的虚拟化是对服务器虚拟化的极大推动，而不是限制VMware这样的虚拟化产品的推广。</p>
<p>误区7：数据中心虚拟化会极大地降低服务器的性能</p>
<p>虚拟化有两种基本架构：寄居架构和裸金属架构，两种架构如下图所示。寄居架构由于基于传统的操作系统之上，所以性能消耗大，往往会对服务器性能影响很大。而裸金属架构基于专门为虚拟化而设计的虚拟化层而实现，大大降低了虚拟化引入的损耗，可以极大改善虚拟机的性能，是企业级数据中心进行虚拟化的首选架构。</p>
<p>因此，对用户来说，为了满足应用对性能的追求，建议采用企业级虚拟化架构――裸金属架构，这可以尽可能降低数据中心虚拟化对服务器性能的影响，一般影响可以降到10%以下。</p>
<p><img src="" alt=""></p>
<p>下图是采用裸金属架构虚拟化对应用性能的影响情况，这是VMware在中国某个用户现场的实测结果，已经很好说明了虚拟化带来的消耗是很低的。</p>
<p><img src="" alt=""></p>
<p>误区8：虚拟化技术仍然不成熟，数据中心虚拟化还不能提上议事日程</p>
<p>虚拟化技术已经获得了广泛地应用，财富100强的所有用户都已经部署了VMware的虚拟化解决方案，财富1000强中超过800家都是VMware的用户。实际上，VMware的企业级用户数量已经超过20000家，而所有用户的数量已经超过四百万家。VMware的服务器虚拟化方案已经久经考验，成为整个IT业界津津乐道的热点，虚拟化已经成为企业级用户构建新型数据中心的利器，成为值得信赖的可靠、稳定的企业级解决方案。</p>
<p>误区9：虚拟化由于引入了新的层次，会增加数据中心的管理难度</p>
<p>在数据中心引入虚拟化确实增加了一个虚拟化层，但并非因此而增加了管理难度。由于虚拟化的管理软件能够很好的管理控制虚拟平台的同时，简化了杂乱的服务器的管理，从而大大降低了大型数据中心的管理复杂性。如VMware  VirtualCenter就是很好的例证，Virtual Center提供了直观的管理界面，提供了丰富的资料和数据来监控整合虚拟化中心，为数据中心高效管理提供了强大的手段，成为新型虚拟化数据中心的必备工具。下图是Virtual center对虚拟机的管理界面。</p>
<p><img src="" alt=""></p>
<p>误区10：服务器虚拟化技术很美好，从原来架构迁移到虚拟架构耗时费力，而且可能风险巨大</p>
<p>如果迁移到虚拟化平台是很多用户的顾虑之一，因为虚拟化是一种架构决策。VMware已经进行了大量工作来简化从物理架构向虚拟架构的迁移，VMware Converter可以让用户不需要重新安装操作系统和应用，通过打包方式，将原来的物理服务器轻松迁移到虚拟平台上来。这不仅简化了流程，也降低了整个的迁移风险，目前很多企业级的用户都在享受VMware Converter所带来的好处。下图是VMware Converter的一个操作主界面，用户可以从VMware的网站免费下载VMware Converter的试用版来进行迁移试验。</p>
<p>1.3 网文：同类系统及其原理</p>
<p>最近笼统地学习和试用了几款比较有名的虚拟化管理软件。学习的内容包括Eucalyptus,  OpenNebula, OpenStack,  OpenQRM, XenServer, Oracle VM, CloudStack, ConVirt。借这一系列文章，对过去一个月的学习内容作一个阶段性的总结。</p>
<p>摘自<a href="http://zhumeng8337797.blog.163.com/blog/static/1007689142011112035330566/" target="_blank"><a href="http://zhumeng8337797.blog.163.com/blog/static/1007689142011112035330566/">http://zhumeng8337797.blog.163.com/blog/static/1007689142011112035330566/</a></a></p>
<p>（1）授权协议、许可证管理、购买价格等方面的比较</p>
<p><strong>授权协议</strong></p>
<p><strong>许可证管理</strong></p>
<p><strong>商业模式</strong> <strong>Eucalyptus</strong></p>
<p>社区版采用GPLv3授权协议</p>
<p>企业版使用自定义的商业授权协议</p>
<p>社区版不需要安装许可证</p>
<p>企业版需要在云控制器（CLC）节点上安装许可证</p>
<p>社区版免费使用</p>
<p>企业版按处理器核心总数收费，用户购买的许可证针对特定版本永久有效。 <strong>OpenStack</strong></p>
<p>Apache 2.0授权协议</p>
<p>不需要许可证</p>
<p>免费使用 <strong>OpenNebula</strong></p>
<p>Apache 2.0授权协议</p>
<p>不需要许可证</p>
<p>社区版免费使用</p>
<p>企业版将社区版重新打包，提供补丁等程序的访问权限，使得用户能够更容易的安装、配置和管理，以订阅的模式提供服务。</p>
<p>企业版按物理服务器总数收费，每台物理服务器器的服务价格为250欧元每年。 <strong>OpenQRM</strong></p>
<p>社区版使用GPLv2授权协议</p>
<p>企业版使用自定义的商业授权协议</p>
<p>不需要许可证</p>
<p>社区版免费使用</p>
<p>企业版将社区版重新打包，提供补丁等程序的访问权限，使得用户能够更容易的安装、配置和管理，以订阅的模式提供服务。基本、标准和高级服务的价格分别为480、960、1920欧元每月。 <strong>XenServer</strong></p>
<p>Citrix  XenServer系列产品均使用自定义的商业授权协议</p>
<p>基于XenServer的Xen Cloud  Platform使用GPLv2授权协议</p>
<p>不管是XenServer还是Xen Cloud  Platform都需要在每台服务器安装许可证</p>
<p>许可证每年更新一次</p>
<p>XenServer免费版本和开源版本的Xen Cloud Platform可以免费使用</p>
<p>XenServer高级版、企业版和白金版按物理服务器数量收费，分别是1000、2500和5000美元。购买的许可证针对特定版本永久有效 <strong>Oracle VM</strong></p>
<p>Oracle VM  Server是基于Xen开发的，使用GPLv2协议发布，从Oracle的网站可以下载到源代码，但是Oracle并不宣传这一点。</p>
<p>Oracle VM  Manager使用自定义的商业授权协议。</p>
<p>Oracle VM  VirtualBox的二进制版本使用自定义的商业授权协议，源代码使用GPLv2授权协议。</p>
<p>不需要许可证</p>
<p>免费使用，可以购买技术支持。技术支持的费用为每台物理服务器8184人民币每年。 <strong>CloudStack</strong></p>
<p>社区版采用GPLv3授权协议企业版使用自定义的商业授权协议</p>
<p>社区版不需要安装许可证</p>
<p>企业版需要在管理服务器上安装许可证</p>
<p>社区版免费使用企业版提供增强功能和技术支持，收费模式不详。 <strong>ConVirt</strong></p>
<p>社区版使用GPLv2授权协议</p>
<p>企业版使用自定义的商业授权协议</p>
<p>社区版不需要安装许可证</p>
<p>企业版需要在管理服务器上安装许可证</p>
<p>社区版免费使用</p>
<p>企业版提供增强功能和技术支持，按物理服务器数量收费，每个节点费用1090美元。购买的许可证针对特定版本永久有效。</p>
<p>（2）项目历史与运营团队、社区规模和活跃程度、沟通交流等方面的比较</p>
<p>   项目历史与运营团队社区规模和活跃程度沟通交流</p>
<p><strong>项目历史与运营团队</strong></p>
<p><strong>社区规模和活跃程度</strong></p>
<p><strong>沟通交流</strong> <strong>Eucalyptus</strong></p>
<p>最 初是UCSB的HPC研究项目，2009年初成立公司来支持该项目的商业化运营。现任CEO是曾担任MySQL CEO的Marten Mickos，现任工程部门SVP的Tim Cramerc曾担任  Sun公司NetBeans和OpenSolaris项目的执行总监。整个管理团队对开放源代码项目的管理和运营方面具有丰富的经验。</p>
<p>在 同类开放源代码项目当中，Eucalyptus的社区规模最大，活跃程度也最高。主要原因是该项目起源于大学研究项目，次要原因是管理团队对开放源代码理 念的高度认同。Ubuntu 10.04服务器版选择Eucalyptus作为UEC的基础构架，大大地促进了Eucalyptu的推广。</p>
<p>社区发表在论坛上的问题通常在48小时内得到回应，通过技术支持电子邮件提出的问题通常在24小时内得到回应。</p>
<p>Eucalyptus在北京和深圳设有办事处，在中国有工程师提供支持团队。 <strong>OpenStack</strong></p>
<p>OpenStack 是服务器托管公司RackSpace与NASA共同发起的开放源代码项目。在开放源代码项目的管理和运营方面，RackSpace和NASA显然缺乏足够  的经验。针对OpenStack项目的批评集中在（1）RackSpace对项目有过于强烈的控制欲，（2）OpenStack项目的运作对于社区成员来 说基本上是不透明的，（3）OpenStack项目对同类开放源代码项目的攻击性过強。</p>
<p>社 区规模较小，主要参与者为支持／参与该项目的公司人员。有几个公开的邮件列表，流量很小。由于该项目比较新，在网络上可以参考的安装与配置方面的文章不 多。Ubuntu 11.04服务器版同时支持Eucalyptus和OpenStack作为UEC的基础构架，将有助于OpenStack的推广。</p>
<p>通过邮件列表进行技术方面的沟通，通常在48小时内得到回应。商务方面的邮件沟通，没有得到回应。 <strong>OpenNebula</strong></p>
<p>2005年启动的研究性项目，2008年初发布第一个开放源代码版本，2010年初大力推进开源社区的建设。</p>
<p>社区规模较小，主要参与者为支持／参与该项目的公司人员，以及少量的用户。有几个公开的邮件列表，流量比OpenStack项目的流量稍大。在网络上搜索到一些中文版安装和配置方面的文章，基本上是以讹传讹，缺乏可操作性。英文版的相关文章也不多，可操作的更少。</p>
<p>通过邮件列表进行技术方面的沟通，通常在48小时内得到回应。 <strong>OpenQRM</strong></p>
<p>起源于集群管理方面的软件，2006年公开源代码，2008年免费发布，目前版本为4.8。</p>
<p>项目的运营团队较小，似乎只有Matt  Rechenburg一个人。</p>
<p>有一些零星的用户，基本上没有形成社区。虽然功能还在不断更新，但是用户文档的日期是2008年的。相关论坛的活跃程度比OpenStack和OpenNebula更差。</p>
<p>在论坛发布的问题，大约有50％左右没有得到回应。通过电子邮件进行商务沟通，反应迅速，在24小时以内得到回应。 <strong>XenServer</strong></p>
<p>Citrix公司的产品，与Xen项目的发展基本同步。</p>
<p>围绕Xen Cloud Platform有一些开放源代码的项目，用于替代XenCentor提供基于桌面或者是浏览器的管理功能。</p>
<p>初期商务沟通的速度比较快。 <strong>Oracle  VM</strong></p>
<p>Oracle公司的产品，用户量较小。Oracle  VM仅仅是Oracle用户生态系统中的一部分，不是Oracle的关键业务。</p>
<p>有一定数量的用户，但是没有形成社区。在网络上缺少与Oracle相关的讨论与交流。Oracle VM团队有一个博客网站，但是最近两篇文章的日期分别是2010年11月和2008年1 月。产品下载的速度很慢。</p>
<p>初期商务沟通的速度比较快。在技术方面的沟通，Oracle在国内没有相应的技术人员提供支持。 <strong>CloudStack</strong></p>
<p>源于2008年成立的VMOps公司，2010年五月启用cloud.com域名，2010年6 月共同启动OpenStack项目。</p>
<p>用户数量较少，论坛不是很活跃。官方文档非常完备，按照文档操作至少能够顺利地完成安装和配置过程。网络上可以搜索到一些可操作的安装和配置文档（得益于CloudStack的安装和配置比较简单）。</p>
<p>商务沟通比较困难，通过社区论坛和电子邮件提出的问题都没有得到回应。 <strong>ConVirt</strong></p>
<p>起源于2006年发起的XenMan项目，与Xen项目的发展基本同步。目前的版本为ConVirt 2.0。现任CEO和工程部门EVP均来自Oracle。</p>
<p>用户规模与Eucalyptus相当，论坛的活跃程度很高。官方文档非常完备，按照文档操作至少能够顺利地完成安装和配置过程。在网络上搜索到的中英文的安装配置教程也基本可用。</p>
<p>商务沟通非常顺畅，社区发表在论坛上的问题通常在48小时内得到回应，通过技术支持电子邮件提出的问题通常在24小时内得到回应。</p>
<p>（3）综合评估</p>
<p>总 的来说，虚拟化管理软件的用户还不是很多。大部分虚拟化管理软件的社区规模较小，活跃程度也不高。除了Eucalyptus积极地鼓励社区用户参与项目的 开发与测试之外，其他项目选择开放源代码只是一种营销策略。如果排除技术和价格方面的因素，最值得选择的软件无疑是Eucalyptus和  ConVirt。这两个项目拥有最大和最活跃的用户社区，其开发／运营团队与潜在客户之间的沟通最为顺畅。XenServer也是一个值得考虑的对象，但 是XenServer社区版要求对每台物理服务器都要每年更新一次许可证。对于拥有大量物理服务器的公司来说，管理和维护成千上百个许可证将是一个令人头 疼的问题。</p>
<p>架构篇：</p>
<p>（1）系统构架比较</p>
<p><strong>系统构架</strong> <strong>Eucalyptus</strong></p>
<p>Eucalyptus 是一个与Amazon EC2兼容的IaaS系统。Eucalyptus包括云控制器（CLC）、Walrus、集群控制器（CC）、存储控制器（SC）和节点控制器（NC）。 CLC是整个Eucalyptu系统的核心，负责高层次的资源调度，例如向CC请求计算资源。Walrus是 一个与Amazon S3类似的存储服务，主要用于存储虚拟机映像和用户数据。CC是一个集群的前端，负责协调一个集群内的计算资源，并且管理集群内的网络流量。SC是一个与 Amazon EBS类似的存储块设备服务，可以用来存储业务数据。NC是最终的计算节点，通过调用操作系统层的虚拟化技术来启动和关闭虚拟机。在同一个集群（CC）内 的所有计算节点（NC）必须在同一个子网内。 在一个集群（CC）内通常需要部署一台存储服务器（SC），为该集群内的计算节点提供数据存储服务。</p>
<p>Eucalyptus 通过Agent的方式来管理计算资源。在每一个计算节点上，都需要运行一个eucalyptus-nc的服务。该服务在集群控制器（CC）上注册后，云控 制器（CLC）即可通过集群控制器（CLC）将需要运行的虚拟机映像文件（EMI）拷贝到该计算节点上运行。</p>
<p>Eucalyptus 将虚拟机映像文件存储在Walrus上。当用户启动一个虚拟机实例的时候，Eucalyptus首先将相应的虚拟机映像（EMI）从Walrus拷贝到将 要运行该实例的计算节点（NC）上。当用户关闭（或者是由于意外而重启）一个虚拟机实例的时候，对虚拟机所做的修改并不会被写回到Walrus上原来的虚 拟机映像（EMI）上，所有对该虚拟机的修改都会丢失。如果用户需要保存修改过的虚拟机，就需要利用工具（euca2ools）将该虚拟机实例保存为新的  虚拟机映像（EMI）。如果用户需要保存数据，则需要利用存储服务器（SC）所提供的弹性块设备来完成。</p>
<p><a href="http://www.qyjohn.net/wp-content/uploads/2011/05/eee_arch.jpg" target="_blank"><img src="" alt=""></a> <strong>OpenStack</strong></p>
<p>OpenStack是一个与Amazon EC2兼容的IaaS系统。OpenStack包括OpenStack  Compute和OpenStack Object Storage两个部分。</p>
<p>OpenStack Compute又包含Web前端、计算服务、存储服务、身份认证服务、存储块设备（卷）服务、网络服务、任务调度等多个模块。OpenStack Compute的不同模块之间不共享任何信息，通过消息传递进行通讯。因此，不同的模块可以运行在不同的服务器上，也可以运行在同一台服务器上。
<a href="http://www.qyjohn.net/wp-content/uploads/2011/05/NOVA_ARCH.png" target="_blank"><img src="" alt=""></a></p>
<p>OpenStack Object Store可以利用通用服务器搭建可扩展的海量数据仓库，并且通过冗余来保证数据的安全性。同一份数据的在多台服务器上都有副本，将出现故障的服务器从集 群中撤除不会影响数据的完整性，加入新的服务器后系统会自动地在新的服务器上为相应的文件创建新的副本。从功能上讲，OpenStack Object Store同时具备Eucalyptus中的Walrus服务和弹性块设备（SC）服务。不过OpenStack Object Store不是一个文件系统，不能够保证数据的实时性。从这个方面来考虑，OpenStack Object Store更适合用于存储需要长期保存的静态数据，例如操作系统映像文件和多媒体数据。</p>
<p><a href="http://www.qyjohn.net/wp-content/uploads/2011/05/os-os.png" target="_blank"><img src="" alt=""></a></p>
<p>OpenStack通过Agent的方式来管理计算资源。在每一个计算节点上，都需要运行nova- network服务和nova-compute服务。这些服务启动之后，就可以通过消息队列来与云控制器进行交互。</p>
<p>  <strong>OpenNebula</strong></p>
<p>OpenNebula 的构架包括三个部分：驱动层、核心层、工具层。驱动层直接与操作系统打交道，负责虚拟机的创建、启动和关闭，为虚拟机分配存储，监控物理机和虚拟机的运行 状况。核心层负责对虚拟机、存储设备、虚拟网络等进行管理。工具层通过命令行界面／浏览器界面方式提供用户交互接口，通过API方式提供程序调用接口。</p>
<p><a href="http://www.qyjohn.net/wp-content/uploads/2011/05/one-architecture.png" target="_blank"><img src="" alt=""></a></p>
<p>OpenNebula 使用共享存储设备（例如NFS）来提供虚拟机映像服务，使得每一个计算节点都能够访问到相同的虚拟机映像资源。当用户需要启动或者是关闭某个虚拟机 时，OpenNebula通过SSH登陆到计算节点，在计算节点上直接运行相对应的虚拟化管理命令。这种模式也称为无代理模式，由于不需要在计算节点上安 装额外的软件（或者服务），系统的复杂度也相对降低了。</p>
<p><a href="http://www.qyjohn.net/wp-content/uploads/2011/05/one-sample-arch2.png" target="_blank"><img src="" alt=""></a> <strong>OpenQRM</strong></p>
<p>OpenQRM 是为了管理混合虚拟化环境而开发的一个虚拟化管理框架，包括基础层（框架层）和插件。基础层（框架）的作用是管理不同的插件，而对虚拟资源的管理（计算资 源，存储资源，映像资源）都是通过插件来实现的。OpenQRM的框架类似于Java语言中的Interface，定义了一系列虚拟机资源生命周期管理的 方法，例如创建、启动、关闭虚拟机等等。在个框架的基础上，OpenQRM针对不同的虚拟化平台（Xen、KVM)实现了不同的插件，用来管理不同的物理 和虚拟资源。当出现新的资源需要支持的时候，只需要为OpenQRM编写新的插件，就可以无缝地整合到原来的环境中去。</p>
<p><a href="http://www.qyjohn.net/wp-content/uploads/2011/05/openqrm-architecture.png" target="_blank"><img src="" alt=""></a></p>
<p>OpenQRM 插件也是使用无代理模式工作的。当需要管理的目标节点提供SSH登录方式时，OpenQRM插件通过SSH登陆到计算节点，在计算节点上直接运行相对应的 虚拟化管理命令。当需要管理的目标节点提供HTTP／HTTPS／XML－RPC远程调用接口时，OpenQRM插件通过目标节点所提供的远程调用接口实 现对目标平台的管理。</p>
<p>OpenQRM是一个虚拟化管理平台，不提供与Amazon EC2兼容的云管理接口。</p>
<p>  <strong>XenServer</strong></p>
<p>XenServer 是对Xen虚拟化技术的进一步封装，在Dom0上提供一系列命令行和远程调用接口，独立的管理软件XenCenter通过远程调用这些接口来管理多台物理 服务器。XenSever在标准Xen实现之上所实现的远程调用接口类似于其他虚拟化管理平台中所实现的Agent，因此XenServer是通过 Agent方式工作的。由于只考虑对Xen虚拟化技术的支持，XenServer的构架相对简单。</p>
<p><a href="http://www.qyjohn.net/wp-content/uploads/2011/05/xcp-architecture.png" target="_blank"><img src="" alt=""></a></p>
<p>XenServer 是一个虚拟化管理平台，不提供与Amazon EC2兼容的云管理接口。管理软件XenCenter是运行在Windows操作系统上的，对于需要随时随地访问管理功能的系统管理员来说有点不便。目前 有一些第三方提供的开放源代码的基于浏览器的XenServer管理工具，但是都还处于比较早期的阶段。</p>
<p>  <strong>Oracle  VM</strong></p>
<p>Oracle VM包括Oracle VM Server和Oracle VM Manager两个部分。Oracle VM Server在支持Xen的Oracle Linux上（Dom0）运行一个与Xen交互的Agent，该Agent为Oracle VM  Manager提供了远程调用接口。Oracle VM Manager通过一个Java应用程序来对多台Oracle VM Server上的虚拟资源进行管理和调度，同时提供基于浏览器的管理界面。由于只考虑对Xen虚拟化技术的支持，Oracle VM Server / Manager的构架相对简单。</p>
<p><a href="http://www.qyjohn.net/wp-content/uploads/2011/05/GW430.gif" target="_blank"><img src="" alt=""></a></p>
<p>Oracle VM是一个虚拟化管理平台，不提供与Amazon EC2兼容的云管理接口。</p>
<p>值 得注意的是，Oracle VM Manager还通过Web Service的方式提供了虚拟机软件生命周期管理的所有接口，使得用户可以自己使用不同的编程语言来调用这些接口来开发自己的虚拟化管理平台。不过由于 Oracle在开放源代码方面的负面形象，似乎没有看到有这方面的尝试。</p>
<p>  <strong>CloudStack</strong></p>
<p>与 OpenQRM类似，CloudStack采用了“框架 ＋ 插件”的系统构架，通过不同的插件来提供对不同虚拟化技术的支持。对于标准的Xen / KVM计算节点，CloudStack需要在计算节点上安装Agent与控制节点进行交互；对于XenServer / VMWare计算节点，CloudStack通过XenServer / VMWare所提供的XML-RPC远程调用接口与计算节点进行交互。</p>
<p><a href="http://www.qyjohn.net/wp-content/uploads/2011/05/Cloud.com-Architecture.jpg" target="_blank"><img src="" alt=""></a></p>
<p>CloudStack本身是一个虚拟化管理平台，但是它通过CloudBridge提供了与Amazon EC2相兼容的云管理接口，对外提供IaaS服务。</p>
<p>  <strong>ConVirt</strong></p>
<p>ConVirt 是一个虚拟化管理平台，使用无代理模式工作。当需要管理的目标节点提供SSH登录方式时，ConVirt通过SSH登陆到计算节点，在计算节点上直接运行 相对应的虚拟化管 理命令。当需要管理的目标节点提供HTTP／HTTPS／XML－RPC远程调用接口时，ConVirt插件通过目标节点所提供的远程调用接口实现对目标 平台的管理。</p>
<p><a href="http://www.qyjohn.net/wp-content/uploads/2011/05/799px-Architecture.png" target="_blank"><img src="" alt=""></a></p>
<p>ConVirt 是一个虚拟化管理平台，不提供与Amazon EC2兼容的云管理接口。但是ConVirt  3.0提供了与Amazon EC2 / Eucalyptus的用户接口，使得ConVirt用户能够在同一个Web  管理界面下同时管理Amazon EC2 / Eucalyptus提供的虚拟计算资源。</p>
<p>（2）云管理平台还是虚拟化管理平台？</p>
<p>在IaaS这个层面，云管理和虚拟化管理的概念非常接近，但是有一些细微的差别。</p>
<p>虚 拟化是指在同一台物理机器上提供多台虚拟机器（包括CPU、内存、存储、网络等计算资源）的能力。每一台虚拟机器都能够像普通的物理机器一样运行完整的操 作系统以及执行正常的应用程序。当需要管理的物理机器数量较小时，虚拟机生命周期管理（资源配置、启动、关闭等等）可以通过手工去操作。当需要管理的物理 机器数量较大时，就需要写一些脚本／程序来提高虚拟机生命周期管理的自动化程度。以管理和调度大量物理／虚拟计算资源为目的系统，属于虚拟化管理系统。这 样一个系统，通常用于管理企业内部计算资源。</p>
<p>云 计算是指通过网络访问物理／虚拟计算机并利用其计算资源的实践。通常来讲，云计算提供商以虚拟机的方式向用户提供计算资源。用户无须了解虚拟机背后实际的 物理资源状况，只需了解自己所能够使用的计算资源配额。因此，虚拟化技术是云计算的基础。任何一个云计算管理平台，都是构建在虚拟化管理平台的基础之上 的。如果某个虚拟化管理平台仅对某个集团内部提供服务，那么这个虚拟化管理平台也可以被称为“私有云”；如果某个虚拟化管理平台对公众提供服务，那么这个 虚拟化管理平台也可以被称为“公有云”。服务对象的不同，对虚拟化管理平台的构架和功能提出了不同的需求。</p>
<p>私 有云服务于集团内部的不同部门（或者应用），强调虚拟资源调度的灵活性。系统管理员需要为不同的部门（或者应用）定制不同的虚拟机，根据部门（或者应用） 对计算资源的需求对分配给某些虚拟机的计算资源进行调整。从这个意义上来讲，OpenQRM、XenServer、Oracle VM、CloudStack和ConVirt比较适合提供私有云服务。</p>
<p>公 有云服务于公众，强调虚拟资源的标准性。通过将计算资源切割成标准化的虚拟机配置（多个系列的产品，每个产品配置相同数量的CPU、内存、磁盘空间、网络 流量配额），公有云提供商可以通过标准的服务合同（Service  Level Agreement, SLA）以标准的价格出售计算资源。当用户对计算资源的需求出现改变的时候，用户只需要缩减或者是增加自己所使用的产品数量。由于Amazon  EC2是目前比较成功的公有云提供商，大部分云管理平台都在某种程度上模仿Amazon EC2的构架。从这个意义上来讲，Eucalyptus、OpenNebula和OpenStack提供了与Amazon EC2兼容或者是类似的接口，比较适合提供公有云服务。</p>
<p>公有云和私有云之间的界限，就像“内部／外部”和“部门／合作伙伴”的概念一样，并不十分明显。根据项目需求的不同，可能会有不同的解释。</p>
<p>功能篇：</p>
<p>（1）支持的虚拟化技术</p>
<p><strong>Xen</strong></p>
<p><strong>KVM</strong></p>
<p><strong>XenServer  / XCP</strong></p>
<p><strong>VMWare</strong></p>
<p><strong>LXC</strong></p>
<p><strong>openVZ</strong> <strong>Eucalyptus</strong></p>
<p>Y</p>
<p>Y</p>
<p>Y</p>
<p>  <strong>OpenStack</strong></p>
<p>Y</p>
<p>Y</p>
<p>Y</p>
<p>Y</p>
<p>Y</p>
<p>  <strong>OpenNebula</strong></p>
<p>Y</p>
<p>Y</p>
<p>Y</p>
<p>  <strong>OpenQRM</strong></p>
<p>Y</p>
<p>Y</p>
<p>Y</p>
<p>Y</p>
<p>Y</p>
<p>Y <strong>XenServer</strong></p>
<p>Y</p>
<p>  <strong>Oracle VM</strong></p>
<p>Y</p>
<p>  <strong>CloudStack</strong></p>
<p>Y</p>
<p>Y</p>
<p>Y</p>
<p>  <strong>ConVirt</strong></p>
<p>Y</p>
<p>Y</p>
<p>可以看出，Xen和KVM是目前获得最广泛的厂商虚拟化技术，紧随其后的是VMWare。需要注意的是，XenServer是对Xen的进一步封装，可以认为是一种新的虚拟化平台（用户在XenServer上不能直接执行Xend相关命令）。</p>
<p>（2）系统安装和配置</p>
<p>（3）前端  计算节点 备注</p>
<p><strong>前端</strong></p>
<p><strong>计算节点</strong></p>
<p><strong>备注</strong> <strong>Eucalyptus</strong></p>
<p>使用Ubuntu 10.04或者CentOS  5.5操作系统，通过apt-get  install或者yum install的方式直接安装二进制包，构建一个包含CLC、 Walrus、SC、CC的前端。根据官方网站提供的文档进行操作，是比较容易实现的。</p>
<p>使用Ubuntu 10.04或者CentOS 5.5操作系统，通过apt-get install或者yum  install的方式直接安装二进制包，构建一个提供NC服务的计算节点。根据官方网站提供的文档进行操作，是比较容易实现的。</p>
<p>Eucalyptus 包含了一个dhcpd，如果配置不好的话，会造成一定的麻烦。另外，计算节点（NC）与集群控制器（CC）必须在一个C类子网里（例如，掩码为  255.255.255.0）。如果NC和CC在一个超网里（例如，掩码为255.255.0.0），在注册服务的时候会出现一些问题。</p>
<p>  <strong>OpenStack</strong></p>
<p>在Ubuntu 10.04上利用官方网站提供的nova- install脚本进行安装，基本上没有遇到问题。</p>
<p>在Ubuntu 10.04上利用官方网站提供的nova- install脚本进行安装，基本上没有遇到问题。</p>
<p>对于一个简单的系统，安装配置比较简单。 <strong>OpenNebula</strong></p>
<p>使 用CentOS 5.5操作系统，配置好CentOS Karan源，启用kbs- CentOS- Testing条目。下载对应的rpm包，直接yum localinstall  -nogpgcheck  opennebula/*.rpm，就可以直接完成安装过程。按照官方文档创建/srv/cloud/one和/srv/cloud/images目录，通 过NFS共享/srv/cloud目录。创建cloud用户组和属于cloud用户组的oneadmin用户。</p>
<p>按照官方文档创建/srv/cloud/one和/srv/cloud/images目录，通过NFS共享/srv/cloud目录。创建cloud用户组和属于cloud用户组的oneadmin用户。</p>
<p>将前端服务器上oneadmin用户的ssh  key拷贝到计算节点上oneadmin用户的authorized_keys中。这样前端服务器才可以通过SSH登陆到计算节点上。</p>
<p>在CentOS  5.5 x86_64上进行安装的时候，如果按照官方网站提供的文档进行操作，先配置好必要的软件依赖关系再安装opennebula，就会出现xmlrpc-c包版本不对的错误。</p>
<p>网络上可以搜索到一些安装配置方面的文档和教程，但是对于熟悉Linux但是不熟悉OpenNebula的开发人员来说，很难按照这些文档完成安装和配置过程。</p>
<p>  <strong>OpenQRM</strong></p>
<p>在Ubuntu 10.04上通过SVN下载OpenQRM源代码，进入源代码目录后依次执行make / make  install / make start命令。按照官方文档的描述创建数据库，然后通过Web界面进行下一步的安装和配置。</p>
<p>计算节点配置好网桥和虚拟化支持之外不需要特别的安装和配置。在OpenQRM管理界面中启用相对应的插件即可通过插件对计算节点进行管理。</p>
<p>在Ubuntu  10.04上安装前端时，可能需要手工安装dhcp3- server。</p>
<p>启用插件管理虚拟资源的操作流程不够直观，并且缺乏详细的文档。</p>
<p>  <strong>XenServer</strong></p>
<p>前端为基于Windows操作系统的XenCenter。在Windows XP上可以安装，需要.NET  Framework Update 2的支持。安转过程非常简单，基本上不需要配置。</p>
<p>从Citrix的网站下载ISO，刻盘直接安装在裸机上即可。计算节点安装完毕后，在XenCenter中把新增计算资源添加到资源池即可。</p>
<p>每一台XenServer服务器都需要安装从Citrix获得License，并且每年更新一次。 <strong>Oracle VM</strong></p>
<p>在CentOS 5.5 x86_64上进行安装。将ISO文件mount起来后，执行runinstaller.sh即可。</p>
<p>从Oracle的网站下载ISO，刻盘直接安装在裸机上即可。计算节点安装完毕后，在Oracle VM Manager中把新增计算资源添加到资源池即可。</p>
<p>最好从Oracle的官方网站下载，不过速度很慢。通过迅雷等途径下载的文件，看起来似乎没有问题，但是ISO刻盘后在启动操作系统安装过程中会出现错误。</p>
<p>如果在Oracle  VM Server上安装Oracle  VM  Manager，建议分区的时候把/ 分得大一点，不然的话会由于磁盘空间不够而无法安装Oracle VM  Manager。</p>
<p>  <strong>CloudStack</strong></p>
<p>在CentOS 5.5和Ubuntu 10.4上，按照官方网站的安装文档顺序操作，基本没有问题。</p>
<p>计算节点上必须安装相应的Agent。</p>
<p>安装配置相对简单，但是在删除物理资源的时候存在较多的问题。 <strong>ConVirt</strong></p>
<p>在CentOS 5.5和Ubuntu 10.4上，按照官方网站的安装文档顺序操作，基本没有问题。</p>
<p>在Ubuntu 10.04上安装企业版，需要手工sudo apt- get install  libmysqlclient- dev。</p>
<p>在计算节点上的root用户必须允许管理节点上运行ConVirt服务的用户通过key auth方式登录。</p>
<p>安装配置相对简单。</p>
<p>不 同的虚拟化管理软件有不同的设计理念，采用不同的系统构架，类似的概念也采用不同的术语来表述，其学习曲线也各不相同。对于大部分用户来说，虚拟化管理软 件还是个新生事物。即使是粗略地尝试一下利用不同的虚拟化管理软件来安装、配置和测试一个最小规模的私有云系统，也需要花费不少的时间和精力。在这个过程 当中，遇见各种各样的问题都在所难免。不过，也只有亲身经验过这些形形色色的问题，才能够切身体会不同虚拟化管理软件的优点和缺点，并且在分析、总结、归 纳的基础上形成自己独特的观点。</p>
<p>用户界面</p>
<p><strong>概述</strong></p>
<p><strong>用户权限</strong></p>
<p><strong>资源池和虚拟机管理</strong> <strong>Eucalyptus</strong></p>
<p>Eucalyptus提供了一个基于浏览器的简单用户界面，可以完成用户注册，下载credentials，对提供的产品类型进行简单配置等。资源池和虚拟机生命周期管理需要通过euca2ools在命令行模式下完成。</p>
<p>euca2ools是一组基于命令行的工具，可以与Amazon  EC2/S3相兼容的Web  Service进行交互。该用具可以管理基于Amazon EC2、Eucalyptus和OpenStack，OpenNebula的云计算服务。</p>
<p>euca2tools的主要功能包括：</p>
<ul>
<li>查询可以使用的域</li>
<li>管理SSH Key</li>
<li>虚拟机生命周期管理</li>
<li>安全组管理</li>
<li>管理卷和快照</li>
<li>管理虚拟机映像</li>
<li>管理IP</li>
</ul>
<p>在Eucalyptus社区版中只有两种类型的用户：管理员，普通用户。在Eucalyptus企业版中进一步提供了用户组，属于某个用户组的用户可以管理属于该用户组的计算资源。</p>
<p>管理员可以通过注册或者是撤销注册某个计算节点，配置标准产品类型的计算资源（CPU、内存、存储）。普通用户只能够在标准配置的基础上创建、启动、关闭虚拟机，不能够定制化自己所需要的计算资源。</p>
<p>虚 拟机映像文件（EMI）的制作，以及虚拟机生命周期管理等等操作，需要通过euca2ools在命令行模式下完成。在FireFox浏览器中，可以利用 ElasticFox插件，在浏览器中启动、监控和关闭虚拟机。ElasticFox的界面不够美观，并且提供的功能非常有限。</p>
<p>Eucalyptus不提供console功能。用户可以通过SSH连接到自己所管理的虚拟机。</p>
<p>每一个公开发布的虚拟机映像（EMI），都是一个模板。用户创建虚拟机实例的时候，系统根据用户选择的EMI将相应的虚拟机映像拷贝到目标计算节点上运行。Eucalyptus根据某种算法自动决定用户的虚拟机将在哪个物理服务器上运行，用户对物理服务器的状况一无所知。</p>
<p>Eucalyptus 中的虚拟机实例只是原虚拟机映像（EMI）的一个副本，用户在运行的实例中对虚拟机所做的任何修改，不会被保存到原来的虚拟机映像中。如果用户将运行的虚  拟机实例关闭（例如：shutdown），用户对虚拟机所作的任何修改都会丢失。如果用户需要保存自己对虚拟机所做的修改，用户可以选择使用弹性块设备来 保存数据，或者将正在运行的虚拟机实例发布为新的EMI。（Amazon EC2自动地将停止运行的虚拟机实例保存为新的AMI，直到用户销毁该虚拟机实例为止。因此，用户可以shutdown自己的虚拟机实例，但是保存自己对 虚拟机所作的修改，直到用户选择销毁该虚拟机实例为止。）</p>
<p>  <strong>OpenStack</strong></p>
<p>OpenStack 不缺省地提供基于浏览器的用户界面。系统管理员需要手工创建用户。大部分的管理操作，需要在命令行下进行。 尽管OpenStack和Eucalyptus在构架上有很大的不同，但是所暴露给用户的界面是类似的（两者都模仿了Amazon EC2的用户接口规范）。因此，OpenStack同样可以使用Eucalyptus所提供的euca2ools进行管理。</p>
<p>OpenStack的openstack- dashboard项目和django- nova项目提供了一个基于浏览器的用户界面，没有被集成到OpenStack安装脚本中，需要单独安装。</p>
<p>OpenStack将用户分成如下几个类别：</p>
<p>admin - 云服务管理员，拥有所有管理权限。</p>
<p>itsec - IT安全管理员，具有隔离有问题的虚拟机实例的权限。</p>
<p>projectmanager - 项目管理员，可以增加属于该项目的新用户，管理虚拟机映像，管理虚拟机生命周期。</p>
<p>netadmin - 网络管理员，负责IP分配，管理防火墙。</p>
<p>developer - 开发人员，可以登录进入属于本项目的虚拟机，管理虚拟机生命周期</p>
<p>在模仿Amazon EC2的云平台（Eucalyptus,  OpenStack,  OpenNebula）中，OpenStack提供了颗粒度最细的用户权限管理模式。</p>
<p>与Eucalyptus类似，虚拟机映像文件（EMI）的制作，以及虚拟机生命周期管理等等操作，需要通过euca2ools在命令行模式下完成。同样，在FireFox浏览器中，可 以利用ElasticFox插件，在浏览器中启动、监控和关闭虚拟机。</p>
<p>OpenStack不提供虚拟机console功能。用户可以通过SSH连接到自己所管理的虚拟机。</p>
<p>正在开发中的openstack- dashboard，基于浏览器提供了比较完整的资源池管理功能和虚拟机生命周期管理功能。虽然界面还比较简单，但是已经处于可用的状态。</p>
<p>OpenStack的模板和虚拟机实例机制与Eucalyptus类似。与Eucalyptus类似，OpenStack根据某种算法自动决定用户的虚拟机将在哪个物理服务器上运行，用户对物理服务器的状况一无所知。</p>
<p>  <strong>OpenNebula</strong></p>
<p>OpenNebula不缺省地提供基于浏览器的用户界面。系统管理员需要手工创建用户。大部分的管理操作，需要在命令行下进行。</p>
<p>OpenNebula目前有两个基于浏览器的用户界面：SunStone和OneMC。这两个项目需要单独安装。</p>
<p>同样，OpenNebula提供了与Amazon EC2相兼容的Web Service接口。因此，可以通过FireFox所提供的ElasticFox插件和Eucalyptus提供的euca2ools工具集与OpenNebula云平台进行交互。</p>
<p>OpenNebula只有两种类型的用户：管理员，普通用户。</p>
<p>在早期版本中，OpenNebula管理员可以在后台通过命令行来管理资源池和虚拟机生命周期。 同样，在FireFox浏览器中，可 以利用ElasticFox插件，在浏览器中启动、监控和关闭虚拟机。</p>
<p>SunStone和OneMC这两个项目都提供了比较完整的资源池管理和虚拟机生命周期管理功能。两个项目的界面都比较简单，但是基本上处于可用的状态。SunStone没有提供虚拟机console功能，OneMC通过VNC协议提供了虚拟机console功能。</p>
<p>OpenNebula的模板和虚拟机实例机制与Eucalyptus类似。但是并不缺省地使用euca2ools作为工具。</p>
<p>与Eucalyptus类似，OpenNebula根据某种算法自动决定用户的虚拟机将在哪个物理服务器上运行，用户对物理服务器的状况一无所知。</p>
<p>  <strong>OpenQRM</strong></p>
<p>基于浏览器的用户界面，功能比较丰富。</p>
<p>OpenQRM的管理界面只有两种用户：管理用户，普通用户。普通用户只有查看权限，没有管理权限。</p>
<p>通过启用不同的插件，可以管理不同的计算资源。所有的资源池和虚拟机生命周期管理操作都可以通过浏览器界面完成。</p>
<p>OpenQRM的novnc插件可以提供基于VNC协议的虚拟机console功能。</p>
<p>  <strong>XenServer</strong></p>
<p>XenCenter是基于Windows的桌面应用，安装与操作都非常简单，界面美观，功能强大。</p>
<p>在参与评测的8 个软件中，XenCenter的用户界面是表现最出色的。基于Windows桌面的应用能够迅速地对用户的点击动作作出反应，从而提高用户体验的满意度。</p>
<p>系统管理员登录XenCenter之后，可以结合Active Directory在用户和用户组的层面分配管理权限。</p>
<p>授权用户可以通过图形界面方便地进行资源池和虚拟机生命周期管理。在图形界面上可以直观地监控物理服务器和虚拟机的计算资源使用情况（CPU、内存、存储、网络活动）。</p>
<p>提供基于VNC的虚拟机console。</p>
<p>可以基于模板的部署新的虚拟机。</p>
<p>  <strong>Oracle VM</strong></p>
<p>Oracle VM Manager提供了基于浏览器的管理界面。</p>
<p>Oracle VM Manager同时提供了role和group的概念。其中role定义了用户所具备的权限，属于同一个group的用户拥有该group所被授予的权限。</p>
<p>Oracle VM Manager提供了三种role：</p>
<p>user - 拥有指定资源池的虚拟机生命周期管理权限。</p>
<p>manager - 拥有除了用户管理之外的所有管理权限。</p>
<p>administrator - 拥有整个系统的管理权限。</p>
<p>授权用户可以通过图形界面方便地进行资源池和虚拟机生命周期管理。在图形界面上可以直观地监控物理服务器和虚拟机的计算资源使用情况（CPU、内存、存储、网络活动）。</p>
<p>提供基于VNC的虚拟机console。</p>
<p>可以基于模板的部署新的虚拟机。 <strong>CloudStack</strong></p>
<p>基于浏览器的用户界面，功能丰富，美观大方。</p>
<p>CloudStack根据用户的role将用户分成三个类型：</p>
<p>admin - 全局管理员。</p>
<p>domain－admin - 域管理员，可以对某个域下的物理和虚拟资源进行管理。</p>
<p>user - 个体用户，可以管理自己名下的虚拟机资源。</p>
<p>CloudStack 对物理资源的管理完整地模拟了一个物理机房的实际情况，按照“机房（Zones）－》机柜（Pods）－》集群（Cluster）－》服务器 （Server）”的结构对物理服务器进行组织，使得管理员能够在管理界面里面的计算资源和机房里面的计算资源建立起直观的一一对应关系。</p>
<p>授权用户可以通过图形界面方便地进行资源池和虚拟机生命周期管理。在图形界面上可以直观地监控物理服务器和虚拟机的计算资源使用情况（CPU、内存、存储、网络活动）。</p>
<p>提供基于VNC的虚拟机console。</p>
<p>可以基于模板的部署新的虚拟机。</p>
<p>  <strong>ConVirt</strong></p>
<p>基于浏览器的用户界面，功能丰富，美观大方。</p>
<p>社区版可以注册多个用户，并可将用户按照用户组进行分类，但是所有的用户拥有相同的全局管理权限。企业版则提供了更细致的用户权限管理机制。除此之外，企业版还提供了对LDAP的支持。</p>
<p>授权用户可以通过图形界面方便地进行资源池和虚拟机生命周期管理。在图形界面上可以直观地监控物理服务器和虚拟机的计算资源使用情况（CPU、内存、存储、网络活动）。提供基于VNC的虚拟机console。</p>
<p>可以基于模板的部署新的虚拟机。</p>
<p>ConVirt 的最大优点，在于其通过时程图的方式在不同的层次上直观地展示计算资源（包括物理资源和虚拟资源）的利用情况和健康状况。在整个数据中心和资源池的层 面，ConVirt实时显示资源池数量、物理服务器和虚拟机数量、虚拟机密度、存储资源使用状况、负载最高的N 台物理服务器和虚拟机。在物理服务器和虚拟机的层面，ConVirt实时显示CPU和内存使用情况，监控人员可以通过CPU和内存时程图及时地发现或者是 调查系统异常情况。</p>
<p>在 所有参与评测的虚拟化管理软件中，XenServer / XCP和ConVirt的图形用户界面是做的最好的。XenCenter的图形界面的优点在于提供了独一无二的用户体验，ConVirt的图形界面的优点 在于以图形的方式直观地展示了从机房到虚拟机的健康状况。CloudStack的图形界面非常大气，但是在功能上不如ConVirt那么实用。不过按照 CloudStack的目前的发展势头来看，下一个版本可能比较值得期待。</p>
<p>由于进行评测的时间较短，并且测试系统规模较小的原因，暂时无法对各个软件的稳定性、健壮性、扩展性等等关键问题作出评估。</p>
<p>商务篇：</p>
<p>目前市面上形形色色的虚拟化管理软件总数很多，这一系列文章所提及的几个软件仅仅其中的几个代表。作为一个机构、或者是一家企业，在向虚拟化过渡时都不可避免地要面临软件选型的问题。本文作为这一系列文章的最后一篇，从商务和功能两个方面提出自己的一点粗浅意见。</p>
<p>（1）商务评估</p>
<p>从 商务上进行软件选型，性价比通常是一个决定性的因素。在假定参与选型的软件全部满足技术要求的前提下，企业（机构）需要考虑的因素包括软件的授权协议是否 友好、许可证管理的难易程度、软件和服务的价格高低、运营团队在业界的声誉、开发者社区和用户社区的规模和活跃程度、商业与技术沟通的难易程度。</p>
<p>授 权协议/许可证管理 — 以全部开放源代码为10分，部分开放源代码（例如以企业版的形式提供某些高级功能，或者以服务的形式提供特别版本的安装包和补丁）扣1 分。商业版本需要在控制节点安装许可证不扣分，需要在所有计算节点安装许可证扣1 分，许可证需要每年更新者扣1 分。</p>
<p>价格指数 — 以全部功能免费使用为10分，以企业版的模式提供全部功能的软件，每台物理服务器每花费500美元扣1  分。</p>
<p>运营团队 — 以运营团队的规模、背景、影响力评分，存在的主观因素较多。</p>
<p>社区因素 — 以开发者和用户社区的规模和活跃程度评分，存在的主观因素较多。</p>
<p>沟通交流 — 以个人与运营团队、开发者社区、用户社区之间的沟通顺畅程度评分，存在的主观因素较多。</p>
<p>   授权协议</p>
<p><strong>授权协议</strong></p>
<p><strong>许可证管理</strong></p>
<p><strong>价格指数</strong></p>
<p><strong>运营团队</strong></p>
<p><strong>社区因素</strong></p>
<p><strong>沟通交流</strong></p>
<p><strong>总分</strong> <strong>Eucalyptus</strong></p>
<p>9</p>
<p>8</p>
<p>9</p>
<p>9</p>
<p>10</p>
<p>45 <strong>OpenStack</strong></p>
<p>10</p>
<p>10</p>
<p>8</p>
<p>8</p>
<p>7</p>
<p>43 <strong>OpenNebula</strong></p>
<p>9</p>
<p>9</p>
<p>7</p>
<p>8</p>
<p>9</p>
<p>42 <strong>OpenQRM</strong></p>
<p>9</p>
<p>8</p>
<p>6</p>
<p>7</p>
<p>8</p>
<p>37 <strong>XenServer</strong></p>
<p>7</p>
<p>8</p>
<p>9</p>
<p>10</p>
<p>9</p>
<p>43 <strong>Oracle VM</strong></p>
<p>9</p>
<p>7</p>
<p>7</p>
<p>6</p>
<p>7</p>
<p>36 <strong>CloudStack</strong></p>
<p>9</p>
<p>8</p>
<p>7</p>
<p>6</p>
<p>7</p>
<p>37 <strong>ConVirt</strong></p>
<p>9</p>
<p>8</p>
<p>8</p>
<p>9</p>
<p>10</p>
<p>44</p>
<p>（2）功能评估</p>
<p>从功能上进行虚拟化管理软件选型，需要考虑的因素包括该软件所支持的虚拟化技术、安装配置的难易程度、开发和使用文档的详尽程度、所提供的功能是否全面以及用户界面是否直观友好、二次开发的难易程度、是否提供物理资源和虚拟资源的监控报表等等。</p>
<p>虚拟化技术支持 — 仅支持一种虚拟化技术为6 分，每增加一种虚拟化技术加1 分，10分封顶。</p>
<p>安装配置 — 以按照官方文档进行安装配置的难易程度评分，存在的主观因素较多。</p>
<p>开发/使用文档 — 以官方所提供的开发与使用文档的详尽程度评分，文档详尽程度越高者得分越高。</p>
<p>功能与界面 — 综合评分，涵盖用户进行物理资源和虚拟资源管理、虚拟机生命周期管理、访问虚拟机资源和存储资源的难易程度，用户界面的美观易用程度，以及综合用户体验。</p>
<p>二次开发 — 基础得分6 分，提供与Amazon EC2相兼容的程序调用接口者加3 分，提供二次开发接口但是与Amazon  EC2不兼容者加2 分。</p>
<p>监控报表 — 基础得分6 分，依系统所提供监控与分析功能的详尽程度加分。</p>
<p><strong>虚拟化技术支持</strong></p>
<p><strong>安装配置</strong></p>
<p><strong>开发／使用文档</strong></p>
<p><strong>功能与界面</strong></p>
<p><strong>二次开发</strong></p>
<p><strong>监控报表</strong></p>
<p><strong>总分</strong> <strong>Eucalyptus</strong></p>
<p>8</p>
<p>8</p>
<p>9</p>
<p>4</p>
<p>9  (Amazon  WS)</p>
<p>6</p>
<p>44 <strong>OpenStack</strong></p>
<p>10</p>
<p>8</p>
<p>8</p>
<p>4</p>
<p>9  (Amazon  WS)</p>
<p>6</p>
<p>45 <strong>OpenNebula</strong></p>
<p>8</p>
<p>8</p>
<p>7</p>
<p>4</p>
<p>9  (Amazon  WS)</p>
<p>6</p>
<p>42 <strong>OpenQRM</strong></p>
<p>10</p>
<p>9</p>
<p>5</p>
<p>10</p>
<p>6 (OS)</p>
<p>7</p>
<p>47 <strong>XenServer</strong></p>
<p>6</p>
<p>10</p>
<p>10</p>
<p>10</p>
<p>8 (Plugin)</p>
<p>9</p>
<p>53 <strong>Oracle VM</strong></p>
<p>6</p>
<p>9</p>
<p>8</p>
<p>7</p>
<p>8 (WS)</p>
<p>7</p>
<p>45 <strong>CloudStack</strong></p>
<p>8</p>
<p>9</p>
<p>8</p>
<p>10</p>
<p>6 (OS)</p>
<p>8</p>
<p>49 <strong>ConVirt</strong></p>
<p>7</p>
<p>10</p>
<p>10</p>
<p>10</p>
<p>8 (API)</p>
<p>10</p>
<p>55</p>
<p>（3）综合评估</p>
<p>从 商务上考虑，Eucalyptus和ConVirt以微弱 的优势领先于其他选项。Eucalyptus是私有云管理平台的先行者。Ubuntu 10.04选择捆绑Eucalyptus作为UEC的基础构架，使得Ecualyptus比其他的私有云管理平台拥有更多的用户和更加活跃的社区。此 外，Ecualyptus在中国国内有销售和技术支持人员，在沟通上比选择其他软件要更加容易。ConVirt排名第二，根本原因在于其销售和技术支持团 队与（潜在的）客户保持积极而有效的沟通。Citrix XenServer仅仅与其他两个选项并列排名第三，输在其过于严苛的许可证管理政策。的确，要给100台以上的服务器单独安装许可证并且每年更新一次， 可不是一件有意思的事情。</p>
<p>从 功能上考虑，ConVirt与XenServer遥遥领先于其他选项。虽然ConVirt仅仅支持Xen和KVM两种虚拟化技术，但是其安装配置相对简 单，文档详尽、功能齐全、界面美观、是比较容易上手的虚拟化管理软件。更重要的是，ConVirt的监控报表功能直观地展示了从数据中心到虚拟机的 CPU、内存利用情况，使得用户对整个数据中心的健康状况一目了然。同样，XenServer虽然仅支持Xen一种虚拟化技术，但是在安装配置、操作文 档、用户界面等方面都不亚于ConVirt。如果用户对基于Windows的界面没有强烈的抵触情绪的话，XenServer是比较值得考虑的一个选型。</p>
<p>综 合如上考虑，对于希望利用虚拟化管理软件提高硬件资源利用率和虚拟化管理自动化程度的企业（机构）来说，建议使用ConVirt来管理企业（机构）的计算 资源。如果网管人员不希望深入了解Linux操作系统，并且所管理的物理服务器数量有限的话，XenServer也是一个不错的选择。ConVirt的浏 览器界面是开放源代码的，用户可以对其进行定制化，将自己所需要的其他功能添加到同一个用户界面中去。XenCenter则提供了一种插件机制，用户可以 通过插件的方式讲自己的功能集成到XenCenter中。</p>
<p>不 过，你的基础设施是否需要与Amazon EC2相兼容呢？也就是说，你的用户是否需要使用他们用于访问和操作Amazon EC2的脚本和工具来访问你的计算资源呢？如果是这样的话，你可能需要在Eucalyptus和OpenStack之间作一个选择（CloudStack 和OpenNebula同样提供了与Amazon EC2兼容的操作接口，但是CloudStack在商务方面得分不高，OpenNebula在功能方面得分不高）。Eucalyptus的历史比 OpenStack稍长，用户群比OpenStack要大，社区的活跃程度也比OpenStack要高。不过OpenStack的后台老板NASA比 Eucalyptus要财大气粗，Ubuntu 11.04也集成了OpenStack作为其UEC的基础构架之一，表明OpenStack已经得到了社区的重视和支持。总的来说，开放源代码的云构架， 还是一个不断发展之中的新生食物。笔者只能够建议用户亲自去安装使用每一个软件，最终基于自己的经验以及需求达到一个最适合自己的选择。</p>
<p>虚拟化管理软件比较 －－ 幻灯片</p>
<p>结合前段时间对不同虚拟化管理软件的评测工作，准备了一套讲座用的幻灯片。PDF版本的文件可以从这里下载。如果有人需要ODP版本的文件，直接跟我联系吧。</p>
<p><img src="" alt=""></p>
<p><img src="" alt=""></p>
<p><img src="" alt=""></p>
<p><img src="" alt=""></p>
<p><img src="" alt=""></p>
<p><img src="" alt=""></p>
<p><img src="" alt=""></p>
<p><img src="" alt=""></p>
<p><img src="" alt=""></p>
<p><img src="" alt=""></p>
<p><img src="" alt=""></p>
<p><img src="" alt=""></p>
<p><img src="" alt=""></p>
<p><img src="" alt=""></p>
<p><img src="" alt=""></p>
<p><img src="" alt=""></p>
<p><img src="" alt=""></p>
<p>1.4 对我司的作用</p>
<p>建议使用Convirt而不是OpenStack</p>
<p>当其与sunde等代表的云终端（支持由真实单机提供和分配多虚拟机的运行环境，最终用户通过sunde的非PC终端连结各自虚拟机）配合使用时。</p>
<p>可以组成OpenStack管理的真实计算机集群基础上按照业务类型（开发、测试、办公、演示、呼叫、培训）划分的虚拟母机，再分别在虚拟母机上运行sunde的管理端，以管理最终使用者的各个虚拟子机并分别组成各自虚拟子网和公司公关网络，最后由最终用户使用非PC终端接入各自使用的一个或者多个虚拟机进行合理工作。</p>
<p>非移动技术用环境：</p>
<p>比起开发、测试人员、销售使用的个人工作机相比，更紧缺的是各种服务器，譬如：</p>
<p>运维：线上完整模拟测试环境、线上环境备份、工作环境文件共享服务器，公司网站测试环境等。</p>
<p>开发：VS、PD、DB设计服务器、开发测试用完整模拟环境、架构测试实验室等。</p>
<p>测试：QC服务器、浏览器多版本环境测试服务器、独立项目测试用完整模拟环境等。</p>
<p>Call Center：简单应用而需多人处理的办公环境。</p>
<p>等用于管理多虚拟实例的环境管理节点（一台机器、一个环境）甚至线上备份。</p>
<p>但因其不能跨物理机调度、通信、资源整合、虚拟机效率低等原因，不可以用于：</p>
<p>分布式大数据量存储控制业务节点、分布式数据中心存储控制节点等需要整合物理机资源进行分布式并行处理的环境。</p>
<p>2 <img src="" alt=""> Hadoop</p>
<p>2.1 简介</p>
<p>一款分布式数据存储与业务系统架构平台，由Apache基金会开发。用户可以在不了解分布式底层细节的情况下，开发分布式程序。充分利用集群的威力高速运算和存储。</p>
<p>其可以在多台计算机（PC或者小型机）组成的网络集群上跨物理机器统一调配单机资源以适应多种可分布并行处理的业务服务。</p>
<p>2.2 原理</p>
<p>其架构由低到高分为 HDFS-&gt;MapReduce+BigTable（NoSQLDB）</p>
<p>HDFS算法中</p>
<p>结构分为星型的NameNode核心与DataNode外围，其中NameNode为其瓶颈，当NameNode再大时，只能再整体复制一个集群出去做分布或者将单一机器的NameNode再通过把Key分级做Hadoop化处理。</p>
<p>有效容积率：</p>
<p>假设容许损坏的机器为x台，机器总数为y台</p>
<p>那么y台机器有效数据量为y/(x+1),其有效数据容积率为(y/(x+1))/y=1/(x+1)</p>
<p>健壮率：</p>
<p>假设容许损坏的机器为x台，机器总数为y台</p>
<p>那么y台机器健壮率(最小需要几台机器才能稳定)为1-(y-x)/y = x/y</p>
<p>如果综合考虑 有效容积率与健壮率 那么 1/(x+1)=x/y 所以y = x（x+1）。</p>
<p>MapReduce算法：</p>
<p>典型分散综合分布式处理算法。</p>
<p>基础是处理可以分散处理再综合统计数据的业务类型。</p>
<p>2.3 对我司的作用</p>
<p>其分布并行Map-reduce算法可以用于很多非即时反馈的非事务性业务处理：且不依赖HDFS一种实现，只要是支持节点运算即可。</p>
<p>其HDFS系统可用于大量数据文件的存储。但是不论其节点数量多少，其有效容积率都是由可容忍的宕机数量决定的。可见HDFS算法中更侧重的是稳定而不是并行处理高效和负载，更针对建立索引等搜索类业务处理要求而对少写多读商务类业务处理针对性不强，从百度淘宝的实践看也都证明这一点。</p>
<p>所以其HDFS系统设计之上应根据实际情况加入中央NOSQL缓存扩展与单数据节点线性NOSQL缓存负载。再配合业务服务器自身各种优化措施才能成为公司分布式DB、FS处理负载设计框架。hadoop设计思路可以借鉴不能照搬。</p>

      
    </div>
    
    
      
    
    
        <footer id="post-meta">
        <span class="categories">Posted in<span class="breadcrumb fa fa-folder"><li><a href="/categories/hadoop/">hadoop</a></li></span></span> | <span class="tags">Tagged <a href="/tags/hadoop/" class="label label-primary">hadoop</a></span> | <span class="time">recent updated:<time title="2014-03-07 09:54:36"datetime="2014-03-07 09:54:36"> mar. 7 2014</time></span> | <span class="comment-link">
<a href="http://itsolife.com/2014/02/02/2014-02-02-hadoop--OpenStack_Hadoop/#comments" class="ds-thread-count comment-link" data-thread-key="2014-02-02-hadoop--OpenStack_Hadoop" data-count-type="comments">暂无评论</a></span>
        </footer>
    
    <div class="clearfix"></div>
  </div>
</div>
</article>



  <article>
<div id="post" class="post well">
  <div class="post-content">
    <header class="well-sm">
      <i class="fa icon fa-5x pull-left"></i>
      <h1 class="title"><a href="/2014/02/02/2014-02-02-hadoop--hdfs_design/">hdfs_design</a></h1>
      
        <span>Posted on<time datetime="2014-02-02T01:54:36.000Z"> <a href="/2014/02/02/2014-02-02-hadoop--hdfs_design/">feb. 2 2014</a></time></span>      
    </header>
    
    
    <div class="entry">
      
        <h1 id="hdfs_design">hdfs_design</h1>
<p>HDFS Architecture
by Dhruba Borthakur
Table of contents
1 2
Introduction .......................................................................................................................3 Assumptions and Goals .....................................................................................................3
2.1 2.2 2.3 2.4 2.5 2.6
Hardware Failure .......................................................................................................... 3 Streaming Data Access .................................................................................................3 Large Data Sets .............................................................................................................3 Simple Coherency Model ............................................................................................. 4 “Moving Computation is Cheaper than Moving Data” ................................................4 Portability Across Heterogeneous Hardware and Software Platforms .........................4
3 4 5
NameNode and DataNodes ...............................................................................................4 The File System Namespace ............................................................................................. 5 Data Replication ................................................................................................................6
5.1 5.2 5.3
Replica Placement: The First Baby Steps .................................................................... 7 Replica Selection .......................................................................................................... 8 Safemode ...................................................................................................................... 8
6 7 8
The Persistence of File System Metadata ......................................................................... 8 The Communication Protocols ......................................................................................... 9 Robustness ........................................................................................................................ 9
8.1 8.2 8.3 8.4 8.5
Data Disk Failure, Heartbeats and Re-Replication .....................................................10 Cluster Rebalancing ....................................................................................................10 Data Integrity ..............................................................................................................10 Metadata Disk Failure ................................................................................................ 10 Snapshots ....................................................................................................................11
Copyright © 2008 The Apache Software Foundation. All rights reserved.
HDFS Architecture
9
Data Organization ........................................................................................................... 11
9.1 9.2 9.3
Data Blocks ................................................................................................................ 11 Staging ........................................................................................................................11 Replication Pipelining ................................................................................................ 12 FS Shell .....................................................................................................................12 DFSAdmin ................................................................................................................ 13 Browser Interface ......................................................................................................13 File Deletes and Undeletes ....................................................................................... 13 Decrease Replication Factor ..................................................................................... 14
10
Accessibility .................................................................................................................. 12
10.1 10.2 10.3 11
Space Reclamation ........................................................................................................ 13
11.1 11.2 12
References ..................................................................................................................... 14
Page 2
Copyright © 2008 The Apache Software Foundation. All rights reserved.
HDFS Architecture</p>
<ol>
<li>Introduction
The Hadoop Distributed File System (HDFS) is a distributed file system designed to run on commodity hardware. It has many similarities with existing distributed file systems. However, the differences from other distributed file systems are significant. HDFS is highly fault-tolerant and is designed to be deployed on low-cost hardware. HDFS provides high throughput access to application data and is suitable for applications that have large data sets. HDFS relaxes a few POSIX requirements to enable streaming access to file system data. HDFS was originally built as infrastructure for the Apache Nutch web search engine project. HDFS is part of the Apache Hadoop Core project. The project URL is <a href="http://hadoop.apache.org/core/" target="_blank">http://hadoop.apache.org/core/</a>.</li>
<li>Assumptions and Goals
2.1. Hardware Failure
Hardware failure is the norm rather than the exception. An HDFS instance may consist of hundreds or thousands of server machines, each storing part of the file system’s data. The fact that there are a huge number of components and that each component has a non-trivial probability of failure means that some component of HDFS is always non-functional. Therefore, detection of faults and quick, automatic recovery from them is a core architectural goal of HDFS.
2.2. Streaming Data Access
Applications that run on HDFS need streaming access to their data sets. They are not general purpose applications that typically run on general purpose file systems. HDFS is designed more for batch processing rather than interactive use by users. The emphasis is on high throughput of data access rather than low latency of data access. POSIX imposes many hard requirements that are not needed for applications that are targeted for HDFS. POSIX semantics in a few key areas has been traded to increase data throughput rates.
2.3. Large Data Sets
Applications that run on HDFS have large data sets. A typical file in HDFS is gigabytes to terabytes in size. Thus, HDFS is tuned to support large files. It should provide high aggregate data bandwidth and scale to hundreds of nodes in a single cluster. It should support tens of millions of files in a single instance.
Page 3
Copyright © 2008 The Apache Software Foundation. All rights reserved.
HDFS Architecture
2.4. Simple Coherency Model
HDFS applications need a write-once-read-many access model for files. A file once created, written, and closed need not be changed. This assumption simplifies data coherency issues and enables high throughput data access. A Map/Reduce application or a web crawler application fits perfectly with this model. There is a plan to support appending-writes to files in the future.
2.5. “Moving Computation is Cheaper than Moving Data”
A computation requested by an application is much more efficient if it is executed near the data it operates on. This is especially true when the size of the data set is huge. This minimizes network congestion and increases the overall throughput of the system. The assumption is that it is often better to migrate the computation closer to where the data is located rather than moving the data to where the application is running. HDFS provides interfaces for applications to move themselves closer to where the data is located.
2.6. Portability Across Heterogeneous Hardware and Software Platforms
HDFS has been designed to be easily portable from one platform to another. This facilitates widespread adoption of HDFS as a platform of choice for a large set of applications.</li>
<li>NameNode and DataNodes
HDFS has a master/slave architecture. An HDFS cluster consists of a single NameNode, a master server that manages the file system namespace and regulates access to files by clients. In addition, there are a number of DataNodes, usually one per node in the cluster, which manage storage attached to the nodes that they run on. HDFS exposes a file system namespace and allows user data to be stored in files. Internally, a file is split into one or more blocks and these blocks are stored in a set of DataNodes. The NameNode executes file system namespace operations like opening, closing, and renaming files and directories. It also determines the mapping of blocks to DataNodes. The DataNodes are responsible for serving read and write requests from the file system’s clients. The DataNodes also perform block creation, deletion, and replication upon instruction from the NameNode.
Page 4
Copyright © 2008 The Apache Software Foundation. All rights reserved.
<img src="" alt=""> HDFS Architecture
The NameNode and DataNode are pieces of software designed to run on commodity machines. These machines typically run a GNU/Linux operating system (OS). HDFS is built using the Java language; any machine that supports Java can run the NameNode or the DataNode software. Usage of the highly portable Java language means that HDFS can be deployed on a wide range of machines. A typical deployment has a dedicated machine that runs only the NameNode software. Each of the other machines in the cluster runs one instance of the DataNode software. The architecture does not preclude running multiple DataNodes on the same machine but in a real deployment that is rarely the case. The existence of a single NameNode in a cluster greatly simplifies the architecture of the system. The NameNode is the arbitrator and repository for all HDFS metadata. The system is designed in such a way that user data never flows through the NameNode.</li>
<li>The File System Namespace
HDFS supports a traditional hierarchical file organization. A user or an application can create directories and store files inside these directories. The file system namespace hierarchy is
Page 5
Copyright © 2008 The Apache Software Foundation. All rights reserved.
HDFS Architecture
similar to most other existing file systems; one can create and remove files, move a file from one directory to another, or rename a file. HDFS does not yet implement user quotas or access permissions. HDFS does not support hard links or soft links. However, the HDFS architecture does not preclude implementing these features. The NameNode maintains the file system namespace. Any change to the file system namespace or its properties is recorded by the NameNode. An application can specify the number of replicas of a file that should be maintained by HDFS. The number of copies of a file is called the replication factor of that file. This information is stored by the NameNode.</li>
<li>Data Replication
HDFS is designed to reliably store very large files across machines in a large cluster. It stores each file as a sequence of blocks; all blocks in a file except the last block are the same size. The blocks of a file are replicated for fault tolerance. The block size and replication factor are configurable per file. An application can specify the number of replicas of a file. The replication factor can be specified at file creation time and can be changed later. Files in HDFS are write-once and have strictly one writer at any time. The NameNode makes all decisions regarding replication of blocks. It periodically receives a Heartbeat and a Blockreport from each of the DataNodes in the cluster. Receipt of a Heartbeat implies that the DataNode is functioning properly. A Blockreport contains a list of all blocks on a DataNode.
Page 6
Copyright © 2008 The Apache Software Foundation. All rights reserved.
<img src="" alt=""> HDFS Architecture
5.1. Replica Placement: The First Baby Steps
The placement of replicas is critical to HDFS reliability and performance. Optimizing replica placement distinguishes HDFS from most other distributed file systems. This is a feature that needs lots of tuning and experience. The purpose of a rack-aware replica placement policy is to improve data reliability, availability, and network bandwidth utilization. The current implementation for the replica placement policy is a first effort in this direction. The short-term goals of implementing this policy are to validate it on production systems, learn more about its behavior, and build a foundation to test and research more sophisticated policies. Large HDFS instances run on a cluster of computers that commonly spread across many racks. Communication between two nodes in different racks has to go through switches. In most cases, network bandwidth between machines in the same rack is greater than network bandwidth between machines in different racks. The NameNode determines the rack id each DataNode belongs to via the process outlined in Rack Awareness. A simple but non-optimal policy is to place replicas on unique racks. This prevents losing data when an entire rack fails and allows use of bandwidth from multiple
Page 7
Copyright © 2008 The Apache Software Foundation. All rights reserved.
HDFS Architecture
racks when reading data. This policy evenly distributes replicas in the cluster which makes it easy to balance load on component failure. However, this policy increases the cost of writes because a write needs to transfer blocks to multiple racks. For the common case, when the replication factor is three, HDFS’s placement policy is to put one replica on one node in the local rack, another on a different node in the local rack, and the last on a different node in a different rack. This policy cuts the inter-rack write traffic which generally improves write performance. The chance of rack failure is far less than that of node failure; this policy does not impact data reliability and availability guarantees. However, it does reduce the aggregate network bandwidth used when reading data since a block is placed in only two unique racks rather than three. With this policy, the replicas of a file do not evenly distribute across the racks. One third of replicas are on one node, two thirds of replicas are on one rack, and the other third are evenly distributed across the remaining racks. This policy improves write performance without compromising data reliability or read performance. The current, default replica placement policy described here is a work in progress.
5.2. Replica Selection
To minimize global bandwidth consumption and read latency, HDFS tries to satisfy a read request from a replica that is closest to the reader. If there exists a replica on the same rack as the reader node, then that replica is preferred to satisfy the read request. If angg/ HDFS cluster spans multiple data centers, then a replica that is resident in the local data center is preferred over any remote replica.
5.3. Safemode
On startup, the NameNode enters a special state called Safemode. Replication of data blocks does not occur when the NameNode is in the Safemode state. The NameNode receives Heartbeat and Blockreport messages from the DataNodes. A Blockreport contains the list of data blocks that a DataNode is hosting. Each block has a specified minimum number of replicas. A block is considered safely replicated when the minimum number of replicas of that data block has checked in with the NameNode. After a configurable percentage of safely replicated data blocks checks in with the NameNode (plus an additional 30 seconds), the NameNode exits the Safemode state. It then determines the list of data blocks (if any) that still have fewer than the specified number of replicas. The NameNode then replicates these blocks to other DataNodes.</li>
<li>The Persistence of File System Metadata
Page 8
Copyright © 2008 The Apache Software Foundation. All rights reserved.
HDFS Architecture
The HDFS namespace is stored by the NameNode. The NameNode uses a transaction log called the EditLog to persistently record every change that occurs to file system metadata. For example, creating a new file in HDFS causes the NameNode to insert a record into the EditLog indicating this. Similarly, changing the replication factor of a file causes a new record to be inserted into the EditLog. The NameNode uses a file in its local host OS file system to store the EditLog. The entire file system namespace, including the mapping of blocks to files and file system properties, is stored in a file called the FsImage. The FsImage is stored as a file in the NameNode’s local file system too. The NameNode keeps an image of the entire file system namespace and file Blockmap in memory. This key metadata item is designed to be compact, such that a NameNode with 4 GB of RAM is plenty to support a huge number of files and directories. When the NameNode starts up, it reads the FsImage and EditLog from disk, applies all the transactions from the EditLog to the in-memory representation of the FsImage, and flushes out this new version into a new FsImage on disk. It can then truncate the old EditLog because its transactions have been applied to the persistent FsImage. This process is called a checkpoint. In the current implementation, a checkpoint only occurs when the NameNode starts up. Work is in progress to support periodic checkpointing in the near future. The DataNode stores HDFS data in files in its local file system. The DataNode has no knowledge about HDFS files. It stores each block of HDFS data in a separate file in its local file system. The DataNode does not create all files in the same directory. Instead, it uses a heuristic to determine the optimal number of files per directory and creates subdirectories appropriately. It is not optimal to create all local files in the same directory because the local file system might not be able to efficiently support a huge number of files in a single directory. When a DataNode starts up, it scans through its local file system, generates a list of all HDFS data blocks that correspond to each of these local files and sends this report to the NameNode: this is the Blockreport.</li>
<li>The Communication Protocols
All HDFS communication protocols are layered on top of the TCP/IP protocol. A client establishes a connection to a configurable TCP port on the NameNode machine. It talks the ClientProtocol with the NameNode. The DataNodes talk to the NameNode using the DataNode Protocol. A Remote Procedure Call (RPC) abstraction wraps both the Client Protocol and the DataNode Protocol. By design, the NameNode never initiates any RPCs. Instead, it only responds to RPC requests issued by DataNodes or clients.</li>
<li>Robustness
The primary objective of HDFS is to store data reliably even in the presence of failures. The
Page 9
Copyright © 2008 The Apache Software Foundation. All rights reserved.
HDFS Architecture
three common types of failures are NameNode failures, DataNode failures and network partitions.
8.1. Data Disk Failure, Heartbeats and Re-Replication
Each DataNode sends a Heartbeat message to the NameNode periodically. A network partition can cause a subset of DataNodes to lose connectivity with the NameNode. The NameNode detects this condition by the absence of a Heartbeat message. The NameNode marks DataNodes without recent Heartbeats as dead and does not forward any new IO requests to them. Any data that was registered to a dead DataNode is not available to HDFS any more. DataNode death may cause the replication factor of some blocks to fall below their specified value. The NameNode constantly tracks which blocks need to be replicated and initiates replication whenever necessary. The necessity for re-replication may arise due to many reasons: a DataNode may become unavailable, a replica may become corrupted, a hard disk on a DataNode may fail, or the replication factor of a file may be increased.
8.2. Cluster Rebalancing
The HDFS architecture is compatible with data rebalancing schemes. A scheme might automatically move data from one DataNode to another if the free space on a DataNode falls below a certain threshold. In the event of a sudden high demand for a particular file, a scheme might dynamically create additional replicas and rebalance other data in the cluster. These types of data rebalancing schemes are not yet implemented.
8.3. Data Integrity
It is possible that a block of data fetched from a DataNode arrives corrupted. This corruption can occur because of faults in a storage device, network faults, or buggy software. The HDFS client software implements checksum checking on the contents of HDFS files. When a client creates an HDFS file, it computes a checksum of each block of the file and stores these checksums in a separate hidden file in the same HDFS namespace. When a client retrieves file contents it verifies that the data it received from each DataNode matches the checksum stored in the associated checksum file. If not, then the client can opt to retrieve that block from another DataNode that has a replica of that block.
8.4. Metadata Disk Failure
The FsImage and the EditLog are central data structures of HDFS. A corruption of these files can cause the HDFS instance to be non-functional. For this reason, the NameNode can be configured to support maintaining multiple copies of the FsImage and EditLog. Any update
Page 10
Copyright © 2008 The Apache Software Foundation. All rights reserved.
HDFS Architecture
to either the FsImage or EditLog causes each of the FsImages and EditLogs to get updated synchronously. This synchronous updating of multiple copies of the FsImage and EditLog may degrade the rate of namespace transactions per second that a NameNode can support. However, this degradation is acceptable because even though HDFS applications are very data intensive in nature, they are not metadata intensive. When a NameNode restarts, it selects the latest consistent FsImage and EditLog to use. The NameNode machine is a single point of failure for an HDFS cluster. If the NameNode machine fails, manual intervention is necessary. Currently, automatic restart and failover of the NameNode software to another machine is not supported.
8.5. Snapshots
Snapshots support storing a copy of data at a particular instant of time. One usage of the snapshot feature may be to roll back a corrupted HDFS instance to a previously known good point in time. HDFS does not currently support snapshots but will in a future release.</li>
<li>Data Organization
9.1. Data Blocks
HDFS is designed to support very large files. Applications that are compatible with HDFS are those that deal with large data sets. These applications write their data only once but they read it one or more times and require these reads to be satisfied at streaming speeds. HDFS supports write-once-read-many semantics on files. A typical block size used by HDFS is 64 MB. Thus, an HDFS file is chopped up into 64 MB chunks, and if possible, each chunk will reside on a different DataNode.
9.2. Staging
A client request to create a file does not reach the NameNode immediately. In fact, initially the HDFS client caches the file data into a temporary local file. Application writes are transparently redirected to this temporary local file. When the local file accumulates data worth over one HDFS block size, the client contacts the NameNode. The NameNode inserts the file name into the file system hierarchy and allocates a data block for it. The NameNode responds to the client request with the identity of the DataNode and the destination data block. Then the client flushes the block of data from the local temporary file to the specified DataNode. When a file is closed, the remaining un-flushed data in the temporary local file is transferred to the DataNode. The client then tells the NameNode that the file is closed. At this point, the NameNode commits the file creation operation into a persistent store. If the
Page 11
Copyright © 2008 The Apache Software Foundation. All rights reserved.
HDFS Architecture
NameNode dies before the file is closed, the file is lost. The above approach has been adopted after careful consideration of target applications that run on HDFS. These applications need streaming writes to files. If a client writes to a remote file directly without any client side buffering, the network speed and the congestion in the network impacts throughput considerably. This approach is not without precedent. Earlier distributed file systems, e.g. AFS, have used client side caching to improve performance. A POSIX requirement has been relaxed to achieve higher performance of data uploads.
9.3. Replication Pipelining
When a client is writing data to an HDFS file, its data is first written to a local file as explained in the previous section. Suppose the HDFS file has a replication factor of three. When the local file accumulates a full block of user data, the client retrieves a list of DataNodes from the NameNode. This list contains the DataNodes that will host a replica of that block. The client then flushes the data block to the first DataNode. The first DataNode starts receiving the data in small portions (4 KB), writes each portion to its local repository and transfers that portion to the second DataNode in the list. The second DataNode, in turn starts receiving each portion of the data block, writes that portion to its repository and then flushes that portion to the third DataNode. Finally, the third DataNode writes the data to its local repository. Thus, a DataNode can be receiving data from the previous one in the pipeline and at the same time forwarding data to the next one in the pipeline. Thus, the data is pipelined from one DataNode to the next.</li>
<li>Accessibility
HDFS can be accessed from applications in many different ways. Natively, HDFS provides a FileSystem Java API for applications to use. A C language wrapper for this Java API is also available. In addition, an HTTP browser can also be used to browse the files of an HDFS instance. Work is in progress to expose HDFS through the WebDAV protocol.
10.1. FS Shell
HDFS allows user data to be organized in the form of files and directories. It provides a commandline interface called FS shell that lets a user interact with the data in HDFS. The syntax of this command set is similar to other shells (e.g. bash, csh) that users are already familiar with. Here are some sample action/command pairs:
Action Create a directory named /foodir Command bin/hadoop dfs -mkdir /foodir
Page 12
Copyright © 2008 The Apache Software Foundation. All rights reserved.
HDFS Architecture
Remove a directory named /foodir View the contents of a file named /foodir/myfile.txt
bin/hadoop dfs -rmr /foodir bin/hadoop dfs -cat /foodir/myfile.txt
FS shell is targeted for applications that need a scripting language to interact with the stored data.
10.2. DFSAdmin
The DFSAdmin command set is used for administering an HDFS cluster. These are commands that are used only by an HDFS administrator. Here are some sample action/command pairs:
Action Put the cluster in Safemode Generate a list of DataNodes Recommission or decommission DataNode(s) Command bin/hadoop dfsadmin -safemode enter bin/hadoop dfsadmin -report bin/hadoop dfsadmin -refreshNodes
10.3. Browser Interface
A typical HDFS install configures a web server to expose the HDFS namespace through a configurable TCP port. This allows a user to navigate the HDFS namespace and view the contents of its files using a web browser.</li>
<li>Space Reclamation
11.1. File Deletes and Undeletes
When a file is deleted by a user or an application, it is not immediately removed from HDFS. Instead, HDFS first renames it to a file in the /trash directory. The file can be restored quickly as long as it remains in /trash. A file remains in /trash for a configurable amount of time. After the expiry of its life in /trash, the NameNode deletes the file from the HDFS namespace. The deletion of a file causes the blocks associated with the file to be freed. Note that there could be an appreciable time delay between the time a file is deleted by a user and the time of the corresponding increase in free space in HDFS. A user can Undelete a file after deleting it as long as it remains in the /trash directory. If a user wants to undelete a file that he/she has deleted, he/she can navigate the /trash directory and retrieve the file. The /trash directory contains only the latest copy of the file
Page 13
Copyright © 2008 The Apache Software Foundation. All rights reserved.
HDFS Architecture
that was deleted. The /trash directory is just like any other directory with one special feature: HDFS applies specified policies to automatically delete files from this directory. The current default policy is to delete files from /trash that are more than 6 hours old. In the future, this policy will be configurable through a well defined interface.
11.2. Decrease Replication Factor
When the replication factor of a file is reduced, the NameNode selects excess replicas that can be deleted. The next Heartbeat transfers this information to the DataNode. The DataNode then removes the corresponding blocks and the corresponding free space appears in the cluster. Once again, there might be a time delay between the completion of the setReplication API call and the appearance of free space in the cluster.</li>
<li>References
Hadoop JavaDoc API. HDFS source code: <a href="http://hadoop.apache.org/core/version_control.html" target="_blank">http://hadoop.apache.org/core/version_control.html</a>
Page 14
Copyright © 2008 The Apache Software Foundation. All rights reserved.</li>
</ol>

      
    </div>
    
    
      
    
    
        <footer id="post-meta">
        <span class="categories">Posted in<span class="breadcrumb fa fa-folder"><li><a href="/categories/hadoop/">hadoop</a></li></span></span> | <span class="tags">Tagged <a href="/tags/hadoop/" class="label label-primary">hadoop</a></span> | <span class="time">recent updated:<time title="2014-03-07 09:54:36"datetime="2014-03-07 09:54:36"> mar. 7 2014</time></span> | <span class="comment-link">
<a href="http://itsolife.com/2014/02/02/2014-02-02-hadoop--hdfs_design/#comments" class="ds-thread-count comment-link" data-thread-key="2014-02-02-hadoop--hdfs_design" data-count-type="comments">暂无评论</a></span>
        </footer>
    
    <div class="clearfix"></div>
  </div>
</div>
</article>





<ul id="pagination" class="pagination pagination-lg">
  <li><a class="extend prev" href="/page/114/">&laquo;</a></li><li><a class="page-number" href="/">1</a></li><li><a class="page-number" href="/page/2/">2</a></li><li><span class="space">&hellip;</span></li><li><a class="page-number" href="/page/112/">112</a></li><li><a class="page-number" href="/page/113/">113</a></li><li><a class="page-number" href="/page/114/">114</a></li><li class="active"><li><span class="page-number current">115</span></li><li><a class="page-number" href="/page/116/">116</a></li><li><a class="page-number" href="/page/117/">117</a></li><li><a class="page-number" href="/page/118/">118</a></li><li><span class="space">&hellip;</span></li></li><li><a class="page-number" href="/page/164/">164</a></li><li><a class="page-number" href="/page/165/">165</a></li><li><a class="extend next" href="/page/116/">&raquo;</a></li>
  <div class="clearfix"></div>
</ul></div><!--wapper-->
       </div><!-- ID main-col END -->
       <aside id="sidebar" class="alignright col-sx-6 col-sm-4 col-md-3 col-lg-3">
<div id="widget_search" class="widget panel panel-primary">
    <form action="//google.com/search" method="get" accept-charset="utf-8">
  <div class="input-group">
    <input class="form-control" id="searchbox" type="search" name="q" results="0" placeholder="search">
    <span class="input-group-btn">
      <button class="btn btn-default" type="submit">Go!</button>
    </span>
    <input type="hidden" name="q" value="site:itsolife.com">
  </div>
</form>
</div>

<div id="widget_category" class="widget panel panel-primary">
  <div class="panel-heading">category</div>  <div data-src='category' class='ajax_widgets'>正在加载...</div>
</div>

<div id="widget_recent_posts" class="widget panel panel-primary">
  <div class="panel-heading">recent posts</div>  <div data-src='recent_posts' class='ajax_widgets'>正在加载...</div></div>

<div id="widget_tagcloud" class="widget panel panel-primary">
  <div class="panel-heading">tagcloud</div>  <div data-src='tagcloud' class='ajax_widgets'>正在加载...</div></div>

<div id="widget_latest_update_posts" class="widget panel panel-primary">
  <div class="panel-heading">最近更新</div>  <div data-src='latest_update_posts' class='ajax_widgets'>正在加载...</div></div>

<div id="widget_recent_comments" class="widget panel panel-primary">
  <div class="panel-heading">recent comments</div>  

<div class="list-group-item ds-recent-comments" data-num-items="6" data-show-avatars="1" data-show-time="1" data-show-title="1" data-show-admin="1" data-excerpt-length="50"></div>



</div>

</aside>
       <div class="clearfix"></div>
     </div><!-- row END -->
  </div>
  <footer id="footer" class="container">
  <div class="panel panel-info">
  <section id='footer_widget'></section>  <div class="panel-footer">
  <div id="site-info">
    <span class='author'>
  
  &copy; 2014 RobinChia
  
    &nbsp;&nbsp;</span>
  
  <span id='analytics-51la'></span><span id='analytics-google'>
<script type="text/javascript">
  var _gaq = _gaq || [];
  _gaq.push(['_setAccount', 'UA-48559895-1']);
  _gaq.push(['_trackPageview']);
  _js2load.push({src:('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js'});
</script></span><span id='analytics-cnzz'>
<script type="text/javascript">var cnzz_protocol = (("https:" == document.location.protocol) ? " https://" : " http://");document.write(unescape("%3Cspan id='cnzz_stat_icon_5774006'%3E%3C/span%3E%3Cscript src='" + cnzz_protocol + "s17.cnzz.com/stat.php%3Fid%3D5774006%26show%3Dpic' type='text/javascript'%3E%3C/script%3E"));</script>
</span><span id='analytics-baidu'>
<script>
var _hmt = _hmt || [];
_js2load.push({src:"//hm.baidu.com/hm.js?1442f50724afc42380b51f097c43082c"});
</script>
</span>  </div>
  <div id="copyright">Blog powered by <a href='http://zespia.tw/hexo/'><strong>hexo</strong></a> Theme <strong><a href='https://github.com/chenall/hexo-theme-chenall'>chenall</a></strong>(Some change in it)<span class="pull-right"> 更新时间: <em>2014-03-15 20:45:44</em></span></div>
</div>
<div class="clearfix"></div>


  </div>
  </footer>
  
        <script src="http://cdn.bootcss.com/jquery/1.10.2/jquery.min.js"></script>        
        <script src="http://cdn.staticfile.org/twitter-bootstrap/3.1.0/js/bootstrap.min.js"></script>        
                
        <script src="http://cdn.bootcss.com/prettify/r298/prettify.min.js"></script>    <script type="text/javascript">
   var lang=["bsh", "c", "cc", "cpp", "cs", "csh", "cyc", "cv", "htm", "html",
    "java", "js", "m", "mxml", "perl", "pl", "pm", "py", "rb", "sh",
    "xhtml", "xml", "xsl"];
   var pretty_base='';
   $('script').each(function(){
	var c = $(this).attr('src');
	if (!c)
	    return;
	if (c.match(/(\/)?prettify(\.min)?\.js/i))
	{
	    var index = c.lastIndexOf('/');
	    if (index != -1)
		pretty_base = c.substr(0,index + 1);
	    return false;
	}
   })
   $('pre code').each(function(){
	var c = $(this).attr('class')
	if (!c)
	    return;
	c = c.match(/\s?(lang\-\w+)/i);
	if (c && lang.indexOf(c[1]) == -1)
	{
	    lang.push(c[1]);
	    $.getScript(pretty_base + c[1] + '.min.js');
	}
   })

    $(window).load(function(){
       $("pre").addClass("prettyprint");
       prettyPrint();
    })
</script>    
            <script type="text/javascript">
var duoshuoQuery = {short_name:"robinchia"};
_js2load.push({src:'http://static.duoshuo.com/embed.js',charset:'UTF-8'});
</script>
    
            <!--wumii_relatedItems-->
    
        <script src="http://cdn.bootcss.com/fancybox/2.1.5/jquery.fancybox.min.js"></script>    <script type="text/javascript">
(function($){
  $('.entry').each(function(i){
    $(this).find('img').each(function(){
      var alt = this.alt;

      if (alt){
        $(this).before('<span class="caption">' + alt + '</span>');
      }

      $(this).wrap('<a href="' + this.src + '" title="' + alt + '" class="fancybox" rel="fancybox' + i + '" />');
    });
  });

  $('.fancybox').fancybox();
})(jQuery);
</script>    
        <script src="http://cdn.bootcss.com/mathjax/2.3/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>    <script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      processEscapes: true
    }
  });
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>
    
<script type="text/javascript">
$('.ajax_widgets').each(function(){var src=$(this).attr('data-src');if(src)$(this).load('/widgets/'+src+'.html');});
$.each(_js2load,function(index,obj){loadjs(obj.src,obj.charset)});
</script>

<div id="scroll2top">
<img src="/scroll2top/arrow.png"/>
</div>
<script src="/scroll2top/scroll2top.min.js"></script>
<div id="winterland">
  <canvas></canvas>
</div>
<script src="/js/winterland.min.js"></script>

  </body>
</html>
