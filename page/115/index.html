
<!DOCTYPE HTML>
<html>
<head>
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<title>The 115 Page | It so life</title>
<meta name="author" content="RobinChia">

<meta name="description" content="It so life">


<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">


<meta property="og:site_name" content="It so life"/>

<!--[if IE]><style>.testIE.IE{display:inline;}</style><![endif]-->
<!--[if lte IE 7]><link rel="stylesheet" href="/css/ie7.css" type="text/css"><![endif]-->
<!--[if (lt IE 9)&(gt IE 7)]><style>.testIE.IE8{display:inline;}</style><![endif]-->
<!--[if gt IE 8]><style>.testIE.IE9{display:inline;}</style><![endif]-->

<link href="/favicon.png" rel="icon">
<link rel="alternate" href="/atom.xml" title="It so life Feed" type="application/atom+xml">

<link rel="stylesheet" href="http://cdn.staticfile.org/twitter-bootstrap/3.1.0/css/bootstrap.min.css" type="text/css">
<link rel="stylesheet" href="http://cdn.bootcss.com/font-awesome/4.0.3/css/font-awesome.min.css" type="text/css">
<link rel="stylesheet" href="http://cdn.bootcss.com/prettify/r298/prettify.min.css" type="text/css">
<link rel="stylesheet" href="http://cdn.bootcss.com/fancybox/2.1.5/jquery.fancybox.min.css" type="text/css">
<link rel="stylesheet" href="/css/style.css" type="text/css">
<!--[if lt IE 9]>
   <style>article,aside,dialog,footer,header,section,footer,nav,figure,menu{display:block}</style>
   <script src="http://cdn.staticfile.org/html5shiv/r29/html5.min.js"></script>
   <script src="http://cdn.staticfile.org/respond.js/1.4.2/respond.min.js"></script>
<link href="http://cdn.staticfile.org/respond.js/1.4.2/respond-proxy.html" id="respond-proxy" rel="respond-proxy" />
<link href="/js/respond.proxy.gif" id="respond-redirect" rel="respond-redirect" />
<script src="/js/respond.proxy.js"></script>
   <script src="http://cdn.bootcss.com/selectivizr/1.0.2/selectivizr-min.js"></script>
<![endif]-->
<script type="text/javascript">
function loadjs(c,d){var a=document.createElement("script");a.async=!0;a.type="text/javascript";a.src=c;a.charset=d||"gbk";(document.getElementsByTagName("HEAD")[0]||document.getElementsByTagName("BODY")[0]).appendChild(a)};
var _protocol = (("https:" == document.location.protocol) ? " https://" : " http://");
var _js2load = [];
</script>

</head>
<body>
      <header id="header" class="container"><nav id="main-nav" class="navbar navbar-default navbar-fixed-top " role="navigation">
  <div class="container">
    <div class="navbar-header">
    <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".navbar-collapse">
      <span class="sr-only">Toggle navigation</span>
      <span class="icon-bar"></span>
      <span class="icon-bar"></span>
      <span class="icon-bar"></span>
    </button>
    <a class="navbar-brand" href="/">It so life</a>
    </div>
    <div  class="collapse navbar-collapse">
      <ul  class="nav navbar-nav">
  
        <li><a href="/" title="Home">Home</a></li>      
        <li><a href="/about/" title="About">About</a></li>      
        <li><a href="/archives/" title="Archives">Archives</a></li>      
      <li class='dropdown'>
        <a class='dropdown-toggle' data-toggle='dropdown' href='#'>website<b class='caret'></b></a>
        <ul class='dropdown-menu pure-menu-selected'>
    
          <li><a href="//groups.google.com/forum/#!forum/pongba" title="TopLanguage">TopLanguage</a></li>    
          <li><a href="//itpub.net/" title="ITPub">ITPub</a></li>    
          <li><a href="//blog.jobbole.com/" title="Bole">Bole</a></li>    
          <li><a href="//nosql-database.org/" title="nosql">nosql</a></li>    
          <li><a href="//gitimmersion.googol.im/" title="Git">Git</a></li>    
        </ul>
      </li>
    
      </ul>
      <ul class="nav navbar-nav navbar-right">
      
        <li><a href="/atom.xml">RSS</a></li>
      
      
        <li><a href="https://twitter.com/robinchia">twitter</a></li>
      
      
      
      
        <li><a href="https://github.com/robinchia">github</a></li>
      
      </ul>
    </div>
  </div>
</nav>
<div class="clearfix"></div>
</header>
  <div id='content' class="container">
     <div class="page-header-wrapper">
      <!--[if lt IE 9]><div class="alert alert-warning alert-dismissable"><button type="button" class="close" data-dismiss="alert" aria-hidden="true">&times;</button><strong>提示:</strong>您的浏览器版本太低了,建议升级到 <strong><a href="http://windows.microsoft.com/zh-cn/internet-explorer/download-ie" title="IE9">IE9</a></strong> 以上,本站使用<a href="https://www.google.com/intl/zh-CN/chrome/">Chrome浏览器</a>可以获得最好的显示效果.</div><![endif]-->
      <div class="page-header"><h1 align="center"><big>It so life</big> </h1>
        <h5 align="center"><big>love as life</big></h5>
      </div>
     </div>
     <div class="row">
       <div id="main-col" class="alignleft col-sx-12 col-sm-8 col-md-9 col-lg-9">
      <section id='header_widget'></section>
          <div id="wrapper"><article>
<div id="post" class="post well">
  <div class="post-content">
    <header class="well-sm">
      <i class="fa icon fa-5x pull-left"></i>
      <h1 class="title"><a href="/2014/02/02/2014-02-02-hadoop--CentOS的Hadoop集群配置/">CentOS的Hadoop集群配置</a></h1>
      
        <span>Posted on<time datetime="2014-02-02T01:54:36.000Z"> <a href="/2014/02/02/2014-02-02-hadoop--CentOS的Hadoop集群配置/">feb. 2 2014</a></time></span>      
    </header>
    
    
    <div class="entry">
      
        <h1 id="centos-hadoop-">CentOS的Hadoop集群配置</h1>
<h3 id="-centos-hadoop-http-blog-csdn-net-inte_sleeper-article-details-6569985-"><a href="http://blog.csdn.net/inte_sleeper/article/details/6569985" target="_blank">CentOS的Hadoop集群配置（一）</a></h3>
<h3 id="-"> </h3>
<p>参考资料：</p>
<p><a href="http://www.michael-noll.com/tutorials/running-hadoop-on-ubuntu-linux-single-node-cluster/" target="_blank"><a href="http://www.michael-noll.com/tutorials/running-hadoop-on-ubuntu-linux-single-node-cluster/">http://www.michael-noll.com/tutorials/running-hadoop-on-ubuntu-linux-single-node-cluster/</a></a></p>
<p><a href="http://www.michael-noll.com/tutorials/running-hadoop-on-ubuntu-linux-multi-node-cluster/" target="_blank"><a href="http://www.michael-noll.com/tutorials/running-hadoop-on-ubuntu-linux-multi-node-cluster/">http://www.michael-noll.com/tutorials/running-hadoop-on-ubuntu-linux-multi-node-cluster/</a></a></p>
<p><a href="http://www.michael-noll.com/tutorials/writing-an-hadoop-mapreduce-program-in-python/" target="_blank"><a href="http://www.michael-noll.com/tutorials/writing-an-hadoop-mapreduce-program-in-python/">http://www.michael-noll.com/tutorials/writing-an-hadoop-mapreduce-program-in-python/</a></a></p>
<p><a href="http://hadoop.apache.org/common/docs/current/cluster_setup.html" target="_blank"><a href="http://hadoop.apache.org/common/docs/current/cluster_setup.html">http://hadoop.apache.org/common/docs/current/cluster_setup.html</a></a></p>
<p>以下集群配置内容，以两台机器为例。其中一台是 master ，另一台是 slave1 。</p>
<p>master 上运行 name node, data node, task tracker, job tracker ， secondary name node ；</p>
<p>slave1 上运行 data node, task tracker 。</p>
<p>前面加 /* 表示对两台机器采取相同的操作</p>
<ol>
<li>安装 JDK /*</li>
</ol>
<p>yum install java-1.6.0-openjdk-devel</p>
<ol>
<li>设置环境变量 /*</li>
</ol>
<p>编辑 /etc/profile 文件，设置 JAVA_HOME 环境变量以及类路径：</p>
<p>export JAVA_HOME=&quot;/usr/lib/jvm/java-1.6.0-openjdk-1.6.0.0.x86_64&quot;</p>
<p>export PATH=$PATH:$JAVA_HOME/bin</p>
<p>export CLASSPATH=.:$JAVA_HOME/lib/tools.jar:$JAVA_HOME/lib/dt.jar</p>
<ol>
<li>添加 hosts 的映射 /*</li>
</ol>
<p>编辑 /etc/hosts 文件，<strong>注意</strong> <strong>host name **</strong>不要有下划线，见下步骤 9**</p>
<p>192.168.225.16 master</p>
<p>192.168.225.66 slave1</p>
<ol>
<li>配置 SSH /*</li>
</ol>
<p>cd /root &amp; mkdir .ssh</p>
<p>chmod 700 .ssh &amp; cd .ssh</p>
<p>创建密码为空的 RSA 密钥对：</p>
<p>ssh-keygen -t rsa -P &quot;&quot;</p>
<p>在提示的对称密钥名称中输入 id_rsa</p>
<p>将公钥添加至 authorized_keys 中：</p>
<p>cat id_rsa.pub &gt;&gt; authorized_keys</p>
<p>chmod 644 authorized_keys <strong>/#</strong> <strong>重要</strong></p>
<p>编辑 sshd 配置文件 /etc/ssh/sshd_config ，把 /#AuthorizedKeysFile  .ssh/authorized_keys 前面的注释取消掉。</p>
<p>重启 sshd 服务：</p>
<p>service sshd restart</p>
<p>测试 SSH 连接。连接时会提示是否连接，按回车后会将此公钥加入至 knows_hosts 中：</p>
<p>ssh localhost</p>
<ol>
<li>配置 master 和 slave1 的 ssh 互通</li>
</ol>
<p>在 slave1 中重复步骤 4 ，然后把 slave1 中的 .ssh/authorized_keys 复制至 master 的 .ssh/authorized_keys中。注意复制过去之后，要看最后的类似 root@localhost 的字符串，修改成 root@slave1 。同样将 master的 key 也复制至 slave1 ，并将最后的串修改成 root@master 。</p>
<p>或者使用如下命令：</p>
<p>ssh-copy-id -i ~/.ssh/id_rsa.pub root@slave1</p>
<p>测试 SSH 连接：</p>
<p>在 master 上运行：</p>
<p>ssh slave1</p>
<p>在 slave1 上运行：</p>
<p>ssh master</p>
<ol>
<li>安装 Hadoop</li>
</ol>
<p>下载 hadoop 安装包：</p>
<p>wget <a href="http://mirror.bjtu.edu.cn/apache/hadoop/common/hadoop-0.20.203.0/hadoop-0.20.203.0rc1.tar.gz" target="_blank"><a href="http://mirror.bjtu.edu.cn/apache/hadoop/common/hadoop-0.20.203.0/hadoop-0.20.203.0rc1.tar.gz">http://mirror.bjtu.edu.cn/apache/hadoop/common/hadoop-0.20.203.0/hadoop-0.20.203.0rc1.tar.gz</a></a></p>
<p>复制安装包至 slave1 ：</p>
<p>scp hadoop-0.20.203.0rc1.tar.gz root@slave1:/root/</p>
<p>解压：</p>
<p>tar xzvf hadoop-0.20.203.0rc1.tar.gz</p>
<p>mkdir /usr/local/hadoop</p>
<p>mv hadoop-0.20.203.0//* /usr/local/hadoop</p>
<pre><code>     修改 .bashrc 文件（位于用户目录下，即 ~/.bashrc ，对于 root ，即为 /root/.bashrc ）

     添加环境变量：

     export HADOOP_HOME=/usr/local/hadoop

export PATH=$PATH:$HADOOP_HOME/bin
</code></pre><ol>
<li>配置 Hadoop 环境变量 /*</li>
</ol>
<p><strong>以下所有 hadoop **</strong>目录下的文件，均以相对路径 hadoop <strong>**开始</strong></p>
<p>修改 hadoop/conf/hadoop-env.sh 文件，将里面的 JAVA_HOME 改成步骤 2 中设置的值。</p>
<ol>
<li>创建 Hadoop 本地临时文件夹 /*</li>
</ol>
<p>mkdir /root/hadoop_tmp （<strong>注意这一步，千万不要放在</strong> <strong>/tmp **</strong>目录下面！！因为 <strong><strong>/tmp </strong></strong>默认分配的空间是很小的，往 <strong><strong>hdfs </strong></strong>里放几个大文件就会导致空间满了，就会报错）**</p>
<p>修改权限：</p>
<p>chown -R hadoop:hadoop /root/hadoop_tmp</p>
<p>更松地，也可以这样：</p>
<p>chmod –R 777 /root/hadoop_tmp</p>
<ol>
<li>配置 Hadoop</li>
</ol>
<p>修改 master 的 hadoop/conf/core-site.xml ，在 <configuration> 节中添加如下内容：</p>
<p>注意： <strong>fs.default.name **</strong>的值不能带下划线**</p>
<property>

    <name>hadoop.tmp.dir</name>

    <value>/root/hadoop<em>tmp/hadoop</em>${user.name}</value>

</property>

<property>

    <name>fs.default.name</name>

    <value>hdfs://localhost:54310</value> 

</property>

<property>

    <name>io.sort.mb</name>

    <value>1024</value> 

</property>

<pre><code>     其中 io.sort.mb 值，指定了排序使用的内存，大的内存可以加快 job 的处理速度。



     修改 hadoop/conf/mapred-site.xml ，在 &lt;configuration&gt; 节中添加如下内容：
</code></pre><property>

    <name>mapred.job.tracker</name>

    <value>localhost:54311</value>

</property>

<property>

    <name>mapred.map.child.java.opts</name>

    <value>-Xmx4096m</value>

</property>

<property>

    <name>mapred.reduce.child.java.opts</name>

    <value>-Xmx4096m</value>

</property>

<pre><code>     其中 mapred.map.child.java.opts, mapred.reduce.child.java.opts 分别指定 map/reduce 任务使用的最大堆内存。较小的内存可能导致程序抛出 OutOfMemoryException 。
</code></pre><p>修改 conf/hdfs -site.xml ，在 <configuration> 节中添加如下内容：</p>
<property>

    <name>dfs.replication</name>

    <value>2</value>

</property>



<p>同样，修改 slave1 的 /usr/local/hadoop/conf/core-site.xml ，在 <configuration> 节中添加如下内容：</p>
<property>

    <name>hadoop.tmp.dir</name>

    <value>/root/hadoop<em>tmp/hadoop</em>${user.name}</value>

</property>

<property>

    <name>fs.default.name</name>

    <value>hdfs://localhost:54310</value> 

</property>

<property>

    <name>io.sort.mb</name>

    <value>1024</value> 

</property>



<pre><code>     修改 conf/mapred-site.xml ，在 &lt;configuration&gt; 节中添加如下内容：
</code></pre><property>

    <name>mapred.job.tracker</name>

    <value>localhost:54311</value>

</property>

<property>

    <name>mapred.map.child.java.opts</name>

    <value>-Xmx4096m</value>

</property>

<property>

    <name>mapred.reduce.child.java.opts</name>

    <value>-Xmx4096m</value>

</property>



<pre><code>     修改 conf/hdfs -site.xml ，在 &lt;configuration&gt; 节中添加如下内容：
</code></pre><property>

    <name>dfs.replication</name>

    <value>2</value>

    </property>



<ol>
<li>修改 hadoop/bin/hadoop 文件</li>
</ol>
<p>把 221 行修改成如下。因为对于 root 用户， -jvm 参数是有问题的，所以需要加一个判断 ( 或者以非 root 用户运行这个脚本也没问题 )</p>
<p>HADOOP_OPTS=&quot;$HADOOP_OPTS -jvm server $HADOOP_DATANODE_OPTS&quot;  à</p>
<pre><code>/#for root, -jvm option is invalid.

CUR_USER=`whoami`

if [ &quot;$CUR_USER&quot; = &quot;root&quot; ]; then

    HADOOP_OPTS=&quot;$HADOOP_OPTS -server $HADOOP_DATANODE_OPTS&quot;

else

    HADOOP_OPTS=&quot;$HADOOP_OPTS -jvm server $HADOOP_DATANODE_OPTS&quot;

fi 
</code></pre><p>unset $CUR_USER</p>
<p>至此， master 和 slave1 都已经完成了 single_node 的搭建，可以分别在两台机器上测试单节点。</p>
<p>启动节点：</p>
<p>hadoop/bin/start-all.sh</p>
<p>运行 jps 命令，应能看到类似如下的输出：</p>
<p>937 DataNode</p>
<p>9232 Jps</p>
<p>8811 NameNode</p>
<p>12033 JobTracker</p>
<p>12041 TaskTracker
来源： <a href="[http://blog.csdn.net/inte_sleeper/article/details/6569985](http://blog.csdn.net/inte_sleeper/article/details/6569985)">[http://blog.csdn.net/inte_sleeper/article/details/6569985](http://blog.csdn.net/inte_sleeper/article/details/6569985)</a> </p>
<p><a href="http://blog.csdn.net/inte_sleeper/article/details/6569990" target="_blank">CentOS的Hadoop集群配置（二）</a>
下面的教程把它们合并至 multi-node cluster 。</p>
<ol>
<li>合并 single-node 至 multi-node cluster</li>
</ol>
<p>修改 master 的 hadoop/conf/core-site.xml ：</p>
<property>

    <name>hadoop.tmp.dir</name>

    <value>/root/hadoop<em>tmp/hadoop</em>${user.name}</value>

</property>

<property>

    <name>fs.default.name</name>

    <value>hdfs://<strong>master</strong> :54310</value>

</property>

<property>

    <name>io.sort.mb</name>

    <value>1024</value> 

</property>



<pre><code>     修改 conf/mapred-site.xml ，在 &lt;configuration&gt; 节中添加如下内容：
</code></pre><property>

    <name>mapred.job.tracker</name>

    <value><strong>master</strong> :54311</value>

</property>

<property>

    <name>mapred.map.child.java.opts</name>

    <value>-Xmx4096m</value>

</property>

<property>

    <name>mapred.reduce.child.java.opts</name>

    <value>-Xmx4096m</value>

</property>

<pre><code>     修改 conf/hdfs -site.xml ，在 &lt;configuration&gt; 节中添加如下内容：
</code></pre><property>

    <name>dfs.replication</name>

    <value>2</value>

</property>

<p>把这三个文件复制至 slave1 相应的目录 hadoop/conf 中 <strong>( **</strong>即 master <strong><strong>和 slave1 </strong></strong>的内容完全一致 )**</p>
<pre><code>     修改所有节点的 hadoop/conf/masters ，把文件内容改成： master

     修改所有节点的 hadoop/conf/slaves ，把文件内容改成：
</code></pre><p>master</p>
<pre><code>slave1



     分别删除 master 和 slave1 的 dfs/data 文件：

     rm –rf /root/hadoop_tmp/hadoop_root/dfs/data
</code></pre><p>重新格式化 namenode ：</p>
<pre><code>     hadoop/bin/hadoop namenode -format



     测试，在 master 上运行：

     hadoop/bin/start-all.sh

     在 master 上运行 jps 命令
</code></pre><p>此时输出应类似于：</p>
<pre><code>     11648 TaskTracker
</code></pre><p>11166 NameNode</p>
<p>11433 SecondaryNameNode</p>
<p>12552 Jps</p>
<p>11282 DataNode</p>
<p>11525 JobTracker</p>
<p>在 slave1 上运行 jps</p>
<p>此时输出应包含 ( 即至少有 DataNode, 否则即为出错 ) ：</p>
<p>3950 Jps</p>
<p>3121 TaskTracker</p>
<p>3044 DataNode</p>
<ol>
<li>测试一个 JOB</li>
</ol>
<p>首先升级 python( 可选，如果 JOB 是 python 写的 ) ：</p>
<p>cd /etc/yum.repos.d/</p>
<p>wget <a href="http://mirrors.geekymedia.com/centos/geekymedia.repo" target="_blank">http://mirrors.geekymedia.com/centos/geekymedia.repo</a></p>
<p>yum makecache</p>
<p>yum -y install python26</p>
<p><strong>升级 python **</strong>的教程，见另外一篇文档。如果已经通过以上方法安装了 python2.6 <strong>**，那需要先卸载：</strong></p>
<p>yum remove python26 python26-devel</p>
<pre><code>     CentOS 的 yum 依赖于 python2.4 ，而 /usr/bin 中 python 程序即为 python2.4 。我们需要把它修改成python2.6 。



     cd /usr/bin/

     编辑 yum 文件，把第一行的
</code></pre><p>/#!/usr/bin/python   à   /#!/usr/bin/python2.4  </p>
<p>保存文件。</p>
<pre><code>     删除旧版本的 python 可执行文件（这个文件跟该目录下 python2.4 其实是一样的，所以可以直接删除）

     rm -f python

     让 python 指向 python2.6 的可执行程序。

     ln -s python26 python  
</code></pre><ol>
<li>Word count python 版本</li>
</ol>
<p><strong>Map.py</strong></p>
<p>/#! /usr/bin/python</p>
<p>import sys;</p>
<p>for line in sys.stdin:</p>
<p>  line =  line.strip();</p>
<p>  words = line.split();</p>
<p>  for word in words:</p>
<pre><code>  print &#39;%s/t%s&#39; % (word,1);
</code></pre><p><strong>Reduce.py</strong></p>
<p>/#!/usr/bin/python</p>
<p>import sys;</p>
<p>wc = {};</p>
<p>for line in sys.stdin:</p>
<p>  line = line.strip();</p>
<p>  word,count = line.split(&#39;/t&#39;,1);</p>
<p>  try:</p>
<pre><code>  count = int(count);
</code></pre><p>  except Error:</p>
<pre><code>  pass;
</code></pre><p>  if wc.has_key(word):</p>
<pre><code>  wc[word] += count;
</code></pre><p>  else: wc[word] = count;</p>
<p>for key in wc.keys():</p>
<p>  print &#39;%s/t%s&#39; % (key, wc[key]);</p>
<p>本机测试：</p>
<p>echo &quot;foo foo bar bar foo abc&quot; | map.py</p>
<p>echo &quot;foo foo bar bar foo abc&quot; | map.py | sort | reduce.py</p>
<p>在 hadoop 中测试：</p>
<p>hadoop jar /usr/local/hadoop/contrib/streaming/hadoop-streaming-0.20.203.0.jar -file mapper.py -mapper mapper.py -file reducer.py -reducer reducer.py -input wc//* -output wc-out</p>
<p>Job 成功后，会在 HDFS 中生成 wc-out 目录。</p>
<p>查看结果：</p>
<p>hadoop fs –ls wc-out</p>
<p>hadoop fs –cat wc-out/part-00000</p>
<ol>
<li>集群增加新节点</li>
</ol>
<p>a.       执行步骤 1 ， 2.</p>
<p>b.       修改 hosts 文件，将集群中的 hosts 加入本身 /etc/hosts 中。并修改集群中其他节点的 hosts ，将新节点加入。</p>
<p>c.       master 的 conf/slaves 文件中，添加新节点。</p>
<p>d.       启动 datanode 和 task tracker 。</p>
<p>hadoop-daemon.sh start datanode</p>
<p>hadoop-daemon.sh start tasktracker</p>
<ol>
<li>Trouble-shooting</li>
</ol>
<p>hadoop 的日志在 hadoop/logs 中。</p>
<p>其中， logs 根目录包含的是 namenode, datanode, jobtracker, tasktracker 等的日志。分别以 hadoop-{username}-namenode/datanode/jobtracker/tasktracker-hostname.log 命名。</p>
<p>userlogs 目录里包含了具体的 job 日志，每个 job 有一个单独的目录，以 job<em>YYYYmmddHHmm_xxxx 命名。里面包含数个 attempt</em>{jobname}<em>m_xxxxx 或 attempt</em>{jobname}_r_xxxx 等数个目录。其中目录名中的m 表示 map 任务的日志， r 表示 reduce 任务的日志。因此，出错时，可以有针对性地查看特定的日志。</p>
<p>常见错误：</p>
<ol>
<li>出现类似：</li>
</ol>
<p><em>ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: java.io.IOException: Incompatible namespaceIDs …</em></p>
<p>的异常，是因为先格式化了 namenode ，后来又修改了配置导致。将 dfs/data 文件夹内容删除，再重新格式化 namenode 即可。</p>
<ol>
<li>出现类似：</li>
</ol>
<p><em>INFO org.apache.hadoop.ipc.Client: Retrying connect to server:…</em></p>
<p>的异常，首先确认 name node 是否启动。如果已经启动，有可能是 master 或 slave1 中的配置出错，集群配置参考步骤 11 。也有可能是防火墙问题，需添加以下例外：</p>
<p>50010 端口用于数据传输， 50020 用于 RPC 调用， 50030 是 WEB 版的 JOB 状态监控， 54311 是job tracker ， 54310 是与 master 通信的端口。</p>
<p>完整的端口列表见：</p>
<p><a href="http://www.cloudera.com/blog/2009/08/hadoop-default-ports-quick-reference/" target="_blank"><a href="http://www.cloudera.com/blog/2009/08/hadoop-default-ports-quick-reference/">http://www.cloudera.com/blog/2009/08/hadoop-default-ports-quick-reference/</a></a></p>
<p>iptables -A RH-Firewall-1-INPUT -p tcp -m tcp --dport 50010 -j ACCEPT</p>
<p>iptables -A RH-Firewall-1-INPUT -p tcp -m tcp --dport 50020 -j ACCEPT</p>
<p>iptables -A RH-Firewall-1-INPUT -p tcp -m tcp --dport 50030 -j ACCEPT</p>
<p>iptables -A RH-Firewall-1-INPUT -p tcp -m tcp --dport 50060 -j ACCEPT</p>
<p>iptables -A RH-Firewall-1-INPUT -p tcp -m tcp --dport 54310 -j ACCEPT</p>
<p>iptables -A RH-Firewall-1-INPUT -p tcp -m tcp --dport 54311 -j ACCEPT</p>
<p>iptables -A OUTPUT -p tcp -m tcp --dport 50010 -j ACCEPT</p>
<p>iptables -A OUTPUT -p tcp -m tcp --dport 50020 -j ACCEPT</p>
<p>iptables -A OUTPUT -p tcp -m tcp --sport 50010 -j ACCEPT</p>
<p>iptables -A OUTPUT -p tcp -m tcp --sport 50020 -j ACCEPT</p>
<p>iptables -A OUTPUT -p tcp -m tcp --dport 50030 -j ACCEPT</p>
<p>iptables -A OUTPUT -p tcp -m tcp --sport 50030 -j ACCEPT</p>
<p>iptables -A OUTPUT -p tcp -m tcp --dport 50060 -j ACCEPT</p>
<p>iptables -A OUTPUT -p tcp -m tcp --sport 50060 -j ACCEPT</p>
<p>iptables -A OUTPUT -p tcp -m tcp --dport 54310 -j ACCEPT</p>
<p>iptables -A OUTPUT -p tcp -m tcp --sport 54310 -j ACCEPT</p>
<p>iptables -A OUTPUT -p tcp -m tcp --dport 54311 -j ACCEPT</p>
<p>iptables -A OUTPUT -p tcp -m tcp --sport 54311 -j ACCEPT</p>
<p>保存规则：</p>
<p>/etc/init.d/iptables save</p>
<p>重启 iptables 服务：</p>
<p>service iptables restart</p>
<p>如果还是出现问题 2 的错误，那可能需要手工修改 /etc/sysconfig/iptables 的规则。手动添加这些规则。若有 ”reject-with icmp-host-prohibited” 的规则，需将规则加到它的前面。注意修改配置文件的时候，不需要带 iptables 命令。直接为类似于：</p>
<p>-A OUTPUT -p tcp -m tcp --sport 54311 -j ACCEPT</p>
<p>或关闭防火墙 <strong>( **</strong>建议，因为端口太多，要加的例外很多 )**</p>
<p>service iptables stop</p>
<ol>
<li><p>在 /etc/hosts 文件中，确保一个 host 只对应一个 IP ，否则会出错（如同时将 slave1 指向 127.0.0.1 和192.168.225.66 ），<strong>可能导致数据无法从一个节点复制至另一节点。</strong></p>
</li>
<li><p>出现类似：</p>
</li>
</ol>
<p><em>FATAL org.apache.hadoop.mapred.TaskTracker: Error running child : java.lang.OutOfMemoryError: Java heap space…</em></p>
<p>的异常，是因为堆内存不够。有以下几个地方可以考虑配置：</p>
<p>a.       conf/hadoop-env.sh 中， export HADOOP_HEAPSIZE=1000 这一行，默认为注释掉，堆大小为1000M ，可以取消注释，将这个值调大一些（对于 16G 的内存，可以调至 8G ）。</p>
<p>b.       conf/mapred-site.xml 中，添加 mapred.map.child.java.opts 属性，手动指定 JAVA 堆的参数值为 -Xmx2048m 或更大。这个值调整 map 任务的堆大小。即：</p>
<property>

    <name>mapred.map.child.java.opts </name>

    <value>-Xmx2048m</value>

</property>

<p>c.       conf/mapred-site.xml 中，添加 mapred.reduce.child.java.opts 属性，手动指定 JAVA 堆的参数值为 -Xmx2048m 或更大。这个值调整 reduce 任务的堆大小。即：</p>
<property>

    <name>mapred.reduce.child.java.opts </name>

    <value>-Xmx2048m</value>

</property>

<pre><code>               注意调整这些值之后，要重启 name node 。
</code></pre><p><em>5.       </em>出现类似： <em>java.io.IOException: File /user/root/pv_product_110124 could only be replicated to 0 nodes, instead of 1…</em></p>
<p>的异常，首先确保 hadoop 临时文件夹中有足够的空间，空间不够会导致这个错误。</p>
<p>如果空间没问题，那就尝试把临时文件夹中 dfs/data 目录删除，然后重新格式化 name node ：</p>
<p>hadoop namenode -format</p>
<p>注意：此命令会删除 hdfs 上的文件</p>
<p><em>6.       </em>出现类似： <em>java.io.IOException: Broken pipe…</em></p>
<p>的异常，检查你的程序吧，没准输出了不该输出的信息，如调试信息等。
来源： <a href="[http://blog.csdn.net/inte_sleeper/article/details/6569990](http://blog.csdn.net/inte_sleeper/article/details/6569990)">[http://blog.csdn.net/inte_sleeper/article/details/6569990](http://blog.csdn.net/inte_sleeper/article/details/6569990)</a> </p>

      
    </div>
    
    
      
    
    
        <footer id="post-meta">
        <span class="categories">Posted in<span class="breadcrumb fa fa-folder"><li><a href="/categories/hadoop/">hadoop</a></li></span></span> | <span class="tags">Tagged <a href="/tags/hadoop/" class="label label-primary">hadoop</a></span> | <span class="time">recent updated:<time title="2014-03-07 09:54:36"datetime="2014-03-07 09:54:36"> mar. 7 2014</time></span> | <span class="comment-link">
<a href="http://itsolife.com/2014/02/02/2014-02-02-hadoop--CentOS的Hadoop集群配置/#comments" class="ds-thread-count comment-link" data-thread-key="2014-02-02-hadoop--CentOS的Hadoop集群配置" data-count-type="comments">暂无评论</a></span>
        </footer>
    
    <div class="clearfix"></div>
  </div>
</div>
</article>



  <article>
<div id="post" class="post well">
  <div class="post-content">
    <header class="well-sm">
      <i class="fa icon fa-5x pull-left"></i>
      <h1 class="title"><a href="/2014/02/02/2014-02-02-hadoop--HadoopSummit2013见闻：看完基本了解整个Hadoop生态圈格局和趋势了/">Hadoop Summit 2013见闻：看完基本了解整个Hadoop生态圈格局和趋势了</a></h1>
      
        <span>Posted on<time datetime="2014-02-02T01:54:36.000Z"> <a href="/2014/02/02/2014-02-02-hadoop--HadoopSummit2013见闻：看完基本了解整个Hadoop生态圈格局和趋势了/">feb. 2 2014</a></time></span>      
    </header>
    
    
    <div class="entry">
      
        <h1 id="hadoop-summit-2013-hadoop-">Hadoop Summit 2013见闻：看完基本了解整个Hadoop生态圈格局和趋势了</h1>
<p>原文出处： <a href="http://blog.sina.com.cn/s/blog_53a5366c0101dp0q.html" target="_blank">钱五哥の共享空间</a></p>
<p>今天参加了3个keynotes，42个session中的8个，和一大堆厂商讨论技术，真是信息大爆炸的一天。</p>
<p><a href="http://cdn2.jobbole.com/2013/06/53a5366c4e020b68545d6amp.jpg" title="Hadoop Summit 2013见闻：看完基本了解整个Hadoop生态圈格局和趋势了" target="_blank"><img src="&quot;Hadoop Summit 2013见闻：看完基本了解整个Hadoop生态圈格局和趋势了&quot;" alt="Hadoop Summit 2013见闻：看完基本了解整个Hadoop生态圈格局和趋势了"></a></p>
<p>Hadoop从诞生到今年已经有7个年头，今年出现了很多新的变化：</p>
<p>1、Hadoop被公认是一套行业大数据标准开源软件，在分布式环境下提供了海量数据的处理能力（Gartner）。几乎所有主流厂商都围绕Hadoop开发工具、开源软件、商业化工具和技术服务。今年大型IT公司，如EMC、Microsoft、Intel、Teradata、Cisco都明显增加了Hadoop方面的投入，Teradata还公开展示了一个一体机；另一方面创业型Hadoop公司层出不穷，这次看到的几个是Sqrrl、Wandisco、GridGain、InMobi等等，都推出了开源的或者商用的软件。</p>
<p><a href="http://cdn2.jobbole.com/2013/06/53a5366c4e020b79dcc7bamp.jpg" title="Hadoop Summit 2013见闻：看完基本了解整个Hadoop生态圈格局和趋势了" target="_blank"><img src="&quot;Hadoop Summit 2013见闻：看完基本了解整个Hadoop生态圈格局和趋势了&quot;" alt="Hadoop Summit 2013见闻：看完基本了解整个Hadoop生态圈格局和趋势了"></a></p>
<p>2、Hadoop生态系统丰富多彩，但是核心已经被Cloudera、HortonWorks牢牢掌控，基本上没有撼动之可能。今年Hortonworks的宣传是100% open source，Cloudera只好干着急，谁叫他不开放Cloudera Enterprise Manager的源代码呢？Hortonworks介绍Ambari的时候，会场至少5个Cloudera的工程师在仔细聆听，有个小伙不停地在iPad上面速记，竞争可见一斑，个人估计，Cloudera早晚将Enterprise Manager开源。Hortonworks目前Ambari的committer是20+，Contributor 50+，后一个数字可能有些水，但是第一个是没有问题的。目前每天有update，1.25版本比1.0x版本明显好用了。其他大小厂商的生存之道就是搞插件，如Wandisco、vmware、mellanox、GridGain，而且插件均是不用修改内核的外挂 – 这些厂商是没有能力动内核的，持续投入可能会有一些作用，如vmware，但是一线hadoop厂商是绝不会松手的。</p>
<p>3、Hadoop 2.0转型基本上无可阻挡。Hortonworks的VPArun在介绍Tez的时候，给出了很多有趣的ppt，主旨就是一个：MapReduce已经是昨日黄花，Yarn将是未来并行计算的基础设施。我自己还没有使用Yarn，但是Hortonworks已经围绕Yarn开发了很多工具，尤其是Tez，这个玩意可以提升查询计划的执行时间，PIG和Hive将被改写并重装上阵。Hortonworks虽然没有搞出来Impala，但是从更底层的技术上包围Impala，两个老大的布局和较量始终没有停止。</p>
<p><a href="http://cdn2.jobbole.com/2013/06/53a5366c4e020b9af392eamp.jpg" title="Hadoop Summit 2013见闻：看完基本了解整个Hadoop生态圈格局和趋势了" target="_blank"><img src="&quot;Hadoop Summit 2013见闻：看完基本了解整个Hadoop生态圈格局和趋势了&quot;" alt="Hadoop Summit 2013见闻：看完基本了解整个Hadoop生态圈格局和趋势了"></a></p>
<p><a href="http://cdn2.jobbole.com/2013/06/53a5366c4e020bac324d8amp.jpg" title="Hadoop Summit 2013见闻：看完基本了解整个Hadoop生态圈格局和趋势了" target="_blank"><img src="&quot;Hadoop Summit 2013见闻：看完基本了解整个Hadoop生态圈格局和趋势了&quot;" alt="Hadoop Summit 2013见闻：看完基本了解整个Hadoop生态圈格局和趋势了"></a></p>
<p>4、SQL over Hadoop是一个重要的技术趋势。去年Hadoop World时，MPP还吹嘘自己如何牛X。但是Google发布了Dremel和PowerDrill，EMC搞出来HAWQ，Cloudera搞出来Impala之后，所有的MPP都开始反思自己的技术路线。和Parccel技术人员（感觉是售前）讨论了一下，她找出一张卡片说Parccel速度是Hive的100X，领先Impala10年。我感觉这个说话很快就会失灵，首先是Hive的优化一直没有停止，Hortonworks搞出来Tez、Stinger（与Facebook合作）。虽然MPP领先Hadoop很多年，根据80：20原则，如果hadoopSQL只做用户需要的20%特性，那么这个差距最多2年，2年内，hadoopSQL将在部分领域超越MPP。MPP企业的出路就是学习HAWQ。列存储也是推陈出新，近期主要是ORC（MS和Hortonworks合作）、Parquet（Twitter和Cloudera合作），有木有看出来两个巨头PK的身影？有木有看到抱团PK？这些技术在测试中均显示出很大的优势</p>
<p><a href="http://cdn2.jobbole.com/2013/06/53a5366c4e020bc314b2bamp.jpg" title="Hadoop Summit 2013见闻：看完基本了解整个Hadoop生态圈格局和趋势了" target="_blank"><img src="&quot;Hadoop Summit 2013见闻：看完基本了解整个Hadoop生态圈格局和趋势了&quot;" alt="Hadoop Summit 2013见闻：看完基本了解整个Hadoop生态圈格局和趋势了"></a></p>
<p><a href="http://cdn2.jobbole.com/2013/06/53a5366c4e020bd3a31e7amp.jpg" title="Hadoop Summit 2013见闻：看完基本了解整个Hadoop生态圈格局和趋势了" target="_blank"><img src="&quot;Hadoop Summit 2013见闻：看完基本了解整个Hadoop生态圈格局和趋势了&quot;" alt="Hadoop Summit 2013见闻：看完基本了解整个Hadoop生态圈格局和趋势了"></a></p>
<p><a href="http://cdn2.jobbole.com/2013/06/53a5366c4e020be5d05c3amp.jpg" title="Hadoop Summit 2013见闻：看完基本了解整个Hadoop生态圈格局和趋势了" target="_blank"><img src="&quot;Hadoop Summit 2013见闻：看完基本了解整个Hadoop生态圈格局和趋势了&quot;" alt="Hadoop Summit 2013见闻：看完基本了解整个Hadoop生态圈格局和趋势了"></a></p>
<p>5、IT和开源单位合作广泛。这个不仅仅存在IT厂商和开源之间，实际上开源之间也在密切合作。不太清楚合作的内部信息，但是基本上有两种模式：产品/软件交叉集成（含管理系统集成）；合作开发和推广。在技术方面就要求软件有很好的架构，提供开放的接口，这一点Ambari的设计和俺对HT的要求一模一样，可以俺未能如愿，而Amabri已经开发了好几个版本。</p>
<p>6、技术上看，大数据和云的整合也是一个选项（注意，不是趋势，而是选项）。今年新增了OpenStack相关议题，一些集成商和厂商也提出了云上Hadoop的适用场景。这个并不是适用于所有人，但是部分用户可以因此获益。Netflix是一个典型的例子，他们的实例都在AWS上面，显然他们的hadoop是基于虚拟机的，和一个Netflix小伙子（日本人）交流，他们大约有2000个虚拟实例，基于EMR，并开发了Gennie管理系统。</p>
<p><a href="http://cdn2.jobbole.com/2013/06/53a5366c4e020bf6e8c61amp.jpg" title="Hadoop Summit 2013见闻：看完基本了解整个Hadoop生态圈格局和趋势了" target="_blank"><img src="&quot;Hadoop Summit 2013见闻：看完基本了解整个Hadoop生态圈格局和趋势了&quot;" alt="Hadoop Summit 2013见闻：看完基本了解整个Hadoop生态圈格局和趋势了"></a></p>
<p>要睡觉了，4小时后还有一场信息大爆炸！贴一张在宾馆小院乘凉，看到的小松鼠吧，也就距离我5米不到，真要赞一声美帝的环境！</p>
<p><a href="http://cdn2.jobbole.com/2013/06/53a5366c07cd01360d282amp.jpg" title="Hadoop Summit 2013见闻：看完基本了解整个Hadoop生态圈格局和趋势了" target="_blank"><img src="&quot;Hadoop Summit 2013见闻：看完基本了解整个Hadoop生态圈格局和趋势了&quot;" alt="Hadoop Summit 2013见闻：看完基本了解整个Hadoop生态圈格局和趋势了"></a></p>
<p>相关信息：</p>
<ol>
<li><a href="http://blog.sina.com.cn/s/blog_53a5366c0101doch.html" title="http://blog.sina.com.cn/s/blog_53a5366c0101doch.html" target="_blank">Hadoop Summit 2013 Day1：BOF&amp;Meetup</a>
<img src="&quot;1 vote, average: 5.00 out of 5&quot;" alt="1 vote, average: 5.00 out of 5"><img src="&quot;1 vote, average: 5.00 out of 5&quot;" alt="1 vote, average: 5.00 out of 5"><img src="&quot;1 vote, average: 5.00 out of 5&quot;" alt="1 vote, average: 5.00 out of 5"><img src="&quot;1 vote, average: 5.00 out of 5&quot;" alt="1 vote, average: 5.00 out of 5"><img src="&quot;1 vote, average: 5.00 out of 5&quot;" alt="1 vote, average: 5.00 out of 5"> (<strong>*1</strong> 个评分，平均: <strong>5.00*</strong>)</li>
</ol>
<p><img src="&quot;Loading ...&quot;" alt="Loading ..."> Loading ...</p>

      
    </div>
    
    
      
    
    
        <footer id="post-meta">
        <span class="categories">Posted in<span class="breadcrumb fa fa-folder"><li><a href="/categories/hadoop/">hadoop</a></li></span></span> | <span class="tags">Tagged <a href="/tags/hadoop/" class="label label-primary">hadoop</a></span> | <span class="time">recent updated:<time title="2014-03-07 09:54:36"datetime="2014-03-07 09:54:36"> mar. 7 2014</time></span> | <span class="comment-link">
<a href="http://itsolife.com/2014/02/02/2014-02-02-hadoop--HadoopSummit2013见闻：看完基本了解整个Hadoop生态圈格局和趋势了/#comments" class="ds-thread-count comment-link" data-thread-key="2014-02-02-hadoop--HadoopSummit2013见闻：看完基本了解整个Hadoop生态圈格局和趋势了" data-count-type="comments">暂无评论</a></span>
        </footer>
    
    <div class="clearfix"></div>
  </div>
</div>
</article>



  <article>
<div id="post" class="post well">
  <div class="post-content">
    <header class="well-sm">
      <i class="fa icon fa-5x pull-left"></i>
      <h1 class="title"><a href="/2014/02/02/2014-02-02-hadoop--HDFS写入和读取流程/">HDFS写入和读取流程</a></h1>
      
        <span>Posted on<time datetime="2014-02-02T01:54:36.000Z"> <a href="/2014/02/02/2014-02-02-hadoop--HDFS写入和读取流程/">feb. 2 2014</a></time></span>      
    </header>
    
    
    <div class="entry">
      
        <h1 id="hdfs-">HDFS写入和读取流程</h1>
<p>您还未登录！|<a href="https://passport.csdn.net/account/login" target="_blank">登录</a>|<a href="https://passport.csdn.net/account/register" target="_blank">注册</a>|<a href="https://passport.csdn.net/help/faq" target="_blank">帮助</a></p>
<ul>
<li><a href="http://www.csdn.net/" target="_blank">首页</a></li>
<li><a href="http://news.csdn.net/" target="_blank">业界</a></li>
<li><a href="http://mobile.csdn.net/" target="_blank">移动</a></li>
<li><a href="http://cloud.csdn.net/" target="_blank">云计算</a></li>
<li><a href="http://sd.csdn.net/" target="_blank">研发</a></li>
<li><a href="http://bbs.csdn.net/" target="_blank">论坛</a></li>
<li><a href="http://blog.csdn.net/" target="_blank">博客</a></li>
<li><a href="http://download.csdn.net/" target="_blank">下载</a></li>
<li><h2 id="-"><a href="">更多</a></h2>
</li>
</ul>
<h1 id="-guisu-http-blog-csdn-net-hguisu-"><a href="http://blog.csdn.net/hguisu" target="_blank">guisu，程序人生。</a></h1>
<h2 id="-a-clever-person-solves-a-problem-a-wise-person-avoids-it-">能干的人解决问题。智慧的人绕开问题(A clever person solves a problem. A wise person avoids it)</h2>
<ul>
<li><a href="http://blog.csdn.net/hguisu?viewmode=contents" target="_blank"><img src="" alt="">目录视图</a></li>
<li><a href="http://blog.csdn.net/hguisu?viewmode=list" target="_blank"><img src="" alt="">摘要视图</a></li>
<li><a href="http://blog.csdn.net/hguisu/rss/list" target="_blank"><img src="" alt="">订阅</a>
<a href="http://blog.csdn.net/blogdevteam/article/details/11889881" target="_blank">2014年1月微软MVP申请开始啦！</a>      <a href="http://bbs.csdn.net/topics/390594487" target="_blank">CSDN社区中秋晒福利活动正式开始啦！</a>        <a href="http://www.csdn.net/article/2013-09-17/2816962" target="_blank">专访钟声：Java程序员，上班那点事儿</a>      <a href="http://blog.csdn.net/adali/article/details/9813651" target="_blank">独一无二的职位：开源社区经理</a>      <a href="http://blog.csdn.net/blogdevteam/article/details/11975399" target="_blank">“说说家乡的互联网”主题有奖征文</a></li>
</ul>
<h3 id="-hdfs-"><a href="">HDFS写入和读取流程</a></h3>
<p>分类： <a href="http://blog.csdn.net/hguisu/article/category/1072794" target="_blank">云计算hadoop</a>  2012-02-14 23:50 8282人阅读 <a href="">评论</a>(17) <a href="&quot;收藏&quot;">收藏</a> <a href="&quot;举报&quot;">举报</a>
<a href="http://blog.csdn.net/tag/details.html?tag=%e5%ad%98%e5%82%a8" target="_blank">存储</a><a href="http://blog.csdn.net/tag/details.html?tag=hadoop" target="_blank">hadoop</a><a href="http://blog.csdn.net/tag/details.html?tag=image" target="_blank">image</a><a href="http://blog.csdn.net/tag/details.html?tag=system" target="_blank">system</a><a href="http://blog.csdn.net/tag/details.html?tag=mysql" target="_blank">mysql</a></p>
<p>目录<a href="&quot;系统根据文章中H1到H6标签自动生成文章目录&quot;">(?)</a><a href="&quot;展开&quot;">[+]</a></p>
<ol>
<li><a href="">一HDFS</a></li>
<li><a href="">二HDFS的体系结构</a></li>
<li><p><a href="">三读写流程</a></p>
</li>
<li><p><a href="">GFS论文提到的文件读取简单流程</a></p>
</li>
<li><a href=""></a></li>
<li><a href=""></a></li>
<li><a href=""></a></li>
<li><a href="">详细流程</a></li>
<li><a href=""></a></li>
<li><p><a href="">GFS论文提到的写入文件简单流程</a></p>
</li>
<li><p><a href="">详细流程</a></p>
</li>
<li><a href=""></a><h2 id="-hdfs"><a href=""></a>一、HDFS</h2>
</li>
</ol>
<p>HDFS全称是Hadoop Distributed System。HDFS是为以流的方式存取大文件而设计的。适用于几百MB，GB以及TB，并写一次读多次的场合。而对于低延时数据访问、大量小文件、同时写和任意的文件修改，则并不是十分适合。</p>
<p>目前HDFS支持的使用接口除了Java的还有，Thrift、C、FUSE、WebDAV、HTTP等。HDFS是以block-sized chunk组织其文件内容的，默认的block大小为64MB，对于不足64MB的文件，其会占用一个block，但实际上不用占用实际硬盘上的64MB，这可以说是HDFS是在文件系统之上架设的一个中间层。之所以将默认的block大小设置为64MB这么大，是因为block-sized对于文件定位很有帮助，同时大文件更使传输的时间远大于文件寻找的时间，这样可以最大化地减少文件定位的时间在整个文件获取总时间中的比例 。</p>
<h2 id="-hdfs-"><a href=""></a>二、HDFS的体系结构</h2>
<p>构成HDFS主要是Namenode（master）和一系列的Datanode（workers）。Namenode是管理HDFS的目录树和相关的文件元数据，这些信息是以&quot;namespace image&quot;和&quot;edit log&quot;两个文件形式存放在本地磁盘，但是这些文件是在HDFS每次重启的时候重新构造出来的。Datanode则是存取文件实际内容的节点，Datanodes会定时地将block的列表汇报给Namenode。</p>
<p>由于Namenode是元数据存放的节点，如果Namenode挂了那么HDFS就没法正常运行，因此一般使用将元数据持久存储在本地或远程的机器上，或者使用secondary namenode来定期同步Namenode的元数据信息，secondary namenode有点类似于MySQL的Master/Salves中的Slave，&quot;edit log&quot;就类似&quot;bin log&quot;。如果Namenode出现了故障，一般会将原Namenode中持久化的元数据拷贝到secondary namenode中，使secondary namenode作为新的Namenode运行起来。</p>
<pre><code>                        ![]()
</code></pre><h2 id="-"><a href=""></a>三、读写流程</h2>
<h3 id="-gfs-"><a href=""></a>GFS论文提到的文件读取简单流程：</h3>
<h3 id="-"><a href=""></a></h3>
<h3 id="-"><a href=""></a>                <img src="" alt=""></h3>
<h3 id="-"><a href=""></a><em>**</em></h3>
<h3 id="-reading-data-from-hdfs-http-blog-endlesscode-com-wp-content-uploads-2010-06-reading-data-from-hdfs-png-reading-data-from-hdfs-"><a href=""></a><strong>详细流程：</strong><img src="http://blog.endlesscode.com/wp-content/uploads/2010/06/reading-data-from-hdfs.png" alt="reading data from hdfs" title="reading data from hdfs"></h3>
<p>文件读取的过程如下：</p>
<ol>
<li>使用HDFS提供的客户端开发库Client，向远程的Namenode发起RPC请求；</li>
<li>Namenode会视情况返回文件的部分或者全部block列表，对于每个block，Namenode都会返回有该block拷贝的DataNode地址；</li>
<li>客户端开发库Client会选取离客户端最接近的DataNode来读取block；如果客户端本身就是DataNode,那么将从本地直接获取数据.</li>
<li>读取完当前block的数据后，关闭与当前的DataNode连接，并为读取下一个block寻找最佳的DataNode；</li>
<li>当读完列表的block后，且文件读取还没有结束，客户端开发库会继续向Namenode获取下一批的block列表。</li>
<li>读取完一个block都会进行checksum验证，如果读取datanode时出现错误，客户端会通知Namenode，然后再从下一个拥有该block拷贝的datanode继续读。</li>
</ol>
<h3 id="-"><a href=""></a></h3>
<h3 id="-gfs-"><a href=""></a>GFS论文提到的写入文件简单流程：</h3>
<pre><code>                                 ![]()             
</code></pre><h2 id="-writing-data-to-hdfs-http-blog-endlesscode-com-wp-content-uploads-2010-06-writing-data-to-hdfs-png-writing-data-to-hdfs-"><a href=""></a>详细流程：<img src="http://blog.endlesscode.com/wp-content/uploads/2010/06/writing-data-to-hdfs.png" alt="writing data to hdfs" title="writing data to hdfs"></h2>
<p>写入文件的过程比读取较为复杂：</p>
<ol>
<li>使用HDFS提供的客户端开发库Client，向远程的Namenode发起RPC请求；</li>
<li>Namenode会检查要创建的文件是否已经存在，创建者是否有权限进行操作，成功则会为文件<strong>创建一个记录</strong>，否则会让客户端抛出异常；</li>
<li>当客户端开始写入文件的时候，开发库会将文件切分成多个packets，并在内部以数据队列&quot;data queue&quot;的形式管理这些packets，并向Namenode申请新的blocks，获取用来存储replicas的合适的datanodes列表，列表的大小根据在Namenode中对replication的设置而定。</li>
<li>开始以pipeline（管道）的形式将packet写入所有的replicas中。开发库把packet以流的方式写入第一个datanode，该datanode把该packet存储之后，再将其传递给在此pipeline中的下一个datanode，直到最后一个datanode，这种写数据的方式呈流水线的形式。</li>
<li>最后一个datanode成功存储之后会返回一个ack packet，在pipeline里传递至客户端，在客户端的开发库内部维护着&quot;ack queue&quot;，成功收到datanode返回的ack packet后会从&quot;ack queue&quot;移除相应的packet。</li>
<li>如果传输过程中，有某个datanode出现了故障，那么当前的pipeline会被关闭，出现故障的datanode会从当前的pipeline中移除，剩余的block会继续剩下的datanode中继续以pipeline的形式传输，同时Namenode会分配一个新的datanode，保持replicas设定的数量。</li>
</ol>
<h2 id="-"><a href=""></a></h2>
<p>分享到： <a href="&quot;分享到新浪微博&quot;"></a><a href="&quot;分享到腾讯微博&quot;"></a></p>
<ol>
<li>上一篇：<a href="http://blog.csdn.net/hguisu/article/details/7256833" target="_blank">Hadoop Hive sql语法详解</a></li>
<li>下一篇：<a href="http://blog.csdn.net/hguisu/article/details/7261145" target="_blank">Hadoop HDFS分布式文件系统设计要点与架构</a></li>
</ol>
<p>顶 3 踩 1
查看评论<a href=""></a></p>
<p>3楼 <a href="http://blog.csdn.net/huangbo1988911" target="_blank">huangbo1988911</a> 2012-04-09 11:25发表 <a href="&quot;回复&quot;">[回复]</a>  <a href="&quot;引用&quot;">[引用]</a> <a href="&quot;举报&quot;">[举报]</a><a href="http://blog.csdn.net/huangbo1988911" target="_blank"><img src="" alt=""></a>在写数据的过程中，一个文件被分割成很多blocks，这些block是按顺序一个个操作的，还是并发的进行传输的？Re: <a href="http://blog.csdn.net/hguisu" target="_blank">真实的归宿</a> 2012-04-09 12:45发表 <a href="&quot;回复&quot;">[回复]</a>  <a href="&quot;引用&quot;">[引用]</a> <a href="&quot;举报&quot;">[举报]</a><a href="http://blog.csdn.net/hguisu" target="_blank"><img src="" alt=""></a>回复huangbo1988911：写数据的是以流的方式传输，即管道的方式，一个一个block顺序传输。而不是像树形拓扑结构那样分散传输。Re: <a href="http://blog.csdn.net/huangbo1988911" target="_blank">huangbo1988911</a> 2012-04-17 18:03发表 <a href="&quot;回复&quot;">[回复]</a>  <a href="&quot;引用&quot;">[引用]</a> <a href="&quot;举报&quot;">[举报]</a><a href="http://blog.csdn.net/huangbo1988911" target="_blank"><img src="" alt=""></a>回复hguisu：您好，很感谢您回答我，但是我仍有点疑惑，hdfs中的文件大小区分为：chunk&lt;packet&lt;block,在每个packet的传输到多个DN（datanode）的过程中是以pipeline方式，但是当其中一个block在以这种方式传输时，其他的block是要等待还是并发的进行呢？谢谢！Re: <a href="http://blog.csdn.net/hguisu" target="_blank">真实的归宿</a> 2012-04-20 16:23发表 <a href="&quot;回复&quot;">[回复]</a>  <a href="&quot;引用&quot;">[引用]</a> <a href="&quot;举报&quot;">[举报]</a><a href="http://blog.csdn.net/hguisu" target="_blank"><img src="" alt=""></a>回复huangbo1988911：管道方式，即是队列方式传输。只能一个block传完了，接着传下个block。Re: <a href="http://blog.csdn.net/huangbo1988911" target="_blank">huangbo1988911</a> 2012-04-30 18:18发表 <a href="&quot;回复&quot;">[回复]</a>  <a href="&quot;引用&quot;">[引用]</a> <a href="&quot;举报&quot;">[举报]</a><a href="http://blog.csdn.net/huangbo1988911" target="_blank"><img src="" alt=""></a>回复hguisu：你好！如果这样，那hdfs的并发写实如何体现的呢？Re: <a href="http://blog.csdn.net/hguisu" target="_blank">真实的归宿</a> 2012-05-02 09:29发表 <a href="&quot;回复&quot;">[回复]</a>  <a href="&quot;引用&quot;">[引用]</a> <a href="&quot;举报&quot;">[举报]</a><a href="http://blog.csdn.net/hguisu" target="_blank"><img src="" alt=""></a>回复huangbo1988911：hdfs没有并发写入。Re: <a href="http://blog.csdn.net/huangbo1988911" target="_blank">huangbo1988911</a> 2012-05-03 23:39发表 <a href="&quot;回复&quot;">[回复]</a>  <a href="&quot;引用&quot;">[引用]</a> <a href="&quot;举报&quot;">[举报]</a><a href="http://blog.csdn.net/huangbo1988911" target="_blank"><img src="" alt=""></a>回复hguisu：这样的话，那hadoop是比较适合大数据的处理了，对于文件的写的速度并没有多大的提高了?Re: <a href="http://blog.csdn.net/hguisu" target="_blank">真实的归宿</a> 2012-05-04 09:40发表 <a href="&quot;回复&quot;">[回复]</a>  <a href="&quot;引用&quot;">[引用]</a> <a href="&quot;举报&quot;">[举报]</a><a href="http://blog.csdn.net/hguisu" target="_blank"><img src="" alt=""></a>回复huangbo1988911：hadoop本来就是通往云服务的捷径，为处理超大数据集而准备。Re: <a href="http://blog.csdn.net/huangbo1988911" target="_blank">huangbo1988911</a> 2012-05-07 10:57发表 <a href="&quot;回复&quot;">[回复]</a>  <a href="&quot;引用&quot;">[引用]</a> <a href="&quot;举报&quot;">[举报]</a><a href="http://blog.csdn.net/huangbo1988911" target="_blank"><img src="" alt=""></a>回复hguisu：这样hadoop的主要优势是在map/reduce那一块，而其文件系统有什么样的优势呢（在文件的读写方面，和其他的文件系统）？Re: <a href="http://blog.csdn.net/hguisu" target="_blank">真实的归宿</a> 2012-05-07 11:50发表 <a href="&quot;回复&quot;">[回复]</a>  <a href="&quot;引用&quot;">[引用]</a> <a href="&quot;举报&quot;">[举报]</a><a href="http://blog.csdn.net/hguisu" target="_blank"><img src="" alt=""></a>回复huangbo1988911：hdfs可以存储超大数据，而map/reduce要处理的数据存储在hdfs上，即MR分布式运算。Re: <a href="http://blog.csdn.net/huangbo1988911" target="_blank">huangbo1988911</a> 2012-05-09 22:35发表 <a href="&quot;回复&quot;">[回复]</a>  <a href="&quot;引用&quot;">[引用]</a> <a href="&quot;举报&quot;">[举报]</a><a href="http://blog.csdn.net/huangbo1988911" target="_blank"><img src="" alt=""></a>回复hguisu： 那多个文件同时向hdfs写入是如何进行的呢？Re: <a href="http://blog.csdn.net/huangbo1988911" target="_blank">huangbo1988911</a> 2012-05-09 00:39发表 <a href="&quot;回复&quot;">[回复]</a>  <a href="&quot;引用&quot;">[引用]</a> <a href="&quot;举报&quot;">[举报]</a><a href="http://blog.csdn.net/huangbo1988911" target="_blank"><img src="" alt=""></a>回复hguisu：恩，hdfs相对于其他的文件系统，除了更适合存储大数据以外，而且有很强的容错能力，但是对数据的读写等，没有并发性，只是采用了管道的方式，这可能是它的一个小缺点吧。2楼 <a href="http://blog.csdn.net/CD_xiaoxin" target="_blank">CD_xiaoxin</a> 2012-03-19 09:44发表 <a href="&quot;回复&quot;">[回复]</a>  <a href="&quot;引用&quot;">[引用]</a> <a href="&quot;举报&quot;">[举报]</a><a href="http://blog.csdn.net/CD_xiaoxin" target="_blank"><img src="" alt=""></a>很详细 很有帮助 谢谢1楼 <a href="http://blog.csdn.net/lin_FS" target="_blank">lin_FS</a> 2012-03-16 10:01发表 <a href="&quot;回复&quot;">[回复]</a>  <a href="&quot;引用&quot;">[引用]</a> <a href="&quot;举报&quot;">[举报]</a><a href="http://blog.csdn.net/lin_FS" target="_blank"><img src="" alt=""></a>client端的那个queue是在内存中，还是写在临时文件里了？Re: <a href="http://blog.csdn.net/hguisu" target="_blank">真实的归宿</a> 2012-03-19 09:43发表 <a href="&quot;回复&quot;">[回复]</a>  <a href="&quot;引用&quot;">[引用]</a> <a href="&quot;举报&quot;">[举报]</a><a href="http://blog.csdn.net/hguisu" target="_blank"><img src="" alt=""></a>回复lin_FS：client分割数据成一个个block（packet）这些数据都不在内存中，你可以想象，如果一个数据是100G，它你那个放进内存吗？Re: <a href="http://blog.csdn.net/lin_FS" target="_blank">lin_FS</a> 2012-03-21 17:26发表 <a href="&quot;回复&quot;">[回复]</a>  <a href="&quot;引用&quot;">[引用]</a> <a href="&quot;举报&quot;">[举报]</a><a href="http://blog.csdn.net/lin_FS" target="_blank"><img src="" alt=""></a>在client端， 多个block（packet）组成一个队列，然后可以想象把文件（100G）分成若干个packet，如果队列满了就根本写不进去数据了，根本不会出现你想象的那种情况。我想了解的是，这个队列在内存中还是以文件的形式，呵呵Re: <a href="http://blog.csdn.net/hguisu" target="_blank">真实的归宿</a> 2012-03-31 18:47发表 <a href="&quot;回复&quot;">[回复]</a>  <a href="&quot;引用&quot;">[引用]</a> <a href="&quot;举报&quot;">[举报]</a><a href="http://blog.csdn.net/hguisu" target="_blank"><img src="" alt=""></a>回复lin_FS：这个队列肯定是文件的形式存在的。
您还没有登录,请<a href="">[登录]</a>或<a href="http://passport.csdn.net/account/register?from=http%3A%2F%2Fblog.csdn.net%2Fhguisu%2Farticle%2Fdetails%2F7259716" target="_blank">[注册]</a></p>
<p>/* 以上用户言论只代表其个人观点，不代表CSDN网站的观点或立场<a href=""></a><a href=""></a></p>
<p><a href="">hguisu</a>
<a href="&quot;回到顶部&quot;"><img src="" alt="TOP"></a></p>
<p>个人资料</p>
<p><a href="http://my.csdn.net/hguisu" target="_blank"><img src="&quot;访问我的空间&quot;" alt=""></a>
<a href="http://my.csdn.net/hguisu" target="_blank">真实的归宿</a></p>
<p><a href="&quot;[加关注]&quot;"></a> <a href="&quot;[发私信]&quot;"></a>
<a href="http://medal.blog.csdn.net/allmedal.aspx" target="_blank"><img src="" alt=""></a></p>
<ul>
<li>访问：481035次</li>
<li>积分：6666分</li>
<li><p>排名：第614名</p>
</li>
<li><p>原创：190篇</p>
</li>
<li>转载：1篇</li>
<li>译文：0篇</li>
<li>评论：313条</li>
</ul>
<p>文章分类</p>
<ul>
<li><a href="http://blog.csdn.net/hguisu/article/category/1253451" target="_blank">操作系统</a>(5)</li>
<li><a href="http://blog.csdn.net/hguisu/article/category/796967" target="_blank">Linux</a>(17)</li>
<li><a href="http://blog.csdn.net/hguisu/article/category/796963" target="_blank">MySQL</a>(12)</li>
<li><a href="http://blog.csdn.net/hguisu/article/category/796962" target="_blank">PHP</a>(41)</li>
<li><a href="http://blog.csdn.net/hguisu/article/category/1104862" target="_blank">PHP内核</a>(11)</li>
<li><a href="http://blog.csdn.net/hguisu/article/category/796968" target="_blank">技术人生</a>(7)</li>
<li><a href="http://blog.csdn.net/hguisu/article/category/1054628" target="_blank">数据结构与算法</a>(27)</li>
<li><a href="http://blog.csdn.net/hguisu/article/category/1072794" target="_blank">云计算hadoop</a>(20)</li>
<li><a href="http://blog.csdn.net/hguisu/article/category/1075597" target="_blank">网络知识</a>(7)</li>
<li><a href="http://blog.csdn.net/hguisu/article/category/1080443" target="_blank">c/c++</a>(22)</li>
<li><a href="http://blog.csdn.net/hguisu/article/category/1099674" target="_blank">memcache</a>(5)</li>
<li><a href="http://blog.csdn.net/hguisu/article/category/1111071" target="_blank">HipHop</a>(2)</li>
<li><a href="http://blog.csdn.net/hguisu/article/category/1112019" target="_blank">计算机原理</a>(4)</li>
<li><a href="http://blog.csdn.net/hguisu/article/category/1114530" target="_blank">Java</a>(7)</li>
<li><a href="http://blog.csdn.net/hguisu/article/category/1122753" target="_blank">socket网络编程</a>(5)</li>
<li><a href="http://blog.csdn.net/hguisu/article/category/1133340" target="_blank">设计模式</a>(26)</li>
<li><a href="http://blog.csdn.net/hguisu/article/category/1151353" target="_blank">AOP</a>(2)</li>
<li><a href="http://blog.csdn.net/hguisu/article/category/1152364" target="_blank">重构</a>(11)</li>
<li><a href="http://blog.csdn.net/hguisu/article/category/1173389" target="_blank">重构与模式</a>(1)</li>
<li><a href="http://blog.csdn.net/hguisu/article/category/1209788" target="_blank">大数据处理</a>(11)</li>
<li><a href="http://blog.csdn.net/hguisu/article/category/1230933" target="_blank">搜索引擎Search Engine</a>(15)</li>
<li><a href="http://blog.csdn.net/hguisu/article/category/1302430" target="_blank">HTML5</a>(1)</li>
<li><a href="http://blog.csdn.net/hguisu/article/category/1309674" target="_blank">Android</a>(1)</li>
<li><a href="http://blog.csdn.net/hguisu/article/category/1422000" target="_blank">webserver</a>(3)</li>
<li><p><a href="http://blog.csdn.net/hguisu/article/category/1429288" target="_blank">NOSQL</a>(6)
文章存档</p>
</li>
<li><p><a href="http://blog.csdn.net/hguisu/article/month/2013/09" target="_blank">2013年09月</a>(2)</p>
</li>
<li><a href="http://blog.csdn.net/hguisu/article/month/2013/08" target="_blank">2013年08月</a>(1)</li>
<li><a href="http://blog.csdn.net/hguisu/article/month/2013/07" target="_blank">2013年07月</a>(2)</li>
<li><a href="http://blog.csdn.net/hguisu/article/month/2013/06" target="_blank">2013年06月</a>(3)</li>
<li><a href="http://blog.csdn.net/hguisu/article/month/2013/05" target="_blank">2013年05月</a>(3)</li>
<li><a href="http://blog.csdn.net/hguisu/article/month/2013/03" target="_blank">2013年03月</a>(3)</li>
<li><a href="http://blog.csdn.net/hguisu/article/month/2013/02" target="_blank">2013年02月</a>(2)</li>
<li><a href="http://blog.csdn.net/hguisu/article/month/2013/01" target="_blank">2013年01月</a>(2)</li>
<li><a href="http://blog.csdn.net/hguisu/article/month/2012/12" target="_blank">2012年12月</a>(4)</li>
<li><a href="http://blog.csdn.net/hguisu/article/month/2012/11" target="_blank">2012年11月</a>(3)</li>
<li><a href="http://blog.csdn.net/hguisu/article/month/2012/10" target="_blank">2012年10月</a>(2)</li>
<li><a href="http://blog.csdn.net/hguisu/article/month/2012/09" target="_blank">2012年09月</a>(15)</li>
<li><a href="http://blog.csdn.net/hguisu/article/month/2012/08" target="_blank">2012年08月</a>(6)</li>
<li><a href="http://blog.csdn.net/hguisu/article/month/2012/07" target="_blank">2012年07月</a>(8)</li>
<li><a href="http://blog.csdn.net/hguisu/article/month/2012/06" target="_blank">2012年06月</a>(14)</li>
<li><a href="http://blog.csdn.net/hguisu/article/month/2012/05" target="_blank">2012年05月</a>(29)</li>
<li><a href="http://blog.csdn.net/hguisu/article/month/2012/04" target="_blank">2012年04月</a>(26)</li>
<li><a href="http://blog.csdn.net/hguisu/article/month/2012/03" target="_blank">2012年03月</a>(27)</li>
<li><a href="http://blog.csdn.net/hguisu/article/month/2012/02" target="_blank">2012年02月</a>(18)</li>
<li><a href="http://blog.csdn.net/hguisu/article/month/2011/12" target="_blank">2011年12月</a>(7)</li>
<li><a href="http://blog.csdn.net/hguisu/article/month/2011/01" target="_blank">2011年01月</a>(8)</li>
<li><a href="http://blog.csdn.net/hguisu/article/month/2010/07" target="_blank">2010年07月</a>(6)</li>
<li><a href="http://blog.csdn.net/hguisu/article/month/2007/12" target="_blank">2007年12月</a>(2)</li>
</ul>
<p>展开</p>
<p>阅读排行</p>
<ul>
<li><a href="http://blog.csdn.net/hguisu/article/details/7237395" title="Hadoop集群配置（最全面总结）" target="_blank">Hadoop集群配置（最全面总结）</a>(23024)</li>
<li><a href="http://blog.csdn.net/hguisu/article/details/7527842" title="设计模式（五）适配器模式Adapter（结构型）" target="_blank">设计模式（五）适配器模式Adapter（结构型）</a>(21421)</li>
<li><a href="http://blog.csdn.net/hguisu/article/details/7244413" title="hbase安装配置（整合到hadoop）" target="_blank">hbase安装配置（整合到hadoop）</a>(20780)</li>
<li><a href="http://blog.csdn.net/hguisu/article/details/7453390" title="socket阻塞与非阻塞，同步与异步、I/O模型" target="_blank">socket阻塞与非阻塞，同步与异步、I/O模型</a>(12788)</li>
<li><a href="http://blog.csdn.net/hguisu/article/details/7282050" title="Hadoop Hive与Hbase整合" target="_blank">Hadoop Hive与Hbase整合</a>(11869)</li>
<li><a href="http://blog.csdn.net/hguisu/article/details/7558249" title="设计模式 ( 十八 ) 策略模式Strategy（对象行为型）" target="_blank">设计模式 ( 十八 ) 策略模式Strategy（对象行为型）</a>(11737)</li>
<li><a href="http://blog.csdn.net/hguisu/article/details/7786014" title="B-树和B+树的应用：数据搜索和数据库索引" target="_blank">B-树和B+树的应用：数据搜索和数据库索引</a>(11507)</li>
<li><a href="http://blog.csdn.net/hguisu/article/details/5731880" title="Mysql 多表联合查询效率分析及优化" target="_blank">Mysql 多表联合查询效率分析及优化</a>(10454)</li>
<li><a href="http://blog.csdn.net/hguisu/article/details/7244991" title="谷歌三大核心技术（三）Google BigTable中文版" target="_blank">谷歌三大核心技术（三）Google BigTable中文版</a>(9958)</li>
<li><p><a href="&quot;HDFS写入和读取流程&quot;">HDFS写入和读取流程</a>(8282)
评论排行</p>
</li>
<li><p><a href="http://blog.csdn.net/hguisu/article/details/7558249" title="设计模式 ( 十八 ) 策略模式Strategy（对象行为型）" target="_blank">设计模式 ( 十八 ) 策略模式Strategy（对象行为型）</a>(33)</p>
</li>
<li><a href="&quot;HDFS写入和读取流程&quot;">HDFS写入和读取流程</a>(17)</li>
<li><a href="http://blog.csdn.net/hguisu/article/details/7529194" title="设计模式（六）桥连模式Bridge（结构型）" target="_blank">设计模式（六）桥连模式Bridge（结构型）</a>(14)</li>
<li><a href="http://blog.csdn.net/hguisu/article/details/7505909" title="设计模式（一）工厂模式Factory（创建型）" target="_blank">设计模式（一）工厂模式Factory（创建型）</a>(13)</li>
<li><a href="http://blog.csdn.net/hguisu/article/details/7880288" title="海量数据处理算法—Bit-Map" target="_blank">海量数据处理算法—Bit-Map</a>(13)</li>
<li><a href="http://blog.csdn.net/hguisu/article/details/7237395" title="Hadoop集群配置（最全面总结）" target="_blank">Hadoop集群配置（最全面总结）</a>(13)</li>
<li><a href="http://blog.csdn.net/hguisu/article/details/7448528" title="PHP SOCKET编程" target="_blank">PHP SOCKET编程</a>(11)</li>
<li><a href="http://blog.csdn.net/hguisu/article/details/7453390" title="socket阻塞与非阻塞，同步与异步、I/O模型" target="_blank">socket阻塞与非阻塞，同步与异步、I/O模型</a>(11)</li>
<li><a href="http://blog.csdn.net/hguisu/article/details/7282050" title="Hadoop Hive与Hbase整合" target="_blank">Hadoop Hive与Hbase整合</a>(10)</li>
<li><a href="http://blog.csdn.net/hguisu/article/details/7244413" title="hbase安装配置（整合到hadoop）" target="_blank">hbase安装配置（整合到hadoop）</a>(10)</li>
</ul>
<p>推荐文章
最新评论</p>
<ul>
<li><a href="http://blog.csdn.net/hguisu/article/details/7453390#comments" target="_blank">socket阻塞与非阻塞，同步与异步、I/O模型</a></li>
</ul>
<p><a href="http://blog.csdn.net/ctqctq99" target="_blank">ctqctq99</a>: 他的意思可能是阻塞IO和非阻塞IO的区别就在于：应用程序的调用是否立即返回！这句话说反了。应该是非阻...</p>
<ul>
<li><a href="http://blog.csdn.net/hguisu/article/details/7408047#comments" target="_blank">硬盘的读写原理</a></li>
</ul>
<p><a href="http://blog.csdn.net/m1013923728" target="_blank">m1013923728</a>: 写的通俗易懂！</p>
<ul>
<li><a href="http://blog.csdn.net/hguisu/article/details/7470695#comments" target="_blank">C语言中的宏定义</a></li>
</ul>
<p><a href="http://blog.csdn.net/ouwen3536" target="_blank">ouwen3536</a>: 很半，转起！</p>
<ul>
<li><a href="http://blog.csdn.net/hguisu/article/details/7470695#comments" target="_blank">C语言中的宏定义</a></li>
</ul>
<p><a href="http://blog.csdn.net/zhangyongbluesky" target="_blank">zhangyongbluesky</a>: good</p>
<ul>
<li><a href="http://blog.csdn.net/hguisu/article/details/7244413#comments" target="_blank">hbase安装配置（整合到hadoop）</a></li>
</ul>
<p><a href="http://blog.csdn.net/hguisu" target="_blank">真实的归宿</a>: @u012171806:conf/hbase-site.xml你应该看官方文档的快速入门。安装的东西...</p>
<ul>
<li><a href="http://blog.csdn.net/hguisu/article/details/7244413#comments" target="_blank">hbase安装配置（整合到hadoop）</a></li>
</ul>
<p><a href="http://blog.csdn.net/u012171806" target="_blank">JAVA_小陈</a>: 我把hbase下载了，你上面说的需要配置一个xml文件，请问配置在什么地方呢？然后启动的时候是在什么...</p>
<ul>
<li><a href="http://blog.csdn.net/hguisu/article/details/7408047#comments" target="_blank">硬盘的读写原理</a></li>
</ul>
<p><a href="http://blog.csdn.net/mxhlee" target="_blank">mxhlee</a>: 好东西啊！！！（实际是斜切向运动）这句话让我纠结了很长时间、我自己就感觉是这运动 但是没有一个介绍...</p>
<ul>
<li><a href="http://blog.csdn.net/hguisu/article/details/7453390#comments" target="_blank">socket阻塞与非阻塞，同步与异步、I/O模型</a></li>
</ul>
<p><a href="http://blog.csdn.net/hguisu" target="_blank">真实的归宿</a>: @liaokailin:并没有反。实际就是这样的。</p>
<ul>
<li><a href="http://blog.csdn.net/hguisu/article/details/7453390#comments" target="_blank">socket阻塞与非阻塞，同步与异步、I/O模型</a></li>
</ul>
<p><a href="http://blog.csdn.net/hguisu" target="_blank">真实的归宿</a>: @liaokailin:实际就是这样的。</p>
<ul>
<li><a href="http://blog.csdn.net/hguisu/article/details/7453390#comments" target="_blank">socket阻塞与非阻塞，同步与异步、I/O模型</a></li>
</ul>
<p><a href="http://blog.csdn.net/liaokailin" target="_blank">廖凯林</a>: 同步IO和异步IO的区别就在于：数据拷贝的时候进程是否阻塞！阻塞IO和非阻塞IO的区别就在于：应用程...</p>
<p><a href="http://www.csdn.net/company/about.html" target="_blank">公司简介</a>|<a href="http://www.csdn.net/company/recruit.html" target="_blank">招贤纳士</a>|<a href="http://www.csdn.net/company/marketing.html" target="_blank">广告服务</a>|<a href="http://www.csdn.net/company/account.html" target="_blank">银行汇款帐号</a>|<a href="http://www.csdn.net/company/contact.html" target="_blank">联系方式</a>|<a href="http://www.csdn.net/company/statement.html" target="_blank">版权声明</a>|<a href="http://www.csdn.net/company/layer.html" target="_blank">法律顾问</a>|<a href="mailto:webmaster@csdn.net">问题报告</a><a href="http://wpa.qq.com/msgrd?v=3&amp;uin=2355263776&amp;site=qq&amp;menu=yes" target="_blank">QQ客服</a> <a href="http://e.weibo.com/csdnsupport/profile" target="_blank">微博客服</a> <a href="http://bbs.csdn.net/forums/Service" target="_blank">论坛反馈</a> <a href="mailto:webmaster@csdn.net">联系邮箱：webmaster@csdn.net</a> 服务热线：400-600-2320京 ICP 证 070598 号北京创新乐知信息技术有限公司 版权所有世纪乐知(北京)网络技术有限公司 提供技术支持江苏乐知网络技术有限公司 提供商务支持Copyright © 1999-2012, CSDN.NET, All Rights Reserved <a href="http://www.hd315.gov.cn/beian/view.asp?bianhao=010202001032100010" target="_blank"><img src="" alt="GongshangLogo"></a>
<img src="http://counter.csdn.net/pv.aspx?id=24" alt=""></p>

      
    </div>
    
    
      
    
    
        <footer id="post-meta">
        <span class="categories">Posted in<span class="breadcrumb fa fa-folder"><li><a href="/categories/hadoop/">hadoop</a></li></span></span> | <span class="tags">Tagged <a href="/tags/hadoop/" class="label label-primary">hadoop</a></span> | <span class="time">recent updated:<time title="2014-03-07 09:54:36"datetime="2014-03-07 09:54:36"> mar. 7 2014</time></span> | <span class="comment-link">
<a href="http://itsolife.com/2014/02/02/2014-02-02-hadoop--HDFS写入和读取流程/#comments" class="ds-thread-count comment-link" data-thread-key="2014-02-02-hadoop--HDFS写入和读取流程" data-count-type="comments">暂无评论</a></span>
        </footer>
    
    <div class="clearfix"></div>
  </div>
</div>
</article>



  <article>
<div id="post" class="post well">
  <div class="post-content">
    <header class="well-sm">
      <i class="fa icon fa-5x pull-left"></i>
      <h1 class="title"><a href="/2014/02/02/2014-02-02-hadoop--hdfs_design/">hdfs_design</a></h1>
      
        <span>Posted on<time datetime="2014-02-02T01:54:36.000Z"> <a href="/2014/02/02/2014-02-02-hadoop--hdfs_design/">feb. 2 2014</a></time></span>      
    </header>
    
    
    <div class="entry">
      
        <h1 id="hdfs_design">hdfs_design</h1>
<p>HDFS Architecture
by Dhruba Borthakur
Table of contents
1 2
Introduction .......................................................................................................................3 Assumptions and Goals .....................................................................................................3
2.1 2.2 2.3 2.4 2.5 2.6
Hardware Failure .......................................................................................................... 3 Streaming Data Access .................................................................................................3 Large Data Sets .............................................................................................................3 Simple Coherency Model ............................................................................................. 4 “Moving Computation is Cheaper than Moving Data” ................................................4 Portability Across Heterogeneous Hardware and Software Platforms .........................4
3 4 5
NameNode and DataNodes ...............................................................................................4 The File System Namespace ............................................................................................. 5 Data Replication ................................................................................................................6
5.1 5.2 5.3
Replica Placement: The First Baby Steps .................................................................... 7 Replica Selection .......................................................................................................... 8 Safemode ...................................................................................................................... 8
6 7 8
The Persistence of File System Metadata ......................................................................... 8 The Communication Protocols ......................................................................................... 9 Robustness ........................................................................................................................ 9
8.1 8.2 8.3 8.4 8.5
Data Disk Failure, Heartbeats and Re-Replication .....................................................10 Cluster Rebalancing ....................................................................................................10 Data Integrity ..............................................................................................................10 Metadata Disk Failure ................................................................................................ 10 Snapshots ....................................................................................................................11
Copyright © 2008 The Apache Software Foundation. All rights reserved.
HDFS Architecture
9
Data Organization ........................................................................................................... 11
9.1 9.2 9.3
Data Blocks ................................................................................................................ 11 Staging ........................................................................................................................11 Replication Pipelining ................................................................................................ 12 FS Shell .....................................................................................................................12 DFSAdmin ................................................................................................................ 13 Browser Interface ......................................................................................................13 File Deletes and Undeletes ....................................................................................... 13 Decrease Replication Factor ..................................................................................... 14
10
Accessibility .................................................................................................................. 12
10.1 10.2 10.3 11
Space Reclamation ........................................................................................................ 13
11.1 11.2 12
References ..................................................................................................................... 14
Page 2
Copyright © 2008 The Apache Software Foundation. All rights reserved.
HDFS Architecture</p>
<ol>
<li>Introduction
The Hadoop Distributed File System (HDFS) is a distributed file system designed to run on commodity hardware. It has many similarities with existing distributed file systems. However, the differences from other distributed file systems are significant. HDFS is highly fault-tolerant and is designed to be deployed on low-cost hardware. HDFS provides high throughput access to application data and is suitable for applications that have large data sets. HDFS relaxes a few POSIX requirements to enable streaming access to file system data. HDFS was originally built as infrastructure for the Apache Nutch web search engine project. HDFS is part of the Apache Hadoop Core project. The project URL is <a href="http://hadoop.apache.org/core/" target="_blank">http://hadoop.apache.org/core/</a>.</li>
<li>Assumptions and Goals
2.1. Hardware Failure
Hardware failure is the norm rather than the exception. An HDFS instance may consist of hundreds or thousands of server machines, each storing part of the file system’s data. The fact that there are a huge number of components and that each component has a non-trivial probability of failure means that some component of HDFS is always non-functional. Therefore, detection of faults and quick, automatic recovery from them is a core architectural goal of HDFS.
2.2. Streaming Data Access
Applications that run on HDFS need streaming access to their data sets. They are not general purpose applications that typically run on general purpose file systems. HDFS is designed more for batch processing rather than interactive use by users. The emphasis is on high throughput of data access rather than low latency of data access. POSIX imposes many hard requirements that are not needed for applications that are targeted for HDFS. POSIX semantics in a few key areas has been traded to increase data throughput rates.
2.3. Large Data Sets
Applications that run on HDFS have large data sets. A typical file in HDFS is gigabytes to terabytes in size. Thus, HDFS is tuned to support large files. It should provide high aggregate data bandwidth and scale to hundreds of nodes in a single cluster. It should support tens of millions of files in a single instance.
Page 3
Copyright © 2008 The Apache Software Foundation. All rights reserved.
HDFS Architecture
2.4. Simple Coherency Model
HDFS applications need a write-once-read-many access model for files. A file once created, written, and closed need not be changed. This assumption simplifies data coherency issues and enables high throughput data access. A Map/Reduce application or a web crawler application fits perfectly with this model. There is a plan to support appending-writes to files in the future.
2.5. “Moving Computation is Cheaper than Moving Data”
A computation requested by an application is much more efficient if it is executed near the data it operates on. This is especially true when the size of the data set is huge. This minimizes network congestion and increases the overall throughput of the system. The assumption is that it is often better to migrate the computation closer to where the data is located rather than moving the data to where the application is running. HDFS provides interfaces for applications to move themselves closer to where the data is located.
2.6. Portability Across Heterogeneous Hardware and Software Platforms
HDFS has been designed to be easily portable from one platform to another. This facilitates widespread adoption of HDFS as a platform of choice for a large set of applications.</li>
<li>NameNode and DataNodes
HDFS has a master/slave architecture. An HDFS cluster consists of a single NameNode, a master server that manages the file system namespace and regulates access to files by clients. In addition, there are a number of DataNodes, usually one per node in the cluster, which manage storage attached to the nodes that they run on. HDFS exposes a file system namespace and allows user data to be stored in files. Internally, a file is split into one or more blocks and these blocks are stored in a set of DataNodes. The NameNode executes file system namespace operations like opening, closing, and renaming files and directories. It also determines the mapping of blocks to DataNodes. The DataNodes are responsible for serving read and write requests from the file system’s clients. The DataNodes also perform block creation, deletion, and replication upon instruction from the NameNode.
Page 4
Copyright © 2008 The Apache Software Foundation. All rights reserved.
<img src="" alt=""> HDFS Architecture
The NameNode and DataNode are pieces of software designed to run on commodity machines. These machines typically run a GNU/Linux operating system (OS). HDFS is built using the Java language; any machine that supports Java can run the NameNode or the DataNode software. Usage of the highly portable Java language means that HDFS can be deployed on a wide range of machines. A typical deployment has a dedicated machine that runs only the NameNode software. Each of the other machines in the cluster runs one instance of the DataNode software. The architecture does not preclude running multiple DataNodes on the same machine but in a real deployment that is rarely the case. The existence of a single NameNode in a cluster greatly simplifies the architecture of the system. The NameNode is the arbitrator and repository for all HDFS metadata. The system is designed in such a way that user data never flows through the NameNode.</li>
<li>The File System Namespace
HDFS supports a traditional hierarchical file organization. A user or an application can create directories and store files inside these directories. The file system namespace hierarchy is
Page 5
Copyright © 2008 The Apache Software Foundation. All rights reserved.
HDFS Architecture
similar to most other existing file systems; one can create and remove files, move a file from one directory to another, or rename a file. HDFS does not yet implement user quotas or access permissions. HDFS does not support hard links or soft links. However, the HDFS architecture does not preclude implementing these features. The NameNode maintains the file system namespace. Any change to the file system namespace or its properties is recorded by the NameNode. An application can specify the number of replicas of a file that should be maintained by HDFS. The number of copies of a file is called the replication factor of that file. This information is stored by the NameNode.</li>
<li>Data Replication
HDFS is designed to reliably store very large files across machines in a large cluster. It stores each file as a sequence of blocks; all blocks in a file except the last block are the same size. The blocks of a file are replicated for fault tolerance. The block size and replication factor are configurable per file. An application can specify the number of replicas of a file. The replication factor can be specified at file creation time and can be changed later. Files in HDFS are write-once and have strictly one writer at any time. The NameNode makes all decisions regarding replication of blocks. It periodically receives a Heartbeat and a Blockreport from each of the DataNodes in the cluster. Receipt of a Heartbeat implies that the DataNode is functioning properly. A Blockreport contains a list of all blocks on a DataNode.
Page 6
Copyright © 2008 The Apache Software Foundation. All rights reserved.
<img src="" alt=""> HDFS Architecture
5.1. Replica Placement: The First Baby Steps
The placement of replicas is critical to HDFS reliability and performance. Optimizing replica placement distinguishes HDFS from most other distributed file systems. This is a feature that needs lots of tuning and experience. The purpose of a rack-aware replica placement policy is to improve data reliability, availability, and network bandwidth utilization. The current implementation for the replica placement policy is a first effort in this direction. The short-term goals of implementing this policy are to validate it on production systems, learn more about its behavior, and build a foundation to test and research more sophisticated policies. Large HDFS instances run on a cluster of computers that commonly spread across many racks. Communication between two nodes in different racks has to go through switches. In most cases, network bandwidth between machines in the same rack is greater than network bandwidth between machines in different racks. The NameNode determines the rack id each DataNode belongs to via the process outlined in Rack Awareness. A simple but non-optimal policy is to place replicas on unique racks. This prevents losing data when an entire rack fails and allows use of bandwidth from multiple
Page 7
Copyright © 2008 The Apache Software Foundation. All rights reserved.
HDFS Architecture
racks when reading data. This policy evenly distributes replicas in the cluster which makes it easy to balance load on component failure. However, this policy increases the cost of writes because a write needs to transfer blocks to multiple racks. For the common case, when the replication factor is three, HDFS’s placement policy is to put one replica on one node in the local rack, another on a different node in the local rack, and the last on a different node in a different rack. This policy cuts the inter-rack write traffic which generally improves write performance. The chance of rack failure is far less than that of node failure; this policy does not impact data reliability and availability guarantees. However, it does reduce the aggregate network bandwidth used when reading data since a block is placed in only two unique racks rather than three. With this policy, the replicas of a file do not evenly distribute across the racks. One third of replicas are on one node, two thirds of replicas are on one rack, and the other third are evenly distributed across the remaining racks. This policy improves write performance without compromising data reliability or read performance. The current, default replica placement policy described here is a work in progress.
5.2. Replica Selection
To minimize global bandwidth consumption and read latency, HDFS tries to satisfy a read request from a replica that is closest to the reader. If there exists a replica on the same rack as the reader node, then that replica is preferred to satisfy the read request. If angg/ HDFS cluster spans multiple data centers, then a replica that is resident in the local data center is preferred over any remote replica.
5.3. Safemode
On startup, the NameNode enters a special state called Safemode. Replication of data blocks does not occur when the NameNode is in the Safemode state. The NameNode receives Heartbeat and Blockreport messages from the DataNodes. A Blockreport contains the list of data blocks that a DataNode is hosting. Each block has a specified minimum number of replicas. A block is considered safely replicated when the minimum number of replicas of that data block has checked in with the NameNode. After a configurable percentage of safely replicated data blocks checks in with the NameNode (plus an additional 30 seconds), the NameNode exits the Safemode state. It then determines the list of data blocks (if any) that still have fewer than the specified number of replicas. The NameNode then replicates these blocks to other DataNodes.</li>
<li>The Persistence of File System Metadata
Page 8
Copyright © 2008 The Apache Software Foundation. All rights reserved.
HDFS Architecture
The HDFS namespace is stored by the NameNode. The NameNode uses a transaction log called the EditLog to persistently record every change that occurs to file system metadata. For example, creating a new file in HDFS causes the NameNode to insert a record into the EditLog indicating this. Similarly, changing the replication factor of a file causes a new record to be inserted into the EditLog. The NameNode uses a file in its local host OS file system to store the EditLog. The entire file system namespace, including the mapping of blocks to files and file system properties, is stored in a file called the FsImage. The FsImage is stored as a file in the NameNode’s local file system too. The NameNode keeps an image of the entire file system namespace and file Blockmap in memory. This key metadata item is designed to be compact, such that a NameNode with 4 GB of RAM is plenty to support a huge number of files and directories. When the NameNode starts up, it reads the FsImage and EditLog from disk, applies all the transactions from the EditLog to the in-memory representation of the FsImage, and flushes out this new version into a new FsImage on disk. It can then truncate the old EditLog because its transactions have been applied to the persistent FsImage. This process is called a checkpoint. In the current implementation, a checkpoint only occurs when the NameNode starts up. Work is in progress to support periodic checkpointing in the near future. The DataNode stores HDFS data in files in its local file system. The DataNode has no knowledge about HDFS files. It stores each block of HDFS data in a separate file in its local file system. The DataNode does not create all files in the same directory. Instead, it uses a heuristic to determine the optimal number of files per directory and creates subdirectories appropriately. It is not optimal to create all local files in the same directory because the local file system might not be able to efficiently support a huge number of files in a single directory. When a DataNode starts up, it scans through its local file system, generates a list of all HDFS data blocks that correspond to each of these local files and sends this report to the NameNode: this is the Blockreport.</li>
<li>The Communication Protocols
All HDFS communication protocols are layered on top of the TCP/IP protocol. A client establishes a connection to a configurable TCP port on the NameNode machine. It talks the ClientProtocol with the NameNode. The DataNodes talk to the NameNode using the DataNode Protocol. A Remote Procedure Call (RPC) abstraction wraps both the Client Protocol and the DataNode Protocol. By design, the NameNode never initiates any RPCs. Instead, it only responds to RPC requests issued by DataNodes or clients.</li>
<li>Robustness
The primary objective of HDFS is to store data reliably even in the presence of failures. The
Page 9
Copyright © 2008 The Apache Software Foundation. All rights reserved.
HDFS Architecture
three common types of failures are NameNode failures, DataNode failures and network partitions.
8.1. Data Disk Failure, Heartbeats and Re-Replication
Each DataNode sends a Heartbeat message to the NameNode periodically. A network partition can cause a subset of DataNodes to lose connectivity with the NameNode. The NameNode detects this condition by the absence of a Heartbeat message. The NameNode marks DataNodes without recent Heartbeats as dead and does not forward any new IO requests to them. Any data that was registered to a dead DataNode is not available to HDFS any more. DataNode death may cause the replication factor of some blocks to fall below their specified value. The NameNode constantly tracks which blocks need to be replicated and initiates replication whenever necessary. The necessity for re-replication may arise due to many reasons: a DataNode may become unavailable, a replica may become corrupted, a hard disk on a DataNode may fail, or the replication factor of a file may be increased.
8.2. Cluster Rebalancing
The HDFS architecture is compatible with data rebalancing schemes. A scheme might automatically move data from one DataNode to another if the free space on a DataNode falls below a certain threshold. In the event of a sudden high demand for a particular file, a scheme might dynamically create additional replicas and rebalance other data in the cluster. These types of data rebalancing schemes are not yet implemented.
8.3. Data Integrity
It is possible that a block of data fetched from a DataNode arrives corrupted. This corruption can occur because of faults in a storage device, network faults, or buggy software. The HDFS client software implements checksum checking on the contents of HDFS files. When a client creates an HDFS file, it computes a checksum of each block of the file and stores these checksums in a separate hidden file in the same HDFS namespace. When a client retrieves file contents it verifies that the data it received from each DataNode matches the checksum stored in the associated checksum file. If not, then the client can opt to retrieve that block from another DataNode that has a replica of that block.
8.4. Metadata Disk Failure
The FsImage and the EditLog are central data structures of HDFS. A corruption of these files can cause the HDFS instance to be non-functional. For this reason, the NameNode can be configured to support maintaining multiple copies of the FsImage and EditLog. Any update
Page 10
Copyright © 2008 The Apache Software Foundation. All rights reserved.
HDFS Architecture
to either the FsImage or EditLog causes each of the FsImages and EditLogs to get updated synchronously. This synchronous updating of multiple copies of the FsImage and EditLog may degrade the rate of namespace transactions per second that a NameNode can support. However, this degradation is acceptable because even though HDFS applications are very data intensive in nature, they are not metadata intensive. When a NameNode restarts, it selects the latest consistent FsImage and EditLog to use. The NameNode machine is a single point of failure for an HDFS cluster. If the NameNode machine fails, manual intervention is necessary. Currently, automatic restart and failover of the NameNode software to another machine is not supported.
8.5. Snapshots
Snapshots support storing a copy of data at a particular instant of time. One usage of the snapshot feature may be to roll back a corrupted HDFS instance to a previously known good point in time. HDFS does not currently support snapshots but will in a future release.</li>
<li>Data Organization
9.1. Data Blocks
HDFS is designed to support very large files. Applications that are compatible with HDFS are those that deal with large data sets. These applications write their data only once but they read it one or more times and require these reads to be satisfied at streaming speeds. HDFS supports write-once-read-many semantics on files. A typical block size used by HDFS is 64 MB. Thus, an HDFS file is chopped up into 64 MB chunks, and if possible, each chunk will reside on a different DataNode.
9.2. Staging
A client request to create a file does not reach the NameNode immediately. In fact, initially the HDFS client caches the file data into a temporary local file. Application writes are transparently redirected to this temporary local file. When the local file accumulates data worth over one HDFS block size, the client contacts the NameNode. The NameNode inserts the file name into the file system hierarchy and allocates a data block for it. The NameNode responds to the client request with the identity of the DataNode and the destination data block. Then the client flushes the block of data from the local temporary file to the specified DataNode. When a file is closed, the remaining un-flushed data in the temporary local file is transferred to the DataNode. The client then tells the NameNode that the file is closed. At this point, the NameNode commits the file creation operation into a persistent store. If the
Page 11
Copyright © 2008 The Apache Software Foundation. All rights reserved.
HDFS Architecture
NameNode dies before the file is closed, the file is lost. The above approach has been adopted after careful consideration of target applications that run on HDFS. These applications need streaming writes to files. If a client writes to a remote file directly without any client side buffering, the network speed and the congestion in the network impacts throughput considerably. This approach is not without precedent. Earlier distributed file systems, e.g. AFS, have used client side caching to improve performance. A POSIX requirement has been relaxed to achieve higher performance of data uploads.
9.3. Replication Pipelining
When a client is writing data to an HDFS file, its data is first written to a local file as explained in the previous section. Suppose the HDFS file has a replication factor of three. When the local file accumulates a full block of user data, the client retrieves a list of DataNodes from the NameNode. This list contains the DataNodes that will host a replica of that block. The client then flushes the data block to the first DataNode. The first DataNode starts receiving the data in small portions (4 KB), writes each portion to its local repository and transfers that portion to the second DataNode in the list. The second DataNode, in turn starts receiving each portion of the data block, writes that portion to its repository and then flushes that portion to the third DataNode. Finally, the third DataNode writes the data to its local repository. Thus, a DataNode can be receiving data from the previous one in the pipeline and at the same time forwarding data to the next one in the pipeline. Thus, the data is pipelined from one DataNode to the next.</li>
<li>Accessibility
HDFS can be accessed from applications in many different ways. Natively, HDFS provides a FileSystem Java API for applications to use. A C language wrapper for this Java API is also available. In addition, an HTTP browser can also be used to browse the files of an HDFS instance. Work is in progress to expose HDFS through the WebDAV protocol.
10.1. FS Shell
HDFS allows user data to be organized in the form of files and directories. It provides a commandline interface called FS shell that lets a user interact with the data in HDFS. The syntax of this command set is similar to other shells (e.g. bash, csh) that users are already familiar with. Here are some sample action/command pairs:
Action Create a directory named /foodir Command bin/hadoop dfs -mkdir /foodir
Page 12
Copyright © 2008 The Apache Software Foundation. All rights reserved.
HDFS Architecture
Remove a directory named /foodir View the contents of a file named /foodir/myfile.txt
bin/hadoop dfs -rmr /foodir bin/hadoop dfs -cat /foodir/myfile.txt
FS shell is targeted for applications that need a scripting language to interact with the stored data.
10.2. DFSAdmin
The DFSAdmin command set is used for administering an HDFS cluster. These are commands that are used only by an HDFS administrator. Here are some sample action/command pairs:
Action Put the cluster in Safemode Generate a list of DataNodes Recommission or decommission DataNode(s) Command bin/hadoop dfsadmin -safemode enter bin/hadoop dfsadmin -report bin/hadoop dfsadmin -refreshNodes
10.3. Browser Interface
A typical HDFS install configures a web server to expose the HDFS namespace through a configurable TCP port. This allows a user to navigate the HDFS namespace and view the contents of its files using a web browser.</li>
<li>Space Reclamation
11.1. File Deletes and Undeletes
When a file is deleted by a user or an application, it is not immediately removed from HDFS. Instead, HDFS first renames it to a file in the /trash directory. The file can be restored quickly as long as it remains in /trash. A file remains in /trash for a configurable amount of time. After the expiry of its life in /trash, the NameNode deletes the file from the HDFS namespace. The deletion of a file causes the blocks associated with the file to be freed. Note that there could be an appreciable time delay between the time a file is deleted by a user and the time of the corresponding increase in free space in HDFS. A user can Undelete a file after deleting it as long as it remains in the /trash directory. If a user wants to undelete a file that he/she has deleted, he/she can navigate the /trash directory and retrieve the file. The /trash directory contains only the latest copy of the file
Page 13
Copyright © 2008 The Apache Software Foundation. All rights reserved.
HDFS Architecture
that was deleted. The /trash directory is just like any other directory with one special feature: HDFS applies specified policies to automatically delete files from this directory. The current default policy is to delete files from /trash that are more than 6 hours old. In the future, this policy will be configurable through a well defined interface.
11.2. Decrease Replication Factor
When the replication factor of a file is reduced, the NameNode selects excess replicas that can be deleted. The next Heartbeat transfers this information to the DataNode. The DataNode then removes the corresponding blocks and the corresponding free space appears in the cluster. Once again, there might be a time delay between the completion of the setReplication API call and the appearance of free space in the cluster.</li>
<li>References
Hadoop JavaDoc API. HDFS source code: <a href="http://hadoop.apache.org/core/version_control.html" target="_blank">http://hadoop.apache.org/core/version_control.html</a>
Page 14
Copyright © 2008 The Apache Software Foundation. All rights reserved.</li>
</ol>

      
    </div>
    
    
      
    
    
        <footer id="post-meta">
        <span class="categories">Posted in<span class="breadcrumb fa fa-folder"><li><a href="/categories/hadoop/">hadoop</a></li></span></span> | <span class="tags">Tagged <a href="/tags/hadoop/" class="label label-primary">hadoop</a></span> | <span class="time">recent updated:<time title="2014-03-07 09:54:36"datetime="2014-03-07 09:54:36"> mar. 7 2014</time></span> | <span class="comment-link">
<a href="http://itsolife.com/2014/02/02/2014-02-02-hadoop--hdfs_design/#comments" class="ds-thread-count comment-link" data-thread-key="2014-02-02-hadoop--hdfs_design" data-count-type="comments">暂无评论</a></span>
        </footer>
    
    <div class="clearfix"></div>
  </div>
</div>
</article>



  <article>
<div id="post" class="post well">
  <div class="post-content">
    <header class="well-sm">
      <i class="fa icon fa-5x pull-left"></i>
      <h1 class="title"><a href="/2014/02/02/2014-02-02-linux--Vim命令合集/">Vim命令合集</a></h1>
      
        <span>Posted on<time datetime="2014-02-02T01:54:36.000Z"> <a href="/2014/02/02/2014-02-02-linux--Vim命令合集/">feb. 2 2014</a></time></span>      
    </header>
    
    
    <div class="entry">
      
        <h1 id="vim-">Vim命令合集</h1>
<p>命令历史</p>
<p>以:和/开头的命令都有历史纪录，可以首先键入:或/然后按上下箭头来选择某个历史命令。</p>
<h1 id="-vim">启动vim</h1>
<p>在命令行窗口中输入以下命令即可</p>
<p>vim 直接启动vim</p>
<p>vim filename 打开vim并创建名为filename的文件</p>
<h1 id="-">文件命令</h1>
<p>打开单个文件</p>
<p>vim file</p>
<p>同时打开多个文件</p>
<p>vim file1 file2 file3 ...</p>
<p>在vim窗口中打开一个新文件</p>
<p>:open file</p>
<p>在新窗口中打开文件</p>
<p>:split file</p>
<p>切换到下一个文件</p>
<p>:bn</p>
<p>切换到上一个文件</p>
<p>:bp</p>
<p>查看当前打开的文件列表，当前正在编辑的文件会用[]括起来。</p>
<p>:args</p>
<p>打开远程文件，比如ftp或者share folder</p>
<p>:e ftp://192.168.10.76/abc.txt</p>
<p>:e \qadrive\test\1.txt</p>
<h1 id="vim-">vim的模式</h1>
<p>正常模式（按Esc或Ctrl+[进入） 左下角显示文件名或为空
插入模式（按i键进入） 左下角显示--INSERT--
可视模式（不知道如何进入） 左下角显示--VISUAL--</p>
<h1 id="-">导航命令</h1>
<p>% 括号匹配</p>
<h1 id="-">插入命令</h1>
<p>i 在当前位置生前插入</p>
<p>I 在当前行首插入</p>
<p>a 在当前位置后插入</p>
<p>A 在当前行尾插入</p>
<p>o 在当前行之后插入一行</p>
<p>O 在当前行之前插入一行</p>
<h1 id="-">查找命令</h1>
<p>/text　　查找text，按n健查找下一个，按N健查找前一个。</p>
<p>?text　　查找text，反向查找，按n健查找下一个，按N健查找前一个。</p>
<p>vim中有一些特殊字符在查找时需要转义　　./*[]^%/?~$</p>
<p>:set ignorecase　　忽略大小写的查找</p>
<p>:set noignorecase　　不忽略大小写的查找</p>
<p>查找很长的词，如果一个词很长，键入麻烦，可以将光标移动到该词上，按/*或/#键即可以该单词进行搜索，相当于/搜索。而/#命令相当于?搜索。</p>
<p>:set hlsearch　　高亮搜索结果，所有结果都高亮显示，而不是只显示一个匹配。</p>
<p>:set nohlsearch　　关闭高亮搜索显示</p>
<p>:nohlsearch　　关闭当前的高亮显示，如果再次搜索或者按下n或N键，则会再次高亮。</p>
<p>:set incsearch　　逐步搜索模式，对当前键入的字符进行搜索而不必等待键入完成。</p>
<p>:set wrapscan　　重新搜索，在搜索到文件头或尾时，返回继续搜索，默认开启。</p>
<h1 id="-">替换命令</h1>
<p>ra 将当前字符替换为a，当期字符即光标所在字符。</p>
<p>s/old/new/ 用old替换new，替换当前行的第一个匹配</p>
<p>s/old/new/g 用old替换new，替换当前行的所有匹配</p>
<p>%s/old/new/ 用old替换new，替换所有行的第一个匹配</p>
<p>%s/old/new/g 用old替换new，替换整个文件的所有匹配</p>
<p>:10,20 s/^/    /g 在第10行知第20行每行前面加四个空格，用于缩进。</p>
<p>ddp 交换光标所在行和其下紧邻的一行。</p>
<h1 id="-">移动命令</h1>
<p>h 左移一个字符
l 右移一个字符，这个命令很少用，一般用w代替。
k 上移一个字符
j 下移一个字符
以上四个命令可以配合数字使用，比如20j就是向下移动20行，5h就是向左移动5个字符，在Vim中，很多命令都可以配合数字使用，比如删除10个字符10x，在当前位置后插入3个！，3a！<Esc>，这里的Esc是必须的，否则命令不生效。</p>
<p>w 向前移动一个单词（光标停在单词首部），如果已到行尾，则转至下一行行首。此命令快，可以代替l命令。</p>
<p>b 向后移动一个单词 2b 向后移动2个单词</p>
<p>e，同w，只不过是光标停在单词尾部</p>
<p>ge，同b，光标停在单词尾部。</p>
<p>^ 移动到本行第一个非空白字符上。</p>
<p>0（数字0）移动到本行第一个字符上，</p>
<p><HOME> 移动到本行第一个字符。同0健。</p>
<p>$ 移动到行尾 3$ 移动到下面3行的行尾</p>
<p>gg 移动到文件头。 = [[</p>
<p>G（shift + g） 移动到文件尾。 = ]]</p>
<p>f（find）命令也可以用于移动，fx将找到光标后第一个为x的字符，3fd将找到第三个为d的字符。</p>
<p>F 同f，反向查找。</p>
<p>跳到指定行，冒号+行号，回车，比如跳到240行就是 :240回车。另一个方法是行号+G，比如230G跳到230行。</p>
<p>Ctrl + e 向下滚动一行</p>
<p>Ctrl + y 向上滚动一行</p>
<p>Ctrl + d 向下滚动半屏</p>
<p>Ctrl + u 向上滚动半屏</p>
<p>Ctrl + f 向下滚动一屏</p>
<p>Ctrl + b 向上滚动一屏</p>
<h1 id="-">撤销和重做</h1>
<p>u 撤销（Undo）
U 撤销对整行的操作
Ctrl + r 重做（Redo），即撤销的撤销。</p>
<h1 id="-">删除命令</h1>
<p>x 删除当前字符</p>
<p>3x 删除当前光标开始向后三个字符</p>
<p>X 删除当前字符的前一个字符。X=dh</p>
<p>dl 删除当前字符， dl=x</p>
<p>dh 删除前一个字符</p>
<p>dd 删除当前行</p>
<p>dj 删除上一行</p>
<p>dk 删除下一行</p>
<p>10d 删除当前行开始的10行。</p>
<p>D 删除当前字符至行尾。D=d$</p>
<p>d$ 删除当前字符之后的所有字符（本行）</p>
<p>kdgg 删除当前行之前所有行（不包括当前行）</p>
<p>jdG（jd shift + g）   删除当前行之后所有行（不包括当前行）</p>
<p>:1,10d 删除1-10行</p>
<p>:11,$d 删除11行及以后所有的行</p>
<p>:1,$d 删除所有行</p>
<p>J(shift + j)　　删除两行之间的空行，实际上是合并两行。</p>
<h1 id="-">拷贝和粘贴</h1>
<p>yy 拷贝当前行</p>
<p>nyy 拷贝当前后开始的n行，比如2yy拷贝当前行及其下一行。</p>
<p>p  在当前光标后粘贴,如果之前使用了yy命令来复制一行，那么就在当前行的下一行粘贴。</p>
<p>shift+p 在当前行前粘贴</p>
<p>:1,10 co 20 将1-10行插入到第20行之后。</p>
<p>:1,$ co $ 将整个文件复制一份并添加到文件尾部。</p>
<p>正常模式下按v（逐字）或V（逐行）进入可视模式，然后用jklh命令移动即可选择某些行或字符，再按y即可复制</p>
<p>ddp交换当前行和其下一行</p>
<p>xp交换当前字符和其后一个字符</p>
<h1 id="-">剪切命令</h1>
<p>正常模式下按v（逐字）或V（逐行）进入可视模式，然后用jklh命令移动即可选择某些行或字符，再按d即可剪切</p>
<p>ndd 剪切当前行之后的n行。利用p命令可以对剪切的内容进行粘贴</p>
<p>:1,10d 将1-10行剪切。利用p命令可将剪切后的内容进行粘贴。</p>
<p>:1, 10 m 20 将第1-10行移动到第20行之后。</p>
<h1 id="-">退出命令</h1>
<p>:wq 保存并退出</p>
<p>ZZ 保存并退出</p>
<p>:q! 强制退出并忽略所有更改</p>
<p>:e! 放弃所有修改，并打开原来文件。</p>
<h1 id="-">窗口命令</h1>
<p>:split或new 打开一个新窗口，光标停在顶层的窗口上</p>
<p>:split file或:new file 用新窗口打开文件</p>
<p>split打开的窗口都是横向的，使用vsplit可以纵向打开窗口。</p>
<p>Ctrl+ww 移动到下一个窗口</p>
<p>Ctrl+wj 移动到下方的窗口</p>
<p>Ctrl+wk 移动到上方的窗口</p>
<p>关闭窗口</p>
<p>:close 最后一个窗口不能使用此命令，可以防止意外退出vim。</p>
<p>:q 如果是最后一个被关闭的窗口，那么将退出vim。</p>
<p>ZZ 保存并退出。</p>
<p>关闭所有窗口，只保留当前窗口</p>
<p>:only</p>
<p>录制宏</p>
<p>按q键加任意字母开始录制，再按q键结束录制（这意味着vim中的宏不可嵌套），使用的时候@加宏名，比如qa。。。q录制名为a的宏，@a使用这个宏。</p>
<h1 id="-shell-">执行shell命令</h1>
<p>:!command</p>
<p>:!ls 列出当前目录下文件</p>
<p>:!perl -c script.pl 检查perl脚本语法，可以不用退出vim，非常方便。</p>
<p>:!perl script.pl 执行perl脚本，可以不用退出vim，非常方便。</p>
<p>:suspend或Ctrl - Z 挂起vim，回到shell，按fg可以返回vim。</p>
<h1 id="-">注释命令</h1>
<p>perl程序中/#开始的行为注释，所以要注释某些行，只需在行首加入/#</p>
<p>3,5 s/^//#/g 注释第3-5行</p>
<p>3,5 s/^/#//g 解除3-5行的注释</p>
<p>1,$ s/^//#/g 注释整个文档。</p>
<p>:%s/^//#/g 注释整个文档，此法更快。</p>
<h1 id="-">帮助命令</h1>
<p>:help or F1 显示整个帮助
:help xxx 显示xxx的帮助，比如 :help i, :help CTRL-[（即Ctrl+[的帮助）。
:help &#39;number&#39; Vim选项的帮助用单引号括起
:help <Esc> 特殊键的帮助用&lt;&gt;扩起
:help -t Vim启动参数的帮助用-
：help i<em><Esc> 插入模式下Esc的帮助，某个模式下的帮助用模式</em>主题的模式
帮助文件中位于||之间的内容是超链接，可以用Ctrl+]进入链接，Ctrl+o（Ctrl + t）返回</p>
<h1 id="-">其他非编辑命令</h1>
<p>. 重复前一次命令</p>
<p>:set ruler?　　查看是否设置了ruler，在.vimrc中，使用set命令设制的选项都可以通过这个命令查看</p>
<p>:scriptnames　　查看vim脚本文件的位置，比如.vimrc文件，语法文件及plugin等。</p>
<p>:set list 显示非打印字符，如tab，空格，行尾等。如果tab无法显示，请确定用set lcs=tab:&gt;-命令设置了.vimrc文件，并确保你的文件中的确有tab，如果开启了expendtab，那么tab将被扩展为空格。</p>
<p>Vim教程
在Unix系统上
$ vimtutor
在Windows系统上
:help tutor
:syntax 列出已经定义的语法项
:syntax clear 清除已定义的语法规则
:syntax case match 大小写敏感，int和Int将视为不同的语法元素
:syntax case ignore 大小写无关，int和Int将视为相同的语法元素，并使用同样的配色方案</p>

      
    </div>
    
    
      
    
    
        <footer id="post-meta">
        <span class="categories">Posted in<span class="breadcrumb fa fa-folder"><li><a href="/categories/linux/">linux</a></li></span></span> | <span class="tags">Tagged <a href="/tags/linux/" class="label label-primary">linux</a></span> | <span class="time">recent updated:<time title="2014-03-07 09:54:36"datetime="2014-03-07 09:54:36"> mar. 7 2014</time></span> | <span class="comment-link">
<a href="http://itsolife.com/2014/02/02/2014-02-02-linux--Vim命令合集/#comments" class="ds-thread-count comment-link" data-thread-key="2014-02-02-linux--Vim命令合集" data-count-type="comments">暂无评论</a></span>
        </footer>
    
    <div class="clearfix"></div>
  </div>
</div>
</article>





<ul id="pagination" class="pagination pagination-lg">
  <li><a class="extend prev" href="/page/114/">&laquo;</a></li><li><a class="page-number" href="/">1</a></li><li><a class="page-number" href="/page/2/">2</a></li><li><span class="space">&hellip;</span></li><li><a class="page-number" href="/page/112/">112</a></li><li><a class="page-number" href="/page/113/">113</a></li><li><a class="page-number" href="/page/114/">114</a></li><li class="active"><li><span class="page-number current">115</span></li><li><a class="page-number" href="/page/116/">116</a></li><li><a class="page-number" href="/page/117/">117</a></li><li><a class="page-number" href="/page/118/">118</a></li><li><span class="space">&hellip;</span></li></li><li><a class="page-number" href="/page/162/">162</a></li><li><a class="page-number" href="/page/163/">163</a></li><li><a class="extend next" href="/page/116/">&raquo;</a></li>
  <div class="clearfix"></div>
</ul></div><!--wapper-->
       </div><!-- ID main-col END -->
       <aside id="sidebar" class="alignright col-sx-6 col-sm-4 col-md-3 col-lg-3">
<div id="widget_search" class="widget panel panel-primary">
    <form action="//google.com/search" method="get" accept-charset="utf-8">
  <div class="input-group">
    <input class="form-control" id="searchbox" type="search" name="q" results="0" placeholder="search">
    <span class="input-group-btn">
      <button class="btn btn-default" type="submit">Go!</button>
    </span>
    <input type="hidden" name="q" value="site:itsolife.com">
  </div>
</form>
</div>

<div id="widget_category" class="widget panel panel-primary">
  <div class="panel-heading">category</div>  <div data-src='category' class='ajax_widgets'>正在加载...</div>
</div>

<div id="widget_recent_posts" class="widget panel panel-primary">
  <div class="panel-heading">recent posts</div>  <div data-src='recent_posts' class='ajax_widgets'>正在加载...</div></div>

<div id="widget_tagcloud" class="widget panel panel-primary">
  <div class="panel-heading">tagcloud</div>  <div data-src='tagcloud' class='ajax_widgets'>正在加载...</div></div>

<div id="widget_latest_update_posts" class="widget panel panel-primary">
  <div class="panel-heading">最近更新</div>  <div data-src='latest_update_posts' class='ajax_widgets'>正在加载...</div></div>

<div id="widget_recent_comments" class="widget panel panel-primary">
  <div class="panel-heading">recent comments</div>  

<div class="list-group-item ds-recent-comments" data-num-items="6" data-show-avatars="1" data-show-time="1" data-show-title="1" data-show-admin="1" data-excerpt-length="50"></div>



</div>

</aside>
       <div class="clearfix"></div>
     </div><!-- row END -->
  </div>
  <footer id="footer" class="container">
  <div class="panel panel-info">
  <section id='footer_widget'></section>  <div class="panel-footer">
  <div id="site-info">
    <span class='author'>
  
  &copy; 2014 RobinChia
  
    &nbsp;&nbsp;</span>
  
  <span id='analytics-51la'></span><span id='analytics-google'>
<script type="text/javascript">
  var _gaq = _gaq || [];
  _gaq.push(['_setAccount', 'UA-48559895-1']);
  _gaq.push(['_trackPageview']);
  _js2load.push({src:('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js'});
</script></span><span id='analytics-cnzz'>
<script type="text/javascript">var cnzz_protocol = (("https:" == document.location.protocol) ? " https://" : " http://");document.write(unescape("%3Cspan id='cnzz_stat_icon_5774006'%3E%3C/span%3E%3Cscript src='" + cnzz_protocol + "s17.cnzz.com/stat.php%3Fid%3D5774006%26show%3Dpic' type='text/javascript'%3E%3C/script%3E"));</script>
</span><span id='analytics-baidu'>
<script>
var _hmt = _hmt || [];
_js2load.push({src:"//hm.baidu.com/hm.js?1442f50724afc42380b51f097c43082c"});
</script>
</span>  </div>
  <div id="copyright">Site powered by <a href='http://zespia.tw/hexo/'><strong>hexo</strong></a>  update time: <em>2014-03-30 14:30:16</em></span></div>
</div>
<div class="clearfix"></div>


  </div>
  </footer>
  
        <script src="http://cdn.bootcss.com/jquery/1.10.2/jquery.min.js"></script>        
        <script src="http://cdn.staticfile.org/twitter-bootstrap/3.1.0/js/bootstrap.min.js"></script>        
                
        <script src="http://cdn.bootcss.com/prettify/r298/prettify.min.js"></script>    <script type="text/javascript">
   var lang=["bsh", "c", "cc", "cpp", "cs", "csh", "cyc", "cv", "htm", "html",
    "java", "js", "m", "mxml", "perl", "pl", "pm", "py", "rb", "sh",
    "xhtml", "xml", "xsl"];
   var pretty_base='';
   $('script').each(function(){
	var c = $(this).attr('src');
	if (!c)
	    return;
	if (c.match(/(\/)?prettify(\.min)?\.js/i))
	{
	    var index = c.lastIndexOf('/');
	    if (index != -1)
		pretty_base = c.substr(0,index + 1);
	    return false;
	}
   })
   $('pre code').each(function(){
	var c = $(this).attr('class')
	if (!c)
	    return;
	c = c.match(/\s?(lang\-\w+)/i);
	if (c && lang.indexOf(c[1]) == -1)
	{
	    lang.push(c[1]);
	    $.getScript(pretty_base + c[1] + '.min.js');
	}
   })

    $(window).load(function(){
       $("pre").addClass("prettyprint");
       prettyPrint();
    })
</script>    
            <script type="text/javascript">
var duoshuoQuery = {short_name:"robinchia"};
_js2load.push({src:'http://static.duoshuo.com/embed.js',charset:'UTF-8'});
</script>
    
            <!--wumii_relatedItems-->
    
        <script src="http://cdn.bootcss.com/fancybox/2.1.5/jquery.fancybox.min.js"></script>    <script type="text/javascript">
(function($){
  $('.entry').each(function(i){
    $(this).find('img').each(function(){
      var alt = this.alt;

      if (alt){
        $(this).before('<span class="caption">' + alt + '</span>');
      }

      $(this).wrap('<a href="' + this.src + '" title="' + alt + '" class="fancybox" rel="fancybox' + i + '" />');
    });
  });

  $('.fancybox').fancybox();
})(jQuery);
</script>    
        <script src="http://cdn.bootcss.com/mathjax/2.3/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>    <script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      processEscapes: true
    }
  });
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>
    
<script type="text/javascript">
$('.ajax_widgets').each(function(){var src=$(this).attr('data-src');if(src)$(this).load('/widgets/'+src+'.html');});
$.each(_js2load,function(index,obj){loadjs(obj.src,obj.charset)});
</script>

<div id="scroll2top">
<img src="/scroll2top/arrow.png"/>
</div>
<script src="/scroll2top/scroll2top.min.js"></script>
<div id="winterland">
  <canvas></canvas>
</div>
<script src="/js/winterland.min.js"></script>

  </body>
</html>
