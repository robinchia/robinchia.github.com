
<!DOCTYPE HTML>
<html>
<head>
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<title>The 108 Page | It so life</title>
<meta name="author" content="RobinChia">

<meta name="description" content="It so life">


<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">


<meta property="og:site_name" content="It so life"/>

<!--[if IE]><style>.testIE.IE{display:inline;}</style><![endif]-->
<!--[if lte IE 7]><link rel="stylesheet" href="/css/ie7.css" type="text/css"><![endif]-->
<!--[if (lt IE 9)&(gt IE 7)]><style>.testIE.IE8{display:inline;}</style><![endif]-->
<!--[if gt IE 8]><style>.testIE.IE9{display:inline;}</style><![endif]-->

<link href="/favicon.png" rel="icon">
<link rel="alternate" href="/atom.xml" title="It so life Feed" type="application/atom+xml">

<link rel="stylesheet" href="http://cdn.staticfile.org/twitter-bootstrap/3.1.0/css/bootstrap.min.css" type="text/css">
<link rel="stylesheet" href="http://cdn.bootcss.com/font-awesome/4.0.3/css/font-awesome.min.css" type="text/css">
<link rel="stylesheet" href="http://cdn.bootcss.com/prettify/r298/prettify.min.css" type="text/css">
<link rel="stylesheet" href="http://cdn.bootcss.com/fancybox/2.1.5/jquery.fancybox.min.css" type="text/css">
<link rel="stylesheet" href="/css/style.css" type="text/css">
<!--[if lt IE 9]>
   <style>article,aside,dialog,footer,header,section,footer,nav,figure,menu{display:block}</style>
   <script src="http://cdn.staticfile.org/html5shiv/r29/html5.min.js"></script>
   <script src="http://cdn.staticfile.org/respond.js/1.4.2/respond.min.js"></script>
<link href="http://cdn.staticfile.org/respond.js/1.4.2/respond-proxy.html" id="respond-proxy" rel="respond-proxy" />
<link href="/js/respond.proxy.gif" id="respond-redirect" rel="respond-redirect" />
<script src="/js/respond.proxy.js"></script>
   <script src="http://cdn.bootcss.com/selectivizr/1.0.2/selectivizr-min.js"></script>
<![endif]-->
<script type="text/javascript">
function loadjs(c,d){var a=document.createElement("script");a.async=!0;a.type="text/javascript";a.src=c;a.charset=d||"gbk";(document.getElementsByTagName("HEAD")[0]||document.getElementsByTagName("BODY")[0]).appendChild(a)};
var _protocol = (("https:" == document.location.protocol) ? " https://" : " http://");
var _js2load = [];
</script>

</head>
<body>
      <header id="header" class="container"><nav id="main-nav" class="navbar navbar-default navbar-fixed-top " role="navigation">
  <div class="container">
    <div class="navbar-header">
    <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".navbar-collapse">
      <span class="sr-only">Toggle navigation</span>
      <span class="icon-bar"></span>
      <span class="icon-bar"></span>
      <span class="icon-bar"></span>
    </button>
    <a class="navbar-brand" href="/">It so life</a>
    </div>
    <div  class="collapse navbar-collapse">
      <ul  class="nav navbar-nav">
  
        <li><a href="/" title="Home">Home</a></li>      
        <li><a href="/about/" title="About">About</a></li>      
        <li><a href="/archives/" title="Archives">Archives</a></li>      
      <li class='dropdown'>
        <a class='dropdown-toggle' data-toggle='dropdown' href='#'>website<b class='caret'></b></a>
        <ul class='dropdown-menu pure-menu-selected'>
    
          <li><a href="//groups.google.com/forum/#!forum/pongba" title="TopLanguage">TopLanguage</a></li>    
          <li><a href="//itpub.net/" title="ITPub">ITPub</a></li>    
          <li><a href="//blog.jobbole.com/" title="Bole">Bole</a></li>    
          <li><a href="//nosql-database.org/" title="nosql">nosql</a></li>    
          <li><a href="//gitimmersion.googol.im/" title="Git">Git</a></li>    
        </ul>
      </li>
    
      </ul>
      <ul class="nav navbar-nav navbar-right">
      
        <li><a href="/atom.xml">RSS</a></li>
      
      
        <li><a href="https://twitter.com/robinchia">twitter</a></li>
      
      
      
      
        <li><a href="https://github.com/robinchia">github</a></li>
      
      </ul>
    </div>
  </div>
</nav>
<div class="clearfix"></div>
</header>
  <div id='content' class="container">
     <div class="page-header-wrapper">
      <!--[if lt IE 9]><div class="alert alert-warning alert-dismissable"><button type="button" class="close" data-dismiss="alert" aria-hidden="true">&times;</button><strong>提示:</strong>您的浏览器版本太低了,建议升级到 <strong><a href="http://windows.microsoft.com/zh-cn/internet-explorer/download-ie" title="IE9">IE9</a></strong> 以上,本站使用<a href="https://www.google.com/intl/zh-CN/chrome/">Chrome浏览器</a>可以获得最好的显示效果.</div><![endif]-->
      <div class="page-header"><h1 align="center"><big>It so life</big> </h1>
        <h5 align="center"><big>love as life</big></h5>
      </div>
     </div>
     <div class="row">
       <div id="main-col" class="alignleft col-sx-12 col-sm-8 col-md-9 col-lg-9">
      <section id='header_widget'></section>
          <div id="wrapper"><article>
<div id="post" class="post well">
  <div class="post-content">
    <header class="well-sm">
      <i class="fa icon fa-5x pull-left"></i>
      <h1 class="title"><a href="/2014/02/02/2014-02-02-hadoop--Hadoop知识分享文稿byquqi99-技术并艺术着/">Hadoop知识分享文稿 ( by quqi99 ) </a></h1>
      
        <span>Posted on<time datetime="2014-02-02T01:54:36.000Z"> <a href="/2014/02/02/2014-02-02-hadoop--Hadoop知识分享文稿byquqi99-技术并艺术着/">feb. 2 2014</a></time></span>      
    </header>
    
    
    <div class="entry">
      
        <h1 id="hadoop-by-quqi99-">Hadoop知识分享文稿 ( by quqi99 ) - 技术并艺术着</h1>
<p>您还未登录！|<a href="https://passport.csdn.net/account/login" target="_blank">登录</a>|<a href="https://passport.csdn.net/account/register" target="_blank">注册</a>|<a href="https://passport.csdn.net/help/faq" target="_blank">帮助</a></p>
<ul>
<li><a href="http://www.csdn.net/" target="_blank">首页</a></li>
<li><a href="http://news.csdn.net/" target="_blank">业界</a></li>
<li><a href="http://mobile.csdn.net/" target="_blank">移动</a></li>
<li><a href="http://cloud.csdn.net/" target="_blank">云计算</a></li>
<li><a href="http://sd.csdn.net/" target="_blank">研发</a></li>
<li><a href="http://bbs.csdn.net/" target="_blank">论坛</a></li>
<li><a href="http://blog.csdn.net/" target="_blank">博客</a></li>
<li><a href="http://download.csdn.net/" target="_blank">下载</a></li>
<li><h2 id="-"><a href="">更多</a></h2>
</li>
</ul>
<h1 id="-http-blog-csdn-net-quqi99-"><a href="http://blog.csdn.net/quqi99" target="_blank">技术并艺术着</a></h1>
<h2 id="-blog">张华的技术Blog</h2>
<ul>
<li><a href="http://blog.csdn.net/quqi99?viewmode=contents" target="_blank"><img src="" alt="">目录视图</a></li>
<li><a href="http://blog.csdn.net/quqi99?viewmode=list" target="_blank"><img src="" alt="">摘要视图</a></li>
<li><a href="http://blog.csdn.net/quqi99/rss/list" target="_blank"><img src="" alt="">订阅</a>
<a href="https://code.csdn.net/blog/12" target="_blank">公告：博客新增直接引用代码功能</a>        <a href="http://www.csdn.net/article/2013-08-06/2816471" target="_blank">专访李铁军：从医生到金山首席安全专家的转变</a>      <a href="http://blog.csdn.net/csdnproduct/article/details/9495139" target="_blank">CSDN博客频道自定义域名、标签搜索功能上线啦！</a>      <a href="http://blog.csdn.net/adali/article/details/9813651" target="_blank">独一无二的职位：开源社区经理</a></li>
</ul>
<h3 id="-hadoop-by-quqi99-"><a href="">[置顶] Hadoop知识分享文稿 ( by quqi99 )</a></h3>
<p>分类： <a href="http://blog.csdn.net/quqi99/article/category/328029" target="_blank">Seach Engine</a>  2011-03-31 15:19 1977人阅读 <a href="">评论</a>(0) <a href="&quot;收藏&quot;">收藏</a> <a href="&quot;举报&quot;">举报</a>
<a href="http://blog.csdn.net/tag/details.html?tag=hadoop" target="_blank">hadoop</a><a href="http://blog.csdn.net/tag/details.html?tag=%e4%bb%bb%e5%8a%a1" target="_blank">任务</a><a href="http://blog.csdn.net/tag/details.html?tag=mapreduce" target="_blank">mapreduce</a><a href="http://blog.csdn.net/tag/details.html?tag=%e4%bb%bb%e5%8a%a1%e8%b0%83%e5%ba%a6" target="_blank">任务调度</a><a href="http://blog.csdn.net/tag/details.html?tag=%e9%9b%86%e7%be%a4" target="_blank">集群</a><a href="http://blog.csdn.net/tag/details.html?tag=%e4%bd%9c%e4%b8%9a" target="_blank">作业</a></p>
<p>目录<a href="&quot;系统根据文章中H1到H6标签自动生成文章目录&quot;">(?)</a><a href="&quot;展开&quot;">[+]</a></p>
<ol>
<li><a href="">作者张华 写于2010-08-15   发表于2011-03-31 版权声明可以任意转载转载时请务必以超链接形式标明文章原始出处和作者信息及本版权声明</a></li>
<li><a href="">httpblogcsdnnetquqi99</a></li>
<li><p><a href="">hadoop 理论基础</a></p>
</li>
<li><p><a href="">hadoop 是什么</a></p>
</li>
<li><a href="">hadoop 项目</a></li>
<li><a href="">MapReduce 任务的运行流程</a></li>
<li><p><a href="">MapReduce 任务的数据流图</a></p>
</li>
<li><p><a href="">hadoop 入门实战</a></p>
</li>
<li><p><a href="">测试环境</a></p>
</li>
<li><a href="">测试程序</a></li>
<li><a href="">属性配置</a></li>
<li><a href="">免密码 SSH 设置</a></li>
<li><a href="">配置 hosts</a></li>
<li><a href="">格式化 HDFS 文件系统</a></li>
<li><a href="">启动守护进程</a></li>
<li><p><a href="">运行程序</a></p>
</li>
<li><p><a href="">hadoop 高级进阶</a></p>
</li>
<li><a href="">hadoop 应用案例</a></li>
<li><a href="">参考文献</a><pre><code>                       **Hadoop知识分享文稿 ( by quqi99 )**
</code></pre></li>
</ol>
<h2 id="-2010-08-15-2011-03-31"><a href=""></a>作者：张华 写于：2010-08-15   发表于：2011-03-31</h2>
<p>版权声明：可以任意转载，转载时请务必以超链接形式标明文章原始出处和作者信息及本版权声明</p>
<h2 id="-http-blog-csdn-net-quqi99-"><a href=""></a>( <a href="http://blog.csdn.net/quqi99">http://blog.csdn.net/quqi99</a> )</h2>
<p><strong>内容目录</strong></p>
<p>目 录</p>
<p>1 hadoop 理论基础 3</p>
<p>1.1 hadoop 是什么 3</p>
<p>1.2 hadoop 项目 3</p>
<p>1.3 Map/Reduce 任务的运行流程 4</p>
<p>1.4 Map/Reduce 任务的数据流图 5</p>
<p>2 hadoop 入门实战 7</p>
<p>2.1  测试环境 7</p>
<p>2.2  测试程序 7</p>
<p>2.3  属性配置 9</p>
<p>2.4  免密码SSH 设置 10</p>
<p>2.5  配置hosts 11</p>
<p>2.6  格式化HDFS 文件系统 11</p>
<p>2.7  启动守护进程 11</p>
<p>2.8  运行程序 11</p>
<p>3 hadoop 高级进阶 12</p>
<p>4 hadoop 应用案例 12</p>
<p>5  参考文献 12</p>
<h1 id="-1-hadoop-"><a href=""></a>1 hadoop  理论基础</h1>
<h2 id="-1-1-hadoop-"><a href=""></a>1.1 hadoop  是什么</h2>
<p>Hadoop  是 Doug Cutting  开发的，他是一个相当牛的哥们，他同时是大名鼎鼎的 Lucene  及 Nutch  的作者。</p>
<p>我是这样理解 hadoop  的，它就是用来对海量数据进行存储与分析的一个开源软件。它包括两块：</p>
<p>1  ） HDFS ( Hadoop Distrubuted File System )  ，可以对重要数据进行冗余存储，有点类似于冗余磁盘陈列。</p>
<p>2  ）对 Map/Reduce  编程模型的一个实现。当然，关系型数据库（ RDBMS  ）也能做类似的事情，但为什么不用 RDBMS  呢？我们知道，让计算移动于数据上比让数据移动到计算更有效率。这使得 Map/Reduce  适合数据被一次写入和多次读取的应用，而 RDBMS  更适合持续更新的数据集。</p>
<h2 id="-1-2-hadoop-"><a href=""></a>1.2 hadoop  项目</h2>
<p>如今，广义上的 Hadoop  已经发展成为一个分布式计算基础架构这把“大伞”下相关子项目的集合，其技术栈如下图所示：</p>
<p>图：</p>
<pre><code>                                     ![]()
</code></pre><p><img src="http://blog.csdn.net/root/Library/Caches/TemporaryItems/moz-screenshot-4.png" alt=""></p>
<pre><code>                                                图1 hadoop 的子项目
</code></pre><ul>
<li>Core ： 一系列分布式文件系统和通用I/O 的组件和接口( 序列化、Java RPC 和持久化数据结构) 。</li>
<li>Avro ： 用于数据的序列化，当然，JDK 中也有Seriable 接口，但hadoop 中有它自己的序列化方式，具说更有效率。</li>
<li>MapReduce ： 分布式数据处理模式和执行环境，运行于大型商用机集群。</li>
<li>HDFS ： 分布式文件系统，运行于大型商用机集群。</li>
<li>Pig ： HDFS 上的数据检索语言，类似于RDBMS 中的SQL 语言。</li>
<li>Hbase ： 一个分布式的、列存储数据库。HBase 使用HDFS 作为底层存储，同时支持MapReduce 的批量式计算和点查询( 随机读取) 。</li>
<li>ZooKeeper ： 一个分布式的、高可用性的协调服务。ZooKeeper 提供分布式锁之类的基本服务用于构建分布式应用。</li>
<li>Hive ： 分布式数据仓库。Hive 管理HDFS 中存储的数据，并提供基于SQL 的查询语言( 由运行时引擎翻译成MapReduce 作业) 用以查询数据。</li>
<li>Chukwa ： 分布式数据收集和分析系统。Chukwa 运行HDFS 中存储数据的收集器，它使用MapReduce 来生成报告。</li>
</ul>
<h2 id="-1-3-map-reduce-"><a href=""></a> 1.3 Map/Reduce  任务的运行流程</h2>
<pre><code>                 ![]()
</code></pre><p>JobClient  的  submitJob()  方法的作业提交过程如下：</p>
<p>1  ）向 Jobtraker  请求一个新作业 ID</p>
<p>2  ） 调用 JobTracker  的 getNewJobId()</p>
<p>3  ）  JobClient  进行作业划分，并将划分后的输入及作业的 JAR  文件、配置文件等复制到 HDFS  中去</p>
<p>4  ） 提交作业，会把此调用放入到一个内部的队列中，交由作业调度器进行调度。值得一提的是，针对  Map  任务与 Reduce  任务，任务调度器是优先选择 Map  任务的，另外，任务调度器在选择 Reduce  任务时并没有考虑数据的本地化。然而，针对一个 Map  任务，它考虑的是 Tasktracker  网络位置和选取一个距离其输入划分文件最近的 Tasktracker  ，它可能是数据本地化的，也可能是机架本地化的，还可能得到不同的机架上取数据。</p>
<p>5  ） 初始化包括创建一个代表该正在运行的作业的对象，它封装任务和记录信息，以便跟踪任务的状态和进度。</p>
<p>6  ） JobTracker  任务调度器首先从共享文件系统中获取 JobClient  已计算好的输入划分信息，然后为每个划分创建一个 Map  任务。创建 的 reduce  任务的数量是由 JobConf  的 Mapred.reduce.tasks  属性决定，它是用 setNumReduceTask()  方法来设置的。</p>
<p>7  ） TaskTracker  执行一个简单的循环，定期发送心跳（ Heartbeat  ）方法调用 Jobtracker  告诉是否还活着，同时，心跳还会报告任务运行的是否已经准备运行新的任务。</p>
<p>8  ） TaskTracker  已经被分配了任务，下一步是运行任务。首先它需要将它所需的全部文件从 HDFS  中复制到本地磁盘。</p>
<p>9  ）紧接着，它要启动一个新的 Java  虚拟机来运行每个任务，这使得用户所定义的 Map  和 Reduce  函数的任务缺陷都不会影响 TaskTracker  （比如导致它崩溃或者挂起）</p>
<p>10  ）运行 Map  任务或者 Reduce  任务，值得一提的是，这些任务使用标准输入与输出流，换句话说，你可以用任务语言（如 JAVA  ， C++  ， Shell  等）来实现 Map  和 Reduce  ，只要保证它们也使用标准输入与输出流，就可以将输出的键值对传回给 JAVA  进程了。</p>
<h2 id="-1-4-map-reduce-"><a href=""></a> 1.4 Map/Reduce  任务的数据流图</h2>
<p><img src="" alt=""></p>
<pre><code>    图3  Map/Reduce  中单一 Reduce  任务的数据流图
</code></pre><p><img src="" alt=""></p>
<pre><code>             图4  Map/Reduce  中多个 Reduce  任务的数据流图
</code></pre><p><img src="" alt=""></p>
<pre><code>            图5  MapReduce  中没有 Reduce  任务的数据流图
</code></pre><p><strong>任务粒度</strong>   ： 分片的个数，在将原始大数据切割成小数据集时，通常让小数据集小于或等于 HDFS  中的一个 Block  的大小（缺省是 64M)  ，这样能够保证一个小数据集位于一台计算机上，便于本地计算。 有 M   个 小数据集 待处理，就启动 M   个 Map   任务，注意这 M   个 Map   任务分布于 N   台计算机上并行运行，Reduce   任务的数量 R   则可由用户指定 。</p>
<p><strong>Map</strong>   ： 输入 <k1, v1>   输出 List(<k2,v2>)</p>
<p><strong>Reduce</strong>   ： 输入 <k2,List(v2)>   输出 <k3,v3></p>
<p><strong>分区（</strong>  <strong>Partition)</strong>  :   把 Map   任务输出的中间结果按 key   的范围划分成 R   份 ( R  是预先定义的 Reduce  任务的个数) ，划分时通常使用 hash  函数如: hash(key) mod R ，这样可以保证某一段范围内的 key ，一定是由一个 Reduce  任务来处理，可以简化 Reduce  的过程。</p>
<p><strong>Combine</strong>   :   在  partition   之前，还可以对中间结果先做  combine  ，即将中间结果中有相同  key  的  <key, value>   对合并成一对。 combine   的过程与  Reduce   的过程类似，很多情况下就可以直接使用  Reduce   函数，但  combine   是作为  Map   任务的一部分，在执行完  Map   函数后紧接着执行的。 Combine   能够减少中间结果中  <key, value>   对的数目，从而减少网络流量。</p>
<p>下面举个例子来着重说明 Combine  ， hadoop  允许用户声明一个 combiner  运行在 Map  的输出上，它的输出再作为 Reduce  的输入。例如，找出每一年的最调气温：</p>
<p>假如用户的输入的分片数是 2  ，那么：</p>
<p>1  ）第一个 Map  的输出如下：</p>
<p>（ 1950  ， 0  ）</p>
<p>（ 1950  ， 20  ）</p>
<p>（ 1950  ， 10  ）</p>
<p>2  ） 第二个 Map  的输出如下：</p>
<p>（ 1950  ， 25  ）</p>
<p>（ 1950  ， 15  ）</p>
<p>3  ） Reduce  的输入如下：</p>
<p>（ 1950  ，［ 0  ， 20  ， 10  ， 25  ， 15  ］）</p>
<p><strong>注意：如果有</strong>   <strong>combine</strong>    <strong>的话，此时</strong>   <strong>Reduce</strong>    <strong>的输入应该是：</strong></p>
<p><strong>max(0, 20, 10, 25, 15) = max(max(0,20,10), max(25,15)) = max(20,25)</strong></p>
<p><strong>combine</strong>    <strong>并不能取代</strong>   <strong>reduce,</strong>    <strong>例如，如果我们计算平均气温，便不能使用</strong>   <strong>combine</strong>    <strong>，因为：</strong></p>
<p><strong>mean(0,20,10,25,15) = 14</strong></p>
<p><strong>但是：</strong></p>
<p><strong>mean(mean(0,20,10), mean(25,15)) = mean(10,20) = 15</strong></p>
<p>4  ） Reduce  的输出如下：</p>
<p>（ 1950  ， 25  ）</p>
<h1 id="-2-hadoop-"><a href=""></a>2 hadoop  入门实战</h1>
<p>hadoop  有三种部署模式：</p>
<ul>
<li>单机模式：没有守护进程，一切都运行在单个 JVM  上，适合测试与调试。</li>
<li>伪集群模式：守护进程在本地运行，适合模拟集群。</li>
<li>集群模式：守护进程运行在集群的某台机器上。</li>
</ul>
<p>所以，在以上任一特定模式运行 hadoop  时，只需要做两件事情：</p>
<p>1  ） 设置适当属性</p>
<p>2  ）启动 hadoop  的守护进程（名称节点，二级名称节名，数据节点）</p>
<p>hadoop  默认的是单机模式，下面，我们将着重介绍在集群模式是如何部署？</p>
<h2 id="-2-1-"><a href=""></a>2.1   测试环境</h2>
<p>用两台机器做为测试环境 ,   通常，集群里的一台机器被指定为  NameNode  ，另一台不同的机器被指定为 JobTracker  ，这些机器是 <strong>masters;</strong>  余下的机器即作为 DataNode  <strong>也</strong> 作为 TaskTracker  ，这些机器是 <strong>slaves</strong>  <strong>。</strong></p>
<p>1  ）  master (JobTracker &amp; NameNode)  ：我的工作机  ( zhanghua  .quqi.com)</p>
<p>2  ）  slave (TaskTracker &amp; DataNode)  ：我的开发机 ( tadev03  .quqi.com)</p>
<p>3)   两机均已安装 ssh   与  rsync</p>
<h2 id="-2-2-"><a href=""></a>2.2   测试程序</h2>
<p>1  ）  /home/workspace/hadoopExample/input/file01:</p>
<p>Hello World Bye World</p>
<p>2) /home/workspace/hadoopExample/input/file02:</p>
<p>Hello  Hadoop    Goodbye  Hadoop</p>
<ol>
<li>WordCount.java</li>
</ol>
<p><strong>package</strong>    com.TripResearch.hadoop;</p>
<p><strong>import</strong>   java.io.IOException;</p>
<p><strong>import</strong>   java.util.Iterator;</p>
<p><strong>import</strong>   java.util.StringTokenizer;</p>
<p><strong>import</strong>   org.apache.hadoop.fs.Path;</p>
<p><strong>import</strong>   org.apache.hadoop.io.IntWritable;</p>
<p><strong>import</strong>   org.apache.hadoop.io.LongWritable;</p>
<p><strong>import</strong>   org.apache.hadoop.io.Text;</p>
<p><strong>import</strong>   org.apache.hadoop.mapred. FileInputFormat  ;</p>
<p><strong>import</strong>   org.apache.hadoop.mapred.FileOutputFormat;</p>
<p><strong>import</strong>   org.apache.hadoop.mapred.JobClient;</p>
<p><strong>import</strong>   org.apache.hadoop.mapred. JobConf  ;</p>
<p><strong>import</strong>   org.apache.hadoop.mapred. MapReduceBase  ;</p>
<p><strong>import</strong>   org.apache.hadoop.mapred. Mapper  ;</p>
<p><strong>import</strong>   org.apache.hadoop.mapred.OutputCollector;</p>
<p><strong>import</strong>   org.apache.hadoop.mapred. Reducer  ;</p>
<p><strong>import</strong>   org.apache.hadoop.mapred.Reporter;</p>
<p><strong>import</strong>   org.apache.hadoop.mapred. TextInputFormat  ;</p>
<p><strong>import</strong>   org.apache.hadoop.mapred. TextOutputFormat  ;</p>
<p>//<em>/</em></p>
<p>/<em>  <em>*@author</em></em>    huazhang</p>
<p>/*/</p>
<p>@SuppressWarnings ( &quot;deprecation&quot; )</p>
<p><strong>public</strong>    <strong>class</strong>   WordCount {</p>
<p><strong>public</strong>    <strong>static</strong>    <strong>class</strong>   MyMap  <strong>extends</strong>    MapReduceBase    <strong>implements</strong></p>
<p>Mapper <LongWritable, Text, Text, IntWritable> {</p>
<p><strong>private</strong>    <strong>final</strong>    <strong>static</strong>   IntWritable  <em>one</em>   =  <strong>new</strong>   IntWritable(1);</p>
<p><strong>private</strong>   Text  word  =  <strong>new</strong>   Text();</p>
<p><strong>public</strong>    <strong>void</strong>   map(LongWritable key, Text value,</p>
<p>OutputCollector<Text, IntWritable> output, Reporter reporter)</p>
<p><strong>throws</strong>   IOException {</p>
<p>String line = value.toString();</p>
<p>StringTokenizer tokenizer =  <strong>new</strong>   StringTokenizer(line);</p>
<p><strong>while</strong>   (tokenizer.hasMoreTokens()) {</p>
<p>word .set(tokenizer.nextToken());</p>
<p>output.collect( word ,  <em>one</em>  );</p>
<p>}</p>
<p>}</p>
<p>}</p>
<p><strong>public</strong>    <strong>static</strong>    <strong>class</strong>   MyReduce  <strong>extends</strong>    MapReduceBase    <strong>implements</strong></p>
<p>Reducer <Text, IntWritable, Text, IntWritable> {</p>
<p><strong>public</strong>    <strong>void</strong>   reduce(Text key, Iterator<IntWritable> values,</p>
<p>OutputCollector<Text, IntWritable> output, Reporter reporter)</p>
<p><strong>throws</strong>   IOException {</p>
<p><strong>int</strong>   sum = 0;</p>
<p><strong>while</strong>   (values.hasNext()) {</p>
<p>sum += values.next().get();</p>
<p>}</p>
<p>output.collect(key,  <strong>new</strong>   IntWritable(sum));</p>
<p>}</p>
<p>}</p>
<p><strong>public</strong>    <strong>static</strong>    <strong>void</strong>   main(String[] args)  <strong>throws</strong>   Exception {</p>
<p>JobConf   conf =  <strong>new</strong>   JobConf(WordCount. <strong>class</strong>  );</p>
<p>conf.setJobName( &quot;wordcount&quot; );</p>
<p>conf.setOutputKeyClass(Text. <strong>class</strong>  );</p>
<p>conf.setOutputValueClass(IntWritable. <strong>class</strong>  );</p>
<p>conf.setMapperClass(MyMap. <strong>class</strong>  );</p>
<p>conf.setCombinerClass(MyReduce. <strong>class</strong>  );</p>
<p>conf.setReducerClass(MyReduce. <strong>class</strong>  );</p>
<p>conf.setInputFormat( TextInputFormat  . <strong>class</strong>  );</p>
<p>conf.setOutputFormat( TextOutputFormat  . <strong>class</strong>  );</p>
<p>FileInputFormat  . <em>setInputPaths</em>  (conf,  <strong>new</strong>   Path(args[0]));</p>
<p>FileOutputFormat. <em>setOutputPath</em>  (conf,  <strong>new</strong>   Path(args[1]));</p>
<p>JobClient.<em>runJob</em> (conf);</p>
<p>}</p>
<p>}</p>
<p><img src="" alt=""></p>
<h2 id="-2-3-"><a href=""></a>2.3   属性配置</h2>
<p>按下图所示修改至少 3  个属性, 如下图所示：</p>
<p>   <img src="" alt=""></p>
<ol>
<li></li>
<li><p>conf/core-site.xml</p>
</li>
</ol>
<configuration>

<property>

<name>fs.default.name</name>

<value>hdfs://zhanghua  .quqi.com:9000</value>

</property>

</configuration>

<p>注意：此处如果是伪集群模式可配置为  hdfs://localhost:9000 ,    是本地模式则为：  localhost:9000   。另外，其他输入输入路径，是本地模式是本地文件系统的路径，是非地模式，用 hdfs  文件系统的路径格式。</p>
<ol>
<li>conf/hdfs-site.xml</li>
</ol>
<configuration>

<property>

<name>dfs.replication</name>

<value>1</value>

</property>

<p></configuration></p>
<ol>
<li>conf/mapred-site.xml</li>
</ol>
<configuration>

<property>

<name>mapred.job.tracker</name>

<value>zhanghua  .quqi.com:8021</value>

</property>

<p></configuration></p>
<ol>
<li>masters</li>
</ol>
<p>zhanghua  .quqi.com (   伪分布模式就配成  localhost)</p>
<ol>
<li>slaves</li>
</ol>
<p>tadev03  .quqi.com  (   伪分布模式就配成 localhost)</p>
<ol>
<li>将以上配置好的 hadoop  文件夹拷到所有机器的相同目录下：</li>
</ol>
<p>scp -r /home/soft/hadoop-0.20.2 <a href="mailto:root@tadev03.daodao.com">root@tadev03</a>   <a href="mailto:root@tadev03.daodao.com">.quqi.com</a>  :/home/soft/hadoop-0.20.2</p>
<p>注意：确保两台机器的  JAVA_HOME   的路径一致，如果不一致，就要改 。</p>
<p>hadoop  所有可配置的配置文件说明如下：</p>
<p>hadoop-env.sh   运行 hadoop  的脚本中使用的环境变量</p>
<p>core-site.xml hadoop  的核心配置，如 HDFS  和 MapReduce  中很普遍的 I/O  设置</p>
<p>hdfs-site.xml HDFS  后台程序设置的配置：名称节点，第二名称节点及数据节点</p>
<p>mapred-site.xml MapReduce  后台程序设置的配置： jobtracker  和 tasktracker</p>
<p>masters   记录运行第二名称节点 的机器（一行一个）的列表</p>
<p>slaves   记录运行数据节点的机器（一行一个）的列表</p>
<h2 id="-2-4-ssh-"><a href=""></a>2.4   免密码 SSH  设置</h2>
<p>免密码  ssh   设置， 保证至少从   master    可以不用口令登陆所有的   slaves    。</p>
<p>1  ）生成密钥对： ssh-keygen -t rsa -P &#39;&#39; -f /root/.ssh/id_rsa (  这样密钥就留在了客户端 )</p>
<p>2)   将公钥拷到要连接的服务器，</p>
<p>scp /root/.ssh/id_rsa.pub root@tadev03  .quqi.com:/tmp</p>
<p>ssh -l root tadev03  .quqi.com</p>
<p>more /tmp/id_rsa.pub &gt;&gt; /root/.ssh/authorized_keys</p>
<ol>
<li>ssh tadev03  .quqi.com   不需要输入密码即为成功。</li>
</ol>
<p>（注意：伪分布模式也要配置  ssh localhost   无密码登录，如果是  mac   ，请将  ssh   打开）</p>
<p>(  另外，在 mac  中请在 hadoop-config.sh  文件中配置  export JAVA_HOME=/System/Library/Frameworks/JavaVM.framework/Versions/1.6/Home)</p>
<p>三条控制线线：</p>
<p>SSH →   这样就可以直接从主节点远程启动从节点上的脚本，如  ssh tadev03  .quqi.com &#39;/var/aa.sh&#39;</p>
<p>NameNode (<a href="http://localhost:50030/" target="_blank"><a href="http://localhost:50070">http://localhost:50070</a></a> ) → DataNode</p>
<p>JobTracker ( <a href="http://localhost:50030/" target="_blank"><a href="http://localhost:50030">http://localhost:50030</a></a> )→ TaskTracker (<a href="http://localhost:50030/" target="_blank"><a href="http://localhost:50060">http://localhost:50060</a></a> )</p>
<h2 id="-2-5-hosts"><a href=""></a>2.5   配置 hosts</h2>
<p>必须配置 master   和 slaves   之间的双向 hosts.   修改 /etc/hosts   进行配置，略。</p>
<h2 id="-2-6-hdfs-"><a href=""></a>2.6   格式化 HDFS  文件系统</h2>
<p>和我们常见的 NTFS  ， FAT32  文件系统一样， NDFS  最开始也是需要格式化的。格式化过程用来创建存储目录以及名称节点的永久数据结构的初始版本来创建一个空的文件系统。命令如下：</p>
<p>hadoop namenode -format</p>
<p>已知问题：在重新格式化时，可能会报： SHUTDOWN_MSG: Shutting down NameNode</p>
<p>解决办法： rm -rf /tmp/hadoop-root/dfs/name</p>
<h2 id="-2-7-"><a href=""></a>2.7   启动守护进程</h2>
<p>1    ）启动   HDFS    守护进程：    start-dfs.sh</p>
<p>(      start-dfs.sh    脚本会参照 NameNode    上 ${HADOOP_CONF_DIR}/slaves    文件的内容，在所有列出的 slave    上启动 DataNode    守护进程。   )</p>
<p>已知问题：在已设置   JAVA_HOME    的情况下仍会报：   Error: JAVA_HOME is not set</p>
<p>解决办法：我是在  hadoop.sh  文件中加下面一句解决的：</p>
<p>JAVA_HOME=/System/Library/Frameworks/JavaVM.framework/Versions/1.6/Home</p>
<p>2  ）启动  Map/Reduce  守护进程：   start-mapred.sh</p>
<p>(      start-mapred.sh   脚本会参照 JobTracker   上 ${HADOOP_CONF_DIR}/slaves   文件的内容，在所有列出的 slave   上启动 TaskTracker   守护进程  )</p>
<p>3)   启动成功后，可以通过访问  <a href="http://localhost:50030" target="_blank">http://localhost:50030</a>   验证。</p>
<p>注意：也可直接使用  start-all.sh       与  stop-all.sh       脚本  ,       在主节点   master    上面启动   hadoop    ，主节点会启动  /    停止所有从节点的   hadoop    。会启动  5       个   java        进程  ,        同时会在   /tmp        目录下创建五个   pid        文件记录这些进程   ID        号。通过这五个文件，可以得知   namenode, datanode, secondary namenode, jobtracker, tasktracker        分别对应于哪一个   Java        进程。</p>
<p>已知问题：启动后，日志中报：  java.io.IOException: File /tmp/hadoop-root/mapred/system/jobtracker.info could only be replicated to 0 nodes, instead of 1</p>
<p>解决办法：原因是    从  tadev03     .quqi.com       机器上无法  ping zhanghua     .quqi.com</p>
<h2 id="-2-8-"><a href=""></a>2.8   运行程序</h2>
<p>先将测试数据及其他输入由本地文件系统拷到  HFDS  文件系统中去（注意：   jar   除外 ）</p>
<ol>
<li></li>
<li><p>hadoop fs -mkdir input</p>
</li>
<li>hadoop fs -ls .</li>
<li>hadoop fs -copyFromLocal /home/workspace/hadoopExample/input/file01 input/file01</li>
<li>hadoop fs -copyFromLocal /home/workspace/hadoopExample/input/file02 input/file02</li>
</ol>
<p>这时候就可以执行下列命令运行程序了，注意：后面的input , output  等目录都是HDFS  文件系统的路径。(  如果是本地模式，就用本地文件系统的绝对路径）</p>
<ol>
<li></li>
</ol>
<p>hadoop     jar   /home/workspace/hadoopExample/hadoopExample.jar com.TripResearch.hadoop.WordCount input/ output</p>
<p>已知问题：在集群模式下运行时任务会Pending</p>
<p>最后，运行下列命令查看结果：</p>
<p>/home/soft/hadoop-0.20.2/bin/hadoop fs -cat output/part-00000</p>
<p>也可访问下列地址查看状态：</p>
<p>NameNode – <a href="http://localhost:50070/" target="_blank"><a href="http://zhanghua">http://zhanghua</a></a>   <a href="http://localhost:50070/" target="_blank">.quqi.com</a> <a href="http://localhost:50070/" target="_blank">:50070/</a></p>
<p>JobTracker - <a href="http://localhost:50030/" target="_blank"><a href="http://zhanghua">http://zhanghua</a></a>   <a href="http://localhost:50030/" target="_blank">.quqi.com</a> <a href="http://localhost:50030/" target="_blank">:50030/</a></p>
<p>常用命令说明如下：</p>
<p>hadoop dfs –ls   查看 /usr/root  目录下的内容径；
hadoop dfs –rmr xxx xxx  就是删除目录；
hadoop dfsadmin -report   这个命令可以全局的查看 DataNode  的情况；
hadoop job -list   后面增加参数是对于当前运行的 Job  的操作，例如 list,kill  等；
hadoop balancer   均衡磁盘负载的命令。</p>
<h1 id="-3-hadoop-"><a href=""></a>3 hadoop  高级进阶</h1>
<h1 id="-4-hadoop-"><a href=""></a>4 hadoop  应用案例</h1>
<h1 id="-5-"><a href=""></a>5   参考文献</h1>
<ol>
<li><a href="http://hadoop.apache.org/common/docs/r0.18.2/cn/" target="_blank"><a href="http://hadoop.apache.org/common/docs/r0.18.2/cn/">http://hadoop.apache.org/common/docs/r0.18.2/cn/</a></a></li>
<li>hadoop 0.20.2  集群配置入门 <a href="http://dev.firnow.com/course/3_program/java/javajs/20100719/453042.html" target="_blank"><a href="http://dev.firnow.com/course/3_program/java/javajs/">http://dev.firnow.com/course/3_program/java/javajs/</a></a></li>
<li>Hadoop 分布式文件系统（HDFS ）初步实践 <a href="http://huatai.me/?p=352" target="_blank"><a href="http://huatai.me/?p=352">http://huatai.me/?p=352</a></a></li>
<li>Hadoop 分布式部署实验2_ 格式化分布式文件系统 <a href="http://hi.baidu.com/thinke365/blog/item/15602aa8f9074cf41e17a235.html" target="_blank"><a href="http://hi.baidu.com/thinke365/blog/item/15602aa8f9074cf41e17a235.html">http://hi.baidu.com/thinke365/blog/item/15602aa8f9074cf41e17a235.html</a></a></li>
<li>hadoop 安装出现问题（紧急），请前辈指教 <a href="http://forum.hadoop.tw/viewtopic.php?f=4&amp;t=90" target="_blank"><a href="http://forum.hadoop.tw/viewtopic.php?f=4&amp;t=90">http://forum.hadoop.tw/viewtopic.php?f=4&amp;t=90</a></a></li>
<li>用 Hadoop  进行分布式并行编程 <a href="http://www.ibm.com/developerworks/cn/opensource/os-cn-hadoop1/index.html" target="_blank"><a href="http://www.ibm.com/developerworks/cn/opensource/os-cn-hadoop1/index.html">http://www.ibm.com/developerworks/cn/opensource/os-cn-hadoop1/index.html</a></a></li>
<li>用 Hadoop  进行分布式数据处理 <a href="http://tech.ddvip.com/2010-06/1275983295155033.html" target="_blank"><a href="http://tech.ddvip.com/2010-06/1275983295155033.html">http://tech.ddvip.com/2010-06/1275983295155033.html</a></a></li>
</ol>
<p>分享到： <a href="&quot;分享到新浪微博&quot;"></a><a href="&quot;分享到腾讯微博&quot;"></a></p>
<ol>
<li>上一篇：<a href="http://blog.csdn.net/quqi99/article/details/6160846" target="_blank">Lucene Scoring 评分机制 （ by quqi99 )</a></li>
<li><p>下一篇：<a href="http://blog.csdn.net/quqi99/article/details/6292472" target="_blank">深入理解各JEE服务器Web层集群原理 ( by quqi99 )</a>
查看评论<a href=""></a></p>
<p>暂无评论
您还没有登录,请<a href="">[登录]</a>或<a href="http://passport.csdn.net/account/register?from=http%3A%2F%2Fblog.csdn.net%2Fquqi99%2Farticle%2Fdetails%2F6291788" target="_blank">[注册]</a></p>
</li>
</ol>
<p>/* 以上用户言论只代表其个人观点，不代表CSDN网站的观点或立场<a href=""></a><a href=""></a>
<a href="&quot;回到顶部&quot;"><img src="" alt="TOP"></a></p>
<p>个人资料</p>
<p><a href="http://my.csdn.net/quqi99" target="_blank"><img src="&quot;访问我的空间&quot;" alt=""></a>
<a href="http://my.csdn.net/quqi99" target="_blank">quqi99</a></p>
<p><a href="&quot;[加关注]&quot;"></a> <a href="&quot;[发私信]&quot;"></a>
<a href="http://medal.blog.csdn.net/allmedal.aspx" target="_blank"><img src="" alt=""></a></p>
<ul>
<li>访问：198660次</li>
<li>积分：3337分</li>
<li><p>排名：第1895名</p>
</li>
<li><p>原创：146篇</p>
</li>
<li>转载：23篇</li>
<li>译文：0篇</li>
<li>评论：123条</li>
</ul>
<p>文章搜索</p>
<p><a href=""></a></p>
<p>文章分类</p>
<ul>
<li><a href="http://blog.csdn.net/quqi99/article/category/875141" target="_blank">VM / Cloud</a>(7)</li>
<li><a href="http://blog.csdn.net/quqi99/article/category/557281" target="_blank">Middleware / Java AppServer</a>(13)</li>
<li><a href="http://blog.csdn.net/quqi99/article/category/328029" target="_blank">Seach Engine</a>(5)</li>
<li><a href="http://blog.csdn.net/quqi99/article/category/674417" target="_blank">Linux / Unix / Shell</a>(24)</li>
<li><a href="http://blog.csdn.net/quqi99/article/category/328188" target="_blank">J2SE / JEE</a>(40)</li>
<li><a href="http://blog.csdn.net/quqi99/article/category/347580" target="_blank">DB / NoSQL</a>(9)</li>
<li><a href="http://blog.csdn.net/quqi99/article/category/803236" target="_blank">Architecture</a>(0)</li>
<li><a href="http://blog.csdn.net/quqi99/article/category/351802" target="_blank">Android</a>(3)</li>
<li><a href="http://blog.csdn.net/quqi99/article/category/803239" target="_blank">Life</a>(4)</li>
<li><a href="http://blog.csdn.net/quqi99/article/category/689016" target="_blank">Other</a>(9)</li>
<li><a href="http://blog.csdn.net/quqi99/article/category/1112756" target="_blank">OpenStack</a>(37)</li>
<li><a href="http://blog.csdn.net/quqi99/article/category/1139084" target="_blank">Python</a>(1)</li>
<li><a href="http://blog.csdn.net/quqi99/article/category/1167554" target="_blank">C / C++</a>(2)</li>
<li><p><a href="http://blog.csdn.net/quqi99/article/category/1490633" target="_blank">Networking</a>(1)
文章存档</p>
</li>
<li><p><a href="http://blog.csdn.net/quqi99/article/month/2013/08" target="_blank">2013年08月</a>(4)</p>
</li>
<li><a href="http://blog.csdn.net/quqi99/article/month/2013/07" target="_blank">2013年07月</a>(13)</li>
<li><a href="http://blog.csdn.net/quqi99/article/month/2013/06" target="_blank">2013年06月</a>(6)</li>
<li><a href="http://blog.csdn.net/quqi99/article/month/2013/05" target="_blank">2013年05月</a>(2)</li>
<li><a href="http://blog.csdn.net/quqi99/article/month/2013/04" target="_blank">2013年04月</a>(2)</li>
<li><a href="http://blog.csdn.net/quqi99/article/month/2013/03" target="_blank">2013年03月</a>(3)</li>
<li><a href="http://blog.csdn.net/quqi99/article/month/2013/02" target="_blank">2013年02月</a>(1)</li>
<li><a href="http://blog.csdn.net/quqi99/article/month/2013/01" target="_blank">2013年01月</a>(9)</li>
<li><a href="http://blog.csdn.net/quqi99/article/month/2012/12" target="_blank">2012年12月</a>(3)</li>
<li><a href="http://blog.csdn.net/quqi99/article/month/2012/11" target="_blank">2012年11月</a>(2)</li>
<li><a href="http://blog.csdn.net/quqi99/article/month/2012/08" target="_blank">2012年08月</a>(1)</li>
<li><a href="http://blog.csdn.net/quqi99/article/month/2012/07" target="_blank">2012年07月</a>(1)</li>
<li><a href="http://blog.csdn.net/quqi99/article/month/2012/06" target="_blank">2012年06月</a>(7)</li>
<li><a href="http://blog.csdn.net/quqi99/article/month/2012/05" target="_blank">2012年05月</a>(2)</li>
<li><a href="http://blog.csdn.net/quqi99/article/month/2012/04" target="_blank">2012年04月</a>(6)</li>
<li><a href="http://blog.csdn.net/quqi99/article/month/2012/03" target="_blank">2012年03月</a>(2)</li>
<li><a href="http://blog.csdn.net/quqi99/article/month/2012/02" target="_blank">2012年02月</a>(1)</li>
<li><a href="http://blog.csdn.net/quqi99/article/month/2011/12" target="_blank">2011年12月</a>(1)</li>
<li><a href="http://blog.csdn.net/quqi99/article/month/2011/09" target="_blank">2011年09月</a>(5)</li>
<li><a href="http://blog.csdn.net/quqi99/article/month/2011/08" target="_blank">2011年08月</a>(2)</li>
<li><a href="http://blog.csdn.net/quqi99/article/month/2011/06" target="_blank">2011年06月</a>(2)</li>
<li><a href="http://blog.csdn.net/quqi99/article/month/2011/04" target="_blank">2011年04月</a>(5)</li>
<li><a href="http://blog.csdn.net/quqi99/article/month/2011/03" target="_blank">2011年03月</a>(2)</li>
<li><a href="http://blog.csdn.net/quqi99/article/month/2011/01" target="_blank">2011年01月</a>(4)</li>
<li><a href="http://blog.csdn.net/quqi99/article/month/2010/12" target="_blank">2010年12月</a>(1)</li>
<li><a href="http://blog.csdn.net/quqi99/article/month/2010/07" target="_blank">2010年07月</a>(1)</li>
<li><a href="http://blog.csdn.net/quqi99/article/month/2010/05" target="_blank">2010年05月</a>(1)</li>
<li><a href="http://blog.csdn.net/quqi99/article/month/2010/04" target="_blank">2010年04月</a>(2)</li>
<li><a href="http://blog.csdn.net/quqi99/article/month/2010/03" target="_blank">2010年03月</a>(2)</li>
<li><a href="http://blog.csdn.net/quqi99/article/month/2010/02" target="_blank">2010年02月</a>(4)</li>
<li><a href="http://blog.csdn.net/quqi99/article/month/2010/01" target="_blank">2010年01月</a>(6)</li>
<li><a href="http://blog.csdn.net/quqi99/article/month/2009/12" target="_blank">2009年12月</a>(6)</li>
<li><a href="http://blog.csdn.net/quqi99/article/month/2009/11" target="_blank">2009年11月</a>(5)</li>
<li><a href="http://blog.csdn.net/quqi99/article/month/2009/10" target="_blank">2009年10月</a>(6)</li>
<li><a href="http://blog.csdn.net/quqi99/article/month/2009/09" target="_blank">2009年09月</a>(1)</li>
<li><a href="http://blog.csdn.net/quqi99/article/month/2009/08" target="_blank">2009年08月</a>(1)</li>
<li><a href="http://blog.csdn.net/quqi99/article/month/2009/06" target="_blank">2009年06月</a>(4)</li>
<li><a href="http://blog.csdn.net/quqi99/article/month/2009/03" target="_blank">2009年03月</a>(1)</li>
<li><a href="http://blog.csdn.net/quqi99/article/month/2008/11" target="_blank">2008年11月</a>(1)</li>
<li><a href="http://blog.csdn.net/quqi99/article/month/2008/10" target="_blank">2008年10月</a>(2)</li>
<li><a href="http://blog.csdn.net/quqi99/article/month/2008/08" target="_blank">2008年08月</a>(1)</li>
<li><a href="http://blog.csdn.net/quqi99/article/month/2008/06" target="_blank">2008年06月</a>(3)</li>
<li><a href="http://blog.csdn.net/quqi99/article/month/2008/04" target="_blank">2008年04月</a>(1)</li>
<li><a href="http://blog.csdn.net/quqi99/article/month/2008/03" target="_blank">2008年03月</a>(1)</li>
<li><a href="http://blog.csdn.net/quqi99/article/month/2008/02" target="_blank">2008年02月</a>(1)</li>
<li><a href="http://blog.csdn.net/quqi99/article/month/2008/01" target="_blank">2008年01月</a>(3)</li>
<li><a href="http://blog.csdn.net/quqi99/article/month/2007/12" target="_blank">2007年12月</a>(4)</li>
<li><a href="http://blog.csdn.net/quqi99/article/month/2007/11" target="_blank">2007年11月</a>(1)</li>
<li><a href="http://blog.csdn.net/quqi99/article/month/2007/10" target="_blank">2007年10月</a>(4)</li>
<li><a href="http://blog.csdn.net/quqi99/article/month/2007/08" target="_blank">2007年08月</a>(5)</li>
<li><a href="http://blog.csdn.net/quqi99/article/month/2007/07" target="_blank">2007年07月</a>(6)</li>
<li><a href="http://blog.csdn.net/quqi99/article/month/2007/06" target="_blank">2007年06月</a>(1)</li>
<li><a href="http://blog.csdn.net/quqi99/article/month/2007/05" target="_blank">2007年05月</a>(8)</li>
</ul>
<p>展开</p>
<p>阅读排行</p>
<ul>
<li><a href="http://blog.csdn.net/quqi99/article/details/1916553" title="Android学习笔记 ( by quqi99 )" target="_blank">Android学习笔记 ( by quqi99 )</a>(22132)</li>
<li><a href="http://blog.csdn.net/quqi99/article/details/1680647" title="java.lang.NoClassDefFoundError: org/apache/commons/io/output/DeferredFileOutputStream" target="_blank">java.lang.NoClassDefFoundError: org/apache/commons/io/output/DeferredFileOutputStream</a>(17851)</li>
<li><a href="http://blog.csdn.net/quqi99/article/details/7433285" title="建立openstack quantum开发环境" target="_blank">建立openstack quantum开发环境</a>(6747)</li>
<li><a href="http://blog.csdn.net/quqi99/article/details/1755642" title="在linux中安装字库( by quqi99 )" target="_blank">在linux中安装字库( by quqi99 )</a>(5151)</li>
<li><a href="http://blog.csdn.net/quqi99/article/details/1930304" title="Android手机客户端与Servlet交换数据(by quqi99)" target="_blank">Android手机客户端与Servlet交换数据(by quqi99)</a>(5140)</li>
<li><a href="http://blog.csdn.net/quqi99/article/details/5298017" title="ReentrantLock与synchronized的区别 ( by quqi99 )" target="_blank">ReentrantLock与synchronized的区别 ( by quqi99 )</a>(4864)</li>
<li><a href="http://blog.csdn.net/quqi99/article/details/5604732" title="个人户口档案转移笔记（适用北京集体户口）" target="_blank">个人户口档案转移笔记（适用北京集体户口）</a>(4802)</li>
<li><a href="http://blog.csdn.net/quqi99/article/details/1624225" title="Magnolia学习笔记（一个基于JSR170的内容管理系统） ( by quqi99 )" target="_blank">Magnolia学习笔记（一个基于JSR170的内容管理系统） ( by quqi99 )</a>(4342)</li>
<li><a href="http://blog.csdn.net/quqi99/article/details/1624218" title="JSpider学习笔记 ( by quqi99 )" target="_blank">JSpider学习笔记 ( by quqi99 )</a>(4149)</li>
<li><p><a href="http://blog.csdn.net/quqi99/article/details/3099945" title="Plone学习笔记 ( by quqi99 )" target="_blank">Plone学习笔记 ( by quqi99 )</a>(4057)
评论排行</p>
</li>
<li><p><a href="http://blog.csdn.net/quqi99/article/details/1916553" title="Android学习笔记 ( by quqi99 )" target="_blank">Android学习笔记 ( by quqi99 )</a>(21)</p>
</li>
<li><a href="http://blog.csdn.net/quqi99/article/details/1680647" title="java.lang.NoClassDefFoundError: org/apache/commons/io/output/DeferredFileOutputStream" target="_blank">java.lang.NoClassDefFoundError: org/apache/commons/io/output/DeferredFileOutputStream</a>(18)</li>
<li><a href="http://blog.csdn.net/quqi99/article/details/1930304" title="Android手机客户端与Servlet交换数据(by quqi99)" target="_blank">Android手机客户端与Servlet交换数据(by quqi99)</a>(12)</li>
<li><a href="http://blog.csdn.net/quqi99/article/details/5604732" title="个人户口档案转移笔记（适用北京集体户口）" target="_blank">个人户口档案转移笔记（适用北京集体户口）</a>(8)</li>
<li><a href="http://blog.csdn.net/quqi99/article/details/1624225" title="Magnolia学习笔记（一个基于JSR170的内容管理系统） ( by quqi99 )" target="_blank">Magnolia学习笔记（一个基于JSR170的内容管理系统） ( by quqi99 )</a>(7)</li>
<li><a href="http://blog.csdn.net/quqi99/article/details/2591768" title="使用itext生成word格式的报表(by quqi99)" target="_blank">使用itext生成word格式的报表(by quqi99)</a>(5)</li>
<li><a href="http://blog.csdn.net/quqi99/article/details/1755642" title="在linux中安装字库( by quqi99 )" target="_blank">在linux中安装字库( by quqi99 )</a>(4)</li>
<li><a href="http://blog.csdn.net/quqi99/article/details/6305061" title="Android分享文稿 ( by quqi99 )" target="_blank">Android分享文稿 ( by quqi99 )</a>(4)</li>
<li><a href="http://blog.csdn.net/quqi99/article/details/9156497" title="OpenDaylight学习 ( by quqi99 )" target="_blank">OpenDaylight学习 ( by quqi99 )</a>(3)</li>
<li><a href="http://blog.csdn.net/quqi99/article/details/2590703" title="使用jacob生成word(by quqi99)" target="_blank">使用jacob生成word(by quqi99)</a>(3)</li>
</ul>
<p>推荐文章
最新评论</p>
<ul>
<li><a href="http://blog.csdn.net/quqi99/article/details/9156497#comments" target="_blank">OpenDaylight学习 ( by quqi99 )</a></li>
</ul>
<p><a href="http://blog.csdn.net/quqi99" target="_blank">quqi99</a>: hi whoeversucks, 谢谢你的实时信息，非常有用，我已经更新到博客里了。另外，问个问题，...</p>
<ul>
<li><a href="http://blog.csdn.net/quqi99/article/details/9156497#comments" target="_blank">OpenDaylight学习 ( by quqi99 )</a></li>
</ul>
<p><a href="http://blog.csdn.net/whoeversucks" target="_blank">whoeversucks</a>: 注意，OpenDayLight Controller和OSCP实际上2个独立的SDN控制器项目（分别...</p>
<ul>
<li><a href="http://blog.csdn.net/quqi99/article/details/9102745#comments" target="_blank">给Linux虚机扩充硬盘空间 ( by quqi99 )</a></li>
</ul>
<p><a href="http://blog.csdn.net/quqi99" target="_blank">quqi99</a>: hi dalinhuang, 谢谢你的回复，你给的这个方法是只适合LVM场景的啊，我没有使用LVM。</p>
<ul>
<li><a href="http://blog.csdn.net/quqi99/article/details/9102745#comments" target="_blank">给Linux虚机扩充硬盘空间 ( by quqi99 )</a></li>
</ul>
<p><a href="http://blog.csdn.net/dalinhuang" target="_blank">dalinhuang</a>: 给根（/）扩充的步骤：（以你的virtualbox并使用LVM为例）1. 新增一块虚拟硬盘，给虚机。...</p>
<ul>
<li><a href="http://blog.csdn.net/quqi99/article/details/9156497#comments" target="_blank">OpenDaylight学习 ( by quqi99 )</a></li>
</ul>
<p><a href="http://blog.csdn.net/piaochenping" target="_blank">piaochenping</a>: 你好，为什么我安装时老是出现这个错误呢？ Failed to execute goal org.co...</p>
<ul>
<li><a href="http://blog.csdn.net/quqi99/article/details/7659905#comments" target="_blank">OpenStack CI测试之devstack-gate ( by quqi99 )</a></li>
</ul>
<p><a href="http://blog.csdn.net/quqi99" target="_blank">quqi99</a>: @sunyilong2012: 这种错误应该是差模块吧，可以单独安装一下试试, sudo pip i...</p>
<ul>
<li><a href="http://blog.csdn.net/quqi99/article/details/7411091#comments" target="_blank">Fedora 16上源码建立pydev + eclipse的OpenStack开发环境笔记草稿 ( by quqi99 )</a></li>
</ul>
<p><a href="http://blog.csdn.net/quqi99" target="_blank">quqi99</a>: openstack因为用到了一些linux特有的东西，如iptables，所以目前只能跑在linux...</p>
<ul>
<li><a href="http://blog.csdn.net/quqi99/article/details/7411091#comments" target="_blank">Fedora 16上源码建立pydev + eclipse的OpenStack开发环境笔记草稿 ( by quqi99 )</a></li>
</ul>
<p><a href="http://blog.csdn.net/javaerss" target="_blank">javaerss</a>: 大神...看哭了，为此特地跑去下载fedora 16来做实验。之前用ubuntu下用eclipse ...</p>
<ul>
<li><a href="http://blog.csdn.net/quqi99/article/details/6576375#comments" target="_blank">玩转play framework ( by quqi99 )</a></li>
</ul>
<p><a href="http://blog.csdn.net/nanfu08" target="_blank">nanfu08</a>: 你能看得清，如果只是自己看的话我没话说，这样的文字叫人怎么读？？</p>
<ul>
<li><a href="http://blog.csdn.net/quqi99/article/details/7659905#comments" target="_blank">OpenStack CI测试之devstack-gate ( by quqi99 )</a></li>
</ul>
<p><a href="http://blog.csdn.net/sunyilong2012" target="_blank">dragonsun</a>: 您好，我在做这个测试的时候遇到了无法导入statsd的问题，请问您有解决的方法吗？+ /home/j...</p>
<p><a href="http://www.csdn.net/company/about.html" target="_blank">公司简介</a>|<a href="http://www.csdn.net/company/recruit.html" target="_blank">招贤纳士</a>|<a href="http://www.csdn.net/company/marketing.html" target="_blank">广告服务</a>|<a href="http://www.csdn.net/company/account.html" target="_blank">银行汇款帐号</a>|<a href="http://www.csdn.net/company/contact.html" target="_blank">联系方式</a>|<a href="http://www.csdn.net/company/statement.html" target="_blank">版权声明</a>|<a href="http://www.csdn.net/company/layer.html" target="_blank">法律顾问</a>|<a href="mailto:webmaster@csdn.net">问题报告</a><a href="http://wpa.qq.com/msgrd?v=3&amp;uin=2355263776&amp;site=qq&amp;menu=yes" target="_blank">QQ客服</a> <a href="http://e.weibo.com/csdnsupport/profile" target="_blank">微博客服</a> <a href="http://bbs.csdn.net/forums/Service" target="_blank">论坛反馈</a> <a href="mailto:webmaster@csdn.net">联系邮箱：webmaster@csdn.net</a> 服务热线：400-600-2320京 ICP 证 070598 号北京创新乐知信息技术有限公司 版权所有世纪乐知(北京)网络技术有限公司 提供技术支持江苏乐知网络技术有限公司 提供商务支持Copyright © 1999-2012, CSDN.NET, All Rights Reserved <a href="http://www.hd315.gov.cn/beian/view.asp?bianhao=010202001032100010" target="_blank"><img src="" alt="GongshangLogo"></a>
<img src="http://counter.csdn.net/pv.aspx?id=24" alt=""></p>

      
    </div>
    
    
      
    
    
        <footer id="post-meta">
        <span class="categories">Posted in<span class="breadcrumb fa fa-folder"><li><a href="/categories/hadoop/">hadoop</a></li></span></span> | <span class="tags">Tagged <a href="/tags/hadoop/" class="label label-primary">hadoop</a></span> | <span class="time">recent updated:<time title="2014-03-07 09:54:36"datetime="2014-03-07 09:54:36"> mar. 7 2014</time></span> | <span class="comment-link">
<a href="http://itsolife.com/2014/02/02/2014-02-02-hadoop--Hadoop知识分享文稿byquqi99-技术并艺术着/#comments" class="ds-thread-count comment-link" data-thread-key="2014-02-02-hadoop--Hadoop知识分享文稿byquqi99-技术并艺术着" data-count-type="comments">暂无评论</a></span>
        </footer>
    
    <div class="clearfix"></div>
  </div>
</div>
</article>



  <article>
<div id="post" class="post well">
  <div class="post-content">
    <header class="well-sm">
      <i class="fa icon fa-5x pull-left"></i>
      <h1 class="title"><a href="/2014/02/02/2014-02-02-hadoop--Hadoop版本梳理/">Hadoop版本梳理</a></h1>
      
        <span>Posted on<time datetime="2014-02-02T01:54:36.000Z"> <a href="/2014/02/02/2014-02-02-hadoop--Hadoop版本梳理/">feb. 2 2014</a></time></span>      
    </header>
    
    
    <div class="entry">
      
        <h1 id="hadoop-">Hadoop版本梳理</h1>
<p>由于Hadoop版本混乱多变，因此，Hadoop的版本选择问题一直令很多初级用户苦恼。本文总结了Apache Hadoop和Cloudera Hadoop的版本衍化过程，并给出了选择Hadoop版本的一些建议。</p>
<h3 id="1-apache-hadoop">1. Apache Hadoop</h3>
<h3 id="1-1-apache-">1.1  Apache版本衍化</h3>
<p>截至目前（2012年12月23日），Apache Hadoop版本分为两代，我们将第一代Hadoop称为Hadoop 1.0，第二代Hadoop称为Hadoop 2.0。第一代Hadoop包含三个大版本，分别是0.20.x，0.21.x和0.22.x，其中，0.20.x最后演化成1.0.x，变成了稳定版，而0.21.x和0.22.x则NameNode HA等新的重大特性。第二代Hadoop包含两个版本，分别是0.23.x和2.x，它们完全不同于Hadoop 1.0，是一套全新的架构，均包含HDFS Federation和YARN两个系统，相比于0.23.x，2.x增加了NameNode HA和Wire-compatibility两个重大特性。</p>
<p>经过上面的大体解释，大家可能明白了Hadoop以重大特性区分各个版本的，总结起来，用于区分Hadoop版本的特性有以下几个：</p>
<p><strong>（1）Append</strong> 支持文件追加功能，如果想使用HBase，需要这个特性。</p>
<p><strong>（2）RAID </strong>在保证数据可靠的前提下，通过引入校验码较少数据块数目。详细链接：</p>
<p><a href="https://issues.apache.org/jira/browse/HDFS/component/12313080" target="_blank">https://issues.apache.org/jira/browse/HDFS/component/12313080</a></p>
<p><strong>（3）Symlink</strong> 支持HDFS文件链接，具体可参考： <a href="https://issues.apache.org/jira/browse/HDFS-245" target="_blank"><a href="https://issues.apache.org/jira/browse/HDFS-245">https://issues.apache.org/jira/browse/HDFS-245</a></a></p>
<p><strong>（4）Security</strong> Hadoop安全，具体可参考：<a href="https://issues.apache.org/jira/browse/HADOOP-4487" target="_blank"><a href="https://issues.apache.org/jira/browse/HADOOP-4487">https://issues.apache.org/jira/browse/HADOOP-4487</a></a></p>
<p><strong>（5） NameNode HA</strong> 具体可参考：<a href="https://issues.apache.org/jira/browse/HDFS-1064" target="_blank"><a href="https://issues.apache.org/jira/browse/HDFS-1064">https://issues.apache.org/jira/browse/HDFS-1064</a></a></p>
<p><strong>（6） HDFS Federation和YARN</strong></p>
<p><img src="&quot;apache-hadoop-versions&quot;" alt=""></p>
<p>需要注意的是，Hadoop 2.0主要由Yahoo独立出来的hortonworks公司主持开发。</p>
<h3 id="1-2-apache-">1.2  Apache版本下载</h3>
<p>（1） 各版本说明：<a href="http://hadoop.apache.org/releases.html" target="_blank"><a href="http://hadoop.apache.org/releases.html">http://hadoop.apache.org/releases.html</a></a>。</p>
<p>（2） 下载稳定版：找到一个镜像，下载stable文件夹下的版本。</p>
<p>（3） Hadoop最全版本：<a href="http://svn.apache.org/repos/asf/hadoop/common/branches/" target="_blank"><a href="http://svn.apache.org/repos/asf/hadoop/common/branches/">http://svn.apache.org/repos/asf/hadoop/common/branches/</a></a>，可直接导到eclipse中。</p>
<h3 id="2-cloudera-hadoop">2. Cloudera Hadoop</h3>
<h3 id="2-1-cdh-">2.1  CDH版本衍化</h3>
<p>Apache当前的版本管理是比较混乱的，各种版本层出不穷，让很多初学者不知所措，相比之下，Cloudera公司的Hadoop版本管理的要很多。</p>
<p>我们知道，Hadoop遵从Apache开源协议，用户可以免费地任意使用和修改Hadoop，也正因此，市面上出现了很多Hadoop版本，其中比较出名的一是Cloudera公司的发行版，我们将该版本称为CDH（Cloudera Distribution Hadoop）。截至目前为止，CDH共有4个版本，其中，前两个已经不再更新，最近的两个，分别是CDH3（在Apache Hadoop 0.20.2版本基础上演化而来的）和CDH4在Apache Hadoop 2.0.0版本基础上演化而来的），分别对应Apache的Hadoop 1.0和Hadoop 2.0，它们每隔一段时间便会更新一次。</p>
<p><img src="&quot;cloudera-hadoop-versions&quot;" alt=""></p>
<p>Cloudera以patch level划分小版本，比如patch level为923.142表示在原生态Apache Hadoop 0.20.2基础上添加了1065个patch（这些patch是各个公司或者个人贡献的，在Hadoop jira上均有记录），其中923个是最后一个beta版本添加的patch，而142个是稳定版发行后新添加的patch。由此可见，patch level越高，功能越完备且解决的bug越多。</p>
<p>Cloudera版本层次更加清晰，且它提供了适用于各种操作系统的Hadoop安装包，可直接使用apt-get或者yum命令进行安装，更加省事。</p>
<h3 id="2-2-cdh-">2.2 CDH版本下载</h3>
<p>（1） 版本含义介绍：</p>
<p><a href="https://ccp.cloudera.com/display/DOC/CDH+Version+and+Packaging+Information" target="_blank"><a href="https://ccp.cloudera.com/display/DOC/CDH+Version+and+Packaging+Information">https://ccp.cloudera.com/display/DOC/CDH+Version+and+Packaging+Information</a></a></p>
<p>（2）各版本特性查看：</p>
<p><a href="https://ccp.cloudera.com/display/DOC/CDH+Packaging+Information+for+Previous+Releases" target="_blank"><a href="https://ccp.cloudera.com/display/DOC/CDH+Packaging+Information+for+Previous+Releases">https://ccp.cloudera.com/display/DOC/CDH+Packaging+Information+for+Previous+Releases</a></a></p>
<p>（3）各版本下载：</p>
<p>CDH3：<a href="http://archive.cloudera.com/cdh/3/" target="_blank"><a href="http://archive.cloudera.com/cdh/3/">http://archive.cloudera.com/cdh/3/</a></a></p>
<p>CDH4：<a href="http://archive.cloudera.com/cdh4/cdh/4/" target="_blank"><a href="http://archive.cloudera.com/cdh4/cdh/4/">http://archive.cloudera.com/cdh4/cdh/4/</a></a></p>
<p>注意，Hadoop压缩包在这两个链接中的最上层目录中，不在某个文件夹里，很多人进到链接还找不到安装包！</p>
<h3 id="3-hadoop-">3. 如何选择Hadoop版本</h3>
<p>当前Hadoop版本比较混乱，让很多用户不知所措。实际上，当前Hadoop只有两个版本：Hadoop 1.0和Hadoop 2.0，其中，Hadoop 1.0由一个分布式文件系统HDFS和一个离线计算框架MapReduce组成，而Hadoop 2.0则包含一个支持NameNode横向扩展的HDFS，一个资源管理系统YARN和一个运行在YARN上的离线计算框架MapReduce。相比于Hadoop 1.0，Hadoop 2.0功能更加强大，且具有更好的扩展性、性能，并支持多种计算框架。</p>
<p>当我们决定是否采用某个软件用于开源环境时，通常需要考虑以下几个因素：</p>
<p>（1）是否为开源软件，即是否免费。</p>
<p>（2） 是否有稳定版，这个一般软件官方网站会给出说明。</p>
<p>（3） 是否经实践验证，这个可通过检查是否有一些大点的公司已经在生产环境中使用知道。</p>
<p>（4） 是否有强大的社区支持，当出现一个问题时，能够通过社区、论坛等网络资源快速获取解决方法。</p>
<p>如今Hadoop 2.0已经发布了最新的稳定版2.2.0，推荐使用该版本，具体介绍可阅读：“<a href="http://dongxicheng.org/mapreduce-nextgen/hadoop-2-2-0/" target="_blank">Hadoop 2.0稳定版本2.2.0新特性剖析</a>”，升级方法可参考：“<a href="http://dongxicheng.org/mapreduce-nextgen/hadoop-upgrade-to-version-2/" target="_blank">Hadoop升级方案（二）：从Hadoop 1.0升级到2.0（1）</a>”。
来源： <a href="[http://dongxicheng.org/mapreduce-nextgen/how-to-select-hadoop-versions/](http://dongxicheng.org/mapreduce-nextgen/how-to-select-hadoop-versions/)">[http://dongxicheng.org/mapreduce-nextgen/how-to-select-hadoop-versions/](http://dongxicheng.org/mapreduce-nextgen/how-to-select-hadoop-versions/)</a></p>
<p>另外一篇：</p>
<h1 id="-hadoop-http-www-cnblogs-com-xuxm2007-archive-2013-04-04-2999741-html-"><a href="http://www.cnblogs.com/xuxm2007/archive/2013/04/04/2999741.html" target="_blank">hadoop的版本问题</a></h1>
<p>现在hadoop的版本比较乱,常常搞不清楚版本之间的关系,下面简单的摘要了,apache hadoop和cloudera hadoop 的版本的演化.</p>
<p><strong>apache hadoop官方给出的版本说明是:</strong></p>
<p><strong>1.0.X -</strong>current stable version, 1.0 release</p>
<p><strong>1.1.X -</strong>current beta version, 1.1 release</p>
<p><strong>2.X.X -</strong>current alpha version</p>
<p><strong>0.23.X -</strong>simmilar to 2.X.X but missing NN HA.</p>
<p><strong>0.22.X -</strong>does not include security</p>
<p><strong>0.20.203.X -</strong>old legacy stable version</p>
<p><strong>0.20.X -</strong>old legacy version</p>
<p>下图来自<a href="http://blog.cloudera.com/blog/2012/01/an-update-on-apache-hadoop-1-0/" target="_blank"><a href="http://blog.cloudera.com/blog/2012/01/an-update-on-apache-hadoop-1-0/">http://blog.cloudera.com/blog/2012/01/an-update-on-apache-hadoop-1-0/</a></a></p>
<p>可以简单说明apache hadoop和cloudera hadoop版本之间的变化关系</p>
<p><a href="http://images.cnitblog.com/blog/73083/201304/04194843-67590ee16a15440497b1153b688fad40.png" target="_blank"><img src="&quot;diagram-3&quot;" alt="diagram-3"></a></p>
<p>0.20.x版本最后演化成了现在的1.0.x版本</p>
<p>0.23.x版本最后演化成了现在的2.x版本</p>
<p>hadoop 1.0 指的是1.x(0.20.x),0.21,0.22</p>
<p>hadoop 2.0 指的是2.x,0.23.x</p>
<p>CDH3,CDH4分别对应了hadoop1.0 hadoop2.0</p>
<p>董的博客有2篇文章也很清晰的解释了,hadoop版本以及各自的版本特性:</p>
<p><a href="http://dongxicheng.org/mapreduce-nextgen/how-to-select-hadoop-versions/" target="_blank"><a href="http://dongxicheng.org/mapreduce-nextgen/how-to-select-hadoop-versions/">http://dongxicheng.org/mapreduce-nextgen/how-to-select-hadoop-versions/</a></a></p>
<p><a href="http://dongxicheng.org/mapreduce-nextgen/hadoop-2-0-terms-explained/" target="_blank"><a href="http://dongxicheng.org/mapreduce-nextgen/hadoop-2-0-terms-explained/">http://dongxicheng.org/mapreduce-nextgen/hadoop-2-0-terms-explained/</a></a></p>
<p><a href="http://images.cnitblog.com/blog/73083/201304/04195633-b2c2d402daf94e2187a2130c24d441b1.jpg" target="_blank"><img src="&quot;apache-hadoop-versions&quot;" alt="apache-hadoop-versions"></a></p>
<p>最后给出常见的下载hadoop不同版本的地址:</p>
<p><a href="http://archive.apache.org/dist/hadoop/core/" target="_blank"><a href="http://archive.apache.org/dist/hadoop/core/">http://archive.apache.org/dist/hadoop/core/</a></a></p>
<p><a href="http://archive.cloudera.com/cdh/3/" target="_blank"><a href="http://archive.cloudera.com/cdh/3/">http://archive.cloudera.com/cdh/3/</a></a></p>
<p><a href="http://archive.cloudera.com/cdh4/cdh/4/" target="_blank"><a href="http://archive.cloudera.com/cdh4/cdh/4/">http://archive.cloudera.com/cdh4/cdh/4/</a></a></p>
<p>另外附注一个 hadoop各商业发行版的比较:</p>
<p><a href="http://www.xiaohui.org/archives/795.html" target="_blank"><a href="http://www.xiaohui.org/archives/795.html">http://www.xiaohui.org/archives/795.html</a></a>
来源： &lt;<a href="http://www.cnblogs.com/xuxm2007/archive/2013/04/04/2999741.html" target="_blank">hadoop的版本问题 - 阿笨猫 - 博客园</a>&gt;  </p>

      
    </div>
    
    
      
    
    
        <footer id="post-meta">
        <span class="categories">Posted in<span class="breadcrumb fa fa-folder"><li><a href="/categories/hadoop/">hadoop</a></li></span></span> | <span class="tags">Tagged <a href="/tags/hadoop/" class="label label-primary">hadoop</a></span> | <span class="time">recent updated:<time title="2014-03-07 09:54:36"datetime="2014-03-07 09:54:36"> mar. 7 2014</time></span> | <span class="comment-link">
<a href="http://itsolife.com/2014/02/02/2014-02-02-hadoop--Hadoop版本梳理/#comments" class="ds-thread-count comment-link" data-thread-key="2014-02-02-hadoop--Hadoop版本梳理" data-count-type="comments">暂无评论</a></span>
        </footer>
    
    <div class="clearfix"></div>
  </div>
</div>
</article>



  <article>
<div id="post" class="post well">
  <div class="post-content">
    <header class="well-sm">
      <i class="fa icon fa-5x pull-left"></i>
      <h1 class="title"><a href="/2014/02/02/2014-02-02-hadoop--Hadoop在CentOS下的单机配置/">Hadoop在CentOS下的单机配置</a></h1>
      
        <span>Posted on<time datetime="2014-02-02T01:54:36.000Z"> <a href="/2014/02/02/2014-02-02-hadoop--Hadoop在CentOS下的单机配置/">feb. 2 2014</a></time></span>      
    </header>
    
    
    <div class="entry">
      
        <h1 id="hadoop-centos-">Hadoop在CentOS下的单机配置</h1>
<p>前言的前言</p>
<p>如果你做某件从未接触过的事的时候很纠结很曲折，那么为你自己高兴吧，你能学到很多东西！</p>
<p>以下的东西都是贴图，所以你们只有手敲了。我也不清楚这个东西是不是应该花很多时间去做，有得有失，某些付出不知道到底值多少。据/<em>/</em>说一下午都能配出来，谁叫我傻呢，谁叫我蠢呢，不过该走的路咱还是踏实点走吧，不去跟人比。所以现在我把细节写出来，供大家参考，让你能在两小时内完成。希望它能帮助你学习，而不是让你变得更依赖。如有不对的地方请指正，我也是初学者。谢谢！</p>
<p>前言</p>
<p>做事总有个原因吧，那么我们为什么安装单机的<a href="http://www.linuxidc.com/topicnews.aspx?tid=13" title="Hadoop" target="_blank">Hadoop</a>呢？因为官网上有安装单机hadoop，因为某权威网站有<a href="http://www.linuxidc.com/topicnews.aspx?tid=2" title="Ubuntu" target="_blank">Ubuntu</a>下安装单机hadoop，但是没有一个网站有<a href="http://www.linuxidc.com/topicnews.aspx?tid=14" title="CentOS" target="_blank">CentOS</a>下单机安装，所以我现在CentOS下面单机配置hadoop。</p>
<p>其实单机hadoop的安装没有什么实质的用处，主要用于初学者熟悉指令，以及对hadoop配置有个大致了解，以便于安装分布式。</p>
<p>首先，我们来理清思路。</p>
<p>目的：安装hadoop</p>
<p>Hadoop是需要在java环境下面运行，所以，首先要保证你的系统下面装有JDK。那么步骤是：配置SSH——安装JDK——安装hadoop（当然你愿意先安装它也完全没问题）——配置java的环境变量（需要知道java的安装路径）——配置namenode下面3个配置文件——格式化hadoop——启动hadoop。</p>
<p>我们用一般用户登录，然后切换到root下面，因为权限的问题，这样相比下会更安全点，注意linux下面尽量不要用root登录。</p>
<p>开始了</p>
<p>所需软件</p>
<p>CentOS、Java、Hadoop安装软件。本人用的版本为Linux Cent OS 5.5、jdk1.6.0_13、hadoop-0.20.2.tar.gz。</p>
<p>我们要提醒一下，linux下面很注意权限问题。我们应该以一般用户登录，然后切换至root用户才能使用某些命令，并能使系统处于相对安全的状态。</p>
<p>所以做如下处理，来切换到root用户。
<img src="" alt="Hadoop在CentOS下的单机配置"></p>
<ol>
<li>SSH无密码验证配置（更建议放到最后一步进行，为非核心步骤，只是方便而已）</li>
</ol>
<p>Hadoop 需要使用SSH 协议。</p>
<p>namenode 将使用SSH 协议启动 namenode和datanode 进程，配置 SSH localhost无密码验证。</p>
<p>(1)生成密钥对
<img src="" alt="Hadoop在CentOS下的单机配置"></p>
<p>前面是为了切换到root下面</p>
<p>通过以上命令将在/root/.ssh/ 目录下生成id_rsa私钥和id_rsa.pub公钥。</p>
<p>（2）进入/root/.ssh目录在namenode节点下做如下配置：
<img src="" alt="Hadoop在CentOS下的单机配置"></p>
<p>可以用键入ssh localhost命令来看已经连接，会有这样的显示</p>
<p><img src="" alt="Hadoop在CentOS下的单机配置"></p>
<p>注意最后一行！跟第一行比较，发现我们用ssh进入到localhost了！但已不需要输入密码了。（这样说你们也一定不知道，如果把这个放到最后一步做就会更懂。）</p>
<p>本人认为这样设置会发现后面操作不会让你老是输入密码，并非核心步骤，大家可以试试先配置其它的，再到这一步，就明白为什么了。
来源： <a href="[http://www.linuxidc.com/Linux/2011-07/37992.htm](http://www.linuxidc.com/Linux/2011-07/37992.htm)">[http://www.linuxidc.com/Linux/2011-07/37992.htm](http://www.linuxidc.com/Linux/2011-07/37992.htm)</a> </p>
<ol>
<li>安装JDK</li>
</ol>
<p>(1)下载JDK</p>
<p>建议到sun的官网上下载,地址如下：<a href="https://cds.sun.com/is-bin/INTERSHOP.enfinity/WFS/CDS-CDS_Developer-Site/en_US/-/USD/ViewFilteredProducts-SingleVariationTypeFilter" target="_blank"><a href="https://cds.sun.com/is-bin/INTERSHOP.enfinity/WFS/CDS-CDS_Developer-Site/en_US/-/USD/ViewFilteredProducts-SingleVariationTypeFilter">https://cds.sun.com/is-bin/INTERSHOP.enfinity/WFS/CDS-CDS_Developer-Site/en_US/-/USD/ViewFilteredProducts-SingleVariationTypeFilter</a></a></p>
<p>选择jdk-6u24-linux-i586.bin</p>
<p>(2)安装JDK</p>
<p>我把它装在/opt里面,所以切换到/opt下面。在命令行输入如下指令来执行JDK文件:</p>
<p><img src="" alt=""></p>
<p>权限有问题！我们看看它的权限
<img src="" alt="Hadoop在CentOS下的单机配置"></p>
<p>没有可执行的x标志，那么我们可以通过命令改变。如下操作：</p>
<p><img src="" alt="Hadoop在CentOS下的单机配置"></p>
<p>看到没，变成绿色的了。有人是把所有者、组、其他用户对该文件的权限都设置为可执行，不过我在这就只让它能被所有者执行就行了。（该文件可能不管紧要，其他重要的文件，我认为不能像他们那样设置。）</p>
<p>现在我们再执行它
<img src="" alt="Hadoop在CentOS下的单机配置"></p>
<p>没有问题了吧，在开始解包了。</p>
<p>(1)Java环境变量配置</p>
<p>输入vim /etc/profile，添加如下的内容（在此我建议所有的都编辑都用vim取代vi，因为它有颜色变化，有语法问题的话很容易发现。）
<img src="" alt="Hadoop在CentOS下的单机配置"></p>
<p>保存好退出后，我们需要改变一下改文件的权限，并执行一下该文件使配置生效。（注：大家一定要小心版本和路径啊，）</p>
<p><img src="" alt="Hadoop在CentOS下的单机配置"></p>
<p>配置完后执行java –version</p>
<p><img src="" alt="Hadoop在CentOS下的单机配置"></p>
<p>显示java的版本</p>
<p>来源： <a href="[http://www.linuxidc.com/Linux/2011-07/37992p2.htm](http://www.linuxidc.com/Linux/2011-07/37992p2.htm)">[http://www.linuxidc.com/Linux/2011-07/37992p2.htm](http://www.linuxidc.com/Linux/2011-07/37992p2.htm)</a> </p>
<ol>
<li>安装<a href="http://www.linuxidc.com/topicnews.aspx?tid=13" title="Hadoop" target="_blank">Hadoop</a></li>
</ol>
<p>（1）下载hadoop</p>
<p>到如下网址下载hadoop，存到/opt中,当然也可以手动点击下载。
<img src="" alt="Hadoop在CentOS下的单机配置"></p>
<p>（2）解压hadoop到/opt/hadoop下面，当然没有现成的opt/hadoop这个目录，所以要新建。</p>
<p><img src="" alt="Hadoop在CentOS下的单机配置"></p>
<p>然后解压到/opt/hadoop下</p>
<p><img src="" alt="Hadoop在CentOS下的单机配置"></p>
<p>3.1   进入/opt/hadoop/hadoop-0.20.2/conf，配置Hadoop配置文件。</p>
<p>（1）配置java环境：修改hadoop-env.sh文件
<img src="" alt="Hadoop在CentOS下的单机配置"></p>
<p>在最后加上这样的内容</p>
<p><img src="" alt="Hadoop在CentOS下的单机配置"></p>
<p>(2)配置Namenode的三个配置文件core-site.xml, hdfs-site.xml, mapred-site.xml。对应于/src/core/core-default.xml，但不能直接修改它，（hadoop启动时先读取src下面的core/core-default.xml,hdfs/hdfs-default.xml,apred/mapred-default.xml，里面缺失的变量由conf下面的三个-site文件提供）</p>
<p>这部分的配置建议参考官方网站（建议大家多上官网），如下：<a href="http://hadoop.apache.org/common/docs/current/single_node_setup.html" target="_blank"><a href="http://hadoop.apache.org/common/docs/current/single_node_setup.html">http://hadoop.apache.org/common/docs/current/single_node_setup.html</a></a></p>
<p>(2.1)配置core
<img src="" alt="Hadoop在CentOS下的单机配置"></p>
<p>（2.2）配置hdfs</p>
<p><img src="" alt="Hadoop在CentOS下的单机配置"></p>
<p>（2.3）配置mapred</p>
<p><img src="" alt="Hadoop在CentOS下的单机配置"></p>
<p>来源： <a href="[http://www.linuxidc.com/Linux/2011-07/37992p3.htm](http://www.linuxidc.com/Linux/2011-07/37992p3.htm)">[http://www.linuxidc.com/Linux/2011-07/37992p3.htm](http://www.linuxidc.com/Linux/2011-07/37992p3.htm)</a></p>
<p>4、启动<a href="http://www.linuxidc.com/topicnews.aspx?tid=13" title="Hadoop" target="_blank">Hadoop</a></p>
<p>(1)格式化namenode，（注意看清路径哦）
<img src="" alt="Hadoop在CentOS下的单机配置"></p>
<p>(2) 启动Hadoop守护进程</p>
<p><img src="" alt="Hadoop在CentOS下的单机配置"></p>
<p>这就表示你配置成功了，上面的一个都不能少</p>
<p>这时候你就可以点击进入下面的网站了。</p>
<p>NameNode - <a href="http://localhost:50070/" target="_blank">http://localhost:50070/</a></p>
<p>JobTracker - <a href="http://localhost:50030/" target="_blank">http://localhost:50030/</a></p>
<p>good luck</p>
<p>其实刚刚接触一个东西可能会觉得不好弄，一旦你弄好了以后就会很顺手。那时候你会告诉自己，这个东西装起来怎么这么白痴啊！赶紧开始下一个工作！加油！
来源： <a href="[http://www.linuxidc.com/Linux/2011-07/37992p4.htm](http://www.linuxidc.com/Linux/2011-07/37992p4.htm)">[http://www.linuxidc.com/Linux/2011-07/37992p4.htm](http://www.linuxidc.com/Linux/2011-07/37992p4.htm)</a> </p>

      
    </div>
    
    
      
    
    
        <footer id="post-meta">
        <span class="categories">Posted in<span class="breadcrumb fa fa-folder"><li><a href="/categories/hadoop/">hadoop</a></li></span></span> | <span class="tags">Tagged <a href="/tags/hadoop/" class="label label-primary">hadoop</a></span> | <span class="time">recent updated:<time title="2014-03-07 09:54:36"datetime="2014-03-07 09:54:36"> mar. 7 2014</time></span> | <span class="comment-link">
<a href="http://itsolife.com/2014/02/02/2014-02-02-hadoop--Hadoop在CentOS下的单机配置/#comments" class="ds-thread-count comment-link" data-thread-key="2014-02-02-hadoop--Hadoop在CentOS下的单机配置" data-count-type="comments">暂无评论</a></span>
        </footer>
    
    <div class="clearfix"></div>
  </div>
</div>
</article>



  <article>
<div id="post" class="post well">
  <div class="post-content">
    <header class="well-sm">
      <i class="fa icon fa-5x pull-left"></i>
      <h1 class="title"><a href="/2014/02/02/2014-02-02-hadoop--hdfs_design/">hdfs_design</a></h1>
      
        <span>Posted on<time datetime="2014-02-02T01:54:36.000Z"> <a href="/2014/02/02/2014-02-02-hadoop--hdfs_design/">feb. 2 2014</a></time></span>      
    </header>
    
    
    <div class="entry">
      
        <h1 id="hdfs_design">hdfs_design</h1>
<p>HDFS Architecture
by Dhruba Borthakur
Table of contents
1 2
Introduction .......................................................................................................................3 Assumptions and Goals .....................................................................................................3
2.1 2.2 2.3 2.4 2.5 2.6
Hardware Failure .......................................................................................................... 3 Streaming Data Access .................................................................................................3 Large Data Sets .............................................................................................................3 Simple Coherency Model ............................................................................................. 4 “Moving Computation is Cheaper than Moving Data” ................................................4 Portability Across Heterogeneous Hardware and Software Platforms .........................4
3 4 5
NameNode and DataNodes ...............................................................................................4 The File System Namespace ............................................................................................. 5 Data Replication ................................................................................................................6
5.1 5.2 5.3
Replica Placement: The First Baby Steps .................................................................... 7 Replica Selection .......................................................................................................... 8 Safemode ...................................................................................................................... 8
6 7 8
The Persistence of File System Metadata ......................................................................... 8 The Communication Protocols ......................................................................................... 9 Robustness ........................................................................................................................ 9
8.1 8.2 8.3 8.4 8.5
Data Disk Failure, Heartbeats and Re-Replication .....................................................10 Cluster Rebalancing ....................................................................................................10 Data Integrity ..............................................................................................................10 Metadata Disk Failure ................................................................................................ 10 Snapshots ....................................................................................................................11
Copyright © 2008 The Apache Software Foundation. All rights reserved.
HDFS Architecture
9
Data Organization ........................................................................................................... 11
9.1 9.2 9.3
Data Blocks ................................................................................................................ 11 Staging ........................................................................................................................11 Replication Pipelining ................................................................................................ 12 FS Shell .....................................................................................................................12 DFSAdmin ................................................................................................................ 13 Browser Interface ......................................................................................................13 File Deletes and Undeletes ....................................................................................... 13 Decrease Replication Factor ..................................................................................... 14
10
Accessibility .................................................................................................................. 12
10.1 10.2 10.3 11
Space Reclamation ........................................................................................................ 13
11.1 11.2 12
References ..................................................................................................................... 14
Page 2
Copyright © 2008 The Apache Software Foundation. All rights reserved.
HDFS Architecture</p>
<ol>
<li>Introduction
The Hadoop Distributed File System (HDFS) is a distributed file system designed to run on commodity hardware. It has many similarities with existing distributed file systems. However, the differences from other distributed file systems are significant. HDFS is highly fault-tolerant and is designed to be deployed on low-cost hardware. HDFS provides high throughput access to application data and is suitable for applications that have large data sets. HDFS relaxes a few POSIX requirements to enable streaming access to file system data. HDFS was originally built as infrastructure for the Apache Nutch web search engine project. HDFS is part of the Apache Hadoop Core project. The project URL is <a href="http://hadoop.apache.org/core/" target="_blank">http://hadoop.apache.org/core/</a>.</li>
<li>Assumptions and Goals
2.1. Hardware Failure
Hardware failure is the norm rather than the exception. An HDFS instance may consist of hundreds or thousands of server machines, each storing part of the file system’s data. The fact that there are a huge number of components and that each component has a non-trivial probability of failure means that some component of HDFS is always non-functional. Therefore, detection of faults and quick, automatic recovery from them is a core architectural goal of HDFS.
2.2. Streaming Data Access
Applications that run on HDFS need streaming access to their data sets. They are not general purpose applications that typically run on general purpose file systems. HDFS is designed more for batch processing rather than interactive use by users. The emphasis is on high throughput of data access rather than low latency of data access. POSIX imposes many hard requirements that are not needed for applications that are targeted for HDFS. POSIX semantics in a few key areas has been traded to increase data throughput rates.
2.3. Large Data Sets
Applications that run on HDFS have large data sets. A typical file in HDFS is gigabytes to terabytes in size. Thus, HDFS is tuned to support large files. It should provide high aggregate data bandwidth and scale to hundreds of nodes in a single cluster. It should support tens of millions of files in a single instance.
Page 3
Copyright © 2008 The Apache Software Foundation. All rights reserved.
HDFS Architecture
2.4. Simple Coherency Model
HDFS applications need a write-once-read-many access model for files. A file once created, written, and closed need not be changed. This assumption simplifies data coherency issues and enables high throughput data access. A Map/Reduce application or a web crawler application fits perfectly with this model. There is a plan to support appending-writes to files in the future.
2.5. “Moving Computation is Cheaper than Moving Data”
A computation requested by an application is much more efficient if it is executed near the data it operates on. This is especially true when the size of the data set is huge. This minimizes network congestion and increases the overall throughput of the system. The assumption is that it is often better to migrate the computation closer to where the data is located rather than moving the data to where the application is running. HDFS provides interfaces for applications to move themselves closer to where the data is located.
2.6. Portability Across Heterogeneous Hardware and Software Platforms
HDFS has been designed to be easily portable from one platform to another. This facilitates widespread adoption of HDFS as a platform of choice for a large set of applications.</li>
<li>NameNode and DataNodes
HDFS has a master/slave architecture. An HDFS cluster consists of a single NameNode, a master server that manages the file system namespace and regulates access to files by clients. In addition, there are a number of DataNodes, usually one per node in the cluster, which manage storage attached to the nodes that they run on. HDFS exposes a file system namespace and allows user data to be stored in files. Internally, a file is split into one or more blocks and these blocks are stored in a set of DataNodes. The NameNode executes file system namespace operations like opening, closing, and renaming files and directories. It also determines the mapping of blocks to DataNodes. The DataNodes are responsible for serving read and write requests from the file system’s clients. The DataNodes also perform block creation, deletion, and replication upon instruction from the NameNode.
Page 4
Copyright © 2008 The Apache Software Foundation. All rights reserved.
<img src="" alt=""> HDFS Architecture
The NameNode and DataNode are pieces of software designed to run on commodity machines. These machines typically run a GNU/Linux operating system (OS). HDFS is built using the Java language; any machine that supports Java can run the NameNode or the DataNode software. Usage of the highly portable Java language means that HDFS can be deployed on a wide range of machines. A typical deployment has a dedicated machine that runs only the NameNode software. Each of the other machines in the cluster runs one instance of the DataNode software. The architecture does not preclude running multiple DataNodes on the same machine but in a real deployment that is rarely the case. The existence of a single NameNode in a cluster greatly simplifies the architecture of the system. The NameNode is the arbitrator and repository for all HDFS metadata. The system is designed in such a way that user data never flows through the NameNode.</li>
<li>The File System Namespace
HDFS supports a traditional hierarchical file organization. A user or an application can create directories and store files inside these directories. The file system namespace hierarchy is
Page 5
Copyright © 2008 The Apache Software Foundation. All rights reserved.
HDFS Architecture
similar to most other existing file systems; one can create and remove files, move a file from one directory to another, or rename a file. HDFS does not yet implement user quotas or access permissions. HDFS does not support hard links or soft links. However, the HDFS architecture does not preclude implementing these features. The NameNode maintains the file system namespace. Any change to the file system namespace or its properties is recorded by the NameNode. An application can specify the number of replicas of a file that should be maintained by HDFS. The number of copies of a file is called the replication factor of that file. This information is stored by the NameNode.</li>
<li>Data Replication
HDFS is designed to reliably store very large files across machines in a large cluster. It stores each file as a sequence of blocks; all blocks in a file except the last block are the same size. The blocks of a file are replicated for fault tolerance. The block size and replication factor are configurable per file. An application can specify the number of replicas of a file. The replication factor can be specified at file creation time and can be changed later. Files in HDFS are write-once and have strictly one writer at any time. The NameNode makes all decisions regarding replication of blocks. It periodically receives a Heartbeat and a Blockreport from each of the DataNodes in the cluster. Receipt of a Heartbeat implies that the DataNode is functioning properly. A Blockreport contains a list of all blocks on a DataNode.
Page 6
Copyright © 2008 The Apache Software Foundation. All rights reserved.
<img src="" alt=""> HDFS Architecture
5.1. Replica Placement: The First Baby Steps
The placement of replicas is critical to HDFS reliability and performance. Optimizing replica placement distinguishes HDFS from most other distributed file systems. This is a feature that needs lots of tuning and experience. The purpose of a rack-aware replica placement policy is to improve data reliability, availability, and network bandwidth utilization. The current implementation for the replica placement policy is a first effort in this direction. The short-term goals of implementing this policy are to validate it on production systems, learn more about its behavior, and build a foundation to test and research more sophisticated policies. Large HDFS instances run on a cluster of computers that commonly spread across many racks. Communication between two nodes in different racks has to go through switches. In most cases, network bandwidth between machines in the same rack is greater than network bandwidth between machines in different racks. The NameNode determines the rack id each DataNode belongs to via the process outlined in Rack Awareness. A simple but non-optimal policy is to place replicas on unique racks. This prevents losing data when an entire rack fails and allows use of bandwidth from multiple
Page 7
Copyright © 2008 The Apache Software Foundation. All rights reserved.
HDFS Architecture
racks when reading data. This policy evenly distributes replicas in the cluster which makes it easy to balance load on component failure. However, this policy increases the cost of writes because a write needs to transfer blocks to multiple racks. For the common case, when the replication factor is three, HDFS’s placement policy is to put one replica on one node in the local rack, another on a different node in the local rack, and the last on a different node in a different rack. This policy cuts the inter-rack write traffic which generally improves write performance. The chance of rack failure is far less than that of node failure; this policy does not impact data reliability and availability guarantees. However, it does reduce the aggregate network bandwidth used when reading data since a block is placed in only two unique racks rather than three. With this policy, the replicas of a file do not evenly distribute across the racks. One third of replicas are on one node, two thirds of replicas are on one rack, and the other third are evenly distributed across the remaining racks. This policy improves write performance without compromising data reliability or read performance. The current, default replica placement policy described here is a work in progress.
5.2. Replica Selection
To minimize global bandwidth consumption and read latency, HDFS tries to satisfy a read request from a replica that is closest to the reader. If there exists a replica on the same rack as the reader node, then that replica is preferred to satisfy the read request. If angg/ HDFS cluster spans multiple data centers, then a replica that is resident in the local data center is preferred over any remote replica.
5.3. Safemode
On startup, the NameNode enters a special state called Safemode. Replication of data blocks does not occur when the NameNode is in the Safemode state. The NameNode receives Heartbeat and Blockreport messages from the DataNodes. A Blockreport contains the list of data blocks that a DataNode is hosting. Each block has a specified minimum number of replicas. A block is considered safely replicated when the minimum number of replicas of that data block has checked in with the NameNode. After a configurable percentage of safely replicated data blocks checks in with the NameNode (plus an additional 30 seconds), the NameNode exits the Safemode state. It then determines the list of data blocks (if any) that still have fewer than the specified number of replicas. The NameNode then replicates these blocks to other DataNodes.</li>
<li>The Persistence of File System Metadata
Page 8
Copyright © 2008 The Apache Software Foundation. All rights reserved.
HDFS Architecture
The HDFS namespace is stored by the NameNode. The NameNode uses a transaction log called the EditLog to persistently record every change that occurs to file system metadata. For example, creating a new file in HDFS causes the NameNode to insert a record into the EditLog indicating this. Similarly, changing the replication factor of a file causes a new record to be inserted into the EditLog. The NameNode uses a file in its local host OS file system to store the EditLog. The entire file system namespace, including the mapping of blocks to files and file system properties, is stored in a file called the FsImage. The FsImage is stored as a file in the NameNode’s local file system too. The NameNode keeps an image of the entire file system namespace and file Blockmap in memory. This key metadata item is designed to be compact, such that a NameNode with 4 GB of RAM is plenty to support a huge number of files and directories. When the NameNode starts up, it reads the FsImage and EditLog from disk, applies all the transactions from the EditLog to the in-memory representation of the FsImage, and flushes out this new version into a new FsImage on disk. It can then truncate the old EditLog because its transactions have been applied to the persistent FsImage. This process is called a checkpoint. In the current implementation, a checkpoint only occurs when the NameNode starts up. Work is in progress to support periodic checkpointing in the near future. The DataNode stores HDFS data in files in its local file system. The DataNode has no knowledge about HDFS files. It stores each block of HDFS data in a separate file in its local file system. The DataNode does not create all files in the same directory. Instead, it uses a heuristic to determine the optimal number of files per directory and creates subdirectories appropriately. It is not optimal to create all local files in the same directory because the local file system might not be able to efficiently support a huge number of files in a single directory. When a DataNode starts up, it scans through its local file system, generates a list of all HDFS data blocks that correspond to each of these local files and sends this report to the NameNode: this is the Blockreport.</li>
<li>The Communication Protocols
All HDFS communication protocols are layered on top of the TCP/IP protocol. A client establishes a connection to a configurable TCP port on the NameNode machine. It talks the ClientProtocol with the NameNode. The DataNodes talk to the NameNode using the DataNode Protocol. A Remote Procedure Call (RPC) abstraction wraps both the Client Protocol and the DataNode Protocol. By design, the NameNode never initiates any RPCs. Instead, it only responds to RPC requests issued by DataNodes or clients.</li>
<li>Robustness
The primary objective of HDFS is to store data reliably even in the presence of failures. The
Page 9
Copyright © 2008 The Apache Software Foundation. All rights reserved.
HDFS Architecture
three common types of failures are NameNode failures, DataNode failures and network partitions.
8.1. Data Disk Failure, Heartbeats and Re-Replication
Each DataNode sends a Heartbeat message to the NameNode periodically. A network partition can cause a subset of DataNodes to lose connectivity with the NameNode. The NameNode detects this condition by the absence of a Heartbeat message. The NameNode marks DataNodes without recent Heartbeats as dead and does not forward any new IO requests to them. Any data that was registered to a dead DataNode is not available to HDFS any more. DataNode death may cause the replication factor of some blocks to fall below their specified value. The NameNode constantly tracks which blocks need to be replicated and initiates replication whenever necessary. The necessity for re-replication may arise due to many reasons: a DataNode may become unavailable, a replica may become corrupted, a hard disk on a DataNode may fail, or the replication factor of a file may be increased.
8.2. Cluster Rebalancing
The HDFS architecture is compatible with data rebalancing schemes. A scheme might automatically move data from one DataNode to another if the free space on a DataNode falls below a certain threshold. In the event of a sudden high demand for a particular file, a scheme might dynamically create additional replicas and rebalance other data in the cluster. These types of data rebalancing schemes are not yet implemented.
8.3. Data Integrity
It is possible that a block of data fetched from a DataNode arrives corrupted. This corruption can occur because of faults in a storage device, network faults, or buggy software. The HDFS client software implements checksum checking on the contents of HDFS files. When a client creates an HDFS file, it computes a checksum of each block of the file and stores these checksums in a separate hidden file in the same HDFS namespace. When a client retrieves file contents it verifies that the data it received from each DataNode matches the checksum stored in the associated checksum file. If not, then the client can opt to retrieve that block from another DataNode that has a replica of that block.
8.4. Metadata Disk Failure
The FsImage and the EditLog are central data structures of HDFS. A corruption of these files can cause the HDFS instance to be non-functional. For this reason, the NameNode can be configured to support maintaining multiple copies of the FsImage and EditLog. Any update
Page 10
Copyright © 2008 The Apache Software Foundation. All rights reserved.
HDFS Architecture
to either the FsImage or EditLog causes each of the FsImages and EditLogs to get updated synchronously. This synchronous updating of multiple copies of the FsImage and EditLog may degrade the rate of namespace transactions per second that a NameNode can support. However, this degradation is acceptable because even though HDFS applications are very data intensive in nature, they are not metadata intensive. When a NameNode restarts, it selects the latest consistent FsImage and EditLog to use. The NameNode machine is a single point of failure for an HDFS cluster. If the NameNode machine fails, manual intervention is necessary. Currently, automatic restart and failover of the NameNode software to another machine is not supported.
8.5. Snapshots
Snapshots support storing a copy of data at a particular instant of time. One usage of the snapshot feature may be to roll back a corrupted HDFS instance to a previously known good point in time. HDFS does not currently support snapshots but will in a future release.</li>
<li>Data Organization
9.1. Data Blocks
HDFS is designed to support very large files. Applications that are compatible with HDFS are those that deal with large data sets. These applications write their data only once but they read it one or more times and require these reads to be satisfied at streaming speeds. HDFS supports write-once-read-many semantics on files. A typical block size used by HDFS is 64 MB. Thus, an HDFS file is chopped up into 64 MB chunks, and if possible, each chunk will reside on a different DataNode.
9.2. Staging
A client request to create a file does not reach the NameNode immediately. In fact, initially the HDFS client caches the file data into a temporary local file. Application writes are transparently redirected to this temporary local file. When the local file accumulates data worth over one HDFS block size, the client contacts the NameNode. The NameNode inserts the file name into the file system hierarchy and allocates a data block for it. The NameNode responds to the client request with the identity of the DataNode and the destination data block. Then the client flushes the block of data from the local temporary file to the specified DataNode. When a file is closed, the remaining un-flushed data in the temporary local file is transferred to the DataNode. The client then tells the NameNode that the file is closed. At this point, the NameNode commits the file creation operation into a persistent store. If the
Page 11
Copyright © 2008 The Apache Software Foundation. All rights reserved.
HDFS Architecture
NameNode dies before the file is closed, the file is lost. The above approach has been adopted after careful consideration of target applications that run on HDFS. These applications need streaming writes to files. If a client writes to a remote file directly without any client side buffering, the network speed and the congestion in the network impacts throughput considerably. This approach is not without precedent. Earlier distributed file systems, e.g. AFS, have used client side caching to improve performance. A POSIX requirement has been relaxed to achieve higher performance of data uploads.
9.3. Replication Pipelining
When a client is writing data to an HDFS file, its data is first written to a local file as explained in the previous section. Suppose the HDFS file has a replication factor of three. When the local file accumulates a full block of user data, the client retrieves a list of DataNodes from the NameNode. This list contains the DataNodes that will host a replica of that block. The client then flushes the data block to the first DataNode. The first DataNode starts receiving the data in small portions (4 KB), writes each portion to its local repository and transfers that portion to the second DataNode in the list. The second DataNode, in turn starts receiving each portion of the data block, writes that portion to its repository and then flushes that portion to the third DataNode. Finally, the third DataNode writes the data to its local repository. Thus, a DataNode can be receiving data from the previous one in the pipeline and at the same time forwarding data to the next one in the pipeline. Thus, the data is pipelined from one DataNode to the next.</li>
<li>Accessibility
HDFS can be accessed from applications in many different ways. Natively, HDFS provides a FileSystem Java API for applications to use. A C language wrapper for this Java API is also available. In addition, an HTTP browser can also be used to browse the files of an HDFS instance. Work is in progress to expose HDFS through the WebDAV protocol.
10.1. FS Shell
HDFS allows user data to be organized in the form of files and directories. It provides a commandline interface called FS shell that lets a user interact with the data in HDFS. The syntax of this command set is similar to other shells (e.g. bash, csh) that users are already familiar with. Here are some sample action/command pairs:
Action Create a directory named /foodir Command bin/hadoop dfs -mkdir /foodir
Page 12
Copyright © 2008 The Apache Software Foundation. All rights reserved.
HDFS Architecture
Remove a directory named /foodir View the contents of a file named /foodir/myfile.txt
bin/hadoop dfs -rmr /foodir bin/hadoop dfs -cat /foodir/myfile.txt
FS shell is targeted for applications that need a scripting language to interact with the stored data.
10.2. DFSAdmin
The DFSAdmin command set is used for administering an HDFS cluster. These are commands that are used only by an HDFS administrator. Here are some sample action/command pairs:
Action Put the cluster in Safemode Generate a list of DataNodes Recommission or decommission DataNode(s) Command bin/hadoop dfsadmin -safemode enter bin/hadoop dfsadmin -report bin/hadoop dfsadmin -refreshNodes
10.3. Browser Interface
A typical HDFS install configures a web server to expose the HDFS namespace through a configurable TCP port. This allows a user to navigate the HDFS namespace and view the contents of its files using a web browser.</li>
<li>Space Reclamation
11.1. File Deletes and Undeletes
When a file is deleted by a user or an application, it is not immediately removed from HDFS. Instead, HDFS first renames it to a file in the /trash directory. The file can be restored quickly as long as it remains in /trash. A file remains in /trash for a configurable amount of time. After the expiry of its life in /trash, the NameNode deletes the file from the HDFS namespace. The deletion of a file causes the blocks associated with the file to be freed. Note that there could be an appreciable time delay between the time a file is deleted by a user and the time of the corresponding increase in free space in HDFS. A user can Undelete a file after deleting it as long as it remains in the /trash directory. If a user wants to undelete a file that he/she has deleted, he/she can navigate the /trash directory and retrieve the file. The /trash directory contains only the latest copy of the file
Page 13
Copyright © 2008 The Apache Software Foundation. All rights reserved.
HDFS Architecture
that was deleted. The /trash directory is just like any other directory with one special feature: HDFS applies specified policies to automatically delete files from this directory. The current default policy is to delete files from /trash that are more than 6 hours old. In the future, this policy will be configurable through a well defined interface.
11.2. Decrease Replication Factor
When the replication factor of a file is reduced, the NameNode selects excess replicas that can be deleted. The next Heartbeat transfers this information to the DataNode. The DataNode then removes the corresponding blocks and the corresponding free space appears in the cluster. Once again, there might be a time delay between the completion of the setReplication API call and the appearance of free space in the cluster.</li>
<li>References
Hadoop JavaDoc API. HDFS source code: <a href="http://hadoop.apache.org/core/version_control.html" target="_blank">http://hadoop.apache.org/core/version_control.html</a>
Page 14
Copyright © 2008 The Apache Software Foundation. All rights reserved.</li>
</ol>

      
    </div>
    
    
      
    
    
        <footer id="post-meta">
        <span class="categories">Posted in<span class="breadcrumb fa fa-folder"><li><a href="/categories/hadoop/">hadoop</a></li></span></span> | <span class="tags">Tagged <a href="/tags/hadoop/" class="label label-primary">hadoop</a></span> | <span class="time">recent updated:<time title="2014-03-07 09:54:36"datetime="2014-03-07 09:54:36"> mar. 7 2014</time></span> | <span class="comment-link">
<a href="http://itsolife.com/2014/02/02/2014-02-02-hadoop--hdfs_design/#comments" class="ds-thread-count comment-link" data-thread-key="2014-02-02-hadoop--hdfs_design" data-count-type="comments">暂无评论</a></span>
        </footer>
    
    <div class="clearfix"></div>
  </div>
</div>
</article>



  <article>
<div id="post" class="post well">
  <div class="post-content">
    <header class="well-sm">
      <i class="fa icon fa-5x pull-left"></i>
      <h1 class="title"><a href="/2014/02/02/2014-02-02-linux-ubuntu--Ubuntu1210软件更新源列表_Linux教程_Linux公社-Linux系统门户网站/">Ubuntu 12.10 软件更新源列表_Linux教程_Linux公社</a></h1>
      
        <span>Posted on<time datetime="2014-02-02T01:54:36.000Z"> <a href="/2014/02/02/2014-02-02-linux-ubuntu--Ubuntu1210软件更新源列表_Linux教程_Linux公社-Linux系统门户网站/">feb. 2 2014</a></time></span>      
    </header>
    
    
    <div class="entry">
      
        <h1 id="ubuntu-12-10-_linux-_linux-linux-">Ubuntu 12.10 软件更新源列表_Linux教程_Linux公社-Linux系统门户网站</h1>
<h3 id="-">分享到</h3>
<ul>
<li><a href="http://www.linuxidc.com/Linux/2012-10/73114.htm#" target="_blank">一键分享</a></li>
<li><a href="http://www.linuxidc.com/Linux/2012-10/73114.htm#" target="_blank">QQ空间</a></li>
<li><a href="http://www.linuxidc.com/Linux/2012-10/73114.htm#" target="_blank">新浪微博</a></li>
<li><a href="http://www.linuxidc.com/Linux/2012-10/73114.htm#" target="_blank">百度搜藏</a></li>
<li><a href="http://www.linuxidc.com/Linux/2012-10/73114.htm#" target="_blank">人人网</a></li>
<li><a href="http://www.linuxidc.com/Linux/2012-10/73114.htm#" target="_blank">腾讯微博</a></li>
<li><a href="http://www.linuxidc.com/Linux/2012-10/73114.htm#" target="_blank">百度相册</a></li>
<li><a href="http://www.linuxidc.com/Linux/2012-10/73114.htm#" target="_blank">开心网</a></li>
<li><a href="http://www.linuxidc.com/Linux/2012-10/73114.htm#" target="_blank">腾讯朋友</a></li>
<li><a href="http://www.linuxidc.com/Linux/2012-10/73114.htm#" target="_blank">百度贴吧</a></li>
<li><a href="http://www.linuxidc.com/Linux/2012-10/73114.htm#" target="_blank">豆瓣网</a></li>
<li><a href="http://www.linuxidc.com/Linux/2012-10/73114.htm#" target="_blank">搜狐微博</a></li>
<li><a href="http://www.linuxidc.com/Linux/2012-10/73114.htm#" target="_blank">百度新首页</a></li>
<li><a href="http://www.linuxidc.com/Linux/2012-10/73114.htm#" target="_blank">QQ</a></li>
<li><a href="http://www.linuxidc.com/Linux/2012-10/73114.htm#" target="_blank">和讯微博</a></li>
<li><a href="http://www.linuxidc.com/Linux/2012-10/73114.htm#" target="_blank">更多...</a></li>
</ul>
<p><a href="http://www.linuxidc.com/Linux/2012-10/73114.htm#" target="_blank">百度分享</a></p>
<p><a href="&quot;點擊以繁體中文方式浏覽&quot;">繁體</a></p>
<p>你好，游客 <a href="http://www.linuxidc.com/Linux/2012-10/73114.htm#" target="_blank">登录</a> <a href="http://www.linuxidc.com/memberreg.aspx" target="_blank">注册</a> <a href="http://www.linuxidc.com/membernewsadd.aspx" target="_blank">发布</a><a href="http://www.linuxidc.com/search.aspx" target="_blank">搜索</a>
<a href="http://www.linuxidc.com/" target="_blank"><img src="" alt="Linux公社"></a>
<a href="http://www.linuxidc.com/topicnews.aspx?tid=11" title="Android专题" target="_blank"><img src="" alt="Android专题"></a></p>
<p><a href="http://www.linuxidc.com/index.htm" target="_blank">首页</a><a href="http://www.linuxidc.com/it/" target="_blank">Linux新闻</a><a href="http://www.linuxidc.com/Linuxit/" target="_blank">Linux教程</a><a href="http://www.linuxidc.com/MySql/" target="_blank">数据库技术</a><a href="http://www.linuxidc.com/RedLinux/" target="_blank">Linux编程</a><a href="http://www.linuxidc.com/Apache/" target="_blank">服务器应用</a><a href="http://www.linuxidc.com/Unix/" target="_blank">Linux安全</a><a href="http://www.linuxidc.com/download/" target="_blank">Linux下载</a><a href="http://www.linuxidc.com/Linuxrz/" target="_blank">Linux认证</a><a href="http://www.linuxidc.com/theme/" target="_blank">Linux主题</a><a href="http://www.linuxidc.com/Linuxwallpaper/" target="_blank">Linux壁纸</a><a href="http://www.linuxidc.com/linuxsoft/" target="_blank">Linux软件</a><a href="http://www.linuxidc.com/digi/" target="_blank">数码</a><a href="http://www.linuxidc.com/mobile/" target="_blank">手机</a><a href="http://www.linuxidc.com/diannao/" target="_blank">电脑</a>
<a href="http://www.linuxidc.com/index.htm" target="_blank">首页</a> → <a href="http://www.linuxidc.com/Linuxit/" target="_blank">Linux教程</a></p>
<p><a href="http://www.yutianedu.com/list.asp?Unid=22239" target="_blank"><img src="" alt=""></a> <a href="http://www.boxue.com.cn/" target="_blank"><img src="" alt=""></a>
背景：<img src="" alt="#EDF0F5"> <img src="" alt="#FAFBE6"> <img src="" alt="#FFF2E2"> <img src="" alt="#FDE6E0"> <img src="" alt="#F3FFE1"> <img src="" alt="#DAFAF3"> <img src="" alt="#EAEAEF"> <img src="" alt="默认"> 阅读新闻</p>
<h1 id="ubuntu-12-10-">Ubuntu 12.10 软件更新源列表</h1>
<p> [日期：2012-10-28] 来源：Linux公社  作者：Linux [字体：<a href="">大</a> <a href="">中</a> <a href="">小</a>]</p>
<p><a href="http://www.upemb.com/page/uealinuxidc12531.php" title="尚观Linux" target="_blank"><img src="" alt=""></a>
<a href="http://www.linuxidc.com/topicnews.aspx?tid=2" title="Ubuntu" target="_blank">Ubuntu</a> 12.10也正式发布了， 安装好后第一件事就是更换源，Ubuntu网易的更新源速度很不错。</p>
<p>Ubuntu 12.10正式版发布下载  <a href="http://www.linuxidc.com/Linux/2012-10/72581.htm" target="_blank"><a href="http://www.linuxidc.com/Linux/2012-10/72581.htm">http://www.linuxidc.com/Linux/2012-10/72581.htm</a></a></p>
<p>废话少说， 上源：</p>
<p>首先，备份一下Ubuntu 12.10 原来的源地址列表文件</p>
<p>sudo cp /etc/apt/sources.list /etc/apt/sources.list.old</p>
<p>然后进行修改
sudo gedit /etc/apt/sources.list</p>
<p>可以在里面添加资源地址，我是直接覆盖掉原来的。</p>
<p>下面是网上找到的一些较好的源，有大型网站的，也有教育网的，可以根据自己的情况添加两三个即可。</p>
<p>/#网易的源（163源，无论是不是教育网，速度都很快）
deb <a href="http://mirrors.163.com/ubuntu/" target="_blank">http://mirrors.163.com/ubuntu/</a> quantal main universe restricted multiverse
deb-src <a href="http://mirrors.163.com/ubuntu/" target="_blank">http://mirrors.163.com/ubuntu/</a> quantal main universe restricted multiverse
deb <a href="http://mirrors.163.com/ubuntu/" target="_blank">http://mirrors.163.com/ubuntu/</a> quantal-security universe main multiverse restricted
deb-src <a href="http://mirrors.163.com/ubuntu/" target="_blank">http://mirrors.163.com/ubuntu/</a> quantal-security universe main multiverse restricted
deb <a href="http://mirrors.163.com/ubuntu/" target="_blank">http://mirrors.163.com/ubuntu/</a> quantal-updates universe main multiverse restricted
deb <a href="http://mirrors.163.com/ubuntu/" target="_blank">http://mirrors.163.com/ubuntu/</a> quantal-proposed universe main multiverse restricted
deb-src <a href="http://mirrors.163.com/ubuntu/" target="_blank">http://mirrors.163.com/ubuntu/</a> quantal-proposed universe main multiverse restricted
deb <a href="http://mirrors.163.com/ubuntu/" target="_blank">http://mirrors.163.com/ubuntu/</a> quantal-backports universe main multiverse restricted
deb-src <a href="http://mirrors.163.com/ubuntu/" target="_blank">http://mirrors.163.com/ubuntu/</a> quantal-backports universe main multiverse restricted
deb-src <a href="http://mirrors.163.com/ubuntu/" target="_blank">http://mirrors.163.com/ubuntu/</a> quantal-updates universe main multiverse restricted</p>
<p>/#搜狐的源（sohu 源今天还没有更新，不过应该快了）
deb <a href="http://mirrors.sohu.com/ubuntu/" target="_blank">http://mirrors.sohu.com/ubuntu/</a> quantal main restricted
deb-src <a href="http://mirrors.sohu.com/ubuntu/" target="_blank">http://mirrors.sohu.com/ubuntu/</a> quantal main restricted
deb <a href="http://mirrors.sohu.com/ubuntu/" target="_blank">http://mirrors.sohu.com/ubuntu/</a> quantal-updates main restricted
deb-src <a href="http://mirrors.sohu.com/ubuntu/" target="_blank">http://mirrors.sohu.com/ubuntu/</a> quantal-updates main restricted
deb <a href="http://mirrors.sohu.com/ubuntu/" target="_blank">http://mirrors.sohu.com/ubuntu/</a> quantal universe
deb-src <a href="http://mirrors.sohu.com/ubuntu/" target="_blank">http://mirrors.sohu.com/ubuntu/</a> quantal universe
deb <a href="http://mirrors.sohu.com/ubuntu/" target="_blank">http://mirrors.sohu.com/ubuntu/</a> quantal-updates universe
deb-src <a href="http://mirrors.sohu.com/ubuntu/" target="_blank">http://mirrors.sohu.com/ubuntu/</a> quantal-updates universe
deb <a href="http://mirrors.sohu.com/ubuntu/" target="_blank">http://mirrors.sohu.com/ubuntu/</a> quantal multiverse
deb-src <a href="http://mirrors.sohu.com/ubuntu/" target="_blank">http://mirrors.sohu.com/ubuntu/</a> quantal multiverse
deb <a href="http://mirrors.sohu.com/ubuntu/" target="_blank">http://mirrors.sohu.com/ubuntu/</a> quantal-updates multiverse
deb-src <a href="http://mirrors.sohu.com/ubuntu/" target="_blank">http://mirrors.sohu.com/ubuntu/</a> quantal-updates multiverse
deb <a href="http://mirrors.sohu.com/ubuntu/" target="_blank">http://mirrors.sohu.com/ubuntu/</a> quantal-backports main restricted universe multiverse
deb-src <a href="http://mirrors.sohu.com/ubuntu/" target="_blank">http://mirrors.sohu.com/ubuntu/</a> quantal-backports main restricted universe multiverse
deb <a href="http://mirrors.sohu.com/ubuntu/" target="_blank">http://mirrors.sohu.com/ubuntu/</a> quantal-security main restricted
deb-src <a href="http://mirrors.sohu.com/ubuntu/" target="_blank">http://mirrors.sohu.com/ubuntu/</a> quantal-security main restricted
deb <a href="http://mirrors.sohu.com/ubuntu/" target="_blank">http://mirrors.sohu.com/ubuntu/</a> quantal-security universe
deb-src <a href="http://mirrors.sohu.com/ubuntu/" target="_blank">http://mirrors.sohu.com/ubuntu/</a> quantal-security universe
deb <a href="http://mirrors.sohu.com/ubuntu/" target="_blank">http://mirrors.sohu.com/ubuntu/</a> quantal-security multiverse
deb-src <a href="http://mirrors.sohu.com/ubuntu/" target="_blank">http://mirrors.sohu.com/ubuntu/</a> quantal-security multiverse
deb <a href="http://extras.ubuntu.com/ubuntu" target="_blank">http://extras.ubuntu.com/ubuntu</a> quantal main
deb-src <a href="http://extras.ubuntu.com/ubuntu" target="_blank">http://extras.ubuntu.com/ubuntu</a> quantal main</p>
<p>/#台湾源（台湾的ubuntu 更新源还是很给力的）
deb <a href="http://tw.archive.ubuntu.com/ubuntu/" target="_blank">http://tw.archive.ubuntu.com/ubuntu/</a> quantal main universe restricted multiverse
deb-src <a href="http://tw.archive.ubuntu.com/ubuntu/" target="_blank">http://tw.archive.ubuntu.com/ubuntu/</a> quantal main universe restricted multiverse
deb <a href="http://tw.archive.ubuntu.com/ubuntu/" target="_blank">http://tw.archive.ubuntu.com/ubuntu/</a> quantal-security universe main multiverse restricted
deb-src <a href="http://tw.archive.ubuntu.com/ubuntu/" target="_blank">http://tw.archive.ubuntu.com/ubuntu/</a> quantal-security universe main multiverse restricted
deb <a href="http://tw.archive.ubuntu.com/ubuntu/" target="_blank">http://tw.archive.ubuntu.com/ubuntu/</a> quantal-updates universe main multiverse restricted
deb-src <a href="http://tw.archive.ubuntu.com/ubuntu/" target="_blank">http://tw.archive.ubuntu.com/ubuntu/</a> quantal-updates universe main multiverse restricted</p>
<p>/#骨头源，骨头源是bones7456架设的一个Ubuntu源 ，提供ubuntu,deepin
deb <a href="http://ubuntu.srt.cn/ubuntu/" target="_blank">http://ubuntu.srt.cn/ubuntu/</a> quantal main universe restricted multiverse
deb-src <a href="http://ubuntu.srt.cn/ubuntu/" target="_blank">http://ubuntu.srt.cn/ubuntu/</a> quantal main universe restricted multiverse
deb <a href="http://ubuntu.srt.cn/ubuntu/" target="_blank">http://ubuntu.srt.cn/ubuntu/</a> quantal-security universe main multiverse restricted
deb-src <a href="http://ubuntu.srt.cn/ubuntu/" target="_blank">http://ubuntu.srt.cn/ubuntu/</a> quantal-security universe main multiverse restricted
deb <a href="http://ubuntu.srt.cn/ubuntu/" target="_blank">http://ubuntu.srt.cn/ubuntu/</a> quantal-updates universe main multiverse restricted
deb <a href="http://ubuntu.srt.cn/ubuntu/" target="_blank">http://ubuntu.srt.cn/ubuntu/</a> quantal-proposed universe main multiverse restricted
deb-src <a href="http://ubuntu.srt.cn/ubuntu/" target="_blank">http://ubuntu.srt.cn/ubuntu/</a> quantal-proposed universe main multiverse restricted
deb <a href="http://ubuntu.srt.cn/ubuntu/" target="_blank">http://ubuntu.srt.cn/ubuntu/</a> quantal-backports universe main multiverse restricted
deb-src <a href="http://ubuntu.srt.cn/ubuntu/" target="_blank">http://ubuntu.srt.cn/ubuntu/</a> quantal-backports universe main multiverse restricted
deb-src <a href="http://ubuntu.srt.cn/ubuntu/" target="_blank">http://ubuntu.srt.cn/ubuntu/</a> quantal-updates universe main multiverse restricted</p>
<p>/#ubuntu.cn99.com源（推荐）:
deb <a href="http://ubuntu.cn99.com/ubuntu/" target="_blank">http://ubuntu.cn99.com/ubuntu/</a> quantal main restricted universe multiverse
deb <a href="http://ubuntu.cn99.com/ubuntu/" target="_blank">http://ubuntu.cn99.com/ubuntu/</a> quantal-updates main restricted universe multiverse
deb <a href="http://ubuntu.cn99.com/ubuntu/" target="_blank">http://ubuntu.cn99.com/ubuntu/</a> quantal-security main restricted universe multiverse
deb <a href="http://ubuntu.cn99.com/ubuntu/" target="_blank">http://ubuntu.cn99.com/ubuntu/</a> quantal-backports main restricted universe multiverse
deb <a href="http://ubuntu.cn99.com/ubuntu-cn/" target="_blank">http://ubuntu.cn99.com/ubuntu-cn/</a> quantal main restricted universe multiverse</p>
<p>/#教育网源
/#电子科技大学
deb <a href="http://ubuntu.uestc.edu.cn/ubuntu/" target="_blank">http://ubuntu.uestc.edu.cn/ubuntu/</a> quantal main restricted universe multiverse
deb <a href="http://ubuntu.uestc.edu.cn/ubuntu/" target="_blank">http://ubuntu.uestc.edu.cn/ubuntu/</a> quantal-backports main restricted universe multiverse
deb <a href="http://ubuntu.uestc.edu.cn/ubuntu/" target="_blank">http://ubuntu.uestc.edu.cn/ubuntu/</a> quantal-proposed main restricted universe multiverse
deb <a href="http://ubuntu.uestc.edu.cn/ubuntu/" target="_blank">http://ubuntu.uestc.edu.cn/ubuntu/</a> quantal-security main restricted universe multiverse
deb <a href="http://ubuntu.uestc.edu.cn/ubuntu/" target="_blank">http://ubuntu.uestc.edu.cn/ubuntu/</a> quantal-updates main restricted universe multiverse
deb-src <a href="http://ubuntu.uestc.edu.cn/ubuntu/" target="_blank">http://ubuntu.uestc.edu.cn/ubuntu/</a> quantal main restricted universe multiverse
deb-src <a href="http://ubuntu.uestc.edu.cn/ubuntu/" target="_blank">http://ubuntu.uestc.edu.cn/ubuntu/</a> quantal-backports main restricted universe multiverse
deb-src <a href="http://ubuntu.uestc.edu.cn/ubuntu/" target="_blank">http://ubuntu.uestc.edu.cn/ubuntu/</a> quantal-proposed main restricted universe multiverse
deb-src <a href="http://ubuntu.uestc.edu.cn/ubuntu/" target="_blank">http://ubuntu.uestc.edu.cn/ubuntu/</a> quantal-security main restricted universe multiverse
deb-src <a href="http://ubuntu.uestc.edu.cn/ubuntu/" target="_blank">http://ubuntu.uestc.edu.cn/ubuntu/</a> quantal-updates main restricted universe multiverse</p>
<p>/#中国科技大学
deb <a href="http://debian.ustc.edu.cn/ubuntu/" target="_blank">http://debian.ustc.edu.cn/ubuntu/</a> quantal main restricted universe multiverse
deb <a href="http://debian.ustc.edu.cn/ubuntu/" target="_blank">http://debian.ustc.edu.cn/ubuntu/</a> quantal-backports restricted universe multiverse
deb <a href="http://debian.ustc.edu.cn/ubuntu/" target="_blank">http://debian.ustc.edu.cn/ubuntu/</a> quantal-proposed main restricted universe multiverse
deb <a href="http://debian.ustc.edu.cn/ubuntu/" target="_blank">http://debian.ustc.edu.cn/ubuntu/</a> quantal-security main restricted universe multiverse
deb <a href="http://debian.ustc.edu.cn/ubuntu/" target="_blank">http://debian.ustc.edu.cn/ubuntu/</a> quantal-updates main restricted universe multiverse
deb-src <a href="http://debian.ustc.edu.cn/ubuntu/" target="_blank">http://debian.ustc.edu.cn/ubuntu/</a> quantal main restricted universe multiverse
deb-src <a href="http://debian.ustc.edu.cn/ubuntu/" target="_blank">http://debian.ustc.edu.cn/ubuntu/</a> quantal-backports main restricted universe multiverse
deb-src <a href="http://debian.ustc.edu.cn/ubuntu/" target="_blank">http://debian.ustc.edu.cn/ubuntu/</a> quantal-proposed main restricted universe multiverse
deb-src <a href="http://debian.ustc.edu.cn/ubuntu/" target="_blank">http://debian.ustc.edu.cn/ubuntu/</a> quantal-security main restricted universe multiverse
deb-src <a href="http://debian.ustc.edu.cn/ubuntu/" target="_blank">http://debian.ustc.edu.cn/ubuntu/</a> quantal-updates main restricted universe multiverse
/#北京理工大学
deb <a href="http://mirror.bjtu.edu.cn/ubuntu/" target="_blank">http://mirror.bjtu.edu.cn/ubuntu/</a> quantal main multiverse restricted universe
deb <a href="http://mirror.bjtu.edu.cn/ubuntu/" target="_blank">http://mirror.bjtu.edu.cn/ubuntu/</a> quantal-backports main multiverse restricted universe
deb <a href="http://mirror.bjtu.edu.cn/ubuntu/" target="_blank">http://mirror.bjtu.edu.cn/ubuntu/</a> quantal-proposed main multiverse restricted universe
deb <a href="http://mirror.bjtu.edu.cn/ubuntu/" target="_blank">http://mirror.bjtu.edu.cn/ubuntu/</a> quantal-security main multiverse restricted universe
deb <a href="http://mirror.bjtu.edu.cn/ubuntu/" target="_blank">http://mirror.bjtu.edu.cn/ubuntu/</a> quantal-updates main multiverse restricted universe
deb-src <a href="http://mirror.bjtu.edu.cn/ubuntu/" target="_blank">http://mirror.bjtu.edu.cn/ubuntu/</a> quantal main multiverse restricted universe
deb-src <a href="http://mirror.bjtu.edu.cn/ubuntu/" target="_blank">http://mirror.bjtu.edu.cn/ubuntu/</a> quantal-backports main multiverse restricted universe
deb-src <a href="http://mirror.bjtu.edu.cn/ubuntu/" target="_blank">http://mirror.bjtu.edu.cn/ubuntu/</a> quantal-proposed main multiverse restricted universe
deb-src <a href="http://mirror.bjtu.edu.cn/ubuntu/" target="_blank">http://mirror.bjtu.edu.cn/ubuntu/</a> quantal-security main multiverse restricted universe
deb-src <a href="http://mirror.bjtu.edu.cn/ubuntu/" target="_blank">http://mirror.bjtu.edu.cn/ubuntu/</a> quantal-updates main multiverse restricted universe</p>
<p>/#兰州大学
deb ftp://mirror.lzu.edu.cn/ubuntu/ quantal main multiverse restricted universe
deb ftp://mirror.lzu.edu.cn/ubuntu/ quantal-backports main multiverse restricted universe
deb ftp://mirror.lzu.edu.cn/ubuntu/ quantal-proposed main multiverse restricted universe
deb ftp://mirror.lzu.edu.cn/ubuntu/ quantal-security main multiverse restricted universe
deb ftp://mirror.lzu.edu.cn/ubuntu/ quantal-updates main multiverse restricted universe
deb ftp://mirror.lzu.edu.cn/ubuntu-cn/ quantal main multiverse restricted universe</p>
<p>/#上海交通大学（上海交大源，教育网的速度不用说了）
deb <a href="http://ftp.sjtu.edu.cn/ubuntu/" target="_blank">http://ftp.sjtu.edu.cn/ubuntu/</a> quantal main multiverse restricted universe
deb <a href="http://ftp.sjtu.edu.cn/ubuntu/" target="_blank">http://ftp.sjtu.edu.cn/ubuntu/</a> quantal-backports main multiverse restricted universe
deb <a href="http://ftp.sjtu.edu.cn/ubuntu/" target="_blank">http://ftp.sjtu.edu.cn/ubuntu/</a> quantal-proposed main multiverse restricted universe
deb <a href="http://ftp.sjtu.edu.cn/ubuntu/" target="_blank">http://ftp.sjtu.edu.cn/ubuntu/</a> quantal-security main multiverse restricted universe
deb <a href="http://ftp.sjtu.edu.cn/ubuntu/" target="_blank">http://ftp.sjtu.edu.cn/ubuntu/</a> quantal-updates main multiverse restricted universe
deb <a href="http://ftp.sjtu.edu.cn/ubuntu-cn/" target="_blank">http://ftp.sjtu.edu.cn/ubuntu-cn/</a> quantal main multiverse restricted universe
deb-src <a href="http://ftp.sjtu.edu.cn/ubuntu/" target="_blank">http://ftp.sjtu.edu.cn/ubuntu/</a> quantal main multiverse restricted universe
deb-src <a href="http://ftp.sjtu.edu.cn/ubuntu/" target="_blank">http://ftp.sjtu.edu.cn/ubuntu/</a> quantal-backports main multiverse restricted universe
deb-src <a href="http://ftp.sjtu.edu.cn/ubuntu/" target="_blank">http://ftp.sjtu.edu.cn/ubuntu/</a> quantal-proposed main multiverse restricted universe
deb-src <a href="http://ftp.sjtu.edu.cn/ubuntu/" target="_blank">http://ftp.sjtu.edu.cn/ubuntu/</a> quantal-security main multiverse restricted universe
deb-src <a href="http://ftp.sjtu.edu.cn/ubuntu/" target="_blank">http://ftp.sjtu.edu.cn/ubuntu/</a> quantal-updates main multiverse restricted universe</p>
<p>添加好后保存，再输入 sudo apt-get update 就可以更新了，等着慢慢下载东西吧。</p>
<ul>
<li>1</li>
<li><a href="http://www.linuxidc.com/Linux/2012-10/73114.htm###" target="_blank">顶一下</a>
<a href="http://www.linuxidc.com/Linux/2012-10/73114.htm#" title="分享到新浪微博" target="_blank"></a> <a href="http://www.linuxidc.com/Linux/2012-10/73114.htm#" title="分享到百度搜藏"></a> <a href="http://www.linuxidc.com/Linux/2012-10/73114.htm#" title="分享到QQ空间" target="_blank"></a> <a href="http://www.linuxidc.com/Linux/2012-10/73114.htm#" title="分享到腾讯微博"></a> <a href="http://www.linuxidc.com/Linux/2012-10/73114.htm#" title="分享到人人网" target="_blank"></a> <a href="http://www.linuxidc.com/Linux/2012-10/73114.htm#" title="分享到搜狐微博"></a> <a href="http://www.linuxidc.com/Linux/2012-10/73114.htm#" title="分享到有道云笔记" target="_blank"></a> <a href="http://www.linuxidc.com/Linux/2012-10/73114.htm#" title="累计分享2次">2</a>
<a href="http://www.linuxidc.com/Linux/2012-10/73111.htm" target="_blank">Ubuntu Grub2启动上一次正确启动的内核</a></li>
</ul>
<p><a href="http://www.linuxidc.com/Linux/2012-10/73115.htm" target="_blank">Linux PAM make err : undefine yywrap()问题</a></p>
<p>相关资讯       <a href="http://www.linuxidc.com/search.aspx?where=nkey&amp;keyword=3408" target="_blank">Ubuntu源</a>  <a href="http://www.linuxidc.com/search.aspx?where=nkey&amp;keyword=3835" target="_blank">ubuntu更新源</a>  <a href="http://www.linuxidc.com/search.aspx?where=nkey&amp;keyword=14209" target="_blank">Ubuntu 12.10更新源</a>  <a href="http://www.linuxidc.com/search.aspx?where=nkey&amp;keyword=14210" target="_blank">Ubuntu教育网更新源</a> </p>
<ul>
<li><a href="http://www.linuxidc.com/Linux/2012-07/64562.htm" target="_blank">Ubuntu 12.04 Server 用户更新源</a>  (07月07日)</li>
<li><a href="http://www.linuxidc.com/Linux/2010-12/30938.htm" target="_blank">Ubuntu 10.10更新源</a>  (12/29/2010 20:10:37)</li>
<li><p><a href="http://www.linuxidc.com/Linux/2009-09/21827.htm" target="_blank">架设Ubuntu源时的两个脚本</a>  (09/22/2009 05:48:40)</p>
</li>
<li><p><a href="http://www.linuxidc.com/Linux/2011-11/47624.htm" target="_blank">Ubuntu 11.10 教育网源</a>  (11/20/2011 04:08:48)</p>
</li>
<li><a href="http://www.linuxidc.com/Linux/2010-01/24170.htm" target="_blank">Ubuntu 9.10更新源的添加和更新</a>  (01/23/2010 12:09:55)</li>
<li><p><a href="http://www.linuxidc.com/Linux/2009-09/21812.htm" title="利用Nginx反向代理功能架设Ubuntu升级源" target="_blank">利用Nginx反向代理功能架设Ubuntu</a>  (09/20/2009 14:38:44)
图片资讯      </p>
</li>
<li><p><a href="http://www.linuxidc.com/Linux/2010-12/30938.htm" target="_blank"><img src="" alt="Ubuntu 10.10更新源">Ubuntu 10.10更新源</a></p>
</li>
<li><a href="http://www.linuxidc.com/Linux/2010-01/24170.htm" title="Ubuntu 9.10更新源的添加和更新" target="_blank"><img src="" alt="Ubuntu 9.10更新源的添加和更新">Ubuntu 9.10更新源的</a></li>
</ul>
<p>本文评论 　　<a href="http://www.linuxidc.com/remark.aspx?id=73114" target="_blank">查看全部评论</a> (0)</p>
<p>表情： <img src="" alt="表情"> 姓名：  匿名 字数
点评：
同意评论声明 　　　发表评论声明</p>
<ul>
<li>尊重网上道德，遵守中华人民共和国的各项有关法律法规</li>
<li>承担一切因您的行为而直接或间接导致的民事或刑事法律责任</li>
<li>本站管理人员有权保留或删除其管辖留言中的任意内容</li>
<li>本站有权在网站内转载或引用您的评论</li>
<li><p>参与本评论即表明您已经阅读并接受上述条款
Digg排行</p>
</li>
<li><p><a href="http://www.linuxidc.com/Linux/2012-05/59564.htm" target="_blank">10Ubuntu 12.04安装QQ2012</a></p>
</li>
<li><a href="http://www.linuxidc.com/Linux/2011-11/47705.htm" title="Linux下除了某个文件外的其他文件全部删除" target="_blank">6Linux下除了某个文件外的其他文</a></li>
<li><a href="http://www.linuxidc.com/Linux/2010-04/25829.htm" title="在VMware虚拟机上安装Ubuntu 10.04" target="_blank">4在VMware虚拟机上安装Ubuntu 10.</a></li>
<li><a href="http://www.linuxidc.com/Linux/2012-10/71796.htm" title="如何从CentOS 6.0升级到CentOS 6.2" target="_blank">3如何从CentOS 6.0升级到CentOS 6</a></li>
<li><a href="http://www.linuxidc.com/Linux/2012-06/63424.htm" target="_blank">3Win7用VMware安装Fedora 16</a></li>
<li><a href="http://www.linuxidc.com/Linux/2012-05/60806.htm" target="_blank">3Ubuntu 12.04 root用户登录设置</a></li>
<li><a href="http://www.linuxidc.com/Linux/2011-12/48609.htm" title="虚拟机下安装BackTrack5 (BT5)教程及BT5汉化" target="_blank">3虚拟机下安装BackTrack5 (BT5)教</a></li>
<li><a href="http://www.linuxidc.com/Linux/2012-10/71874.htm" title="新安装 Ubuntu 12.10 需要做的 10 件事" target="_blank">2新安装 Ubuntu 12.10 需要做的</a></li>
<li><a href="http://www.linuxidc.com/Linux/2012-09/71478.htm" title="Ubuntu使用conky美化监测系统状态" target="_blank">2Ubuntu使用conky美化监测系统状</a></li>
<li><a href="http://www.linuxidc.com/Linux/2012-09/71477.htm" title="Ubuntu在顶部面板显示系统负载、流量，磁盘I/O" target="_blank">2Ubuntu在顶部面板显示系统负载、</a></li>
</ul>
<p><a href="http://www.linuxidc.com/Linux/2012-07/66157.htm" title="Oracle" target="_blank"><img src="" alt=""></a> <a href="http://www.linuxidc.net/" target="_blank"><img src="" alt="LinuxIDC"></a> <a href="http://www.linuxidc.com/search.aspx?Where=Nkey&amp;Keyword=Ubuntu+11.10" target="_blank"><img src="" alt="Ubuntu"></a> <a href="http://www.linuxidc.com/topicnews.aspx?tid=2" target="_blank"><img src="" alt=""></a>
最新资讯</p>
<ul>
<li><a href="http://www.linuxidc.com/Linux/2012-10/73115.htm" target="_blank">Linux PAM make err : undefine yywrap()问题</a></li>
<li><a href="">Ubuntu 12.10 软件更新源列表</a></li>
<li><a href="http://www.linuxidc.com/Linux/2012-10/73113.htm" target="_blank">C/# 使用定时任务 之 Timer类</a></li>
<li><a href="http://www.linuxidc.com/Linux/2012-10/73112.htm" target="_blank">C/# 使用SQLite数据库</a></li>
<li><a href="http://www.linuxidc.com/Linux/2012-10/73111.htm" target="_blank">Ubuntu Grub2启动上一次正确启动的内核</a></li>
<li><a href="http://www.linuxidc.com/Linux/2012-10/73110.htm" target="_blank">Mozilla 向微软送蛋糕 祝贺其 IE10 发布</a></li>
<li><a href="http://www.linuxidc.com/Linux/2012-10/73109.htm" target="_blank">Wine 1.5.16 发布</a></li>
<li><a href="http://www.linuxidc.com/Linux/2012-10/73108.htm" target="_blank">FreeNAS 8.3.0 正式版发布</a></li>
<li><a href="http://www.linuxidc.com/Linux/2012-10/73107.htm" target="_blank">Debian 7.0 “Wheezy” Beta3 发布</a></li>
<li><a href="http://www.linuxidc.com/Linux/2012-10/73106.htm" target="_blank">Hadoop安装配置入门手册</a></li>
<li><a href="http://www.linuxidc.com/Linux/2012-10/73105.htm" target="_blank">配置Hive使用嵌入式derby或客服模式derby方法</a></li>
<li><a href="http://www.linuxidc.com/Linux/2012-10/73104.htm" title="Canonical发布Ubuntu Nexus 7 Desktop Installer" target="_blank">Canonical发布Ubuntu Nexus 7 Desktop</a></li>
</ul>
<p>本周热门</p>
<ul>
<li><a href="http://www.linuxidc.com/Linux/2010-04/25829.htm" target="_blank">在VMware虚拟机上安装Ubuntu 10.04</a></li>
<li><a href="http://www.linuxidc.com/Linux/2012-05/59564.htm" target="_blank">Ubuntu 12.04安装QQ2012</a></li>
<li><a href="http://www.linuxidc.com/Linux/2010-03/24993.htm" target="_blank">Fedora 12 LiveCD安装记录[图文]</a></li>
<li><a href="http://www.linuxidc.com/Linux/2010-09/28435.htm" title="Ubuntu中用VirtualBox虚拟机安装Windows XP完整图解" target="_blank">Ubuntu中用VirtualBox虚拟机安装Windows XP完整</a></li>
<li><a href="http://www.linuxidc.com/Linux/2012-05/59663.htm" target="_blank">Ubuntu 12.04和Windows 7双系统安装图解</a></li>
<li><a href="http://www.linuxidc.com/Linux/2011-12/48609.htm" target="_blank">虚拟机下安装BackTrack5 (BT5)教程及BT5汉化</a></li>
<li><a href="http://www.linuxidc.com/Linux/2012-04/59433.htm" target="_blank">Windows XP硬盘安装Ubuntu 12.04双系统图文详解</a></li>
<li><a href="http://www.linuxidc.com/Linux/2010-04/25573.htm" target="_blank">Virtualbox虚拟机安装Ubuntu图文教程</a></li>
<li><a href="http://www.linuxidc.com/Linux/2006-11/1006.htm" target="_blank">红旗Linux5.0安装教程</a></li>
<li><a href="http://www.linuxidc.com/Linux/2011-10/46327.htm" target="_blank">Windows XP硬盘安装Ubuntu 11.10双系统全程图解</a></li>
<li><a href="http://www.linuxidc.com/Linux/2012-05/60321.htm" target="_blank">Linux卷管理详解--VG LV PV</a></li>
<li><a href="http://www.linuxidc.com/Linux/2012-06/63424.htm" target="_blank">Win7用VMware安装Fedora 16</a></li>
</ul>
<p><a href="http://www.linuxidc.com/aboutus.htm" target="_blank">Linux公社简介</a> - <a href="http://www.linuxidc.com/adsense.htm" target="_blank">广告服务</a> - <a href="http://www.linuxidc.com/sitemap.aspx" target="_blank">网站地图</a> - <a href="http://www.linuxidc.com/help.htm" target="_blank">帮助信息</a> - <a href="http://www.linuxidc.com/contactus.htm" target="_blank">联系我们</a>
本站（LinuxIDC）所刊载文章不代表同意其说法或描述，仅为提供更多信息，也不构成任何建议。
主编：漏网的鱼 (QQ:3165270) 联系邮箱:<img src="" alt=""> (如有版权及广告合作请联系)
本站带宽由[<a href="http://www.6688.cc/" target="_blank">6688.CC</a>]友情提供
关注Linux，关注LinuxIDC.com，请向您的QQ好友宣传LinuxIDC.com，多谢支持！
Copyright © 2006-2011　<a href="http://www.linuxidc.com/" target="_blank">Linux公社</a>　All rights reserved 浙ICP备06018118号</p>
<p>成功接收数据</p>

      
    </div>
    
    
      
    
    
        <footer id="post-meta">
        <span class="categories">Posted in<span class="breadcrumb fa fa-folder"><li><a href="/categories/linux/">linux</a></li></span><span class="breadcrumb"><li><a href="/categories/linux/">linux</a></li><li><a href="/categories/linux/ubuntu/">ubuntu</a></li></span></span> | <span class="tags">Tagged <a href="/tags/linux/" class="label label-primary">linux</a><a href="/tags/ubuntu/" class="label label-success">ubuntu</a></span> | <span class="time">recent updated:<time title="2014-03-07 09:54:36"datetime="2014-03-07 09:54:36"> mar. 7 2014</time></span> | <span class="comment-link">
<a href="http://itsolife.com/2014/02/02/2014-02-02-linux-ubuntu--Ubuntu1210软件更新源列表_Linux教程_Linux公社-Linux系统门户网站/#comments" class="ds-thread-count comment-link" data-thread-key="2014-02-02-linux-ubuntu--Ubuntu1210软件更新源列表_Linux教程_Linux公社-Linux系统门户网站" data-count-type="comments">暂无评论</a></span>
        </footer>
    
    <div class="clearfix"></div>
  </div>
</div>
</article>





<ul id="pagination" class="pagination pagination-lg">
  <li><a class="extend prev" href="/page/107/">&laquo;</a></li><li><a class="page-number" href="/">1</a></li><li><a class="page-number" href="/page/2/">2</a></li><li><span class="space">&hellip;</span></li><li><a class="page-number" href="/page/105/">105</a></li><li><a class="page-number" href="/page/106/">106</a></li><li><a class="page-number" href="/page/107/">107</a></li><li class="active"><li><span class="page-number current">108</span></li><li><a class="page-number" href="/page/109/">109</a></li><li><a class="page-number" href="/page/110/">110</a></li><li><a class="page-number" href="/page/111/">111</a></li><li><span class="space">&hellip;</span></li></li><li><a class="page-number" href="/page/164/">164</a></li><li><a class="page-number" href="/page/165/">165</a></li><li><a class="extend next" href="/page/109/">&raquo;</a></li>
  <div class="clearfix"></div>
</ul></div><!--wapper-->
       </div><!-- ID main-col END -->
       <aside id="sidebar" class="alignright col-sx-6 col-sm-4 col-md-3 col-lg-3">
<div id="widget_search" class="widget panel panel-primary">
    <form action="//google.com/search" method="get" accept-charset="utf-8">
  <div class="input-group">
    <input class="form-control" id="searchbox" type="search" name="q" results="0" placeholder="search">
    <span class="input-group-btn">
      <button class="btn btn-default" type="submit">Go!</button>
    </span>
    <input type="hidden" name="q" value="site:itsolife.com">
  </div>
</form>
</div>

<div id="widget_category" class="widget panel panel-primary">
  <div class="panel-heading">category</div>  <div data-src='category' class='ajax_widgets'>正在加载...</div>
</div>

<div id="widget_recent_posts" class="widget panel panel-primary">
  <div class="panel-heading">recent posts</div>  <div data-src='recent_posts' class='ajax_widgets'>正在加载...</div></div>

<div id="widget_tagcloud" class="widget panel panel-primary">
  <div class="panel-heading">tagcloud</div>  <div data-src='tagcloud' class='ajax_widgets'>正在加载...</div></div>

<div id="widget_latest_update_posts" class="widget panel panel-primary">
  <div class="panel-heading">最近更新</div>  <div data-src='latest_update_posts' class='ajax_widgets'>正在加载...</div></div>

<div id="widget_recent_comments" class="widget panel panel-primary">
  <div class="panel-heading">recent comments</div>  

<div class="list-group-item ds-recent-comments" data-num-items="6" data-show-avatars="1" data-show-time="1" data-show-title="1" data-show-admin="1" data-excerpt-length="50"></div>



</div>

</aside>
       <div class="clearfix"></div>
     </div><!-- row END -->
  </div>
  <footer id="footer" class="container">
  <div class="panel panel-info">
  <section id='footer_widget'></section>  <div class="panel-footer">
  <div id="site-info">
    <span class='author'>
  
  &copy; 2014 RobinChia
  
    &nbsp;&nbsp;</span>
  
  <span id='analytics-51la'></span><span id='analytics-google'>
<script type="text/javascript">
  var _gaq = _gaq || [];
  _gaq.push(['_setAccount', 'UA-48559895-1']);
  _gaq.push(['_trackPageview']);
  _js2load.push({src:('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js'});
</script></span><span id='analytics-cnzz'>
<script type="text/javascript">var cnzz_protocol = (("https:" == document.location.protocol) ? " https://" : " http://");document.write(unescape("%3Cspan id='cnzz_stat_icon_5774006'%3E%3C/span%3E%3Cscript src='" + cnzz_protocol + "s17.cnzz.com/stat.php%3Fid%3D5774006%26show%3Dpic' type='text/javascript'%3E%3C/script%3E"));</script>
</span><span id='analytics-baidu'>
<script>
var _hmt = _hmt || [];
_js2load.push({src:"//hm.baidu.com/hm.js?1442f50724afc42380b51f097c43082c"});
</script>
</span>  </div>
  <div id="copyright">Blog powered by <a href='http://zespia.tw/hexo/'><strong>hexo</strong></a> Theme <strong><a href='https://github.com/chenall/hexo-theme-chenall'>chenall</a></strong>(Some change in it)<span class="pull-right"> 更新时间: <em>2014-03-15 17:47:11</em></span></div>
</div>
<div class="clearfix"></div>


  </div>
  </footer>
  
        <script src="http://cdn.bootcss.com/jquery/1.10.2/jquery.min.js"></script>        
        <script src="http://cdn.staticfile.org/twitter-bootstrap/3.1.0/js/bootstrap.min.js"></script>        
                
        <script src="http://cdn.bootcss.com/prettify/r298/prettify.min.js"></script>    <script type="text/javascript">
   var lang=["bsh", "c", "cc", "cpp", "cs", "csh", "cyc", "cv", "htm", "html",
    "java", "js", "m", "mxml", "perl", "pl", "pm", "py", "rb", "sh",
    "xhtml", "xml", "xsl"];
   var pretty_base='';
   $('script').each(function(){
	var c = $(this).attr('src');
	if (!c)
	    return;
	if (c.match(/(\/)?prettify(\.min)?\.js/i))
	{
	    var index = c.lastIndexOf('/');
	    if (index != -1)
		pretty_base = c.substr(0,index + 1);
	    return false;
	}
   })
   $('pre code').each(function(){
	var c = $(this).attr('class')
	if (!c)
	    return;
	c = c.match(/\s?(lang\-\w+)/i);
	if (c && lang.indexOf(c[1]) == -1)
	{
	    lang.push(c[1]);
	    $.getScript(pretty_base + c[1] + '.min.js');
	}
   })

    $(window).load(function(){
       $("pre").addClass("prettyprint");
       prettyPrint();
    })
</script>    
            <script type="text/javascript">
var duoshuoQuery = {short_name:"robinchia"};
_js2load.push({src:'http://static.duoshuo.com/embed.js',charset:'UTF-8'});
</script>
    
            <!--wumii_relatedItems-->
    
        <script src="http://cdn.bootcss.com/fancybox/2.1.5/jquery.fancybox.min.js"></script>    <script type="text/javascript">
(function($){
  $('.entry').each(function(i){
    $(this).find('img').each(function(){
      var alt = this.alt;

      if (alt){
        $(this).before('<span class="caption">' + alt + '</span>');
      }

      $(this).wrap('<a href="' + this.src + '" title="' + alt + '" class="fancybox" rel="fancybox' + i + '" />');
    });
  });

  $('.fancybox').fancybox();
})(jQuery);
</script>    
        <script src="http://cdn.bootcss.com/mathjax/2.3/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>    <script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      processEscapes: true
    }
  });
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>
    
<script type="text/javascript">
$('.ajax_widgets').each(function(){var src=$(this).attr('data-src');if(src)$(this).load('/widgets/'+src+'.html');});
$.each(_js2load,function(index,obj){loadjs(obj.src,obj.charset)});
</script>

<div id="scroll2top">
<img src="/scroll2top/arrow.png"/>
</div>
<script src="/scroll2top/scroll2top.min.js"></script>
<div id="winterland">
  <canvas></canvas>
</div>
<script src="/js/winterland.min.js"></script>

  </body>
</html>
