
<!DOCTYPE HTML>
<html>
<head>
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<title>The 108 Page | It so life</title>
<meta name="author" content="RobinChia">

<meta name="description" content="It so life">


<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">


<meta property="og:site_name" content="It so life"/>

<!--[if IE]><style>.testIE.IE{display:inline;}</style><![endif]-->
<!--[if lte IE 7]><link rel="stylesheet" href="/css/ie7.css" type="text/css"><![endif]-->
<!--[if (lt IE 9)&(gt IE 7)]><style>.testIE.IE8{display:inline;}</style><![endif]-->
<!--[if gt IE 8]><style>.testIE.IE9{display:inline;}</style><![endif]-->

<link href="/favicon.png" rel="icon">
<link rel="alternate" href="/atom.xml" title="It so life Feed" type="application/atom+xml">

<link rel="stylesheet" href="http://cdn.staticfile.org/twitter-bootstrap/3.1.0/css/bootstrap.min.css" type="text/css">
<link rel="stylesheet" href="http://cdn.bootcss.com/font-awesome/4.0.3/css/font-awesome.min.css" type="text/css">
<link rel="stylesheet" href="http://cdn.bootcss.com/prettify/r298/prettify.min.css" type="text/css">
<link rel="stylesheet" href="http://cdn.bootcss.com/fancybox/2.1.5/jquery.fancybox.min.css" type="text/css">
<link rel="stylesheet" href="/css/style.css" type="text/css">
<!--[if lt IE 9]>
   <style>article,aside,dialog,footer,header,section,footer,nav,figure,menu{display:block}</style>
   <script src="http://cdn.staticfile.org/html5shiv/r29/html5.min.js"></script>
   <script src="http://cdn.staticfile.org/respond.js/1.4.2/respond.min.js"></script>
<link href="http://cdn.staticfile.org/respond.js/1.4.2/respond-proxy.html" id="respond-proxy" rel="respond-proxy" />
<link href="/js/respond.proxy.gif" id="respond-redirect" rel="respond-redirect" />
<script src="/js/respond.proxy.js"></script>
   <script src="http://cdn.bootcss.com/selectivizr/1.0.2/selectivizr-min.js"></script>
<![endif]-->
<script type="text/javascript">
function loadjs(c,d){var a=document.createElement("script");a.async=!0;a.type="text/javascript";a.src=c;a.charset=d||"gbk";(document.getElementsByTagName("HEAD")[0]||document.getElementsByTagName("BODY")[0]).appendChild(a)};
var _protocol = (("https:" == document.location.protocol) ? " https://" : " http://");
var _js2load = [];
</script>

</head>
<body>
      <header id="header" class="container"><nav id="main-nav" class="navbar navbar-default navbar-fixed-top " role="navigation">
  <div class="container">
    <div class="navbar-header">
    <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".navbar-collapse">
      <span class="sr-only">Toggle navigation</span>
      <span class="icon-bar"></span>
      <span class="icon-bar"></span>
      <span class="icon-bar"></span>
    </button>
    <a class="navbar-brand" href="/">It so life</a>
    </div>
    <div  class="collapse navbar-collapse">
      <ul  class="nav navbar-nav">
  
        <li><a href="/" title="Home">Home</a></li>      
        <li><a href="/about/" title="About">About</a></li>      
        <li><a href="/archives/" title="Archives">Archives</a></li>      
      <li class='dropdown'>
        <a class='dropdown-toggle' data-toggle='dropdown' href='#'>website<b class='caret'></b></a>
        <ul class='dropdown-menu pure-menu-selected'>
    
          <li><a href="//groups.google.com/forum/#!forum/pongba" title="TopLanguage">TopLanguage</a></li>    
          <li><a href="//itpub.net/" title="ITPub">ITPub</a></li>    
          <li><a href="//blog.jobbole.com/" title="Bole">Bole</a></li>    
          <li><a href="//nosql-database.org/" title="nosql">nosql</a></li>    
          <li><a href="//gitimmersion.googol.im/" title="Git">Git</a></li>    
        </ul>
      </li>
    
      </ul>
      <ul class="nav navbar-nav navbar-right">
      
        <li><a href="/atom.xml">RSS</a></li>
      
      
        <li><a href="https://twitter.com/robinchia">twitter</a></li>
      
      
      
      
        <li><a href="https://github.com/robinchia">github</a></li>
      
      </ul>
    </div>
  </div>
</nav>
<div class="clearfix"></div>
</header>
  <div id='content' class="container">
     <div class="page-header-wrapper">
      <!--[if lt IE 9]><div class="alert alert-warning alert-dismissable"><button type="button" class="close" data-dismiss="alert" aria-hidden="true">&times;</button><strong>提示:</strong>您的浏览器版本太低了,建议升级到 <strong><a href="http://windows.microsoft.com/zh-cn/internet-explorer/download-ie" title="IE9">IE9</a></strong> 以上,本站使用<a href="https://www.google.com/intl/zh-CN/chrome/">Chrome浏览器</a>可以获得最好的显示效果.</div><![endif]-->
      <div class="page-header"><h1 align="center"><big>It so life</big> </h1>
        <h5 align="center"><big>love as life</big></h5>
      </div>
     </div>
     <div class="row">
       <div id="main-col" class="alignleft col-sx-12 col-sm-8 col-md-9 col-lg-9">
      <section id='header_widget'></section>
          <div id="wrapper"><article>
<div id="post" class="post well">
  <div class="post-content">
    <header class="well-sm">
      <i class="fa icon fa-5x pull-left"></i>
      <h1 class="title"><a href="/2014/02/02/2014-02-02-hadoop--JAVA线程池管理及分布式HADOOP调度框架搭建/">JAVA线程池管理及分布式HADOOP调度框架搭建</a></h1>
      
        <span>Posted on<time datetime="2014-02-02T01:54:36.000Z"> <a href="/2014/02/02/2014-02-02-hadoop--JAVA线程池管理及分布式HADOOP调度框架搭建/">feb. 2 2014</a></time></span>      
    </header>
    
    
    <div class="entry">
      
        <h1 id="java-hadoop-">JAVA线程池管理及分布式HADOOP调度框架搭建</h1>
<p>平时的开发中线程是个少不了的东西，比如tomcat里的servlet就是线程，没有线程我们如何提供多用户访问呢？不过很多刚开始接触线程的开发攻城师却在这个上面吃了不少苦头。怎么做一套简便的线程开发模式框架让大家从单线程开发快速转入多线程开发，这确实是个比较难搞的工程。</p>
<p>那具体什么是线程呢？首先看看进程是什么，进程就是系统中执行的一个程序，这个程序可以使用内存、处理器、文件系统等相关资源。例如 QQ软件、eclipse、tomcat等就是一个exe程序，运行启动起来就是一个进程。为什么需要多线程？如果每个进程都是单独处理一件事情不能多个任务同时处理，比如我们打开qq只能和一个人聊天，我们用eclipse开发代码的时候不能编译代码，我们请求tomcat服务时只能服务一个用户请求，那我想我们还在原始社会。多线程的目的就是让一个进程能够同时处理多件事情或者请求。比如现在我们使用的QQ软件可以同时和多个人聊天，我们用eclipse开发代码时还可以编译代码，tomcat可以同时服务多个用户请求。</p>
<p>线程这么多好处，怎么把单进程程序变成多线程程序呢？不同的语言有不同的实现，这里说下java语言的实现多线程的两种方式：扩展java.lang.Thread类、实现java.lang.Runnable接口。
先看个例子，假设有100个数据需要分发并且计算。看下单线程的处理速度：
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26 package thread;
import java.util.Vector;
public class OneMain {
       public static void main(<a href="http://www.google.com/search?hl=en&amp;q=allinurl%3Astring+java.sun.com&amp;btnI=I%27m%20Feeling%20Lucky" target="_blank">String</a>[] args) throws <a href="http://www.google.com/search?hl=en&amp;q=allinurl%3Ainterruptedexception+java.sun.com&amp;btnI=I%27m%20Feeling%20Lucky" target="_blank">InterruptedException</a> {
            Vector<Integer> list = new Vector<Integer>(100);
             for (int i = 0; i &lt; 100; i++) {
                  list.add(i);
            }
             long start = <a href="http://www.google.com/search?hl=en&amp;q=allinurl%3Asystem+java.sun.com&amp;btnI=I%27m%20Feeling%20Lucky" target="_blank">System</a>.currentTimeMillis();
             while (list.size() &gt; 0) {
                   int val = list.remove(0);
                  <a href="http://www.google.com/search?hl=en&amp;q=allinurl%3Athread+java.sun.com&amp;btnI=I%27m%20Feeling%20Lucky" target="_blank">Thread</a>. sleep(100);//模拟处理
                  <a href="http://www.google.com/search?hl=en&amp;q=allinurl%3Asystem+java.sun.com&amp;btnI=I%27m%20Feeling%20Lucky" target="_blank">System</a>. out.println(val);
            }
             long end = <a href="http://www.google.com/search?hl=en&amp;q=allinurl%3Asystem+java.sun.com&amp;btnI=I%27m%20Feeling%20Lucky" target="_blank">System</a>.currentTimeMillis();
            <a href="http://www.google.com/search?hl=en&amp;q=allinurl%3Asystem+java.sun.com&amp;btnI=I%27m%20Feeling%20Lucky" target="_blank">System</a>. out.println(&quot;消耗 &quot; + (end - start) + &quot; ms&quot;);
      }
       // 消耗 10063 ms
}</p>
<p>再看一下多线程的处理速度，采用了10个线程分别处理：</p>
<p>1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48 package thread;
import java.util.Vector;
import java.util.concurrent.CountDownLatch;
public class MultiThread extends <a href="http://www.google.com/search?hl=en&amp;q=allinurl%3Athread+java.sun.com&amp;btnI=I%27m%20Feeling%20Lucky" target="_blank">Thread</a> {
     static Vector<Integer> list = new Vector<Integer>(100);
     static CountDownLatch count = new CountDownLatch(10);
     public void run() {
          while (list.size() &gt; 0) {
               try {
                    int val = list.remove(0);
                    <a href="http://www.google.com/search?hl=en&amp;q=allinurl%3Asystem+java.sun.com&amp;btnI=I%27m%20Feeling%20Lucky" target="_blank">System</a>.out.println(val);
                    <a href="http://www.google.com/search?hl=en&amp;q=allinurl%3Athread+java.sun.com&amp;btnI=I%27m%20Feeling%20Lucky" target="_blank">Thread</a>.sleep(100);//模拟处理
               } catch (<a href="http://www.google.com/search?hl=en&amp;q=allinurl%3Aexception+java.sun.com&amp;btnI=I%27m%20Feeling%20Lucky" target="_blank">Exception</a> e) {
                    // 可能数组越界，这个地方只是为了说明问题，忽略错误
               }
          }</p>
<pre><code>      count.countDown(); // 删除成功减一
 }
 public static void main([String](http://www.google.com/search?hl=en&amp;q=allinurl%3Astring+java.sun.com&amp;btnI=I%27m%20Feeling%20Lucky)[] args) throws [InterruptedException](http://www.google.com/search?hl=en&amp;q=allinurl%3Ainterruptedexception+java.sun.com&amp;btnI=I%27m%20Feeling%20Lucky) {

      for (int i = 0; i &lt; 100; i++) {
           list.add(i);
      }

      long start = [System](http://www.google.com/search?hl=en&amp;q=allinurl%3Asystem+java.sun.com&amp;btnI=I%27m%20Feeling%20Lucky).currentTimeMillis();
      for (int i = 0; i &lt; 10; i++) {
           new MultiThread().start();
      }

      count.await();
      long end = [System](http://www.google.com/search?hl=en&amp;q=allinurl%3Asystem+java.sun.com&amp;btnI=I%27m%20Feeling%20Lucky).currentTimeMillis();
      [System](http://www.google.com/search?hl=en&amp;q=allinurl%3Asystem+java.sun.com&amp;btnI=I%27m%20Feeling%20Lucky).out.println(&quot;消耗 &quot; + (end - start) + &quot; ms&quot;);
 }
 // 消耗 1001 ms
</code></pre><p>}</p>
<p>大家看到了线程的好处了吧！单线程需要10S，10个线程只需要1S。充分利用了系统资源实现并行计算。也许这里会产生一个误解，是不是增加的线程个数越多效率越高。线程越多处理性能越高这个是错误的，范式都要合适，过了就不好了。需要普及一下计算机硬件的一些知识。我们的cpu是个运算器，线程执行就需要这个运算器来运行。不过这个资源只有一个，大家就会争抢。一般通过以下几种算法实现争抢cpu的调度：</p>
<p>1、队列方式，先来先服务。不管是什么任务来了都要按照队列排队先来后到。
2、时间片轮转，这也是最古老的cpu调度算法。设定一个时间片，每个任务使用cpu的时间不能超过这个时间。如果超过了这个时间就把任务暂停保存状态，放到队列尾部继续等待执行。
3、优先级方式：给任务设定优先级，有优先级的先执行，没有优先级的就等待执行。</p>
<p>这三种算法都有优缺点，实际操作系统是结合多种算法，保证优先级的能够先处理，但是也不能一直处理优先级的任务。硬件方面为了提高效率也有多核cpu、多线程cpu等解决方案。目前看得出来线程增多了会带来cpu调度的负载增加，cpu需要调度大量的线程，包括创建线程、销毁线程、线程是否需要换出cpu、是否需要分配到cpu。这些都是需要消耗系统资源的，由此，我们需要一个机制来统一管理这一堆线程资源。线程池的理念提出解决了频繁创建、销毁线程的代价。线程池指预先创建好一定大小的线程等待随时服务用户的任务处理，不必等到用户需要的时候再去创建。特别是在java开发中，尽量减少垃圾回收机制的消耗就要减少对象的频繁创建和销毁。</p>
<p>之前我们都是自己实现的线程池，不过随之jdk1.5的推出，jdk自带了 java.util.concurrent并发开发框架，解决了我们大部分线程池框架的重复工作。可以使用Executors来建立线程池，列出以下大概的，后面再介绍。
newCachedThreadPool 建立具有缓存功能线程池
newFixedThreadPool 建立固定数量的线程
newScheduledThreadPool 建立具有时间调度的线程</p>
<p>有了线程池后有以下几个问题需要考虑：
1、线程怎么管理，比如新建任务线程。
2、线程如何停止、启动。
3、线程除了scheduled模式的间隔时间定时外能否实现精确时间启动。比如晚上1点启动。
4、线程如何监控，如果线程执行过程中死掉了，异常终止我们怎么知道。</p>
<p>考虑到这几点，我们需要把线程集中管理起来，用java.util.concurrent是做不到的。需要做以下几点：
1、将线程和业务分离，业务的配置单独做成一个表。
2、构建基于concurrent的线程调度框架，包括可以管理线程的状态、停止线程的接口、线程存活心跳机制、线程异常日志记录模块。
3、构建灵活的timer组件，添加quartz定时组件实现精准定时系统。
4、和业务配置信息结合构建线程池任务调度系统。可以通过配置管理、添加线程任务、监控、定时、管理等操作。
组件图为：
<a href="http://www.lanceyan.com/wp-content/uploads/2013/05/task1.png" target="_blank"><img src="" alt="分布式调度框架-lanceyan.com"></a></p>
<p>构建好线程调度框架是不是就可以应对大量计算的需求了呢?答案是否定的。因为一个机器的资源是有限的，上面也提到了cpu是时间周期的，任务一多了也会排队，就算增加cpu，一个机器能承载的cpu也是有限的。所以需要把整个线程池框架做成分布式的任务调度框架才能应对横向扩展，比如一个机器上的资源呢达到瓶颈了，马上增加一台机器部署调度框架和业务就可以增加计算能力了。好了，如何搭建？如下图：
<a href="http://www.lanceyan.com/wp-content/uploads/2013/05/task2.png" target="_blank"><img src="" alt="分布式调度框架-lanceyan.com"></a></p>
<p>基于jeeframework我们封装spring、ibatis、数据库等操作，并且可以调用业务方法完成业务处理。主要组件为：
1、任务集中存储到数据库服务器
2、控制中心负责管理集群中的节点状态，任务分发
3、线程池调度集群负责控制中心分发的任务执行
4、web服务器通过可视化操作任务的分派、管理、监控。</p>
<p>一般这个架构可以应对常用的分布式处理需求了，不过有个缺陷就是随着开发人员的增多和业务模型的增多，单线程的编程模型也会变得复杂。比如需要对1000w数据进行分词，如果这个放到一个线程里来执行，不算计算时间消耗光是查询数据库就需要耗费不少时间。有人说，那我把1000w数据打散放到不同机器去运算，然后再合并不就行了吗？因为这是个特例的模式，专为了这个需求去开发相应的程序没有问题，但是以后又有其他的海量需求如何办？比如把倒退3年的所有用户发的帖子中发帖子最多的粉丝转发的最高的用户作息时间取出来。又得编一套程序实现，太麻烦！分布式云计算架构要解决的就是这些问题，减少开发复杂度并且要高性能，大家会不会想到一个最近很热的一个框架，hadoop，没错就是这个玩意。hadoop解决的就是这个问题，把大的计算任务分解、计算、合并，这不就是我们要的东西吗？不过玩过这个的人都知道他是一个单独的进程。不是！他是一堆进程，怎么和我们的调度框架结合起来？看图说话：
<a href="http://www.lanceyan.com/wp-content/uploads/2013/05/task31.png" target="_blank"><img src="" alt="task31"></a></p>
<p>基本前面的分布式调度框架组件不变，增加如下组件和功能：
1、改造分布式调度框架，可以把本身线程任务变成mapreduce任务并提交到hadoop集群。
2、hadoop集群能够调用业务接口的spring、ibatis处理业务逻辑访问数据库。
3、hadoop需要的数据能够通过hive查询。
4、hadoop可以访问hdfs/hbase读写操作。
5、业务数据要及时加入hive仓库。
6、hive处理离线型数据、hbase处理经常更新的数据、hdfs是hive和hbase的底层结构也可以存放常规文件。</p>
<p>这样，整个改造基本完成。不过需要注意的是架构设计一定要减少开发程序的复杂度。这里虽然引入了hadoop模型，但是框架上开发者还是隐藏的。业务处理类既可以在单机模式下运行也可以在hadoop上运行，并且可以调用spring、ibatis。减少了开发的学习成本，在实战中慢慢体会就学会了一项新技能。</p>
<p>界面截图：
<a href="http://www.lanceyan.com/wp-content/uploads/2013/05/task4.png" target="_blank"><img src="" alt="task4"></a>
来源： <a href="[http://www.lanceyan.com/category/tech/hadoop](http://www.lanceyan.com/category/tech/hadoop)">[http://www.lanceyan.com/category/tech/hadoop](http://www.lanceyan.com/category/tech/hadoop)</a> </p>

      
    </div>
    
    
      
    
    
        <footer id="post-meta">
        <span class="categories">Posted in<span class="breadcrumb fa fa-folder"><li><a href="/categories/hadoop/">hadoop</a></li></span></span> | <span class="tags">Tagged <a href="/tags/hadoop/" class="label label-primary">hadoop</a></span> | <span class="time">recent updated:<time title="2014-03-07 09:54:36"datetime="2014-03-07 09:54:36"> mar. 7 2014</time></span> | <span class="comment-link">
<a href="http://itsolife.com/2014/02/02/2014-02-02-hadoop--JAVA线程池管理及分布式HADOOP调度框架搭建/#comments" class="ds-thread-count comment-link" data-thread-key="2014-02-02-hadoop--JAVA线程池管理及分布式HADOOP调度框架搭建" data-count-type="comments">暂无评论</a></span>
        </footer>
    
    <div class="clearfix"></div>
  </div>
</div>
</article>



  <article>
<div id="post" class="post well">
  <div class="post-content">
    <header class="well-sm">
      <i class="fa icon fa-5x pull-left"></i>
      <h1 class="title"><a href="/2014/02/02/2014-02-02-hadoop--Hadoop知识分享文稿byquqi99-技术并艺术着/">Hadoop知识分享文稿 ( by quqi99 ) </a></h1>
      
        <span>Posted on<time datetime="2014-02-02T01:54:36.000Z"> <a href="/2014/02/02/2014-02-02-hadoop--Hadoop知识分享文稿byquqi99-技术并艺术着/">feb. 2 2014</a></time></span>      
    </header>
    
    
    <div class="entry">
      
        <h1 id="hadoop-by-quqi99-">Hadoop知识分享文稿 ( by quqi99 ) - 技术并艺术着</h1>
<p>您还未登录！|<a href="https://passport.csdn.net/account/login" target="_blank">登录</a>|<a href="https://passport.csdn.net/account/register" target="_blank">注册</a>|<a href="https://passport.csdn.net/help/faq" target="_blank">帮助</a></p>
<ul>
<li><a href="http://www.csdn.net/" target="_blank">首页</a></li>
<li><a href="http://news.csdn.net/" target="_blank">业界</a></li>
<li><a href="http://mobile.csdn.net/" target="_blank">移动</a></li>
<li><a href="http://cloud.csdn.net/" target="_blank">云计算</a></li>
<li><a href="http://sd.csdn.net/" target="_blank">研发</a></li>
<li><a href="http://bbs.csdn.net/" target="_blank">论坛</a></li>
<li><a href="http://blog.csdn.net/" target="_blank">博客</a></li>
<li><a href="http://download.csdn.net/" target="_blank">下载</a></li>
<li><h2 id="-"><a href="">更多</a></h2>
</li>
</ul>
<h1 id="-http-blog-csdn-net-quqi99-"><a href="http://blog.csdn.net/quqi99" target="_blank">技术并艺术着</a></h1>
<h2 id="-blog">张华的技术Blog</h2>
<ul>
<li><a href="http://blog.csdn.net/quqi99?viewmode=contents" target="_blank"><img src="" alt="">目录视图</a></li>
<li><a href="http://blog.csdn.net/quqi99?viewmode=list" target="_blank"><img src="" alt="">摘要视图</a></li>
<li><a href="http://blog.csdn.net/quqi99/rss/list" target="_blank"><img src="" alt="">订阅</a>
<a href="https://code.csdn.net/blog/12" target="_blank">公告：博客新增直接引用代码功能</a>        <a href="http://www.csdn.net/article/2013-08-06/2816471" target="_blank">专访李铁军：从医生到金山首席安全专家的转变</a>      <a href="http://blog.csdn.net/csdnproduct/article/details/9495139" target="_blank">CSDN博客频道自定义域名、标签搜索功能上线啦！</a>      <a href="http://blog.csdn.net/adali/article/details/9813651" target="_blank">独一无二的职位：开源社区经理</a></li>
</ul>
<h3 id="-hadoop-by-quqi99-"><a href="">[置顶] Hadoop知识分享文稿 ( by quqi99 )</a></h3>
<p>分类： <a href="http://blog.csdn.net/quqi99/article/category/328029" target="_blank">Seach Engine</a>  2011-03-31 15:19 1977人阅读 <a href="">评论</a>(0) <a href="&quot;收藏&quot;">收藏</a> <a href="&quot;举报&quot;">举报</a>
<a href="http://blog.csdn.net/tag/details.html?tag=hadoop" target="_blank">hadoop</a><a href="http://blog.csdn.net/tag/details.html?tag=%e4%bb%bb%e5%8a%a1" target="_blank">任务</a><a href="http://blog.csdn.net/tag/details.html?tag=mapreduce" target="_blank">mapreduce</a><a href="http://blog.csdn.net/tag/details.html?tag=%e4%bb%bb%e5%8a%a1%e8%b0%83%e5%ba%a6" target="_blank">任务调度</a><a href="http://blog.csdn.net/tag/details.html?tag=%e9%9b%86%e7%be%a4" target="_blank">集群</a><a href="http://blog.csdn.net/tag/details.html?tag=%e4%bd%9c%e4%b8%9a" target="_blank">作业</a></p>
<p>目录<a href="&quot;系统根据文章中H1到H6标签自动生成文章目录&quot;">(?)</a><a href="&quot;展开&quot;">[+]</a></p>
<ol>
<li><a href="">作者张华 写于2010-08-15   发表于2011-03-31 版权声明可以任意转载转载时请务必以超链接形式标明文章原始出处和作者信息及本版权声明</a></li>
<li><a href="">httpblogcsdnnetquqi99</a></li>
<li><p><a href="">hadoop 理论基础</a></p>
</li>
<li><p><a href="">hadoop 是什么</a></p>
</li>
<li><a href="">hadoop 项目</a></li>
<li><a href="">MapReduce 任务的运行流程</a></li>
<li><p><a href="">MapReduce 任务的数据流图</a></p>
</li>
<li><p><a href="">hadoop 入门实战</a></p>
</li>
<li><p><a href="">测试环境</a></p>
</li>
<li><a href="">测试程序</a></li>
<li><a href="">属性配置</a></li>
<li><a href="">免密码 SSH 设置</a></li>
<li><a href="">配置 hosts</a></li>
<li><a href="">格式化 HDFS 文件系统</a></li>
<li><a href="">启动守护进程</a></li>
<li><p><a href="">运行程序</a></p>
</li>
<li><p><a href="">hadoop 高级进阶</a></p>
</li>
<li><a href="">hadoop 应用案例</a></li>
<li><a href="">参考文献</a><pre><code>                       **Hadoop知识分享文稿 ( by quqi99 )**
</code></pre></li>
</ol>
<h2 id="-2010-08-15-2011-03-31"><a href=""></a>作者：张华 写于：2010-08-15   发表于：2011-03-31</h2>
<p>版权声明：可以任意转载，转载时请务必以超链接形式标明文章原始出处和作者信息及本版权声明</p>
<h2 id="-http-blog-csdn-net-quqi99-"><a href=""></a>( <a href="http://blog.csdn.net/quqi99">http://blog.csdn.net/quqi99</a> )</h2>
<p><strong>内容目录</strong></p>
<p>目 录</p>
<p>1 hadoop 理论基础 3</p>
<p>1.1 hadoop 是什么 3</p>
<p>1.2 hadoop 项目 3</p>
<p>1.3 Map/Reduce 任务的运行流程 4</p>
<p>1.4 Map/Reduce 任务的数据流图 5</p>
<p>2 hadoop 入门实战 7</p>
<p>2.1  测试环境 7</p>
<p>2.2  测试程序 7</p>
<p>2.3  属性配置 9</p>
<p>2.4  免密码SSH 设置 10</p>
<p>2.5  配置hosts 11</p>
<p>2.6  格式化HDFS 文件系统 11</p>
<p>2.7  启动守护进程 11</p>
<p>2.8  运行程序 11</p>
<p>3 hadoop 高级进阶 12</p>
<p>4 hadoop 应用案例 12</p>
<p>5  参考文献 12</p>
<h1 id="-1-hadoop-"><a href=""></a>1 hadoop  理论基础</h1>
<h2 id="-1-1-hadoop-"><a href=""></a>1.1 hadoop  是什么</h2>
<p>Hadoop  是 Doug Cutting  开发的，他是一个相当牛的哥们，他同时是大名鼎鼎的 Lucene  及 Nutch  的作者。</p>
<p>我是这样理解 hadoop  的，它就是用来对海量数据进行存储与分析的一个开源软件。它包括两块：</p>
<p>1  ） HDFS ( Hadoop Distrubuted File System )  ，可以对重要数据进行冗余存储，有点类似于冗余磁盘陈列。</p>
<p>2  ）对 Map/Reduce  编程模型的一个实现。当然，关系型数据库（ RDBMS  ）也能做类似的事情，但为什么不用 RDBMS  呢？我们知道，让计算移动于数据上比让数据移动到计算更有效率。这使得 Map/Reduce  适合数据被一次写入和多次读取的应用，而 RDBMS  更适合持续更新的数据集。</p>
<h2 id="-1-2-hadoop-"><a href=""></a>1.2 hadoop  项目</h2>
<p>如今，广义上的 Hadoop  已经发展成为一个分布式计算基础架构这把“大伞”下相关子项目的集合，其技术栈如下图所示：</p>
<p>图：</p>
<pre><code>                                     ![]()
</code></pre><p><img src="http://blog.csdn.net/root/Library/Caches/TemporaryItems/moz-screenshot-4.png" alt=""></p>
<pre><code>                                                图1 hadoop 的子项目
</code></pre><ul>
<li>Core ： 一系列分布式文件系统和通用I/O 的组件和接口( 序列化、Java RPC 和持久化数据结构) 。</li>
<li>Avro ： 用于数据的序列化，当然，JDK 中也有Seriable 接口，但hadoop 中有它自己的序列化方式，具说更有效率。</li>
<li>MapReduce ： 分布式数据处理模式和执行环境，运行于大型商用机集群。</li>
<li>HDFS ： 分布式文件系统，运行于大型商用机集群。</li>
<li>Pig ： HDFS 上的数据检索语言，类似于RDBMS 中的SQL 语言。</li>
<li>Hbase ： 一个分布式的、列存储数据库。HBase 使用HDFS 作为底层存储，同时支持MapReduce 的批量式计算和点查询( 随机读取) 。</li>
<li>ZooKeeper ： 一个分布式的、高可用性的协调服务。ZooKeeper 提供分布式锁之类的基本服务用于构建分布式应用。</li>
<li>Hive ： 分布式数据仓库。Hive 管理HDFS 中存储的数据，并提供基于SQL 的查询语言( 由运行时引擎翻译成MapReduce 作业) 用以查询数据。</li>
<li>Chukwa ： 分布式数据收集和分析系统。Chukwa 运行HDFS 中存储数据的收集器，它使用MapReduce 来生成报告。</li>
</ul>
<h2 id="-1-3-map-reduce-"><a href=""></a> 1.3 Map/Reduce  任务的运行流程</h2>
<pre><code>                 ![]()
</code></pre><p>JobClient  的  submitJob()  方法的作业提交过程如下：</p>
<p>1  ）向 Jobtraker  请求一个新作业 ID</p>
<p>2  ） 调用 JobTracker  的 getNewJobId()</p>
<p>3  ）  JobClient  进行作业划分，并将划分后的输入及作业的 JAR  文件、配置文件等复制到 HDFS  中去</p>
<p>4  ） 提交作业，会把此调用放入到一个内部的队列中，交由作业调度器进行调度。值得一提的是，针对  Map  任务与 Reduce  任务，任务调度器是优先选择 Map  任务的，另外，任务调度器在选择 Reduce  任务时并没有考虑数据的本地化。然而，针对一个 Map  任务，它考虑的是 Tasktracker  网络位置和选取一个距离其输入划分文件最近的 Tasktracker  ，它可能是数据本地化的，也可能是机架本地化的，还可能得到不同的机架上取数据。</p>
<p>5  ） 初始化包括创建一个代表该正在运行的作业的对象，它封装任务和记录信息，以便跟踪任务的状态和进度。</p>
<p>6  ） JobTracker  任务调度器首先从共享文件系统中获取 JobClient  已计算好的输入划分信息，然后为每个划分创建一个 Map  任务。创建 的 reduce  任务的数量是由 JobConf  的 Mapred.reduce.tasks  属性决定，它是用 setNumReduceTask()  方法来设置的。</p>
<p>7  ） TaskTracker  执行一个简单的循环，定期发送心跳（ Heartbeat  ）方法调用 Jobtracker  告诉是否还活着，同时，心跳还会报告任务运行的是否已经准备运行新的任务。</p>
<p>8  ） TaskTracker  已经被分配了任务，下一步是运行任务。首先它需要将它所需的全部文件从 HDFS  中复制到本地磁盘。</p>
<p>9  ）紧接着，它要启动一个新的 Java  虚拟机来运行每个任务，这使得用户所定义的 Map  和 Reduce  函数的任务缺陷都不会影响 TaskTracker  （比如导致它崩溃或者挂起）</p>
<p>10  ）运行 Map  任务或者 Reduce  任务，值得一提的是，这些任务使用标准输入与输出流，换句话说，你可以用任务语言（如 JAVA  ， C++  ， Shell  等）来实现 Map  和 Reduce  ，只要保证它们也使用标准输入与输出流，就可以将输出的键值对传回给 JAVA  进程了。</p>
<h2 id="-1-4-map-reduce-"><a href=""></a> 1.4 Map/Reduce  任务的数据流图</h2>
<p><img src="" alt=""></p>
<pre><code>    图3  Map/Reduce  中单一 Reduce  任务的数据流图
</code></pre><p><img src="" alt=""></p>
<pre><code>             图4  Map/Reduce  中多个 Reduce  任务的数据流图
</code></pre><p><img src="" alt=""></p>
<pre><code>            图5  MapReduce  中没有 Reduce  任务的数据流图
</code></pre><p><strong>任务粒度</strong>   ： 分片的个数，在将原始大数据切割成小数据集时，通常让小数据集小于或等于 HDFS  中的一个 Block  的大小（缺省是 64M)  ，这样能够保证一个小数据集位于一台计算机上，便于本地计算。 有 M   个 小数据集 待处理，就启动 M   个 Map   任务，注意这 M   个 Map   任务分布于 N   台计算机上并行运行，Reduce   任务的数量 R   则可由用户指定 。</p>
<p><strong>Map</strong>   ： 输入 <k1, v1>   输出 List(<k2,v2>)</p>
<p><strong>Reduce</strong>   ： 输入 <k2,List(v2)>   输出 <k3,v3></p>
<p><strong>分区（</strong>  <strong>Partition)</strong>  :   把 Map   任务输出的中间结果按 key   的范围划分成 R   份 ( R  是预先定义的 Reduce  任务的个数) ，划分时通常使用 hash  函数如: hash(key) mod R ，这样可以保证某一段范围内的 key ，一定是由一个 Reduce  任务来处理，可以简化 Reduce  的过程。</p>
<p><strong>Combine</strong>   :   在  partition   之前，还可以对中间结果先做  combine  ，即将中间结果中有相同  key  的  <key, value>   对合并成一对。 combine   的过程与  Reduce   的过程类似，很多情况下就可以直接使用  Reduce   函数，但  combine   是作为  Map   任务的一部分，在执行完  Map   函数后紧接着执行的。 Combine   能够减少中间结果中  <key, value>   对的数目，从而减少网络流量。</p>
<p>下面举个例子来着重说明 Combine  ， hadoop  允许用户声明一个 combiner  运行在 Map  的输出上，它的输出再作为 Reduce  的输入。例如，找出每一年的最调气温：</p>
<p>假如用户的输入的分片数是 2  ，那么：</p>
<p>1  ）第一个 Map  的输出如下：</p>
<p>（ 1950  ， 0  ）</p>
<p>（ 1950  ， 20  ）</p>
<p>（ 1950  ， 10  ）</p>
<p>2  ） 第二个 Map  的输出如下：</p>
<p>（ 1950  ， 25  ）</p>
<p>（ 1950  ， 15  ）</p>
<p>3  ） Reduce  的输入如下：</p>
<p>（ 1950  ，［ 0  ， 20  ， 10  ， 25  ， 15  ］）</p>
<p><strong>注意：如果有</strong>   <strong>combine</strong>    <strong>的话，此时</strong>   <strong>Reduce</strong>    <strong>的输入应该是：</strong></p>
<p><strong>max(0, 20, 10, 25, 15) = max(max(0,20,10), max(25,15)) = max(20,25)</strong></p>
<p><strong>combine</strong>    <strong>并不能取代</strong>   <strong>reduce,</strong>    <strong>例如，如果我们计算平均气温，便不能使用</strong>   <strong>combine</strong>    <strong>，因为：</strong></p>
<p><strong>mean(0,20,10,25,15) = 14</strong></p>
<p><strong>但是：</strong></p>
<p><strong>mean(mean(0,20,10), mean(25,15)) = mean(10,20) = 15</strong></p>
<p>4  ） Reduce  的输出如下：</p>
<p>（ 1950  ， 25  ）</p>
<h1 id="-2-hadoop-"><a href=""></a>2 hadoop  入门实战</h1>
<p>hadoop  有三种部署模式：</p>
<ul>
<li>单机模式：没有守护进程，一切都运行在单个 JVM  上，适合测试与调试。</li>
<li>伪集群模式：守护进程在本地运行，适合模拟集群。</li>
<li>集群模式：守护进程运行在集群的某台机器上。</li>
</ul>
<p>所以，在以上任一特定模式运行 hadoop  时，只需要做两件事情：</p>
<p>1  ） 设置适当属性</p>
<p>2  ）启动 hadoop  的守护进程（名称节点，二级名称节名，数据节点）</p>
<p>hadoop  默认的是单机模式，下面，我们将着重介绍在集群模式是如何部署？</p>
<h2 id="-2-1-"><a href=""></a>2.1   测试环境</h2>
<p>用两台机器做为测试环境 ,   通常，集群里的一台机器被指定为  NameNode  ，另一台不同的机器被指定为 JobTracker  ，这些机器是 <strong>masters;</strong>  余下的机器即作为 DataNode  <strong>也</strong> 作为 TaskTracker  ，这些机器是 <strong>slaves</strong>  <strong>。</strong></p>
<p>1  ）  master (JobTracker &amp; NameNode)  ：我的工作机  ( zhanghua  .quqi.com)</p>
<p>2  ）  slave (TaskTracker &amp; DataNode)  ：我的开发机 ( tadev03  .quqi.com)</p>
<p>3)   两机均已安装 ssh   与  rsync</p>
<h2 id="-2-2-"><a href=""></a>2.2   测试程序</h2>
<p>1  ）  /home/workspace/hadoopExample/input/file01:</p>
<p>Hello World Bye World</p>
<p>2) /home/workspace/hadoopExample/input/file02:</p>
<p>Hello  Hadoop    Goodbye  Hadoop</p>
<ol>
<li>WordCount.java</li>
</ol>
<p><strong>package</strong>    com.TripResearch.hadoop;</p>
<p><strong>import</strong>   java.io.IOException;</p>
<p><strong>import</strong>   java.util.Iterator;</p>
<p><strong>import</strong>   java.util.StringTokenizer;</p>
<p><strong>import</strong>   org.apache.hadoop.fs.Path;</p>
<p><strong>import</strong>   org.apache.hadoop.io.IntWritable;</p>
<p><strong>import</strong>   org.apache.hadoop.io.LongWritable;</p>
<p><strong>import</strong>   org.apache.hadoop.io.Text;</p>
<p><strong>import</strong>   org.apache.hadoop.mapred. FileInputFormat  ;</p>
<p><strong>import</strong>   org.apache.hadoop.mapred.FileOutputFormat;</p>
<p><strong>import</strong>   org.apache.hadoop.mapred.JobClient;</p>
<p><strong>import</strong>   org.apache.hadoop.mapred. JobConf  ;</p>
<p><strong>import</strong>   org.apache.hadoop.mapred. MapReduceBase  ;</p>
<p><strong>import</strong>   org.apache.hadoop.mapred. Mapper  ;</p>
<p><strong>import</strong>   org.apache.hadoop.mapred.OutputCollector;</p>
<p><strong>import</strong>   org.apache.hadoop.mapred. Reducer  ;</p>
<p><strong>import</strong>   org.apache.hadoop.mapred.Reporter;</p>
<p><strong>import</strong>   org.apache.hadoop.mapred. TextInputFormat  ;</p>
<p><strong>import</strong>   org.apache.hadoop.mapred. TextOutputFormat  ;</p>
<p>//<em>/</em></p>
<p>/<em>  <em>*@author</em></em>    huazhang</p>
<p>/*/</p>
<p>@SuppressWarnings ( &quot;deprecation&quot; )</p>
<p><strong>public</strong>    <strong>class</strong>   WordCount {</p>
<p><strong>public</strong>    <strong>static</strong>    <strong>class</strong>   MyMap  <strong>extends</strong>    MapReduceBase    <strong>implements</strong></p>
<p>Mapper <LongWritable, Text, Text, IntWritable> {</p>
<p><strong>private</strong>    <strong>final</strong>    <strong>static</strong>   IntWritable  <em>one</em>   =  <strong>new</strong>   IntWritable(1);</p>
<p><strong>private</strong>   Text  word  =  <strong>new</strong>   Text();</p>
<p><strong>public</strong>    <strong>void</strong>   map(LongWritable key, Text value,</p>
<p>OutputCollector<Text, IntWritable> output, Reporter reporter)</p>
<p><strong>throws</strong>   IOException {</p>
<p>String line = value.toString();</p>
<p>StringTokenizer tokenizer =  <strong>new</strong>   StringTokenizer(line);</p>
<p><strong>while</strong>   (tokenizer.hasMoreTokens()) {</p>
<p>word .set(tokenizer.nextToken());</p>
<p>output.collect( word ,  <em>one</em>  );</p>
<p>}</p>
<p>}</p>
<p>}</p>
<p><strong>public</strong>    <strong>static</strong>    <strong>class</strong>   MyReduce  <strong>extends</strong>    MapReduceBase    <strong>implements</strong></p>
<p>Reducer <Text, IntWritable, Text, IntWritable> {</p>
<p><strong>public</strong>    <strong>void</strong>   reduce(Text key, Iterator<IntWritable> values,</p>
<p>OutputCollector<Text, IntWritable> output, Reporter reporter)</p>
<p><strong>throws</strong>   IOException {</p>
<p><strong>int</strong>   sum = 0;</p>
<p><strong>while</strong>   (values.hasNext()) {</p>
<p>sum += values.next().get();</p>
<p>}</p>
<p>output.collect(key,  <strong>new</strong>   IntWritable(sum));</p>
<p>}</p>
<p>}</p>
<p><strong>public</strong>    <strong>static</strong>    <strong>void</strong>   main(String[] args)  <strong>throws</strong>   Exception {</p>
<p>JobConf   conf =  <strong>new</strong>   JobConf(WordCount. <strong>class</strong>  );</p>
<p>conf.setJobName( &quot;wordcount&quot; );</p>
<p>conf.setOutputKeyClass(Text. <strong>class</strong>  );</p>
<p>conf.setOutputValueClass(IntWritable. <strong>class</strong>  );</p>
<p>conf.setMapperClass(MyMap. <strong>class</strong>  );</p>
<p>conf.setCombinerClass(MyReduce. <strong>class</strong>  );</p>
<p>conf.setReducerClass(MyReduce. <strong>class</strong>  );</p>
<p>conf.setInputFormat( TextInputFormat  . <strong>class</strong>  );</p>
<p>conf.setOutputFormat( TextOutputFormat  . <strong>class</strong>  );</p>
<p>FileInputFormat  . <em>setInputPaths</em>  (conf,  <strong>new</strong>   Path(args[0]));</p>
<p>FileOutputFormat. <em>setOutputPath</em>  (conf,  <strong>new</strong>   Path(args[1]));</p>
<p>JobClient.<em>runJob</em> (conf);</p>
<p>}</p>
<p>}</p>
<p><img src="" alt=""></p>
<h2 id="-2-3-"><a href=""></a>2.3   属性配置</h2>
<p>按下图所示修改至少 3  个属性, 如下图所示：</p>
<p>   <img src="" alt=""></p>
<ol>
<li></li>
<li><p>conf/core-site.xml</p>
</li>
</ol>
<configuration>

<property>

<name>fs.default.name</name>

<value>hdfs://zhanghua  .quqi.com:9000</value>

</property>

</configuration>

<p>注意：此处如果是伪集群模式可配置为  hdfs://localhost:9000 ,    是本地模式则为：  localhost:9000   。另外，其他输入输入路径，是本地模式是本地文件系统的路径，是非地模式，用 hdfs  文件系统的路径格式。</p>
<ol>
<li>conf/hdfs-site.xml</li>
</ol>
<configuration>

<property>

<name>dfs.replication</name>

<value>1</value>

</property>

<p></configuration></p>
<ol>
<li>conf/mapred-site.xml</li>
</ol>
<configuration>

<property>

<name>mapred.job.tracker</name>

<value>zhanghua  .quqi.com:8021</value>

</property>

<p></configuration></p>
<ol>
<li>masters</li>
</ol>
<p>zhanghua  .quqi.com (   伪分布模式就配成  localhost)</p>
<ol>
<li>slaves</li>
</ol>
<p>tadev03  .quqi.com  (   伪分布模式就配成 localhost)</p>
<ol>
<li>将以上配置好的 hadoop  文件夹拷到所有机器的相同目录下：</li>
</ol>
<p>scp -r /home/soft/hadoop-0.20.2 <a href="mailto:root@tadev03.daodao.com">root@tadev03</a>   <a href="mailto:root@tadev03.daodao.com">.quqi.com</a>  :/home/soft/hadoop-0.20.2</p>
<p>注意：确保两台机器的  JAVA_HOME   的路径一致，如果不一致，就要改 。</p>
<p>hadoop  所有可配置的配置文件说明如下：</p>
<p>hadoop-env.sh   运行 hadoop  的脚本中使用的环境变量</p>
<p>core-site.xml hadoop  的核心配置，如 HDFS  和 MapReduce  中很普遍的 I/O  设置</p>
<p>hdfs-site.xml HDFS  后台程序设置的配置：名称节点，第二名称节点及数据节点</p>
<p>mapred-site.xml MapReduce  后台程序设置的配置： jobtracker  和 tasktracker</p>
<p>masters   记录运行第二名称节点 的机器（一行一个）的列表</p>
<p>slaves   记录运行数据节点的机器（一行一个）的列表</p>
<h2 id="-2-4-ssh-"><a href=""></a>2.4   免密码 SSH  设置</h2>
<p>免密码  ssh   设置， 保证至少从   master    可以不用口令登陆所有的   slaves    。</p>
<p>1  ）生成密钥对： ssh-keygen -t rsa -P &#39;&#39; -f /root/.ssh/id_rsa (  这样密钥就留在了客户端 )</p>
<p>2)   将公钥拷到要连接的服务器，</p>
<p>scp /root/.ssh/id_rsa.pub root@tadev03  .quqi.com:/tmp</p>
<p>ssh -l root tadev03  .quqi.com</p>
<p>more /tmp/id_rsa.pub &gt;&gt; /root/.ssh/authorized_keys</p>
<ol>
<li>ssh tadev03  .quqi.com   不需要输入密码即为成功。</li>
</ol>
<p>（注意：伪分布模式也要配置  ssh localhost   无密码登录，如果是  mac   ，请将  ssh   打开）</p>
<p>(  另外，在 mac  中请在 hadoop-config.sh  文件中配置  export JAVA_HOME=/System/Library/Frameworks/JavaVM.framework/Versions/1.6/Home)</p>
<p>三条控制线线：</p>
<p>SSH →   这样就可以直接从主节点远程启动从节点上的脚本，如  ssh tadev03  .quqi.com &#39;/var/aa.sh&#39;</p>
<p>NameNode (<a href="http://localhost:50030/" target="_blank"><a href="http://localhost:50070">http://localhost:50070</a></a> ) → DataNode</p>
<p>JobTracker ( <a href="http://localhost:50030/" target="_blank"><a href="http://localhost:50030">http://localhost:50030</a></a> )→ TaskTracker (<a href="http://localhost:50030/" target="_blank"><a href="http://localhost:50060">http://localhost:50060</a></a> )</p>
<h2 id="-2-5-hosts"><a href=""></a>2.5   配置 hosts</h2>
<p>必须配置 master   和 slaves   之间的双向 hosts.   修改 /etc/hosts   进行配置，略。</p>
<h2 id="-2-6-hdfs-"><a href=""></a>2.6   格式化 HDFS  文件系统</h2>
<p>和我们常见的 NTFS  ， FAT32  文件系统一样， NDFS  最开始也是需要格式化的。格式化过程用来创建存储目录以及名称节点的永久数据结构的初始版本来创建一个空的文件系统。命令如下：</p>
<p>hadoop namenode -format</p>
<p>已知问题：在重新格式化时，可能会报： SHUTDOWN_MSG: Shutting down NameNode</p>
<p>解决办法： rm -rf /tmp/hadoop-root/dfs/name</p>
<h2 id="-2-7-"><a href=""></a>2.7   启动守护进程</h2>
<p>1    ）启动   HDFS    守护进程：    start-dfs.sh</p>
<p>(      start-dfs.sh    脚本会参照 NameNode    上 ${HADOOP_CONF_DIR}/slaves    文件的内容，在所有列出的 slave    上启动 DataNode    守护进程。   )</p>
<p>已知问题：在已设置   JAVA_HOME    的情况下仍会报：   Error: JAVA_HOME is not set</p>
<p>解决办法：我是在  hadoop.sh  文件中加下面一句解决的：</p>
<p>JAVA_HOME=/System/Library/Frameworks/JavaVM.framework/Versions/1.6/Home</p>
<p>2  ）启动  Map/Reduce  守护进程：   start-mapred.sh</p>
<p>(      start-mapred.sh   脚本会参照 JobTracker   上 ${HADOOP_CONF_DIR}/slaves   文件的内容，在所有列出的 slave   上启动 TaskTracker   守护进程  )</p>
<p>3)   启动成功后，可以通过访问  <a href="http://localhost:50030" target="_blank">http://localhost:50030</a>   验证。</p>
<p>注意：也可直接使用  start-all.sh       与  stop-all.sh       脚本  ,       在主节点   master    上面启动   hadoop    ，主节点会启动  /    停止所有从节点的   hadoop    。会启动  5       个   java        进程  ,        同时会在   /tmp        目录下创建五个   pid        文件记录这些进程   ID        号。通过这五个文件，可以得知   namenode, datanode, secondary namenode, jobtracker, tasktracker        分别对应于哪一个   Java        进程。</p>
<p>已知问题：启动后，日志中报：  java.io.IOException: File /tmp/hadoop-root/mapred/system/jobtracker.info could only be replicated to 0 nodes, instead of 1</p>
<p>解决办法：原因是    从  tadev03     .quqi.com       机器上无法  ping zhanghua     .quqi.com</p>
<h2 id="-2-8-"><a href=""></a>2.8   运行程序</h2>
<p>先将测试数据及其他输入由本地文件系统拷到  HFDS  文件系统中去（注意：   jar   除外 ）</p>
<ol>
<li></li>
<li><p>hadoop fs -mkdir input</p>
</li>
<li>hadoop fs -ls .</li>
<li>hadoop fs -copyFromLocal /home/workspace/hadoopExample/input/file01 input/file01</li>
<li>hadoop fs -copyFromLocal /home/workspace/hadoopExample/input/file02 input/file02</li>
</ol>
<p>这时候就可以执行下列命令运行程序了，注意：后面的input , output  等目录都是HDFS  文件系统的路径。(  如果是本地模式，就用本地文件系统的绝对路径）</p>
<ol>
<li></li>
</ol>
<p>hadoop     jar   /home/workspace/hadoopExample/hadoopExample.jar com.TripResearch.hadoop.WordCount input/ output</p>
<p>已知问题：在集群模式下运行时任务会Pending</p>
<p>最后，运行下列命令查看结果：</p>
<p>/home/soft/hadoop-0.20.2/bin/hadoop fs -cat output/part-00000</p>
<p>也可访问下列地址查看状态：</p>
<p>NameNode – <a href="http://localhost:50070/" target="_blank"><a href="http://zhanghua">http://zhanghua</a></a>   <a href="http://localhost:50070/" target="_blank">.quqi.com</a> <a href="http://localhost:50070/" target="_blank">:50070/</a></p>
<p>JobTracker - <a href="http://localhost:50030/" target="_blank"><a href="http://zhanghua">http://zhanghua</a></a>   <a href="http://localhost:50030/" target="_blank">.quqi.com</a> <a href="http://localhost:50030/" target="_blank">:50030/</a></p>
<p>常用命令说明如下：</p>
<p>hadoop dfs –ls   查看 /usr/root  目录下的内容径；
hadoop dfs –rmr xxx xxx  就是删除目录；
hadoop dfsadmin -report   这个命令可以全局的查看 DataNode  的情况；
hadoop job -list   后面增加参数是对于当前运行的 Job  的操作，例如 list,kill  等；
hadoop balancer   均衡磁盘负载的命令。</p>
<h1 id="-3-hadoop-"><a href=""></a>3 hadoop  高级进阶</h1>
<h1 id="-4-hadoop-"><a href=""></a>4 hadoop  应用案例</h1>
<h1 id="-5-"><a href=""></a>5   参考文献</h1>
<ol>
<li><a href="http://hadoop.apache.org/common/docs/r0.18.2/cn/" target="_blank"><a href="http://hadoop.apache.org/common/docs/r0.18.2/cn/">http://hadoop.apache.org/common/docs/r0.18.2/cn/</a></a></li>
<li>hadoop 0.20.2  集群配置入门 <a href="http://dev.firnow.com/course/3_program/java/javajs/20100719/453042.html" target="_blank"><a href="http://dev.firnow.com/course/3_program/java/javajs/">http://dev.firnow.com/course/3_program/java/javajs/</a></a></li>
<li>Hadoop 分布式文件系统（HDFS ）初步实践 <a href="http://huatai.me/?p=352" target="_blank"><a href="http://huatai.me/?p=352">http://huatai.me/?p=352</a></a></li>
<li>Hadoop 分布式部署实验2_ 格式化分布式文件系统 <a href="http://hi.baidu.com/thinke365/blog/item/15602aa8f9074cf41e17a235.html" target="_blank"><a href="http://hi.baidu.com/thinke365/blog/item/15602aa8f9074cf41e17a235.html">http://hi.baidu.com/thinke365/blog/item/15602aa8f9074cf41e17a235.html</a></a></li>
<li>hadoop 安装出现问题（紧急），请前辈指教 <a href="http://forum.hadoop.tw/viewtopic.php?f=4&amp;t=90" target="_blank"><a href="http://forum.hadoop.tw/viewtopic.php?f=4&amp;t=90">http://forum.hadoop.tw/viewtopic.php?f=4&amp;t=90</a></a></li>
<li>用 Hadoop  进行分布式并行编程 <a href="http://www.ibm.com/developerworks/cn/opensource/os-cn-hadoop1/index.html" target="_blank"><a href="http://www.ibm.com/developerworks/cn/opensource/os-cn-hadoop1/index.html">http://www.ibm.com/developerworks/cn/opensource/os-cn-hadoop1/index.html</a></a></li>
<li>用 Hadoop  进行分布式数据处理 <a href="http://tech.ddvip.com/2010-06/1275983295155033.html" target="_blank"><a href="http://tech.ddvip.com/2010-06/1275983295155033.html">http://tech.ddvip.com/2010-06/1275983295155033.html</a></a></li>
</ol>
<p>分享到： <a href="&quot;分享到新浪微博&quot;"></a><a href="&quot;分享到腾讯微博&quot;"></a></p>
<ol>
<li>上一篇：<a href="http://blog.csdn.net/quqi99/article/details/6160846" target="_blank">Lucene Scoring 评分机制 （ by quqi99 )</a></li>
<li><p>下一篇：<a href="http://blog.csdn.net/quqi99/article/details/6292472" target="_blank">深入理解各JEE服务器Web层集群原理 ( by quqi99 )</a>
查看评论<a href=""></a></p>
<p>暂无评论
您还没有登录,请<a href="">[登录]</a>或<a href="http://passport.csdn.net/account/register?from=http%3A%2F%2Fblog.csdn.net%2Fquqi99%2Farticle%2Fdetails%2F6291788" target="_blank">[注册]</a></p>
</li>
</ol>
<p>/* 以上用户言论只代表其个人观点，不代表CSDN网站的观点或立场<a href=""></a><a href=""></a>
<a href="&quot;回到顶部&quot;"><img src="" alt="TOP"></a></p>
<p>个人资料</p>
<p><a href="http://my.csdn.net/quqi99" target="_blank"><img src="&quot;访问我的空间&quot;" alt=""></a>
<a href="http://my.csdn.net/quqi99" target="_blank">quqi99</a></p>
<p><a href="&quot;[加关注]&quot;"></a> <a href="&quot;[发私信]&quot;"></a>
<a href="http://medal.blog.csdn.net/allmedal.aspx" target="_blank"><img src="" alt=""></a></p>
<ul>
<li>访问：198660次</li>
<li>积分：3337分</li>
<li><p>排名：第1895名</p>
</li>
<li><p>原创：146篇</p>
</li>
<li>转载：23篇</li>
<li>译文：0篇</li>
<li>评论：123条</li>
</ul>
<p>文章搜索</p>
<p><a href=""></a></p>
<p>文章分类</p>
<ul>
<li><a href="http://blog.csdn.net/quqi99/article/category/875141" target="_blank">VM / Cloud</a>(7)</li>
<li><a href="http://blog.csdn.net/quqi99/article/category/557281" target="_blank">Middleware / Java AppServer</a>(13)</li>
<li><a href="http://blog.csdn.net/quqi99/article/category/328029" target="_blank">Seach Engine</a>(5)</li>
<li><a href="http://blog.csdn.net/quqi99/article/category/674417" target="_blank">Linux / Unix / Shell</a>(24)</li>
<li><a href="http://blog.csdn.net/quqi99/article/category/328188" target="_blank">J2SE / JEE</a>(40)</li>
<li><a href="http://blog.csdn.net/quqi99/article/category/347580" target="_blank">DB / NoSQL</a>(9)</li>
<li><a href="http://blog.csdn.net/quqi99/article/category/803236" target="_blank">Architecture</a>(0)</li>
<li><a href="http://blog.csdn.net/quqi99/article/category/351802" target="_blank">Android</a>(3)</li>
<li><a href="http://blog.csdn.net/quqi99/article/category/803239" target="_blank">Life</a>(4)</li>
<li><a href="http://blog.csdn.net/quqi99/article/category/689016" target="_blank">Other</a>(9)</li>
<li><a href="http://blog.csdn.net/quqi99/article/category/1112756" target="_blank">OpenStack</a>(37)</li>
<li><a href="http://blog.csdn.net/quqi99/article/category/1139084" target="_blank">Python</a>(1)</li>
<li><a href="http://blog.csdn.net/quqi99/article/category/1167554" target="_blank">C / C++</a>(2)</li>
<li><p><a href="http://blog.csdn.net/quqi99/article/category/1490633" target="_blank">Networking</a>(1)
文章存档</p>
</li>
<li><p><a href="http://blog.csdn.net/quqi99/article/month/2013/08" target="_blank">2013年08月</a>(4)</p>
</li>
<li><a href="http://blog.csdn.net/quqi99/article/month/2013/07" target="_blank">2013年07月</a>(13)</li>
<li><a href="http://blog.csdn.net/quqi99/article/month/2013/06" target="_blank">2013年06月</a>(6)</li>
<li><a href="http://blog.csdn.net/quqi99/article/month/2013/05" target="_blank">2013年05月</a>(2)</li>
<li><a href="http://blog.csdn.net/quqi99/article/month/2013/04" target="_blank">2013年04月</a>(2)</li>
<li><a href="http://blog.csdn.net/quqi99/article/month/2013/03" target="_blank">2013年03月</a>(3)</li>
<li><a href="http://blog.csdn.net/quqi99/article/month/2013/02" target="_blank">2013年02月</a>(1)</li>
<li><a href="http://blog.csdn.net/quqi99/article/month/2013/01" target="_blank">2013年01月</a>(9)</li>
<li><a href="http://blog.csdn.net/quqi99/article/month/2012/12" target="_blank">2012年12月</a>(3)</li>
<li><a href="http://blog.csdn.net/quqi99/article/month/2012/11" target="_blank">2012年11月</a>(2)</li>
<li><a href="http://blog.csdn.net/quqi99/article/month/2012/08" target="_blank">2012年08月</a>(1)</li>
<li><a href="http://blog.csdn.net/quqi99/article/month/2012/07" target="_blank">2012年07月</a>(1)</li>
<li><a href="http://blog.csdn.net/quqi99/article/month/2012/06" target="_blank">2012年06月</a>(7)</li>
<li><a href="http://blog.csdn.net/quqi99/article/month/2012/05" target="_blank">2012年05月</a>(2)</li>
<li><a href="http://blog.csdn.net/quqi99/article/month/2012/04" target="_blank">2012年04月</a>(6)</li>
<li><a href="http://blog.csdn.net/quqi99/article/month/2012/03" target="_blank">2012年03月</a>(2)</li>
<li><a href="http://blog.csdn.net/quqi99/article/month/2012/02" target="_blank">2012年02月</a>(1)</li>
<li><a href="http://blog.csdn.net/quqi99/article/month/2011/12" target="_blank">2011年12月</a>(1)</li>
<li><a href="http://blog.csdn.net/quqi99/article/month/2011/09" target="_blank">2011年09月</a>(5)</li>
<li><a href="http://blog.csdn.net/quqi99/article/month/2011/08" target="_blank">2011年08月</a>(2)</li>
<li><a href="http://blog.csdn.net/quqi99/article/month/2011/06" target="_blank">2011年06月</a>(2)</li>
<li><a href="http://blog.csdn.net/quqi99/article/month/2011/04" target="_blank">2011年04月</a>(5)</li>
<li><a href="http://blog.csdn.net/quqi99/article/month/2011/03" target="_blank">2011年03月</a>(2)</li>
<li><a href="http://blog.csdn.net/quqi99/article/month/2011/01" target="_blank">2011年01月</a>(4)</li>
<li><a href="http://blog.csdn.net/quqi99/article/month/2010/12" target="_blank">2010年12月</a>(1)</li>
<li><a href="http://blog.csdn.net/quqi99/article/month/2010/07" target="_blank">2010年07月</a>(1)</li>
<li><a href="http://blog.csdn.net/quqi99/article/month/2010/05" target="_blank">2010年05月</a>(1)</li>
<li><a href="http://blog.csdn.net/quqi99/article/month/2010/04" target="_blank">2010年04月</a>(2)</li>
<li><a href="http://blog.csdn.net/quqi99/article/month/2010/03" target="_blank">2010年03月</a>(2)</li>
<li><a href="http://blog.csdn.net/quqi99/article/month/2010/02" target="_blank">2010年02月</a>(4)</li>
<li><a href="http://blog.csdn.net/quqi99/article/month/2010/01" target="_blank">2010年01月</a>(6)</li>
<li><a href="http://blog.csdn.net/quqi99/article/month/2009/12" target="_blank">2009年12月</a>(6)</li>
<li><a href="http://blog.csdn.net/quqi99/article/month/2009/11" target="_blank">2009年11月</a>(5)</li>
<li><a href="http://blog.csdn.net/quqi99/article/month/2009/10" target="_blank">2009年10月</a>(6)</li>
<li><a href="http://blog.csdn.net/quqi99/article/month/2009/09" target="_blank">2009年09月</a>(1)</li>
<li><a href="http://blog.csdn.net/quqi99/article/month/2009/08" target="_blank">2009年08月</a>(1)</li>
<li><a href="http://blog.csdn.net/quqi99/article/month/2009/06" target="_blank">2009年06月</a>(4)</li>
<li><a href="http://blog.csdn.net/quqi99/article/month/2009/03" target="_blank">2009年03月</a>(1)</li>
<li><a href="http://blog.csdn.net/quqi99/article/month/2008/11" target="_blank">2008年11月</a>(1)</li>
<li><a href="http://blog.csdn.net/quqi99/article/month/2008/10" target="_blank">2008年10月</a>(2)</li>
<li><a href="http://blog.csdn.net/quqi99/article/month/2008/08" target="_blank">2008年08月</a>(1)</li>
<li><a href="http://blog.csdn.net/quqi99/article/month/2008/06" target="_blank">2008年06月</a>(3)</li>
<li><a href="http://blog.csdn.net/quqi99/article/month/2008/04" target="_blank">2008年04月</a>(1)</li>
<li><a href="http://blog.csdn.net/quqi99/article/month/2008/03" target="_blank">2008年03月</a>(1)</li>
<li><a href="http://blog.csdn.net/quqi99/article/month/2008/02" target="_blank">2008年02月</a>(1)</li>
<li><a href="http://blog.csdn.net/quqi99/article/month/2008/01" target="_blank">2008年01月</a>(3)</li>
<li><a href="http://blog.csdn.net/quqi99/article/month/2007/12" target="_blank">2007年12月</a>(4)</li>
<li><a href="http://blog.csdn.net/quqi99/article/month/2007/11" target="_blank">2007年11月</a>(1)</li>
<li><a href="http://blog.csdn.net/quqi99/article/month/2007/10" target="_blank">2007年10月</a>(4)</li>
<li><a href="http://blog.csdn.net/quqi99/article/month/2007/08" target="_blank">2007年08月</a>(5)</li>
<li><a href="http://blog.csdn.net/quqi99/article/month/2007/07" target="_blank">2007年07月</a>(6)</li>
<li><a href="http://blog.csdn.net/quqi99/article/month/2007/06" target="_blank">2007年06月</a>(1)</li>
<li><a href="http://blog.csdn.net/quqi99/article/month/2007/05" target="_blank">2007年05月</a>(8)</li>
</ul>
<p>展开</p>
<p>阅读排行</p>
<ul>
<li><a href="http://blog.csdn.net/quqi99/article/details/1916553" title="Android学习笔记 ( by quqi99 )" target="_blank">Android学习笔记 ( by quqi99 )</a>(22132)</li>
<li><a href="http://blog.csdn.net/quqi99/article/details/1680647" title="java.lang.NoClassDefFoundError: org/apache/commons/io/output/DeferredFileOutputStream" target="_blank">java.lang.NoClassDefFoundError: org/apache/commons/io/output/DeferredFileOutputStream</a>(17851)</li>
<li><a href="http://blog.csdn.net/quqi99/article/details/7433285" title="建立openstack quantum开发环境" target="_blank">建立openstack quantum开发环境</a>(6747)</li>
<li><a href="http://blog.csdn.net/quqi99/article/details/1755642" title="在linux中安装字库( by quqi99 )" target="_blank">在linux中安装字库( by quqi99 )</a>(5151)</li>
<li><a href="http://blog.csdn.net/quqi99/article/details/1930304" title="Android手机客户端与Servlet交换数据(by quqi99)" target="_blank">Android手机客户端与Servlet交换数据(by quqi99)</a>(5140)</li>
<li><a href="http://blog.csdn.net/quqi99/article/details/5298017" title="ReentrantLock与synchronized的区别 ( by quqi99 )" target="_blank">ReentrantLock与synchronized的区别 ( by quqi99 )</a>(4864)</li>
<li><a href="http://blog.csdn.net/quqi99/article/details/5604732" title="个人户口档案转移笔记（适用北京集体户口）" target="_blank">个人户口档案转移笔记（适用北京集体户口）</a>(4802)</li>
<li><a href="http://blog.csdn.net/quqi99/article/details/1624225" title="Magnolia学习笔记（一个基于JSR170的内容管理系统） ( by quqi99 )" target="_blank">Magnolia学习笔记（一个基于JSR170的内容管理系统） ( by quqi99 )</a>(4342)</li>
<li><a href="http://blog.csdn.net/quqi99/article/details/1624218" title="JSpider学习笔记 ( by quqi99 )" target="_blank">JSpider学习笔记 ( by quqi99 )</a>(4149)</li>
<li><p><a href="http://blog.csdn.net/quqi99/article/details/3099945" title="Plone学习笔记 ( by quqi99 )" target="_blank">Plone学习笔记 ( by quqi99 )</a>(4057)
评论排行</p>
</li>
<li><p><a href="http://blog.csdn.net/quqi99/article/details/1916553" title="Android学习笔记 ( by quqi99 )" target="_blank">Android学习笔记 ( by quqi99 )</a>(21)</p>
</li>
<li><a href="http://blog.csdn.net/quqi99/article/details/1680647" title="java.lang.NoClassDefFoundError: org/apache/commons/io/output/DeferredFileOutputStream" target="_blank">java.lang.NoClassDefFoundError: org/apache/commons/io/output/DeferredFileOutputStream</a>(18)</li>
<li><a href="http://blog.csdn.net/quqi99/article/details/1930304" title="Android手机客户端与Servlet交换数据(by quqi99)" target="_blank">Android手机客户端与Servlet交换数据(by quqi99)</a>(12)</li>
<li><a href="http://blog.csdn.net/quqi99/article/details/5604732" title="个人户口档案转移笔记（适用北京集体户口）" target="_blank">个人户口档案转移笔记（适用北京集体户口）</a>(8)</li>
<li><a href="http://blog.csdn.net/quqi99/article/details/1624225" title="Magnolia学习笔记（一个基于JSR170的内容管理系统） ( by quqi99 )" target="_blank">Magnolia学习笔记（一个基于JSR170的内容管理系统） ( by quqi99 )</a>(7)</li>
<li><a href="http://blog.csdn.net/quqi99/article/details/2591768" title="使用itext生成word格式的报表(by quqi99)" target="_blank">使用itext生成word格式的报表(by quqi99)</a>(5)</li>
<li><a href="http://blog.csdn.net/quqi99/article/details/1755642" title="在linux中安装字库( by quqi99 )" target="_blank">在linux中安装字库( by quqi99 )</a>(4)</li>
<li><a href="http://blog.csdn.net/quqi99/article/details/6305061" title="Android分享文稿 ( by quqi99 )" target="_blank">Android分享文稿 ( by quqi99 )</a>(4)</li>
<li><a href="http://blog.csdn.net/quqi99/article/details/9156497" title="OpenDaylight学习 ( by quqi99 )" target="_blank">OpenDaylight学习 ( by quqi99 )</a>(3)</li>
<li><a href="http://blog.csdn.net/quqi99/article/details/2590703" title="使用jacob生成word(by quqi99)" target="_blank">使用jacob生成word(by quqi99)</a>(3)</li>
</ul>
<p>推荐文章
最新评论</p>
<ul>
<li><a href="http://blog.csdn.net/quqi99/article/details/9156497#comments" target="_blank">OpenDaylight学习 ( by quqi99 )</a></li>
</ul>
<p><a href="http://blog.csdn.net/quqi99" target="_blank">quqi99</a>: hi whoeversucks, 谢谢你的实时信息，非常有用，我已经更新到博客里了。另外，问个问题，...</p>
<ul>
<li><a href="http://blog.csdn.net/quqi99/article/details/9156497#comments" target="_blank">OpenDaylight学习 ( by quqi99 )</a></li>
</ul>
<p><a href="http://blog.csdn.net/whoeversucks" target="_blank">whoeversucks</a>: 注意，OpenDayLight Controller和OSCP实际上2个独立的SDN控制器项目（分别...</p>
<ul>
<li><a href="http://blog.csdn.net/quqi99/article/details/9102745#comments" target="_blank">给Linux虚机扩充硬盘空间 ( by quqi99 )</a></li>
</ul>
<p><a href="http://blog.csdn.net/quqi99" target="_blank">quqi99</a>: hi dalinhuang, 谢谢你的回复，你给的这个方法是只适合LVM场景的啊，我没有使用LVM。</p>
<ul>
<li><a href="http://blog.csdn.net/quqi99/article/details/9102745#comments" target="_blank">给Linux虚机扩充硬盘空间 ( by quqi99 )</a></li>
</ul>
<p><a href="http://blog.csdn.net/dalinhuang" target="_blank">dalinhuang</a>: 给根（/）扩充的步骤：（以你的virtualbox并使用LVM为例）1. 新增一块虚拟硬盘，给虚机。...</p>
<ul>
<li><a href="http://blog.csdn.net/quqi99/article/details/9156497#comments" target="_blank">OpenDaylight学习 ( by quqi99 )</a></li>
</ul>
<p><a href="http://blog.csdn.net/piaochenping" target="_blank">piaochenping</a>: 你好，为什么我安装时老是出现这个错误呢？ Failed to execute goal org.co...</p>
<ul>
<li><a href="http://blog.csdn.net/quqi99/article/details/7659905#comments" target="_blank">OpenStack CI测试之devstack-gate ( by quqi99 )</a></li>
</ul>
<p><a href="http://blog.csdn.net/quqi99" target="_blank">quqi99</a>: @sunyilong2012: 这种错误应该是差模块吧，可以单独安装一下试试, sudo pip i...</p>
<ul>
<li><a href="http://blog.csdn.net/quqi99/article/details/7411091#comments" target="_blank">Fedora 16上源码建立pydev + eclipse的OpenStack开发环境笔记草稿 ( by quqi99 )</a></li>
</ul>
<p><a href="http://blog.csdn.net/quqi99" target="_blank">quqi99</a>: openstack因为用到了一些linux特有的东西，如iptables，所以目前只能跑在linux...</p>
<ul>
<li><a href="http://blog.csdn.net/quqi99/article/details/7411091#comments" target="_blank">Fedora 16上源码建立pydev + eclipse的OpenStack开发环境笔记草稿 ( by quqi99 )</a></li>
</ul>
<p><a href="http://blog.csdn.net/javaerss" target="_blank">javaerss</a>: 大神...看哭了，为此特地跑去下载fedora 16来做实验。之前用ubuntu下用eclipse ...</p>
<ul>
<li><a href="http://blog.csdn.net/quqi99/article/details/6576375#comments" target="_blank">玩转play framework ( by quqi99 )</a></li>
</ul>
<p><a href="http://blog.csdn.net/nanfu08" target="_blank">nanfu08</a>: 你能看得清，如果只是自己看的话我没话说，这样的文字叫人怎么读？？</p>
<ul>
<li><a href="http://blog.csdn.net/quqi99/article/details/7659905#comments" target="_blank">OpenStack CI测试之devstack-gate ( by quqi99 )</a></li>
</ul>
<p><a href="http://blog.csdn.net/sunyilong2012" target="_blank">dragonsun</a>: 您好，我在做这个测试的时候遇到了无法导入statsd的问题，请问您有解决的方法吗？+ /home/j...</p>
<p><a href="http://www.csdn.net/company/about.html" target="_blank">公司简介</a>|<a href="http://www.csdn.net/company/recruit.html" target="_blank">招贤纳士</a>|<a href="http://www.csdn.net/company/marketing.html" target="_blank">广告服务</a>|<a href="http://www.csdn.net/company/account.html" target="_blank">银行汇款帐号</a>|<a href="http://www.csdn.net/company/contact.html" target="_blank">联系方式</a>|<a href="http://www.csdn.net/company/statement.html" target="_blank">版权声明</a>|<a href="http://www.csdn.net/company/layer.html" target="_blank">法律顾问</a>|<a href="mailto:webmaster@csdn.net">问题报告</a><a href="http://wpa.qq.com/msgrd?v=3&amp;uin=2355263776&amp;site=qq&amp;menu=yes" target="_blank">QQ客服</a> <a href="http://e.weibo.com/csdnsupport/profile" target="_blank">微博客服</a> <a href="http://bbs.csdn.net/forums/Service" target="_blank">论坛反馈</a> <a href="mailto:webmaster@csdn.net">联系邮箱：webmaster@csdn.net</a> 服务热线：400-600-2320京 ICP 证 070598 号北京创新乐知信息技术有限公司 版权所有世纪乐知(北京)网络技术有限公司 提供技术支持江苏乐知网络技术有限公司 提供商务支持Copyright © 1999-2012, CSDN.NET, All Rights Reserved <a href="http://www.hd315.gov.cn/beian/view.asp?bianhao=010202001032100010" target="_blank"><img src="" alt="GongshangLogo"></a>
<img src="http://counter.csdn.net/pv.aspx?id=24" alt=""></p>

      
    </div>
    
    
      
    
    
        <footer id="post-meta">
        <span class="categories">Posted in<span class="breadcrumb fa fa-folder"><li><a href="/categories/hadoop/">hadoop</a></li></span></span> | <span class="tags">Tagged <a href="/tags/hadoop/" class="label label-primary">hadoop</a></span> | <span class="time">recent updated:<time title="2014-03-07 09:54:36"datetime="2014-03-07 09:54:36"> mar. 7 2014</time></span> | <span class="comment-link">
<a href="http://itsolife.com/2014/02/02/2014-02-02-hadoop--Hadoop知识分享文稿byquqi99-技术并艺术着/#comments" class="ds-thread-count comment-link" data-thread-key="2014-02-02-hadoop--Hadoop知识分享文稿byquqi99-技术并艺术着" data-count-type="comments">暂无评论</a></span>
        </footer>
    
    <div class="clearfix"></div>
  </div>
</div>
</article>



  <article>
<div id="post" class="post well">
  <div class="post-content">
    <header class="well-sm">
      <i class="fa icon fa-5x pull-left"></i>
      <h1 class="title"><a href="/2014/02/02/2014-02-02-hadoop--hadoop/">hadoop</a></h1>
      
        <span>Posted on<time datetime="2014-02-02T01:54:36.000Z"> <a href="/2014/02/02/2014-02-02-hadoop--hadoop/">feb. 2 2014</a></time></span>      
    </header>
    
    
    <div class="entry">
      
        <h1 id="hadoop">hadoop</h1>
<p><img src="" alt=""></p>
<h1 id="hadoop">hadoop</h1>
<h2 id="table-of-contents">Table of Contents</h2>
<ul>
<li><p><a href="http://dirlt.com/hadoop.html#sec-1" target="_blank">1 hadoop</a></p>
</li>
<li><p><a href="http://dirlt.com/hadoop.html#sec-1-1" target="_blank">1.1 FAQ</a></p>
</li>
<li><p><a href="http://dirlt.com/hadoop.html#sec-1-1-1" target="_blank">1.1.1 Hadoop可以用来做什么</a></p>
</li>
<li><a href="http://dirlt.com/hadoop.html#sec-1-1-2" target="_blank">1.1.2 Hadoop包括哪些组件</a></li>
<li><a href="http://dirlt.com/hadoop.html#sec-1-1-3" target="_blank">1.1.3 CDH和Apache Hadoop的关系</a></li>
<li><a href="http://dirlt.com/hadoop.html#sec-1-1-4" target="_blank">1.1.4 CDH产品组件构成</a></li>
<li><p><a href="http://dirlt.com/hadoop.html#sec-1-1-5" target="_blank">1.1.5 CDH产品组件端口分布和配置</a></p>
</li>
<li><p><a href="http://dirlt.com/hadoop.html#sec-1-1-5-1" target="_blank">1.1.5.1 Hadoop HDFS</a></p>
</li>
<li><a href="http://dirlt.com/hadoop.html#sec-1-1-5-2" target="_blank">1.1.5.2 Hadoop MRv1</a></li>
<li><a href="http://dirlt.com/hadoop.html#sec-1-1-5-3" target="_blank">1.1.5.3 Hadoop YARN</a></li>
<li><a href="http://dirlt.com/hadoop.html#sec-1-1-5-4" target="_blank">1.1.5.4 HBase</a></li>
<li><a href="http://dirlt.com/hadoop.html#sec-1-1-5-5" target="_blank">1.1.5.5 Hive</a></li>
<li><a href="http://dirlt.com/hadoop.html#sec-1-1-5-6" target="_blank">1.1.5.6 Sqoop</a></li>
<li><a href="http://dirlt.com/hadoop.html#sec-1-1-5-7" target="_blank">1.1.5.7 Zookeeper</a></li>
<li><a href="http://dirlt.com/hadoop.html#sec-1-1-5-8" target="_blank">1.1.5.8 Hue</a></li>
<li><a href="http://dirlt.com/hadoop.html#sec-1-1-5-9" target="_blank">1.1.5.9 Ozzie</a></li>
<li><a href="http://dirlt.com/hadoop.html#sec-1-1-5-10" target="_blank">1.1.5.10 Ganglia</a></li>
<li><a href="http://dirlt.com/hadoop.html#sec-1-1-5-11" target="_blank">1.1.5.11 Kerberos</a></li>
<li><p><a href="http://dirlt.com/hadoop.html#sec-1-2" target="_blank">1.2 观点</a></p>
</li>
<li><p><a href="http://dirlt.com/hadoop.html#sec-1-2-1" target="_blank">1.2.1 Hadoop即将过时了吗？</a></p>
</li>
<li><a href="http://dirlt.com/hadoop.html#sec-1-2-2" target="_blank">1.2.2 Best Practices for Selecting Apache Hadoop Hardware</a></li>
<li><a href="http://dirlt.com/hadoop.html#sec-1-2-3" target="_blank">1.2.3 The dark side of Hadoop - BackType Technology</a></li>
<li><p><a href="http://dirlt.com/hadoop.html#sec-1-3" target="_blank">1.3 使用问题</a></p>
</li>
<li><p><a href="http://dirlt.com/hadoop.html#sec-1-3-1" target="_blank">1.3.1 CDH3u3搭建单节点集群</a></p>
</li>
<li><a href="http://dirlt.com/hadoop.html#sec-1-3-2" target="_blank">1.3.2 CDH4.2.0搭建单节点集群</a></li>
<li><a href="http://dirlt.com/hadoop.html#sec-1-3-3" target="_blank">1.3.3 CDH4.3.0</a></li>
<li><p><a href="http://dirlt.com/hadoop.html#sec-1-3-4" target="_blank">1.3.4 Configuration</a></p>
</li>
<li><p><a href="http://dirlt.com/hadoop.html#sec-1-3-4-1" target="_blank">1.3.4.1 .bash_profile</a></p>
</li>
<li><a href="http://dirlt.com/hadoop.html#sec-1-3-4-2" target="_blank">1.3.4.2 core-site.xml</a></li>
<li><a href="http://dirlt.com/hadoop.html#sec-1-3-4-3" target="_blank">1.3.4.3 hdfs-site.xml</a></li>
<li><a href="http://dirlt.com/hadoop.html#sec-1-3-4-4" target="_blank">1.3.4.4 mapred-site.xml</a></li>
<li><a href="http://dirlt.com/hadoop.html#sec-1-3-4-5" target="_blank">1.3.4.5 hadoop-env.sh</a></li>
<li><a href="http://dirlt.com/hadoop.html#sec-1-3-4-6" target="_blank">1.3.4.6 hbase-site.xml</a></li>
<li><a href="http://dirlt.com/hadoop.html#sec-1-3-4-7" target="_blank">1.3.4.7 hbase-env.sh</a></li>
<li><p><a href="http://dirlt.com/hadoop.html#sec-1-4" target="_blank">1.4 Hadoop权威指南</a></p>
</li>
<li><p><a href="http://dirlt.com/hadoop.html#sec-1-4-1" target="_blank">1.4.1 初识Hadoop</a></p>
</li>
<li><a href="http://dirlt.com/hadoop.html#sec-1-4-2" target="_blank">1.4.2 关于MapReduce</a></li>
<li><a href="http://dirlt.com/hadoop.html#sec-1-4-3" target="_blank">1.4.3 Hadoop分布式文件系统</a></li>
<li><a href="http://dirlt.com/hadoop.html#sec-1-4-4" target="_blank">1.4.4 Hadoop IO</a></li>
<li><a href="http://dirlt.com/hadoop.html#sec-1-4-5" target="_blank">1.4.5 MapReduce应用开发</a></li>
<li><a href="http://dirlt.com/hadoop.html#sec-1-4-6" target="_blank">1.4.6 MapReduce的工作机制</a></li>
<li><a href="http://dirlt.com/hadoop.html#sec-1-4-7" target="_blank">1.4.7 MapReduce的类型与格式</a></li>
<li><a href="http://dirlt.com/hadoop.html#sec-1-4-8" target="_blank">1.4.8 MapReduce的特性</a></li>
<li><a href="http://dirlt.com/hadoop.html#sec-1-4-9" target="_blank">1.4.9 构建Hadoop集群</a></li>
<li><a href="http://dirlt.com/hadoop.html#sec-1-4-10" target="_blank">1.4.10 管理Hadoop</a></li>
<li><p><a href="http://dirlt.com/hadoop.html#sec-1-5" target="_blank">1.5 Benchmark</a></p>
</li>
<li><p><a href="http://dirlt.com/hadoop.html#sec-1-5-1" target="_blank">1.5.1 TestDFSIO</a></p>
</li>
<li><a href="http://dirlt.com/hadoop.html#sec-1-5-2" target="_blank">1.5.2 TeraSort</a></li>
<li><a href="http://dirlt.com/hadoop.html#sec-1-5-3" target="_blank">1.5.3 nnbench</a></li>
<li><a href="http://dirlt.com/hadoop.html#sec-1-5-4" target="_blank">1.5.4 mrbench</a></li>
<li><a href="http://dirlt.com/hadoop.html#sec-1-5-5" target="_blank">1.5.5 hbase.PerformanceEvaluation</a></li>
</ul>
<h2 id="1-hadoop">1 hadoop</h2>
<p>参考资源</p>
<ul>
<li>Cloudera <a href="http://www.cloudera.com/" target="_blank"><a href="http://www.cloudera.com/">http://www.cloudera.com/</a></a></li>
<li>Apache Hadoop <a href="http://hadoop.apache.org/" target="_blank"><a href="http://hadoop.apache.org/">http://hadoop.apache.org/</a></a></li>
<li>Apache Hadoop r1.0.3 文档 <a href="http://hadoop.apache.org/common/docs/r1.0.3/" target="_blank"><a href="http://hadoop.apache.org/common/docs/r1.0.3/">http://hadoop.apache.org/common/docs/r1.0.3/</a></a></li>
<li>Apache Hadoop r1.0.3 中文文档 <a href="http://hadoop.apache.org/common/docs/r1.0.3/cn" target="_blank"><a href="http://hadoop.apache.org/common/docs/r1.0.3/cn">http://hadoop.apache.org/common/docs/r1.0.3/cn</a></a></li>
<li>CDH Downloads <a href="https://ccp.cloudera.com/display/SUPPORT/Downloads" target="_blank"><a href="https://ccp.cloudera.com/display/SUPPORT/Downloads">https://ccp.cloudera.com/display/SUPPORT/Downloads</a></a></li>
<li>CDH Documentation <a href="https://ccp.cloudera.com/display/DOC/Documentation" target="_blank"><a href="https://ccp.cloudera.com/display/DOC/Documentation">https://ccp.cloudera.com/display/DOC/Documentation</a></a></li>
<li>CDH Tutorial <a href="https://ccp.cloudera.com/display/SUPPORT/Hadoop+Tutorial" target="_blank"><a href="https://ccp.cloudera.com/display/SUPPORT/Hadoop+Tutorial">https://ccp.cloudera.com/display/SUPPORT/Hadoop+Tutorial</a></a></li>
</ul>
<h3 id="1-1-faq">1.1 FAQ</h3>
<h3 id="1-1-1-hadoop-">1.1.1 Hadoop可以用来做什么</h3>
<p>Why Hadoop? <a href="http://www.cloudera.com/why-hadoop/" target="_blank"><a href="http://www.cloudera.com/why-hadoop/">http://www.cloudera.com/why-hadoop/</a></a></p>
<p>TODO(dirlt):translate it!!!</p>
<p>Simply put, Hadoop can transform the way you store and process data throughout your enterprise. According to analysts, about 80% of the data in the world is unstructured, and until Hadoop, it was essentially unusable in any systematic way. With Hadoop, for the first time you can combine all your data and look at it as one.</p>
<ul>
<li>Make All Your Data Profitable. Hadoop enables you to gain insight from all the data you already have; to ingest the data flowing into your systems 24/7 and leverage it to make optimizations that were impossible before; to make decisions based on hard data, not hunches; to look at complete data, not samples; to look at years of transactions, not days or weeks. In short, Hadoop will change the way you run your organization.</li>
<li>Leverage All Types of Data, From All Types of Systems. Hadoop can handle all types of data from disparate systems: structured, unstructured, log files, pictures, audio files, communications records, email– just about anything you can think of. Even when different types of data have been stored in unrelated systems, you can dump it all into your Hadoop cluster before you even know how you might take advantage of it in the future.</li>
<li>Scale Beyond Anything You Have Today. The largest social network in the world is built on the same open-source technology as Hadoop, and now exceeds 100 petabytes. It’s unlikely your organization has that much data. As you need more capacity, you just add more commodity servers and Hadoop automatically incorporates the new storage and compute capacity.</li>
</ul>
<h3 id="1-1-2-hadoop-">1.1.2 Hadoop包括哪些组件</h3>
<p>TODO(dirlt):translate it!!!</p>
<p>Apache Hadoop包括了下面这些组件：</p>
<ul>
<li><a href="http://hadoop.apache.org/common/" target="_blank">Hadoop Common</a> The common utilities that support the other Hadoop subprojects.</li>
<li><a href="http://hadoop.apache.org/hdfs/" target="_blank">Hadoop Distributed File System(HDFS)</a> A distributed file system that provides high-throughput access to application data.</li>
<li><a href="http://hadoop.apache.org/mapreduce/" target="_blank">Hadoop MapReduce</a> A software framework for distributed processing of large data sets on compute clusters.</li>
</ul>
<p>和Apache Hadoop相关的组件有：</p>
<ul>
<li><a href="http://avro.apache.org/" target="_blank">Avro</a> A data serialization system.</li>
<li><a href="http://cassandra.apache.org/" target="_blank">Cassandra</a> A scalable multi-master database with no single points of failure.</li>
<li><a href="http://incubator.apache.org/chukwa/" target="_blank">Chukwa</a> A data collection system for managing large distributed systems.</li>
<li><a href="http://hbase.apache.org/" target="_blank">HBase</a> A scalable, distributed database that supports structured data storage for large tables.</li>
<li><a href="http://hive.apache.org/" target="_blank">Hive</a> A data warehouse infrastructure that provides data summarization and ad hoc querying.</li>
<li><a href="http://mahout.apache.org/" target="_blank">Mahout</a> A Scalable machine learning and data mining library.</li>
<li><a href="http://pig.apache.org/" target="_blank">Pig</a> A high-level data-flow language and execution framework for parallel computation.</li>
<li><a href="http://zookeeper.apache.org/" target="_blank">ZooKeeper</a> A high-performance coordination service for distributed applications.<h3 id="1-1-3-cdh-apache-hadoop-">1.1.3 CDH和Apache Hadoop的关系</h3>
</li>
</ul>
<p>CDH Hadoop FAQ <a href="https://ccp.cloudera.com/display/SUPPORT/Hadoop+FAQ" target="_blank"><a href="https://ccp.cloudera.com/display/SUPPORT/Hadoop+FAQ">https://ccp.cloudera.com/display/SUPPORT/Hadoop+FAQ</a></a></p>
<p>TODO(dirlt):translate it!!!</p>
<ul>
<li>What exactly is included in CDH? / Cloudera&#39;s Distribution Including Apache Hadoop (CDH) is a certified release of Apache Hadoop. We include some stable patches scheduled to be included in future releases, as well as some patches we have developed for our supported customers, and are in the process of contributing back to Apache.</li>
<li>What license is Cloudera&#39;s Distribution Including Apache Hadoop released under? / Just like Hadoop, Cloudera&#39;s Distribution Including Apache Hadoop is released under the Apache Public License version 2.</li>
<li>Is Cloudera forking Hadoop? / Absolutely not. Cloudera is committed to the Hadoop project and the principles of the Apache Software License and Foundation. We continue to work actively with current releases of Hadoop and deliver certified releases to the community as appropriate.</li>
<li>Does Cloudera contribute their changes back to Apache? / We do, and will continue to contribute all eligible changes back to Apache. We occasionally release code we know to be stable even if our contribution to Apache is still in progress. Some of our changes are not eligible for contribution, as they capture the Cloudera brand, or link to our tools and documentation, but these do not affect compatibility with core project.</li>
</ul>
<h3 id="1-1-4-cdh-">1.1.4 CDH产品组件构成</h3>
<p><a href="http://www.cloudera.com/content/cloudera/en/products/cdh.html" target="_blank"><a href="http://www.cloudera.com/content/cloudera/en/products/cdh.html">http://www.cloudera.com/content/cloudera/en/products/cdh.html</a></a></p>
<p>从这里可以下载CDH4组件 <a href="http://www.cloudera.com/content/cloudera-content/cloudera-docs/CDHTarballs/3.25.2013/CDH4-Downloadable-Tarballs/CDH4-Downloadable-Tarballs.html" target="_blank"><a href="http://www.cloudera.com/content/cloudera-content/cloudera-docs/CDHTarballs/3.25.2013/CDH4-Downloadable-Tarballs/CDH4-Downloadable-Tarballs.html">http://www.cloudera.com/content/cloudera-content/cloudera-docs/CDHTarballs/3.25.2013/CDH4-Downloadable-Tarballs/CDH4-Downloadable-Tarballs.html</a></a></p>
<p><img src="" alt="./images/cloudera-enterprise-diagram.png"></p>
<h3 id="1-1-5-cdh-">1.1.5 CDH产品组件端口分布和配置</h3>
<p>The CDH4 components, and third parties such as Kerberos, use the ports listed in the tables that follow. Before you deploy CDH4, make sure these ports are open on each system.</p>
<h3 id="1-1-5-1-hadoop-hdfs">1.1.5.1 Hadoop HDFS</h3>
<p>ServiceQualifierPortProtocolAccess RequirementConfigurationCommentDataNode50010TCPExternaldfs.datanode.addressDataNode HTTP server portDataNodeSecure1004TCPExternaldfs.datanode.addressDataNode50075TCPExternaldfs.datanode.http.addressDataNodeSecure1006TCPExternaldfs.datanode.http.addressDataNode50020TCPExternaldfs.datanode.ipc.addressNameNode8020TCPExternalfs.default.name or fs.defaultFSfs.default.name is deprecated (but still works)NameNode50070TCPExternaldfs.http.address or dfs.namenode.http-addressdfs.http.address is deprecated (but still works)NameNodeSecure50470TCPExternaldfs.https.address or dfs.namenode.https-addressdfs.https.address is deprecated (but still works)Sec NameNode50090TCPInternaldfs.secondary.http.address or dfs.namenode.secondary.http-addressdfs.secondary.http.address is deprecated (but still works)Sec NameNodeSecure50495TCPInternaldfs.secondary.https.addressJournalNode8485TCPInternaldfs.namenode.shared.edits.dirJournalNode8480TCPInternal</p>
<h3 id="1-1-5-2-hadoop-mrv1">1.1.5.2 Hadoop MRv1</h3>
<p>ServiceQualifierPortProtocolAccess RequirementConfigurationCommentJobTracker8021TCPExternalmapred.job.trackerJobTracker50030TCPExternalmapred.job.tracker.http.addressJobTrackerThrift Plugin9290TCPInternaljobtracker.thrift.addressRequired by Hue and Cloudera Manager Activity MonitorTaskTracker50060TCPExternalmapred.task.tracker.http.addressTaskTracker0TCPLocalhostmapred.task.tracker.report.addressCommunicating with child (umbilical)</p>
<h3 id="1-1-5-3-hadoop-yarn">1.1.5.3 Hadoop YARN</h3>
<p>ServiceQualifierPortProtocolAccess RequirementConfigurationCommentResourceManager8032TCPyarn.resourcemanager.addressResourceManager8030TCPyarn.resourcemanager.scheduler.addressResourceManager8031TCPyarn.resourcemanager.resource-tracker.addressResourceManager8033TCPyarn.resourcemanager.admin.addressResourceManager8088TCPyarn.resourcemanager.webapp.addressNodeManager8040TCPyarn.nodemanager.localizer.addressNodeManager8042TCPyarn.nodemanager.webapp.addressNodeManager8041TCPyarn.nodemanager.addressMapReduce JobHistory Server10020TCPmapreduce.jobhistory.addressMapReduce JobHistory Server19888TCPmapreduce.jobhistory.webapp.address</p>
<h3 id="1-1-5-4-hbase">1.1.5.4 HBase</h3>
<p>ServiceQualifierPortProtocolAccess RequirementConfigurationCommentMaster60000TCPExternalhbase.master.portIPCMaster60010TCPExternalhbase.master.info.portHTTPRegionServer60020TCPExternalhbase.regionserver.portIPCRegionServer60030TCPExternalhbase.regionserver.info.portHTTPHQuorumPeer2181TCPhbase.zookeeper.property.clientPortHBase-managed ZK modeHQuorumPeer2888TCPhbase.zookeeper.peerportHBase-managed ZK modeHQuorumPeer3888TCPhbase.zookeeper.leaderportHBase-managed ZK modeRESTREST Service8080TCPExternalhbase.rest.portThriftServerThrift Server9090TCPExternalPass -p <port> on CLIAvro server9090TCPExternalPass –port <port> on CLI</p>
<h3 id="1-1-5-5-hive">1.1.5.5 Hive</h3>
<p>ServiceQualifierPortProtocolAccess RequirementConfigurationCommentMetastore9083TCPExternalHiveServer10000TCPExternal</p>
<h3 id="1-1-5-6-sqoop">1.1.5.6 Sqoop</h3>
<p>ServiceQualifierPortProtocolAccess RequirementConfigurationCommentMetastore16000TCPExternalsqoop.metastore.server.portSqoop 2 server12000TCPExternal</p>
<h3 id="1-1-5-7-zookeeper">1.1.5.7 Zookeeper</h3>
<p>ServiceQualifierPortProtocolAccess RequirementConfigurationCommentServer (with CDH4 and/or Cloudera Manager 4)2181TCPExternalclientPortClient portServer (with CDH4 only)2888TCPInternalX in server.N=host:X:YPeerServer (with CDH4 only)3888TCPInternalY in server.N=host:X:YPeerServer (with CDH4 and Cloudera Manager 4)3181TCPInternalX in server.N=host:X:YPeerServer (with CDH4 and Cloudera Manager 4)4181TCPInternalY in server.N=host:X:YPeerZooKeeper FailoverController (ZKFC)8019TCPInternalUsed for HAZooKeeper JMX port9010TCPInternal</p>
<p>As JMX port, ZooKeeper will also use another randomly selected port for RMI. In order for Cloudera Manager to monitor ZooKeeper, you must open up all ports when the connection originates from the Cloudera Manager server.</p>
<h3 id="1-1-5-8-hue">1.1.5.8 Hue</h3>
<p>ServiceQualifierPortProtocolAccess RequirementConfigurationCommentServer8888TCPExternalBeeswax Server8002InternalBeeswax Metastore8003Internal</p>
<h3 id="1-1-5-9-ozzie">1.1.5.9 Ozzie</h3>
<p>ServiceQualifierPortProtocolAccess RequirementConfigurationCommentOozie Server11000TCPExternalOOZIE_HTTP_PORT in oozie-env.shHTTPOozie Server11001TCPlocalhostOOZIE_ADMIN_PORT in oozie-env.shShutdown port</p>
<h3 id="1-1-5-10-ganglia">1.1.5.10 Ganglia</h3>
<p>ServiceQualifierPortProtocolAccess RequirementConfigurationCommentganglia-gmond8649UDP/TCPInternalganglia-web80TCPExternalVia Apache httpd</p>
<h3 id="1-1-5-11-kerberos">1.1.5.11 Kerberos</h3>
<p>ServiceQualifierPortProtocolAccess RequirementConfigurationCommentKRB5 KDC ServerSecure88UDP/TCPExternalkdc_ports and kdc_tcp_ports in either the [kdcdefaults] or [realms] sections of kdc.confBy default only UDPKRB5 Admin ServerSecure749TCPInternalkadmind_port in the [realms] section of kdc.conf</p>
<h3 id="1-2-">1.2 观点</h3>
<h3 id="1-2-1-hadoop-">1.2.1 Hadoop即将过时了吗？</h3>
<p><a href="http://www.kuqin.com/database/20120715/322528.html" target="_blank"><a href="http://www.kuqin.com/database/20120715/322528.html">http://www.kuqin.com/database/20120715/322528.html</a></a></p>
<p>google提出的三个东西都是解决hadoop的软肋，最终目的还是需要解决大数据上面的实时性问题。</p>
<ul>
<li>增量索引过滤器（Percolator for incremental indexing）和频繁变化数据集分析。Hadoop是一台大型“机器”，当启动并全速运转时处理数据的性能惊人，你唯一需要操心的就是硬盘的传输速度跟不上。但是每次你准备启动分析数据时，都需要把所有的数据都过一遍，当数据集越来越庞大时，这个问题将导致分析时间无限延长。那么Google是如何解决让搜索结果返回速度越来越接近实时的呢？答案是用增量处理引擎Percolator代替GMR。通过只处理新增的、改动过的或删除的文档和使用二级指数来高效率建目录，返回查询结果。Percolator论文的作者写道：“将索引系统转换成增量系统…将文档处理延迟缩短了100倍。”这意味着索引web新内容的速度比用MapReduce快100倍！类似大型强子对撞机产生的数据将不断变大，Twitter也是如此。这也是为什么HBase中会新增触发流程，而Twitter Storm正在成为实时处理流数据的热门技术。</li>
<li>用于点对点分析的Dremel。Google和Hadoop生态系统都致力于让MapReduce成为可用的点对点分析工具。从Sawzall到Pig和Hive，创建了大量的界面层，但是尽管这让Hadoop看上去更像SQL系统，但是人们忘记了一个基本事实——MapReduce(以及Hadoop)是为组织数据处理任务开发的系统，诞生于工作流内核，而不是点对点分析。今天有大量的BI/分析查询都是点对点模式，属于互动和低延迟的分析。Hadoop的Map和Reduce工作流让很多分析师望而却步，而且工作启动和完成工作流运行的漫长周期对于很多互动性分析来说意味着糟糕的用户体验。于是，Google发明了Dremel（业界也称之为BigQuery产品）专用工具，可以让分析师数秒钟内就扫描成PB（Petabyte）的数据完成点到点查询，而且还能支持可视化。Google在Dremel的论文中声称：“Dremel能够在数秒内完成数万亿行数据的聚合查询，比MapReduce快上100倍！”</li>
<li>分析图数据的Pregel。Google MapReduce的设计初衷是分析世界上最大的数据图谱——互联网。但是在分析人际网络、电信设备、文档和其他一些图数据时就没有那么灵光了，例如MapReduce在计算单源最短路径（SSSP）时效率非常低下，已有的并行图算法库Parallel BGL或者CGMgraph又没有容错。于是Google开发了Pregel，一个可以在分布式通用服务器上处理PB级别图数据的大型同步处理应用。与Hadoop经常在处理图数据时产生指数级数据放大相比，Pregel能够自然高效地处理SSSP或PageRank等图算法，所用时间要短得多，代码也简洁得多。目前唯一能与Pregel媲美的开源选择是Giraph，这是一个早期的Apache孵化项目，调用了HDFS和Zookeeper。Githb上还有一个项目Golden Orb可用。</li>
</ul>
<h3 id="1-2-2-best-practices-for-selecting-apache-hadoop-hardware">1.2.2 Best Practices for Selecting Apache Hadoop Hardware</h3>
<p><a href="http://hortonworks.com/blog/best-practices-for-selecting-apache-hadoop-hardware/" target="_blank"><a href="http://hortonworks.com/blog/best-practices-for-selecting-apache-hadoop-hardware/">http://hortonworks.com/blog/best-practices-for-selecting-apache-hadoop-hardware/</a></a></p>
<p>RAID cards, redundant power supplies and other per-component reliability features are not needed. Buy error-correcting RAM and SATA drives with good MTBF numbers. Good RAM allows you to trust the quality of your computations. Hard drives are the largest source of failures, so buy decent ones.（不需要选购RAID，冗余电源或者是一些满足高可靠性组件，但是选择带有ECC的RAM以及good MTBF的SATA硬盘却是非常需要的。ECC RAM可以让你确保计算结果的正确性，而SATA故障是大部分故障的主要原因）</p>
<ul>
<li>On CPU: It helps to understand your workload, but for most systems I recommend sticking with medium clock speeds and no more than 2 sockets. Both your upfront costs and power costs rise quickly on the high-end. For many workloads, the extra performance per node is not cost-effective.（没有特别要求，普通频率，dual-socket？？？）</li>
<li>On Power: Power is a major concern when designing Hadoop clusters. It is worth understanding how much power the systems you are buying use and not buying the biggest and fastest nodes on the market.In years past we saw huge savings in pricing and significant power savings by avoiding the fastest CPUs, not buying redundant power supplies, etc. Nowadays, vendors are building machines for cloud data centers that are designed to reduce cost and power and that exclude a lot of the niceties that bulk up traditional servers. Spermicro, Dell and HP all have such product lines for cloud providers, so if you are buying in large volume, it is worth looking for stripped-down cloud servers. （根据自己的需要尽量减少能耗开销，撇去一些不需要的部件。而且现在很多厂商也在尽量减少不必要的部件）</li>
<li>On RAM: What you need to consider is the amount of RAM needed to keep the processors busy and where the knee in the cost curve resides. Right now 48GB seems like a pretty good number. You can get this much RAM at commodity prices on low-end server motherboards. This is enough to provide the Hadoop framework with lots of RAM (~4 GB) and still have plenty to run many processes. Don’t worry too much about RAM, you’ll find a use for it, often running more processes in parallel. If you don’t, the system will still use it to good effect, caching disk data and improving performance.（RAM方面的话越大越好，对于48GB的RAM来说普通的主板也是支持的。如果RAM用的上的话那么允许多个进程并行执行，如果暂时永不上的话可以做cache来提高速度）</li>
<li>On Disk: Look to buy high-capacity SATA drives, usually 7200RPM. Hadoop is storage hungry and seek efficient but it does not require fast, expensive hard drives. Keep in mind that with 12-drive systems you are generally getting 24 or 36 TB/node. Until recently, putting this much storage in a node was not practical because, in large clusters, disk failures are a regular occurrence and replicating 24+TB could swamp the network for long enough to really disrupt work and cause jobs to miss SLAs. The most recent release of Hadoop 0.20.204 is engineered to handle the failure of drives more elegantly, allowing machines to continue serving from their remaining drives. With these changes, we expect to see a lot of 12+ drive systems. In general, add disks for storage and not seeks. If your workload does not require huge amounts of storage, dropping disk count to 6 or 4 per box is a reasonable way to economize.（高容量SATA硬盘，最好是7.2KRPM，并且最好单机上面挂在12个硬盘。对于hadoop之前这种方式并不实际，因为磁盘非常容易损坏并且备份这24TB的数据非常耗时。而hadoop可以很好地解决这个问题。</li>
</ul>
<p>小集群来说的话，通常单个机器上面挂在4-6个disk即可）</p>
<ul>
<li>On Network: This is the hardest variable to nail down. Hadoop workloads vary a lot. The key is to buy enough network capacity to allow all nodes in your cluster to communicate with each other at reasonable speeds and for reasonable cost. For smaller clusters, I’d recommend at least 1GB all-to-all bandwidth, which is easily achieved by just connecting all of your nodes to a good switch. With larger clusters this is still a good target although based on workload you can probably go lower. In the very large data centers the Yahoo! built, they are seeing 2/<em>10GB per 20 node rack going up to a pair of central switches, with rack nodes connected with two 1GB links. As a rule of thumb, watch the ratio of network-to-computer cost and aim for network cost being somewhere around 20% of your total cost. Network costs should include your complete network, core switches, rack switches, any network cards needed, etc. We’ve been seeing InfiniBand and 10GB Ethernet networks to the node now. If you can build this cost effectively, that’s great. However, keep in mind that Hadoop grew up with commodity Ethernet, so understand your workload requirements before spending too much on the network.（这个主要还是看需求。通常来说网络整体开销占据所有开销的20%，包括核心交换机，机架之间的交换机以及网卡设备等。yahoo大集群的部署方式是rack之间使用2/</em>10GB的核心交换机工作，而20个节点的rack之间内部使用1GB链路）。<h3 id="1-2-3-the-dark-side-of-hadoop-backtype-technology">1.2.3 The dark side of Hadoop - BackType Technology</h3>
</li>
</ul>
<p><a href="http://web.archive.org/web/20110510125644/http://tech.backtype.com/the-dark-side-of-hadoop" target="_blank"><a href="http://web.archive.org/web/20110510125644/http://tech.backtype.com/the-dark-side-of-hadoop">http://web.archive.org/web/20110510125644/http://tech.backtype.com/the-dark-side-of-hadoop</a></a></p>
<p>谈到了一些在使用hadoop出现的一些问题，而这些问题是hadoop本身的。</p>
<ul>
<li>Critical configuration poorly documented 一些关键的参数和配置并没有很好地说明清楚。</li>
<li><p>Terrible with memory usage 内存使用上面存在问题。hadoop里面有一些非常sloppy的实现，比如chmod以及ln -s等操作，并没有调用fs API而是直接创建一个shell进程来完成。因为fork出一个shell进程需要申请同样大小的内存（虽然实现上是COW），但是这样造成jvm出现oom。解决的办法是开辟一定空间的swap The solution to these memory problems is to allocate a healthy amount of swap space for each machine to protect you from these memory glitches. We couldn&#39;t believe how much more stable everything became when we added swap space to our worker machines.</p>
</li>
<li><p>Thomas Jungblut&#39;s Blog: Dealing with &quot;OutOfMemoryError&quot; in Hadoop <a href="http://codingwiththomas.blogspot.jp/2011/07/dealing-with-outofmemoryerror-in-hadoop.html" target="_blank"><a href="http://codingwiththomas.blogspot.jp/2011/07/dealing-with-outofmemoryerror-in-hadoop.html">http://codingwiththomas.blogspot.jp/2011/07/dealing-with-outofmemoryerror-in-hadoop.html</a></a> 作者给出的解决办法就是修改hadoop的代码，通过调用Java API而不是使用ProcessBuilder来解决。</p>
</li>
<li><strong>NOTE(dirlt):出现OOM的话必须区分JVM还是Linux System本身的OOM。JVM出现OOM是抛出异常，而Linux出现OOM是会触发OOM killer</strong></li>
<li>Zombies hadoop集群出现一些zombie进程，而这些进程会一直持有内存直到大量zombie进程存在最后需要重启。造成这些zombie进程的原因通常是因为jvm oom（增加了swap之后就没有出现这个问题了），但是奇怪的是tasktracker作为这些process的parent，并不负责cleanup这些zombie进程而是依赖这些zombie进程的自己退出，这就是hadoop设计方面的问题。</li>
</ul>
<p>Making Hadoop easy to deploy, use, and operate should be the /#1 priority for the developers of Hadoop.</p>
<h3 id="1-3-">1.3 使用问题</h3>
<h3 id="1-3-1-cdh3u3-">1.3.1 CDH3u3搭建单节点集群</h3>
<p>搭建单节点集群允许我们在单机做一些模拟或者是测试，还是非常有意义的。如何操作的话可以参考链接 <a href="http://localhost/utils/hadoop-0.20.2-cdh3u3/docs/single_node_setup.html" target="_blank"><a href="http://localhost/utils/hadoop-0.20.2-cdh3u3/docs/single_node_setup.html">http://localhost/utils/hadoop-0.20.2-cdh3u3/docs/single_node_setup.html</a></a></p>
<p>这里稍微总结一下：</p>
<ul>
<li>首先安装ssh和rsync /# sudo apt-get install ssh &amp;&amp; sudo apt-get install rsync</li>
<li>本机建立好信任关系 /# cat ~/.ssh/id_dsa.pub &gt;&gt; ~/.ssh/authorized_keys</li>
<li>将{hadoop-package}/conf配置文件修改如下：</li>
<li><p>conf/core-site.xml</p>
<configuration>

   <property>
       <name>fs.default.name</name>

       <value>hdfs://localhost:9000</value>
   </property>

</li>
</ul>
<p></configuration></p>
<ul>
<li><p>conf/hdfs-site.xml</p>
<configuration>

   <property>
       <name>dfs.replication</name>

       <value>1</value>
   </property>

</li>
</ul>
<p></configuration></p>
<ul>
<li><p>conf/mapred-site.xml</p>
<configuration>

   <property>
       <name>mapred.job.tracker</name>

       <value>localhost:9001</value>
   </property>

</li>
</ul>
<p></configuration></p>
<ul>
<li>格式化namenode /# bin/hadoop namenode -format</li>
<li>启动hadoop集群 /# bin/start-all.sh</li>
<li>停止hadoop集群 /# bin/stop-all.sh</li>
<li><p>webconsole</p>
</li>
<li><p>NameNode - <a href="http://localhost:50070/" target="_blank"><a href="http://localhost:50070/">http://localhost:50070/</a></a></p>
</li>
<li>JobTracker - <a href="http://localhost:50030/" target="_blank"><a href="http://localhost:50030/">http://localhost:50030/</a></a></li>
</ul>
<h3 id="1-3-2-cdh4-2-0-">1.3.2 CDH4.2.0搭建单节点集群</h3>
<p>基本流程和CDH3u3是相同的，但是有一些差异我记录下来。</p>
<ul>
<li><p>配置文件</p>
</li>
<li><p>配置文件在etc/hadoop，包括环境配置脚本比如hadoop-env.sh</p>
</li>
<li>bin/sbin目录下面有hadoop集群启动停止工具 <strong>NOTE（dirlt）：不要使用它们</strong></li>
<li>libexec目录下面是公用的配置脚本</li>
<li>mapred-site.xml中jobtracker地址配置key修改为 mapred.jobtracker.address <strong>NOTE(dirlt):this for yarn.如果是mr1那么不用修改,依然是mapred.job.tracker</strong></li>
<li>hadoop-daemons.sh会使用/sbin/slaves.sh来在各个节点启动，但是 /<em>不知道什么原因，很多环境变量没有设置/</em> ，所以在slaves.sh执行ssh命令部分最开始增加了 source ~/.shrc; 来强制设置我的环境变量</li>
<li><strong>NOTE(dirlt):不要使用shell脚本来启动，而是直接使用类似hadoop namenode这种方式来启动单个机器上的实例</strong></li>
<li><p>公共组件</p>
</li>
<li><p>CDH4.2.0 native-library都放在了目录lib/native下面，而不是CDH3u3的lib/native/Linux-amd64-64下面，这点需要注意。</p>
</li>
<li>CDH4.2.0 没有自带libhadoop.so, 所以启动的时候都会出现 ”Unable to load native-hadoop library for your platform… using builtin-java classes where applicable“ 这个警告。需要自己编译放到lib/native目录下面。</li>
<li>CDH4.2.0 lib下面没有任何文件，所有的lib都在share/hadoop//*/lib下面，比如share/hadoop/common/lib. 这点和CDH3有差别，CDH3所有的jar都放在lib目录下面。使用 hadoop classpath 命令可以察看</li>
<li><p>环境变量</p>
</li>
<li><p>JAVA_LIBRARY_PATH用来设置native library path</p>
</li>
<li>HADOOP_CLASSPATH可以用来设置hadoop相关的classpath（比如使用hadoop-lzo等）</li>
<li><p>准备工作</p>
</li>
<li><p>使用hdfs namenode -format来做格式化 <strong>注意如果使用sudo apt-get来安装的话，是其他用户比如hdfs,impala,mapred,yarn来启动的，所以必须确保目录对于这些用户是可写的</strong></p>
</li>
<li>使用命令 hadoop org/apache/hadoop/examples/QuasiMonteCarlo 1 1 确定集群是否可以正常运行。<h3 id="1-3-3-cdh4-3-0">1.3.3 CDH4.3.0</h3>
</li>
</ul>
<p>基本流程和CDH4.2.0是相同的，但是存在一些差异我记录下来的。从4.3.0开始将mr1和mr2分开存放，还是一个比较大的区别的。这里我以使用mr1为例。</p>
<ul>
<li>在libexec/hadoop-config.sh添加source ~/.shrc 来强制设置环境变量。</li>
<li><p>mr1和mr2分开存放主要有</p>
</li>
<li><p>etc目录，hadoop and hadoop-mapreduce1</p>
</li>
<li>bin目录，bin and bin-mapreduce1</li>
<li><p>lib目录。如果需要使用mr1的话，那么将cp -r share/hadoop/mapreduce1/ .</p>
</li>
<li><p><strong>NOTE（dirlt）：似乎只需要最顶层的一些jar文件即可</strong></p>
</li>
<li>在bin/hadoop-config.sh添加source ~/.shrc 来强制设置环境变量。</li>
<li><strong>NOTE（dirlt）：不要使用start-dfs.sh这些脚本启动，似乎这些脚本会去读取master,slaves这些文件然后逐个上去ssh启动。直接使用hadoop namenode这种方式可以只启动单个机器上的实例</strong></li>
</ul>
<h3 id="1-3-4-configuration">1.3.4 Configuration</h3>
<h3 id="1-3-4-1-bash_profile">1.3.4.1 .bash_profile</h3>
<p>export HADOOP_HOME=$HOME/dirlt/hadoop-2.0.0-cdh4.3.0/</p>
<p>export HADOOP_CONF_DIR=$HADOOP_HOME/etc/hadoop
export HBASE_HOME=/home/alium_zhanyinan/dirlt/hbase-0.94.6-cdh4.3.0</p>
<p>export HBASE_CLASSPATH=$HBASE_HOME/hbase-0.94.6-cdh4.3.0-security.jar:$HBASE_HOME/conf
export ZK_HOME=/home/alium_zhanyinan/dirlt/zookeeper-3.4.5-cdh4.3.0</p>
<p>export ZK_CLASSPATH=$ZK_HOME/zookeeper-3.4.5-cdh4.3.0.jar
export HADOOP_CLASSPATH=$HADOOP_CLASSPATH:$HBASE_CLASSPATH:$ZK_CLASSPATH</p>
<p>export JAVA_HOME=/usr/java/default/</p>
<h3 id="1-3-4-2-core-site-xml">1.3.4.2 core-site.xml</h3>
<configuration>

  <property>
    <name>fs.default.name</name>

    <value>hdfs://umengds1.mob.cm3:8020</value>
  </property>


  <property>

    <name>fs.trash.interval</name>
    <value>1440</value>

  </property>
</configuration>


<h3 id="1-3-4-3-hdfs-site-xml">1.3.4.3 hdfs-site.xml</h3>
<configuration>

  <property>
    <name>dfs.name.dir</name>

    <value>/disk1/data/dfs/nn</value>
  </property>


  <property>

    <name>dfs.data.dir</name>
    <value>/disk1/data/dfs/dn</value>

  </property>


  <property>
    <name>fs.checkpoint.dir</name>

    <value>/disk1/data/dfs/snn</value>
  </property>


  <property>

    <name>dfs.replication</name>
    <value>3</value>

  </property>


  <property>
    <name>dfs.block.size</name>

    <value>134217728</value>
  </property>


  <property>

    <name>dfs.datanode.max.xcievers</name>
    <value>8192</value>

  </property>


  <property>
    <name>dfs.datanode.du.reserved</name>

    <value>21474836480</value>
  </property>


  <property>

    <name>dfs.namenode.handler.count</name>
    <value>64</value>

  </property>


  <property>
    <name>dfs.datanode.handler.count</name>

    <value>32</value>
  </property>


  <property>

    <name>dfs.client.read.shortcircuit</name>
    <value>true</value>

  </property>
</configuration>



<h3 id="1-3-4-4-mapred-site-xml">1.3.4.4 mapred-site.xml</h3>
<configuration>

  <property>
    <name>mapred.job.tracker</name>

    <value>umengds2.mob.cm3:8021</value>
  </property>


  <property>

    <name>mapred.system.dir</name>
    <value>/tmp/mapred/system</value>

  </property>


  <property>
    <name>mapreduce.jobtracker.staging.root.dir</name>

    <value>/user</value>
  </property>


  <property>

    <name>mapred.local.dir</name>
    <value>/disk1/data/mapred/local</value>

  </property>


  <property>
    <name>mapred.submit.replication</name>

    <value>3</value>
    <final>true</final>

  </property>


  <property>
    <name>mapred.tasktracker.map.tasks.maximum</name>

    <value>6</value>
  </property>

  <property>
    <name>mapred.tasktracker.reduce.tasks.maximum</name>

    <value>8</value>
  </property>


  <property>

    <name>mapred.child.java.opts</name>
    <value> -Xmx2048M -XX:-UseGCOverheadLimit</value>

  </property>


  <property>
    <name>mapred.job.tracker.handler.count</name>

    <value>64</value>
  </property>


  <property>

    <name>io.sort.mb</name>
    <value>256</value>

  </property>


  <property>
    <name>io.sort.factor</name>

    <value>64</value>
  </property>

</configuration>

<h3 id="1-3-4-5-hadoop-env-sh">1.3.4.5 hadoop-env.sh</h3>
<p>/# The maximum amount of heap to use, in MB. Default is 1000.</p>
<p>export HADOOP_HEAPSIZE=6000</p>
<p>/# Extra Java runtime options. Empty by default.
/# if [&quot;$HADOOP_OPTS&quot; == &quot;&quot; ]; then export HADOOP_OPTS=-server; else HADOOP_OPTS+=&quot; -server&quot;</p>
<p>; fi</p>
<p>/# Command specific options appended to HADOOP_OPTS when specified
export HADOOP_NAMENODE_OPTS=&quot;-Xmx12000m $HADOOP_NAMENODE_OPTS&quot;export HADOOP_SECONDARYNAMENODE_OPTS=&quot;-Xmx12000m $HADOOP_SECONDARYNAMENODE_OPTS&quot;export HADOOP_DATANODE_OPTS=&quot;-Xmx6000m $HADOOP_DATANODE_OPTS&quot;export HADOOP_BALANCER_OPTS=&quot;-Xmx3000m $HADOOP_BALANCER_OPTS&quot;export HADOOP_JOBTRACKER_OPTS=&quot;-Xmx12000m $HADOOP_JOBTRACKER_OPTS&quot;</p>
<h3 id="1-3-4-6-hbase-site-xml">1.3.4.6 hbase-site.xml</h3>
<configuration>

  <property>
    <name>hbase.cluster.distributed</name>

    <value>true</value>
  </property>


  <property>

    <name>hbase.rootdir</name>
    <value>hdfs://umengds1.mob.cm3:8020/hbase</value>

  </property>


  <property>
    <name>hbase.zookeeper.quorum</name>

    <value>umengds1.mob.cm3,umengds2.mob.cm3</value>
  </property>


  <property>

    <name>hbase.hregion.memstore.mslab.enabled</name>
    <value>true</value>

  </property>


  <property>
    <name>hbase.regionserver.handler.count</name>

    <value>128</value>
  </property>


  <property>

    <name>hbase.client.write.buffer</name>
    <value>4194304</value>

  </property>


  <property>
    <name>hbase.hregion.memstore.block.multiplier</name>

    <value>8</value>
  </property>


  <property>

    <name>hbase.server.thread.wakefrequency</name>
    <value>1000</value>

  </property>


  <property>
    <name>hbase.regionserver.lease.period</name>

    <value>600000</value>
  </property>


  <property>

    <name>hbase.hstore.blockingStoreFiles</name>
    <value>15</value>

  </property>


  <property>
    <name>hbase.hregion.max.filesize</name>

    <value>2147483648</value>
  </property>


  <property>

    <name>hbase.ipc.client.tcpnodelay</name>
    <value>true</value>

  </property>


  <property>
    <name>ipc.ping.interval</name>

    <value>10000</value>
  </property>


  <property>

    <name>hbase.hregion.majorcompaction</name>
    <value>0</value>

  </property>


  <property>
    <name>hbase.regionserver.checksum.verify</name>

    <value>true</value>
  </property>

</configuration>

<h3 id="1-3-4-7-hbase-env-sh">1.3.4.7 hbase-env.sh</h3>
<p>/# The maximum amount of heap to use, in MB. Default is 1000.</p>
<p>export HBASE_HEAPSIZE=14000</p>
<p>/# Extra Java runtime options.
/# Below are what we set by default. May only work with SUN JVM.</p>
<p>/# For more on why as well as other possible settings,
/# see <a href="http://wiki.apache.org/hadoop/PerformanceTuning" target="_blank">http://wiki.apache.org/hadoop/PerformanceTuning</a></p>
<p>/# export HBASE_OPTS=
&quot;-ea -XX:+UseConcMarkSweepGC -XX:+CMSIncrementalMode&quot;export HBASE_OPTS=&quot;-ea -XX:+UseParNewGC -XX:+UseConcMarkSweepGC -XX:+UseCMSCompactAtFullCollection -XX:CMSInitiatingOccupancyFraction=90&quot;</p>
<h3 id="1-4-hadoop-">1.4 Hadoop权威指南</h3>
<h3 id="1-4-1-hadoop">1.4.1 初识Hadoop</h3>
<p>古代，人们用牛来拉中午，当一头牛拉不动一根圆木的时候，他们不曾想过培育更大更壮的牛。同样，我们也不需要尝试开发超级计算机，而应试着结合使用更多计算机系统。</p>
<h3 id="1-4-2-mapreduce">1.4.2 关于MapReduce</h3>
<ul>
<li>设置HADOOP_CLASSPATH就可以直接使用hadoop CLASSNAME来在本地运行mapreduce程序。</li>
<li><p>hadoop jar $HADOOP_HOME/contrib/streaming/hadoop-streaming-0.20.2-cdh3u3.jar 可以用来启动streaming任务</p>
</li>
<li><p>使用stdin/stdout来作为输入和输出</p>
</li>
<li><p><strong>NOTE（dirlt）：倒是可以探索一下如何使用，但是觉得能力有限</strong></p>
</li>
<li><p>Input/Output Format</p>
</li>
<li>外围环境的访问比如访问hdfs以及hbase</li>
<li>程序打包。比如使用很多第三方库的话在其他机器上面没有部署。</li>
<li><p>hadoop pipes 可以用来启动pipes任务</p>
</li>
<li><p>Hadoop的Pipes是Hadoop MapReduce的C++接口代称</p>
</li>
<li>使用Unix Domain Socket来作为输入和输出</li>
<li><p><strong>NOTE（dirlt）：可能使用上面还是没有native mr或者是streaming方式方便</strong></p>
<h3 id="1-4-3-hadoop-">1.4.3 Hadoop分布式文件系统</h3>
</li>
<li><p>使用hadoop archive能够将大量小文档打包，存档文件之能够只读访问</p>
</li>
<li><p>使用hadoop archive -archiveName <file>.har -p <parent-path> src dst</p>
</li>
<li><p>存档过程使用mapreduce完成，输出结果为目录</p>
</li>
<li><p>part-0 表示存档内容文件，应该是使用一个reduce做聚合。</p>
</li>
<li>_index,_masterindex 是对存档内容文件的索引文件。</li>
<li><p>har(hadoop archive)文件系统是建立在其他文件系统上面的，比如hdfs或者是local fs.</p>
</li>
<li><p>hadoop fs -ls har:///file.har 那么访问的是默认的文件系统上面的file.har</p>
</li>
<li>如果想显示地访问hdfs文件系统的话，那么可以hadoop fs -ls har://hdfs-localhost:9000/file.har</li>
<li>如果想显示地访问本地文件系统的话，那么可以使用hadoop fs -ls har://file-localhost/file.har</li>
<li>hadoop fs -ls har://schema-<host>/<path> 是通用的访问方式</li>
</ul>
<h3 id="1-4-4-hadoop-io">1.4.4 Hadoop IO</h3>
<ul>
<li><p>文件系统</p>
</li>
<li><p>ChecksumFileSystem</p>
</li>
<li><p>使用decorator设计模式，底层filesystem称为RawFileSystem</p>
</li>
<li>对于每个文件filename都会创建.filename.crc文件存储校验和</li>
<li>计算crc的单位大小通过io.bytes.per.checksum来进行控制</li>
<li>读取文件如果出现错误的话，那么会抛出ChecksumException</li>
<li><p>考虑到存在多副本的情况，如果读取某个副本出错的话，期间那么会调用reportChecksumFailure方法</p>
</li>
<li><p><strong>NOTE（dirlt）：这个部分的代码不太好读，非常绕</strong></p>
</li>
<li><p>RawLocalFileSystem</p>
</li>
<li><p>本地文件系统</p>
</li>
<li><p>LocalFileSystem</p>
</li>
<li><p>RawLocalFileSystem + ChecksumFileSystem</p>
</li>
<li>reportChecksumFailure实现为将校验和存在问题的文件移动到bad_files边际文件夹（side directory）</li>
<li><p>DistributedFileSystem</p>
</li>
<li><p>分布式文件系统</p>
</li>
<li><p>ChecksumDistributedFileSystem</p>
</li>
<li><p>DistributedFileSystem + ChecksumFileSystem</p>
</li>
<li><p>压缩解压</p>
</li>
<li><p>DEFLATE org.apache.hadoop.io.compress.DefaultCodec 扩展名.defalte</p>
</li>
<li>Gzip org.apache.hadoop.io.compress.GzipCodec 扩展名.gz 使用DEFLATE算法但是增加了额外的文件头。</li>
<li>bzip2 org.apache.hadoop.io.compress.BZip2Codec 扩展名.bz2 自身支持文件切分，内置同步点。</li>
<li><p>LZO com.hadoop.compression.lzo.LzopCodec 扩展名.lzo 和lzop工具兼容，LZO算法增加了额外的文件头。</p>
</li>
<li><p>LzopCodec则是纯lzo格式的codec,使用.lzo_deflate作为文件扩展名</p>
</li>
<li>因为LZO代码库拥有GPL许可，因此没有办法包含在Apache的发行版本里面。</li>
<li><p>运行MapReduce时候可能需要针对不同压缩文件解压读取，就需要构造CompressionCodec对象，我们可以通过CompressionCodecFactory来构造这个对象</p>
</li>
<li><p>CompressionCodecFactory读取变量io.compression.codecs</p>
</li>
<li>然后根据输入文件的扩展名来选择使用何种codec.</li>
<li>getDefaultExtension</li>
<li><p>压缩和解压算法可能同时存在Java实现和原生实现</p>
</li>
<li><p>如果是原生实现的话通常是.so，那么需要设置java.library.path或者是在环境变量里面设置LD_LIBRARY_PATH</p>
</li>
<li>如果同时有原生实现和Java实现，我们想只是使用原生实现的话，那么可以设置hadoop.native.lib = false来禁用原生实现。</li>
<li><p>压缩算法涉及到对应的InputFormat,也就涉及到是否支持切分</p>
</li>
<li><p>对于一些不支持切分的文件，可能存在一些外部工具来建立索引，从而支持切分。</p>
</li>
<li><p>下面这些选项可以针对map结果以及mapreduce结果进行压缩</p>
</li>
<li><p>mapred.output.compress = true 将mapreduce结果做压缩</p>
</li>
<li>mapred.output.compression.codec mapreduce压缩格式</li>
<li>mapred.output.compress.type = BLOCK/RECORD 如果输出格式为SequenceFile的话，那么这个参数可以控制是块压缩还是记录压缩</li>
<li><strong>NOTE（dirlt）：我现在强烈感觉MR的中间结果存储格式为SequenceFile</strong></li>
<li><strong>NOTE（dirlt）：应该是IFile，但是是否共享了这个配置呢？</strong></li>
<li>mapred.compress.map.output = true 将map结果做压缩</li>
<li><p>mapred.map.output.compression.codec map压缩格式</p>
</li>
<li><p>序列化</p>
</li>
<li><p>Hadoop的序列化都是基于Writable实现的，WritableComparable则是同时继承Writable,Comparable<T>.</p>
</li>
<li><p>序列化对象需要实现RawComparator，接口为public int compare(byte[] b1, int s1, int l1, byte[] b2, int s2, int l2)进行二进制比较。</p>
</li>
<li><p>WritableComparator简化了这个实现，继承WritableComparator就实现了这个接口</p>
</li>
<li>但是这个接口实现起来非常naive，就是将两个byte stream反序列化然后调用对象的compareTo实现</li>
<li>如果想要提高效率的话，可以考虑通过直接比较两个byte stream来做优化。</li>
<li><p>基于文件的数据结构</p>
</li>
<li><p>SequenceFile 主要用来存储KV数据结构，多条记录之间会穿插一些同步标记，因此允许进行切分。</p>
</li>
<li><p>使用SequenceFileInputFormat和SequenceFileOutputFormat来读取和输出SequenceFile</p>
</li>
<li>hadoop fs -text 可以用来读取文件</li>
<li><p>mapred.output.compress.type = BLOCK/RECORD 可以用来控制压缩方式</p>
</li>
<li><p>如果没有使用压缩的话，那么格式为 recordLength(4byte) + keyLength(4byte) + key + value</p>
</li>
<li>如果使用记录压缩的话，那么格式为 recordLnegth(4byte) + keyLength(4byte) + key + compressedValue</li>
<li>如果使用块压缩的话，那么格式为 numberRecord(1-5byte) + keyLength(4byte) + compressedKeys + valueLength(4byte) + compressedValues.每个block之间会插入sync标记</li>
<li>块压缩大小可以使用io.seqfile.compress.blocksize来控制，默认1MB</li>
<li><p>MapFile 也是用来存储KV数据结构，但是可以认为已经按照了Key进行排序 <strong>NOTE（dirlt）：要求添加顺序就按照Key排序</strong></p>
</li>
<li><p>存储格式实际上也是SequenceFile，data，index都是。</p>
</li>
<li>底层会建立index，index在搜索的时候会加载到内存里面，这样可以减少data上的随机查询次数。</li>
<li>使用io.map.index.interval可以控制多少个item在index里面创建一个条目</li>
<li>使用io.map.index.skip = 0/1/2/n 可以控制skip几个index的item，如果为1的话那么表示只是使用1/2的索引。</li>
<li><p>从SequenceFile创建MapFile非常简单</p>
</li>
<li><p>首先使用sort将SequenceFile进行排序(可以使用hadoop example的sort）</p>
</li>
<li><p>然后调用hadoop MapFileFixer来建立索引</p>
<h3 id="1-4-5-mapreduce-">1.4.5 MapReduce应用开发</h3>
</li>
<li><p>Configuration用来读取配置文件，功能还是比较强大的，有变量替换的功能</p>
</li>
<li><property><name>…</name><value>…</value></property></li>
<li>如果使用<final>true</final>标记的话那么这个变量不允许被重置</li>
<li>变量替换可以使用${variable}</li>
<li><p>通过addResource来添加读取的配置文件</p>
</li>
<li><p>Hadoop集群有三种工作方式，分别为</p>
</li>
<li><p>standalone 使用单个JVM进程来模拟</p>
</li>
<li><p>如果不进行任何配置的话默认使用这个模式 <strong>NOTE（dirlt）：这个模式确实不错</strong></p>
</li>
<li>fs.default.name = file 本地文件系统</li>
<li>mapred.job.tracker = local</li>
<li><p>pseudo-distributed 本地启动单节点集群</p>
</li>
<li><p>fs.default.name = hdfs://localhost</p>
</li>
<li>mapred.job.tracker = localhost:8021</li>
<li><p>fully-distributed 完全分布式环境</p>
</li>
<li><p>fs.default.name = hdfs://<namenode></p>
</li>
<li><p>mapred.job.tracer = <jobtracker>:8021</p>
</li>
<li><p>使用hadoop启动MapReduce任务的常用参数</p>
</li>
</ul>
<ol>
<li>-D property=value 覆盖默认配置属性</li>
<li>-conf filename 添加配置文件</li>
<li>-fs uri 设置默认文件系统</li>
<li>-jt host:port 设置jobtracker</li>
<li>-files file,file2 这些文件可以在tasktracker工作目录下面访问</li>
<li>-archives archive,archive2 和files类似，但是是存档文件</li>
</ol>
<ul>
<li>突然觉得这个差别在files只能是平级结构，而archive可以是层级结构。</li>
<li>-libjars jar1,jar2 和files类似，通常这些JAR文件是MapReduce所需要的。</li>
</ul>
<p>如果希望运行时候动态创建集群的话，可以通过这几个类来创建</p>
<ul>
<li>MiniDFSCluster</li>
<li>MiniMRCluster</li>
<li>MiniHBaseCluster</li>
<li>MiniZooKeeperClutser</li>
<li><strong>NOTE(dirlt):都称为Mini???Cluster？</strong></li>
</ul>
<p>另外还有自带的ClusterMapReduceTestCase以及HBaseTestingUtility来帮助进行mapreduce的testcase. 这些类散步在hadoop,hbase,hadoop-test以及hbase-test里面。</p>
<p><strong>NOTE（dirlt）：但是个人觉得可能还是没有本地测试方便，不过倒是可以试试</strong></p>
<p>job，task and attempt</p>
<ul>
<li><p>jobID常见格式为 job_200904110811_0002</p>
</li>
<li><p>其中200904110811表示jobtracker从2009.04.11的08:11启动的</p>
</li>
<li>0002 表示第三个job,从0000开始计数。超过10000的话就不能够很好地排序</li>
<li><p>taskID常见格式为 task_200904110811_0002_m_000003</p>
</li>
<li><p>前面一串数字和jobID匹配，表示从属于这个job</p>
</li>
<li>m表示map任务，r表示reduce任务</li>
<li>000003表示这是第4个map任务。顺序是在初始化时候指定的，并不反应具体的执行顺序。</li>
<li><p>attemptID常见格式为 attempt_200904110811_0002_m_000003_0</p>
</li>
<li><p>前面一串数字和taskID匹配，表示从属与这个task</p>
</li>
<li>attempt出现的原因是因为一个task可能会因为失败重启或者是预测执行而执行多次</li>
<li>如果jobtracker重启而导致作业重启的话，那么做后面id从1000开始避免和原来的attempt冲突。</li>
</ul>
<p>作业调试</p>
<ul>
<li><p>相关配置</p>
</li>
<li><p>mapred.jobtracker.completeuserjobs.maximum 表示web页面下面展示completed jobs的个数，默认是100，超过的部分放到历史信息页。</p>
</li>
<li>mapred.jobtracker.restart.recover = true jobtracker重启之后自动恢复作业</li>
<li>hadoop.job.history.location 历史作业信息存放位置，超过30天删除，默认在_logs/history</li>
<li>hadoop.job.history.user.location 如果不为none那么历史作业信息在这里也会存在一份，不会删除。</li>
<li><p>相关命令</p>
</li>
<li><p>hadoop fs -getmerge <src> <dst> 能够将hdfs的src下面所有的文件merge合并成为一份文件并且copy到本地</p>
</li>
<li>hadoop job -history 察看作业历史</li>
<li>hadoop job -counter 察看作业计数器</li>
<li><p>相关日志</p>
</li>
<li><p>系统守护进程日志 写入HADOOP_LOG_DIR里面，可以用来监控namenode以及datanode的运行情况</p>
</li>
<li>MapReduce作业历史日志 _logs/history</li>
<li>MapReduce任务日志 写入HADOOP_LOG_DIR/userlogs里面，可以用来监控每个job的运行情况</li>
<li><p>分析任务</p>
</li>
<li><p>JobConf允许设置profile参数 <strong>NOTE（dirlt）：新的接口里面JobConf-&gt;JobContext-&gt;Job，Job没有这些接口，但是可以通过Configuration来设置</strong></p>
</li>
<li><p>setProfileEnabled 打开profile功能，默认false，属性 mapred.task.profile</p>
</li>
<li><p>setProfileParams 设置profile参数</p>
</li>
<li><p>属性 mapred.task.profile.params</p>
</li>
<li>默认使用hprof -agentlib:hprof=cpu=samples,heap=sites,force=n,thread=y,verbose=n,file=%s&quot;</li>
<li>其中%s会替换成为profile输出文件</li>
<li><strong>NOTE（dirlt）：其实这里似乎也可以设置成为jmxremote来通过jvisualvm来调试</strong></li>
<li><p>setProfileTaskRange(boolean,String)</p>
</li>
<li><p>参数1表示针对map还是reduce task做profile, true表示map, false表示reduce</p>
</li>
<li>参数2表示针对哪些tasks做优化，&quot;0-2&quot;表示针对0，1，2三个任务，默认也是&quot;0-2&quot;</li>
<li>map task对应属性mapred.task.profile.maps，reduce task对应属性mapred.task.profile.reduces</li>
<li><p>任务重现</p>
</li>
<li><p>首先将keep.failed.task.files设置为true,这样如果任务失败的话，那么这个任务的输入和输出都会保留下来</p>
</li>
<li><p>如果是map任务的话，那么输入分别会在本地保留</p>
</li>
<li>如果是reduce任务的话，那么对应的map任务输出会在本地保留</li>
<li>然后我们使用hadoop IsolationRunner job.xml来重新运行这个任务</li>
<li>可以修改HADOOP_OPTS添加远程调试选项来启动这个任务。</li>
<li>如果希望任务都保留而不仅仅是失败任务保留的话，那么可以设置 keep.task.files.pattern 为正则表达式（与保留的任务ID匹配）</li>
</ul>
<h3 id="1-4-6-mapreduce-">1.4.6 MapReduce的工作机制</h3>
<p>Hadoop运行MapReduce作业的工作原理</p>
<p><img src="" alt="./images/mapreduce-workflow-architecture.png"></p>
<p>其中有几点需要注意的：</p>
<ul>
<li>计算分片信息是在本地完成的，分片信息和其他resouce(包括jars,files,archives等）一起copy到HDFS上面，然后jobtracker直接读取分片信息。</li>
<li>提交的资源可以设置replication数目，高副本数目可以缓解tasktracker获取resource的压力。参数是mapred.submit.replication.</li>
<li>对于streaming以及pipes的实现，无非就是task并不直接执行任务，而是开辟另外一个子进程来运行streaming或者是pipes的程序。</li>
</ul>
<p><img src="" alt="./images/mapreduce-streamming-pipes.jpg"></p>
<p>进度和状态的更新</p>
<ul>
<li>map任务进度是已经处理输入的比例</li>
<li><p>reduce任务进度分为三个部分</p>
</li>
<li><p>shuffle 1/3</p>
</li>
<li>sort 1/3</li>
<li>reduce 1/3</li>
<li>也就是说如果刚运行完成sort的话，那么进度是2/3</li>
<li><p>状态的更新</p>
</li>
<li><p>触发事件</p>
</li>
<li><p>读取记录</p>
</li>
<li>输出记录</li>
<li>修改状态 reporter的setStatus</li>
<li>计数器修改</li>
<li>reporter的progress</li>
<li><p>子进程有单独线程每隔3秒检查progress位是否设置，如果设置的话那么和tasktracker发起心跳</p>
</li>
<li><p>通过mapred.task.timeout控制</p>
</li>
<li><p>tasktracker每隔5秒和jobtracker做心跳</p>
</li>
<li><p>心跳时间通过 mapred.tasktracker.expircy.interval 设置</p>
</li>
<li><p>jobClient定期会去jobtracker询问job是否完成</p>
</li>
<li><p>jobClient也可以设置属性job.end.notification.url,任务完成jobtracker会调用这个url</p>
</li>
<li>可以认为就是推拉方式的结合。</li>
</ul>
<p>失败检测和处理</p>
<ul>
<li><p>任务失败</p>
</li>
<li><p>子进程抛出异常的话，tasktracker将异常信息记录到日志文件然后标记失败</p>
</li>
<li>对于streaming任务的话非0退出表示出现问题，也可以使用stream.non.zero.exit.is.failure = false来规避（ <strong>这样是否就没有办法判断是否正常退出了？</strong> ）</li>
<li>如果长时间没有响应的话，没有和tasktracker有交互，那么也会认为失败。这个时间使用mapred.task.timeout控制，默认10min</li>
<li><p>如果任务失败的话，jobtracker会尝试进行多次重试</p>
</li>
<li><p>map重试次数通过 mapred.map.max.attempts 配置</p>
</li>
<li>reduce重试次数通过 mapre.reduce.max.attempts 配置</li>
<li><strong>任何任务重试超过4次的话那么会认为整个job失败</strong></li>
<li>另外需要区分KILLED状态和FAILED状态，对于KILLED状态可能是因为推测执行造成的，不会记录到failed attempts里面</li>
<li><p>如果我们希望允许少量任务失败的话，那么可以配置</p>
</li>
<li><p>mapred.max.map.failures.percent 允许map失败的最大比率</p>
</li>
<li>mapred.max.reduce.failures.percent 允许reduce失败的最大比率</li>
<li>如果一个job超过一定的task在某个tt上面运行失败的话，那么就会将这个tt加入到这个job的blacklist. mapred.max.tracker.failures = 4</li>
<li>如果job成功的话，检查运行task失败的tt并且标记，如果超过一定阈值的话，那么会将tt加入到全局的blacklist. mapred.max.tracker.blacklists = 4</li>
</ul>
<p>作业的调度</p>
<ul>
<li><p>fifo scheduler</p>
</li>
<li><p>可以通过mapred.job.priority或者是setJobPriority设置</p>
</li>
<li>当队列中有空闲的槽位需要执行任务时，从等待队列中选择优先级最高的作业</li>
<li>fair scheduler</li>
<li>capacity scheduler</li>
</ul>
<p>shuffle和排序</p>
<p><img src="" alt="./images/mapreduce-shuffle-sort.jpg"></p>
<p><img src="" alt="./images/mapreduce-shuffle-sort-2.png"></p>
<p>有下面这些参数控制shuffle和sort的过程 <strong>NOTE（dirlt）：书上倒是有很多参数，但是好多还是不太理解</strong></p>
<ul>
<li>io.sort.mb map输出缓存空间大小，默认是100MB. 建议设置10/* io.sort.factor.</li>
<li><p>io.sort.spill.percent 如果map输出超过了缓存空间大小的这个阈值的话，那么就会spill,默认是0.8</p>
</li>
<li><p>每次spill之前先会对这个文件进行排序，如果有combiner的话那么会在上面调用combiner</p>
</li>
<li>写磁盘是按照轮询的方式写到mapred.local.dir属性指定的目录下面</li>
<li>如果spill速度太慢的话，那么往缓存空间写入进程就会阻塞，直到spill腾出空间。</li>
<li><p>io.sort.factor 多路归并的数量，默认是10. 建议设置在25-32.</p>
</li>
<li><p>在map阶段，因为最终会存在多个spill文件，所以需要做多路归并。 <strong>TODO（dirlt）：如果归并数量少的话是否可能会多次merge？</strong></p>
</li>
<li>在reduce阶段的话，因为可能存在多路map输出的结果，所以需要做多路归并。</li>
<li>min.num.spill.for.combine 如果指定combiner并且spill次数超过这个值的话就会调用combine,默认为3</li>
<li>tasktracker.http.threads reduce通过HTTP接口来发起数据请求，这个就是HTTP接口相应线程数目，默认为40。 <strong>mapper as server</strong></li>
<li><p>mapred.reduce.parallel.copies reduce启动多少个线程去请求map输出，默认为5。 <strong>reducer as client</strong></p>
</li>
<li><p><strong>NOTE(dirlt):如果reduce和每个map都使用一个线程去请求输出结果的话，只要shuffle阶段没有出现network congestion，那么提高线程数量是有效果的</strong></p>
</li>
<li><strong>NOTE（dirlt）：可以设置到15-50</strong></li>
<li>mapred.reduce.copy.backoff = 300(s) reduce下载线程最大等待时间</li>
<li>mapred.job.shuffle.input.buffer.percent = 0.7 用来缓存shuffle数据的reduce task heap百分比</li>
<li>mapred.job.shuffle.merge.percent = 0.66 缓存的内存中多少百分比后开始做merge操作</li>
<li>mapred.job.reduce.input.buffer.percent = 0.0 sort完成后reduce计算阶段用来缓存数据的百分比. 默认来说不会使用任何内存来缓存，因此完全从磁盘上进行读取。</li>
</ul>
<p>任务的执行</p>
<ul>
<li><p>推测执行参数</p>
</li>
<li><p>如果某个任务执行缓慢的话会执行另外一个备份任务</p>
</li>
<li>mapred.map.tasks.speculative.execution true</li>
<li>mapred.reduce.tasks.speculative.execution true</li>
<li><p>JVM重用</p>
</li>
<li><p>一个JVM实例可以用来执行多个task.</p>
</li>
<li>mapred.job.reuse.jvm.num.tasks/setNumTasksToExecutePerJvm 单个JVM运行任务的最大数目</li>
<li>-1表示没有限制</li>
<li><p>任务执行环境</p>
</li>
<li><p>程序自身可以知道执行环境对于开发还是比较有帮助的</p>
</li>
<li><p>这些属性对于streaming可以通过环境变量获得</p>
</li>
<li><p><strong>对于streaming来说.替换成为_</strong></p>
</li>
<li>mapred.job.id string jobID</li>
<li>mapred.tip.id string taskID</li>
<li>mapred.task.id string attemptID</li>
<li>mapred.task.partition int 作业中任务编号</li>
<li>mapred.task.is.map boolean 是否为map</li>
<li>mapred.work.output.dir / FileOutputFormat.getWorkOutputPath 当前工作目录</li>
<li><p>杂项 <strong>NOTE（dirlt）：from misc articles</strong></p>
</li>
<li><p>mapred.job.map.capacity /# 最大同时运行map数量</p>
</li>
<li>mapred.job.reduce.capacity /# 最大同时运行reduce数量</li>
<li>mapred.job.queue.name /# 选择执行queue<h3 id="1-4-7-mapreduce-">1.4.7 MapReduce的类型与格式</h3>
</li>
</ul>
<p>MapReduce的类型</p>
<p>老API里面还有MapRunner这个类，这个类主要的作用是可以用来控制Mapper运行的方法，比如可以多线程来控制Mapper的运行。 但是在新API里面已经完全集成到Mapper实现里面来了，用户可以重写两个方法来完全控制mapper的运行</p>
<ul>
<li>map 如何处理kv</li>
<li><p>run 如何从context里面读取kv
protected void map(KEYIN key, VALUEIN value,</p>
<pre><code>             Context context) throws IOException, InterruptedException {
</code></pre><p>context.write((KEYOUT) key, (VALUEOUT) value);</p>
</li>
</ul>
<p>}
public void run(Context context) throws IOException, InterruptedException {</p>
<p>  setup(context);
  while (context.nextKeyValue()) {</p>
<pre><code>map(context.getCurrentKey(), context.getCurrentValue(), context);
</code></pre><p>  }</p>
<p>  cleanup(context);
}</p>
<p><strong>NOTE（dirlt）：觉得这个特性不是特别有用</strong></p>
<ul>
<li>mapred.input.format.class setInputFormat</li>
<li>mapred.mapoutput.key.class setMapOutputKeyClass</li>
<li>mapred.mapoutput.value.class setMapOutputValueClass</li>
<li>mapred.output.key.class setOutputKeyClass</li>
<li>mapred.output.value.class setOutputValueClass</li>
<li>mapred.mapper.class setMapperClass</li>
<li>mapred.map.runner.class setMapRunnerClass</li>
<li>mapred.combiner.class setCombinerClass</li>
<li>mapred.partitioner.class setPartitionerClass</li>
<li>mapred.output.key.comparator.class setOutputKeyComparatorClass</li>
<li>mapred.output.value.groupfn.class setOutputValueGroupingComparator</li>
<li>mapred.reducer.class setReducerClass</li>
<li>mapred.output.format.class setOutputFormat</li>
</ul>
<p>输入格式</p>
<p>对于InputFormat来说包含两个任务</p>
<ul>
<li>根据job描述来对输入进行切片（InputSplit）</li>
<li><p>根据切片信息来读取记录（RecordReader）
public abstract class InputFormat<K, V> {</p>
<p>public abstract
  List<InputSplit> getSplits(JobContext context</p>
<pre><code>                         ) throws IOException, InterruptedException;
</code></pre></li>
</ul>
<p>   public abstract
    RecordReader<K,V> createRecordReader(InputSplit split,</p>
<pre><code>                                     TaskAttemptContext context
                                    ) throws IOException,

                                             InterruptedException;
</code></pre><p>}</p>
<p>public abstract class InputSplit {
  public abstract long getLength() throws IOException, InterruptedException;</p>
<p>  public abstract</p>
<pre><code>String[] getLocations() throws IOException, InterruptedException;
</code></pre><p>}</p>
<p>public abstract class RecordReader<KEYIN, VALUEIN> implements Closeable {</p>
<p>  public abstract void initialize(InputSplit split,
                                  TaskAttemptContext context</p>
<pre><code>                              ) throws IOException, InterruptedException;
</code></pre><p>  public abstract
  boolean nextKeyValue() throws IOException, InterruptedException;</p>
<p>  public abstract</p>
<p>  KEYIN getCurrentKey() throws IOException, InterruptedException;</p>
<p>  public abstract
  VALUEIN getCurrentValue() throws IOException, InterruptedException;</p>
<p>  public abstract float getProgress() throws IOException, InterruptedException;</p>
<p>  public abstract void close() throws IOException;</p>
<p>}</p>
<p>下面是一些常见的InputFormat实现</p>
<ul>
<li><p>FileInputFormat</p>
</li>
<li><p>addInputPath或者是setInputPaths修改输入路径 mapred.input.dir</p>
</li>
<li><p>setInputPathFilter可以修改过滤器 mapred.input.path.Filter.class</p>
</li>
<li><p>基本实现会排除隐藏.或者是_开头文件。</p>
</li>
<li>自定义的过滤器是建立在默认过滤器的基础上的。</li>
<li><p>分片大小由下面三个参数控制</p>
</li>
<li><p>mapred.min.split.size 1</p>
</li>
<li>mapred.max.split.size MAX</li>
<li>dfs.block.size 64MB</li>
<li>算法是max(minSplitSize,min(maxSplitSize,blockSize))</li>
<li>isSplitable可以控制输入文件是否需要分片</li>
<li>CombineFileInputFormat 可以处理多个小文件输入，抽象类需要继承实现。</li>
<li><p>TextInputFormat</p>
</li>
<li><p>输入单位是行，key是LongWritable表示行偏移，value是Text表示行内容</p>
</li>
<li><p>KeyValueTextInputFormat</p>
</li>
<li><p>输入单位是行，按照key.value.seperator.in.input.line来进行分隔默认是\t</p>
</li>
<li>key和value的格式都是Text</li>
<li><p>NLineInputFormat</p>
</li>
<li><p>和TextInputFormat非常类似，大师使用多行输入默认为1行</p>
</li>
<li>通过mapred.line.input.format.linespermap来控制行数</li>
<li><p>XML</p>
</li>
<li><p>InputFormat使用StreamInputFormat,</p>
</li>
<li>设置RecordReader使用stream.recordreader.class来设置</li>
<li>RecordReader使用org.apache.hadoop.streaming.StreamXmlRecordReader</li>
<li><strong>NOTE（dirlt）：也有现成的XmlInputFormat的实现</strong></li>
<li>SequenceFileInputFormat</li>
<li><p>SequenceFileAsTextInputFormat</p>
</li>
<li><p>将输入的kv转换成为text对象适合streaming处理方式</p>
</li>
<li>SequenceFileAsBinaryInputFormat <strong>NOTE（dirlt）：似乎没有什么用！</strong></li>
<li>MultipleInputs</li>
<li>DBInputFormat/DBOutputFormat JDBC数据库输入输出</li>
<li>TableInputFormat/TableOutputFormat HBase输入输出</li>
</ul>
<p>输出格式</p>
<ul>
<li><p>TextOutputFormat</p>
</li>
<li><p>使用mpared.textoutputformat.seperator来控制kv的分隔，默认是\t</p>
</li>
<li>对应的输入格式为KeyValueTextInputFormat</li>
<li>可以使用NullWritable来忽略输出的k或者是v</li>
<li>SequenceFileOutputFormat</li>
<li>SequenceFileAsBinaryOutpuFormat <strong>NOTE（dirlt）：似乎没有什么用！</strong></li>
<li>MapFileOutputFormat</li>
<li>MultipleOutputFormat</li>
<li><p>MultipleOutputs</p>
</li>
<li><p>如果不像生成那写part-r-00000这些空文件的话，那么可以将OutputFormat设置成为NullOutputFormat</p>
</li>
<li>但是使用NullOutputFormat的话会没有输出目录，如果想保留目录的话那么可以使用LazyOutputFormat</li>
</ul>
<h3 id="1-4-8-mapreduce-">1.4.8 MapReduce的特性</h3>
<ul>
<li><p>计数器</p>
</li>
<li><p>streaming计数器和可以通过写stderr来提交</p>
</li>
<li><p>reporter:counter:<group>,<counter>,<amount></p>
</li>
<li>reporter:status:<message></li>
<li><p>连接</p>
</li>
<li><p>map端连接</p>
</li>
<li><p>必须确保多路输入文件的reduce数量相同以及键相同。</p>
</li>
<li>使用CompositeInputFormat来运行map端连接。</li>
<li><strong>NOTE（dirlt)；不过我稍微看了一下代码，实现上其实也是针对输入文件对每条记录读取，然后进行join包括inner或者是outer。感觉场景会有限，而且效率不会太高</strong></li>
<li><p>分布式缓存</p>
</li>
<li><p>使用-files以及-archives来添加缓存文件</p>
</li>
<li><p>也可以使用DistributedAPI来完成之间事情</p>
</li>
<li><p>addCacheFile/addCacheArchive</p>
</li>
<li>然后在task里面通过configuration的getLocalCacheFiles以及getLocalCacheArchives来获得这些缓存文件</li>
<li><p>工作原理</p>
</li>
<li><p>缓存文件首先被放到hdfs上面</p>
</li>
<li><p>task需要的话那么会尝试下载，之后会对这个缓存文件进行引用计数，如果为0那么删除</p>
</li>
<li><p>这也就意味着缓存文件可能会被多次下载</p>
</li>
<li>但是运气好的话多个task在一个node上面的话那么就不用重复下载</li>
<li>缓存文件存放在${mapred.local.dir}/taskTracker/archive下面，但是通过软连接指向工作目录</li>
<li>缓存大小通过local.cache.size来配置</li>
<li><p>MapReduce库类</p>
</li>
<li><p>ChainMapper/ChainReducer 能够在一个mapper以及reducer里面运行多次mapper以及reducer</p>
</li>
<li><p>ChainMapper 允许在Map阶段，多个mapper组成一个chain,然后连续进行调用</p>
</li>
<li>ChainReducer 允许在Reuduce阶段，reducer完成之后执行一个mapper chain.</li>
<li>最终达到的效果就是 M+ -&gt; R -&gt; M/* （1个或者是多个mapper, 一个reducer，然后0个或者是多个mapper)</li>
<li><p><strong>TODO(dirlt):这样做倒是可以将各个mapper组合起来用作adapter.</strong></p>
<h3 id="1-4-9-hadoop-">1.4.9 构建Hadoop集群</h3>
</li>
<li><p>很多教程说hadoop集群需要配置ssh,但是配置这个前提是你希望使用start-all.sh这个脚本来启动集群</p>
</li>
<li><p>我现在的公司使用apt-get来安装，使用cssh来登陆到所有的节点上面进行配置，因此没有配置这个信任关系</p>
</li>
<li><p>Hadoop配置</p>
</li>
<li><p>配置文件</p>
</li>
<li><p>hadoop-env.sh 环境变量脚本</p>
</li>
<li>core-site.xml core配置，包括hdfs以及mapred的IO配置等</li>
<li>hdfs-site.xml hadoop进程配置比如namenode以及datanode以及secondary namenode</li>
<li>mapred-site.xml mapred进程配置比如jobtracker以及tasktracker</li>
<li><p>masters 运行namenode（secondary namenode)的机器列表，每行一个, <strong>无需分发到各个节点</strong></p>
</li>
<li><p><strong>在本地启动primary namenode</strong></p>
</li>
<li><p>slaves 运行datanode以及tasktracker的机器列表，每行一个 <strong>无需分发到各个节点</strong></p>
</li>
<li><p><strong>在本地启动jobtracker</strong></p>
</li>
<li>hadoop-metrics.properties 对hadoop做监控的配置文件</li>
<li>log4j.properties 日志配置文件</li>
<li>这些文件在conf目录下面有，如果想使用不同的文件也可以使用-config来另行指定</li>
<li><strong>NOTE(dirlt):所以从上面这个脚本来看，还是具有一定的局限性的</strong></li>
<li><p>hadoop-env.sh</p>
</li>
<li><p>HADOOP_HEAPSIZE = 1000MB 守护进程大小</p>
</li>
<li>HADOOP_NAMENODE_OPTS</li>
<li>HADOOP_SECONDARYNAMENODE_OPTS</li>
<li>HADOOP_IDENT_STRING 用户名称标记，默认为${USER}</li>
<li>HADOOP_LOG_DIR hadoop日志文件，默认是HADOOP_INSTALL/logs</li>
<li><p>core-site.xml</p>
</li>
<li><p>io.file.buffer.size IO操作缓冲区大小，默认是4KB <strong>这个需要提高</strong></p>
</li>
<li><p>hdfs-site.xml</p>
</li>
<li><p>fs.default.name</p>
</li>
<li>hadoop.tmp.dir hadoop临时目录，默认是在/tmp/hadoop-${user.name}</li>
<li>dfs.name.dir namenode数据目录，一系列的目录，namenode内容会同时备份在所有指定的目录中。默认为${hadoop.tmp.dir}/dfs/name</li>
<li>dfs.data.dir datanode数据目录，一系列的目录，循环将数据写在各个目录里面。默认是${hadoop.tmp.dir}/dfs/data</li>
<li>fs.checkpoint.dir secondarynamenode数据目录，一系列目录，所有目录都会写一份。默认为${hadoop.tmp.dir}/dfs/namesecondary</li>
<li>dfs.namenode.handler.count namenode上用来处理请求的线程数目</li>
<li>dfs.datanode.ipc.address 0.0.0.0:50020 datanode的RPC接口，主要和namenode交互</li>
<li>dfs.datanode.address 0.0.0.0:50010 datanode的data block传输接口，主要和client交互</li>
<li>dfs.datanode.http.address 0.0.0.0:50075 datanode的HTTP接口，和user交互</li>
<li>dfs.datanode.handler.count datanode上用来处理请求的线程数目</li>
<li>dfs.datanode.max.xcievers datanode允许最多同时打开的文件数量</li>
<li>dfs.http.address 0.0.0.0:50070 namenode的HTTP接口</li>
<li>dfs.secondary.http.address 0.0.0.0:50090 secondard namenode的HTTP接口</li>
<li>dfs.datanode.dns.interface default 绑定的NIC，默认是绑定默认的NIC比如eth0</li>
<li>dfs.hosts / dfs.hosts.exclude 加入的datanode以及排除的datanode</li>
<li>dfs.replication = 3 副本数目</li>
<li>dfs.block.size = 64MB</li>
<li>dfs.datanode.du.reserved 默认datanode会使用目录所在磁盘所有空间，这个值可以保证有多少空间被reserved的</li>
<li><p>fs.trash.interval 单位分钟，如果不为0的话，那么删除文件会移动到回收站，超过这个单位时间的文件才会完全删除。</p>
</li>
<li><p>回收站位置/home/${user]/.Trash <strong>NOTE(dirlt):回收站这个功能只是对fs shell有效。fs shell remove时候会构造Trash这个类来处理删除文件的请求。如果调用Java API的话那么会直接删除文件</strong></p>
</li>
<li>haddop fs -expunge 强制删除</li>
<li><strong>NOTE（dirlt）：grep代码发现只有NameNode在TrashEmptier里面构造了Trash这个类，因此这个配置之需要在nn上配置即可，决定多久定期删除垃圾文件</strong></li>
<li><p>fs.trash.checkpoint.interval 单位分钟，namenode多久检查一次文件是否需要删除。</p>
</li>
<li><p><strong>NOTE（dirlt）：似乎没有这个参数。如果没有这个参数的话，那么两次检查时长应该是由参数fs.trasn.interval来决定</strong></p>
</li>
<li><p>mapred-site.xml</p>
</li>
<li><p>mapred.job.tracker</p>
</li>
<li>mapred.local.dir MR中间数据存储，一系列目录，分散写到各个目录下面，默认为${hadoop.tmp.dir}/mapred/local</li>
<li>mapred.system.dir MR运行期间存储，比如存放jar或者是缓存文件等。默认${hadoop.tmp.dir}/mapred/system</li>
<li>mapred.tasktracker.map.tasks.maximum = 2 单个tasktracker最多多少map任务</li>
<li>mapred.tasktracker.reduce.tasks.maximum = 2 单个tasktracker最多多少个reduce任务</li>
<li>mapred.tasktracker.dns.interface default 绑定的NIC，默认是绑定默认的NIC比如eth0</li>
<li>mapred.child.ulimit 单个tasktracker允许子进程占用的最大内存空间。通常为2-3/* mapred.child.java.opts.</li>
<li><p>mapred.child.java.opts = -Xmx200m 每个子JVM进程200M. <strong>NOTE（dirlt）：这个是在提交机器上面设置的，而不是每个tasktracker上面设置的，每个job可以不同</strong></p>
</li>
<li><p>不一定支持将map/reduce的jvm参数分开设置 <a href="http://hadoop-common.472056.n3.nabble.com/separate-JVM-flags-for-map-and-reduce-tasks-td743351.html" target="_blank"><a href="http://hadoop-common.472056.n3.nabble.com/separate-JVM-flags-for-map-and-reduce-tasks-td743351.html">http://hadoop-common.472056.n3.nabble.com/separate-JVM-flags-for-map-and-reduce-tasks-td743351.html</a></a></p>
</li>
<li><strong>NOTE（dirlt）：个人折中思路是限制内存大小为1G，然后大内存机器允许同时执行map/reduce数量上限提高，通过增加job的map/reduce数量来提高并发增加性能</strong></li>
<li><p><strong>NOTE（dirlt）：我grep了一下cdh3u3的代码，应该是将map/reduce的jvm参数分开进行了设置</strong></p>
</li>
<li><p>mapred.map.child.java.opts</p>
</li>
<li>mapred.reduce.child.java.opts</li>
<li>mapred.task.tracker.report.address 127.0.0.1:0 tasktracker启动子进程通信的端口，0表示使用任意端口</li>
<li>mapred.task.tracker.expiry.interval 600(sec) tt和jt之间的心跳间隔</li>
<li>mapred.job.tracker.handler.count. jobtracker用来处理请求的线程数目。</li>
<li>mapred.job.tracker.http.address 0.0.0.0:50030 jobtracker的HTTP接口</li>
<li>mapred.task.tracker.http.address 0.0.0.0:50060 tasktrackder的HTTP接口</li>
<li>mapred.hosts / mapred.hosts.exclude 加入的tasktracker以及排除的tasktracker.</li>
<li><p>Hadoop Benchmark <strong>NOTE（dirlt）：try it out</strong></p>
</li>
<li><p>在hadoop安装目录下面有jar可以来做基准测试</p>
</li>
<li>TestDFSIO测试HDFS的IO性能</li>
<li>Sort测试MapReduce性能</li>
<li>MRBench多次运行一个小作业来检验小作业能否快速相应</li>
<li>NNBench测试namenode硬件的负载</li>
</ul>
<h3 id="1-4-10-hadoop">1.4.10 管理Hadoop</h3>
<ul>
<li><p>永久性数据结构</p>
</li>
<li><p>namenode的目录结构</p>
</li>
<li><p>current表示当前的namenode数据（对于辅助节点上这个数据并不是最新的）</p>
</li>
<li><p>previous.checkpoint表示secondarynamenode完成checkpoint的数据（和current可能存在一些编辑差距）</p>
</li>
<li><p>hadoop dfsadmin -saveNamespace 可以强制创建检查点,仅仅在安全模式下面运行</p>
</li>
<li><p>辅助namenode每隔5分钟会检查</p>
</li>
<li><p>如果超过fs.checkpoint.period = 3600（sec），那么会创建检查点</p>
</li>
<li>如果编辑日志大小超过fs.checkpoint.size = 64MB,同样也会创建检查点</li>
<li>除了将文件copy到namenode之外，在辅助节点上面可以使用选项-importCheckpoint来载入</li>
<li><p>VERSION Java属性文件</p>
</li>
<li><p>namespaceID 每次格式化都会重新生成一个ID，这样可以防止错误的datanode加入</p>
</li>
<li>cTime namenode存储系统创建时间，对于刚格式化的存储系统为0.对于升级的话会更新到最新的时间戳</li>
<li>storageType NAME_NODE or DATA_NODE</li>
<li>layoutVersion 负整数表示hdfs文件系统布局版本号，对于hadoop升级的话这个版本号可能不会变化</li>
<li>edits 编辑日志文件</li>
<li>fsimage 镜像文件</li>
<li>fstime ???</li>
<li><p>datanode的目录结构</p>
</li>
<li><p>blk<em><id>以及blk</em><id>.meta 表示块数据以及对应的元信息，元数据主要包括校验和等内容</p>
</li>
<li>如果datanode文件非常多的话，超过dfs.datanode.numblocks = 64的话，那么会创建一个目录单独存放，最终结果就是形成树存储结构。</li>
<li>dfs.data.dir目录是按照round-robin的算法选择的。</li>
<li><p>安全模式</p>
</li>
<li><p>namenode启动的时候会尝试合并edit数据并且新建一个checkpoint，然后进入安全模式，在这个模式内文件系统是只读的</p>
</li>
<li>可以通过hadoop dfsadmin -safemode来操作安全模式</li>
<li><p>当达到下面几个条件的时候会离开安全模式</p>
</li>
<li><p>整个系统的副本数目大于某个阈值的副本数目比率超过一个阈值之后，然后继续等待一段时间就会离开安全模式</p>
</li>
<li>dfs.replication.min = 1 副本数目阈值</li>
<li>dfs.safemode.threshold.pct = 0.999 比率阈值</li>
<li>dfs.safemode.extension = 30000(ms) 等待时间</li>
<li><p>工具</p>
</li>
<li><p>dfsadmin</p>
</li>
<li>fsck</li>
<li><p>scanner</p>
</li>
<li><p>DataBlockScanner每隔一段时间会扫描本地的data block检查是否出现校验和问题</p>
</li>
<li>时间间隔是dfs.datanode.scan.period.hours = 504默认三周</li>
<li>可以通过页面访问每个datanode的block情况 <a href="http://localhost:50075/blockScannerReport" target="_blank"><a href="http://localhost:50075/blockScannerReport">http://localhost:50075/blockScannerReport</a></a></li>
<li>加上listblocks参数可以看每个block情况 <a href="http://localhost:50075/blockScannerReport?listblocks" target="_blank"><a href="http://localhost:50075/blockScannerReport?listblocks">http://localhost:50075/blockScannerReport?listblocks</a></a> <strong>NOTE（dirlt）：可能会很大</strong></li>
<li><p>balancer</p>
</li>
<li><p>通过start-balancer.sh来启动,集群中只允许存在一个均衡器</p>
</li>
<li>均衡的标准是datanode的利用率和集群平均利用率的插值，如果超过某个阈值就会进行block movement</li>
<li>-threshold可以执行阈值，默认为10%</li>
<li>dfs.balance.bandwidthPerSec = 1024 /* 1024 用于balance的带宽上限。</li>
<li><p>监控</p>
</li>
<li><p>日志</p>
</li>
<li><p>jobtracker的stack信息（thread-dump）<a href="http://localhost:50030/stacks" target="_blank"><a href="http://localhost:50030/stacks">http://localhost:50030/stacks</a></a></p>
</li>
<li><p>度量</p>
</li>
<li><p>度量从属于特性的上下文(context),包括下面几个</p>
</li>
<li><p>dfs</p>
</li>
<li>mapred</li>
<li>rpc</li>
<li>jvm</li>
<li><p>下面是几种常见的context</p>
</li>
<li><p>FileContext 度量写到文件</p>
</li>
<li>GangliaContext 度量写到ganglia <strong>(这个似乎比较靠谱）</strong></li>
<li>CompositeContext 组合context</li>
<li>度量可以从hadoop-metrics.properties进行配置</li>
</ul>
<h3 id="1-5-benchmark">1.5 Benchmark</h3>
<ul>
<li>Benchmarking and Stress Testing an Hadoop Cluster with TeraSort, TestDFSIO &amp; Co. - Michael G. Noll <a href="http://www.michael-noll.com/blog/2011/04/09/benchmarking-and-stress-testing-an-hadoop-cluster-with-terasort-testdfsio-nnbench-mrbench/" target="_blank"><a href="http://www.michael-noll.com/blog/2011/04/09/benchmarking-and-stress-testing-an-hadoop-cluster-with-terasort-testdfsio-nnbench-mrbench/">http://www.michael-noll.com/blog/2011/04/09/benchmarking-and-stress-testing-an-hadoop-cluster-with-terasort-testdfsio-nnbench-mrbench/</a></a></li>
<li>intel-hadoop/HiBench · GitHub <a href="https://github.com/intel-hadoop/HiBench" target="_blank"><a href="https://github.com/intel-hadoop/HiBench">https://github.com/intel-hadoop/HiBench</a></a></li>
<li>HBase Performance Testing at hstack <a href="http://hstack.org/hbase-performance-testing/" target="_blank"><a href="http://hstack.org/hbase-performance-testing/">http://hstack.org/hbase-performance-testing/</a></a></li>
<li>Performance testing / Benchmarking a HBase cluster – Sujee Maniyam <a href="http://sujee.net/tech/articles/hadoop/hbase-performance-testing/" target="_blank"><a href="http://sujee.net/tech/articles/hadoop/hbase-performance-testing/">http://sujee.net/tech/articles/hadoop/hbase-performance-testing/</a></a></li>
<li>new Put(&quot;lars&quot;.toBytes(&quot;UTF-8&quot;)) : Performance testing HBase using YCSB <a href="http://blog.lars-francke.de/2010/08/16/performance-testing-hbase-using-ycsb/" target="_blank"><a href="http://blog.lars-francke.de/2010/08/16/performance-testing-hbase-using-ycsb/">http://blog.lars-francke.de/2010/08/16/performance-testing-hbase-using-ycsb/</a></a></li>
<li>Hbase/PerformanceEvaluation - Hadoop Wiki <a href="http://wiki.apache.org/hadoop/Hbase/PerformanceEvaluation" target="_blank"><a href="http://wiki.apache.org/hadoop/Hbase/PerformanceEvaluation">http://wiki.apache.org/hadoop/Hbase/PerformanceEvaluation</a></a></li>
</ul>
<h3 id="1-5-1-testdfsio">1.5.1 TestDFSIO</h3>
<p>测试hdfs吞吐
hdfs@hadoop1:~$ hadoop jar /usr/lib/hadoop/hadoop-test-0.20.2-cdh3u3.jar TestDFSIO</p>
<p>Usage: TestDFSIO [genericOptions] -read | -write | -append | -clean [-nrFiles N] [-fileSize Size[B|KB|MB|GB|TB]] [-resFile resultFileName] [-bufferSize Bytes] [-rootDir]%</p>
<ul>
<li>read / write / append / clean 操作类型 <strong>append和write执行效率差别不大，但是write会创建新文件所以使用比较方便</strong> (default read)</li>
<li>nrFiles 文件数目(default 1) <strong>启动相同数量的map</strong></li>
<li>fileSize 每个文件大小(1MB)</li>
<li>resFile 结果报告文件(TestDFSIO_results.log)</li>
<li>bufferSize write buffer size(单次write写入大小）（1000000 bytes)</li>
<li><p>rootDir 操作文件根目录（/benchmarks/TestDFSIO/）
----- TestDFSIO ----- : write</p>
<pre><code>     Date &amp; time: Thu Apr 25 19:14:21 CST 2013
 Number of files: 2
</code></pre></li>
</ul>
<p>Total MBytes processed: 2.0
     Throughput mb/sec: 7.575757575757576</p>
<p>Average IO rate mb/sec: 7.61113977432251
IO rate std deviation: 0.5189420757292891</p>
<pre><code>Test exec time sec: 14.565
</code></pre><p>----- TestDFSIO ----- : read
           Date &amp; time: Thu Apr 25 19:15:13 CST 2013</p>
<pre><code>   Number of files: 2
</code></pre><p>Total MBytes processed: 2.0</p>
<pre><code> Throughput mb/sec: 27.77777777777778
</code></pre><p>Average IO rate mb/sec: 28.125</p>
<p>IO rate std deviation: 3.125
    Test exec time sec: 14.664</p>
<ul>
<li>throughtput = sum(filesize) / sum(time)</li>
<li>avaerage io rate = sum(filesize/time) / n</li>
<li>io rate std deviation<h3 id="1-5-2-terasort">1.5.2 TeraSort</h3>
</li>
</ul>
<p>通过排序测试MR执行效率 <strong>我看了一下代码map/reduce都有CPU操作，并且这个也非常依靠shuffle/copy.因此这个测试应该会是比较全面的</strong>
hdfs@hadoop1:~$ hadoop jar /usr/lib/hadoop/hadoop-examples-0.20.2-cdh3u3.jar <command></p>
<ul>
<li><p>teragen 产生排序数据</p>
</li>
<li><number of 100-byte rows>
</li>
<li><p>10 bytes key(random characters)</p>
</li>
<li>10 bytes rowid(right justified row id as a int)</li>
<li>78 bytes filler</li>
<li>\r\n</li>
<li><output dir></li>
<li><p>terasort 对数据排序</p>
</li>
<li><input dir></li>
<li><output dir></li>
<li>teravalidate 对排序数据做验证</li>
</ul>
<p>可以使用hadoop job -history all <job-output-dir>来观察程序运行数据，也可以通过web page来分析。</p>
<h3 id="1-5-3-nnbench">1.5.3 nnbench</h3>
<p>测试nn负载能力
➜  ~HADOOP_HOME  hadoop jar hadoop-test-0.20.2-cdh3u3.jar nnbench</p>
<p>NameNode Benchmark 0.4
Usage: nnbench <options></p>
<p>Options:
        -operation <Available operations are create_write open_read rename delete. This option is mandatory></p>
<pre><code>     /* NOTE: The open_read, rename and delete operations assume that the files they operate on, are already available. The create_write operation must be run before running the other operations.
    -maps &lt;number of maps. default is 1. This is not mandatory&gt;

    -reduces &lt;number of reduces. default is 1. This is not mandatory&gt;
    -startTime &lt;time to start, given in seconds from the epoch. Make sure this is far enough into the future, so all maps (operations) will start at the same time&gt;. default is launch time + 2 mins. This is not mandatory

    -blockSize &lt;Block size in bytes. default is 1. This is not mandatory&gt;
    -bytesToWrite &lt;Bytes to write. default is 0. This is not mandatory&gt;

    -bytesPerChecksum &lt;Bytes per checksum for the files. default is 1. This is not mandatory&gt;
    -numberOfFiles &lt;number of files to create. default is 1. This is not mandatory&gt;

    -replicationFactorPerFile &lt;Replication factor for the files. default is 1. This is not mandatory&gt;
    -baseDir &lt;base DFS path. default is /becnhmarks/NNBench. This is not mandatory&gt;

    -readFileAfterOpen &lt;true or false. if true, it reads the file and reports the average time to read. This is valid with the open_read operation. default is false. This is not mandatory&gt;
    -help: Display the help statement
</code></pre><ul>
<li>startTime 作用是为了能够让所有的map同时启动以便对nn造成压力
➜  ~HADOOP_HOME  hadoop jar hadoop-test-0.20.2-cdh3u3.jar nnbench -operation create_write -bytesToWrite 0 -numberOfFiles 1200</li>
</ul>
<p>➜  ~HADOOP_HOME  hadoop jar hadoop-test-0.20.2-cdh3u3.jar nnbench -operation open_read</p>
<p>结果报告文件是 NNBench_results.log</p>
<p>-------------- NNBench -------------- :</p>
<pre><code>                           Version: NameNode Benchmark 0.4
                       Date &amp; time: 2013-04-25 19:41:02,873


                    Test Operation: create_write

                        Start time: 2013-04-25 19:40:21,70
                       Maps to run: 1

                    Reduces to run: 1
                Block Size (bytes): 1

                    Bytes to write: 0
                Bytes per checksum: 1

                   Number of files: 1200
                Replication factor: 1

        Successful file operations: 1200


    /# maps that missed the barrier: 0
                      /# exceptions: 0


           TPS: Create/Write/Close: 75
</code></pre><p>Avg exec time (ms): Create/Write/Close: 26.526666666666667
            Avg Lat (ms): Create/Write: 13.236666666666666</p>
<pre><code>               Avg Lat (ms): Close: 13.164166666666667


             RAW DATA: AL Total /#1: 15884
             RAW DATA: AL Total /#2: 15797

          RAW DATA: TPS Total (ms): 31832
   RAW DATA: Longest Map Time (ms): 31832.0

               RAW DATA: Late maps: 0
         RAW DATA: /# of exceptions: 0
</code></pre><p>-------------- NNBench -------------- :</p>
<pre><code>                           Version: NameNode Benchmark 0.4
                       Date &amp; time: 2013-04-25 19:44:42,354


                    Test Operation: open_read

                        Start time: 2013-04-25 19:44:31,921
                       Maps to run: 1

                    Reduces to run: 1
                Block Size (bytes): 1

                    Bytes to write: 0
                Bytes per checksum: 1

                   Number of files: 1
                Replication factor: 1

        Successful file operations: 1


    /# maps that missed the barrier: 0
                      /# exceptions: 0


                    TPS: Open/Read: 500

     Avg Exec time (ms): Open/Read: 2.0
                Avg Lat (ms): Open: 2.0


             RAW DATA: AL Total /#1: 2

             RAW DATA: AL Total /#2: 0
          RAW DATA: TPS Total (ms): 2

   RAW DATA: Longest Map Time (ms): 2.0
               RAW DATA: Late maps: 0

         RAW DATA: /# of exceptions: 0
</code></pre><ul>
<li>maps that missed the barrier 从代码上分析是，在等待到start time期间中,如果sleep出现异常的话。</li>
<li>exceptions 表示在操作文件系统时候的exception数量</li>
<li>TPS transactions per second</li>
<li>exec（execution） 执行时间</li>
<li>lat（latency） 延迟时间</li>
<li>late maps 和 maps missed the barrier是一个概念。</li>
</ul>
<p>对于后面RAW DATA部分的话，从代码上看，就是为了计算出上面那些指标的，所以没有必要关注。</p>
<h3 id="1-5-4-mrbench">1.5.4 mrbench</h3>
<p>测试运行small mr jobs执行效率，主要关注响应时间。
MRBenchmark.0.0.2</p>
<p>Usage: mrbench [-baseDir <base DFS path for output/input, default is /benchmarks/MRBench>] [-jar <local path to job jar file containing Mapper and Reducer implementations, default is current jar file>] [-numRuns <number of times to run the job, default is 1>] [-maps <number of maps for each run, default is 2>] [-reduces <number of reduces for each run, default is 1>] [-inputLines <number of input lines to generate, default is 1>] [-inputType <type of input to generate, one of ascending (default), descending, random>] [-verbose]</p>
<ul>
<li>baseDir 输入输出目录</li>
<li>jar 通常不需要指定，用默认即可。</li>
<li>inputLines 输入条数</li>
<li>inputType 输入是否有序
hdfs@hadoop1:~$ hadoop jar /usr/lib/hadoop/hadoop-test-0.20.2-cdh3u3.jar mrbench -verbose</li>
</ul>
<p>结果直接输出在终端上面，</p>
<p>Total MapReduce jobs executed: 1</p>
<p>Total lines of data per job: 1
Maps per job: 2</p>
<p>Reduces per job: 1
Total milliseconds for task: 1 = 16452</p>
<p>DataLines       Maps    Reduces AvgTime (milliseconds)
1               2       1       16452</p>
<p>可以看到每个任务平均执行时间在16.452s.</p>
<h3 id="1-5-5-hbase-performanceevaluation">1.5.5 hbase.PerformanceEvaluation</h3>
<p>hdfs@hadoop1:~$ hbase org.apache.hadoop.hbase.PerformanceEvaluation</p>
<p>Usage: java org.apache.hadoop.hbase.PerformanceEvaluation \
  [--miniCluster] [--nomapred] [--rows=ROWS] <command> <nclients></p>
<p>Options:</p>
<p> miniCluster     Run the test on an HBaseMiniCluster
 nomapred        Run multiple clients using threads (rather than use mapreduce)</p>
<p> rows            Rows each client runs. Default: One million
 flushCommits    Used to determine if the test should flush the table.  Default: false</p>
<p> writeToWAL      Set writeToWAL on puts. Default: True</p>
<p>Command:
 filterScan      Run scan test using a filter to find a specific row based on it&#39;s value (make sure to use --rows=20)</p>
<p> randomRead      Run random read test
 randomSeekScan  Run random seek and scan 100 test</p>
<p> randomWrite     Run random write test
 scan            Run scan test (read every row)</p>
<p> scanRange10     Run random seek scan with both start and stop row (max 10 rows)
 scanRange100    Run random seek scan with both start and stop row (max 100 rows)</p>
<p> scanRange1000   Run random seek scan with both start and stop row (max 1000 rows)
 scanRange10000  Run random seek scan with both start and stop row (max 10000 rows)</p>
<p> sequentialRead  Run sequential read test
sequentialWrite Run sequential write test</p>
<p>Args:</p>
<p> nclients        Integer. Required. Total number of clients (and HRegionServers)
                 running: 1 &lt;= value &lt;= 500</p>
<p>Examples:
To run a single evaluation client:</p>
<p>$ bin/hbase org.apache.hadoop.hbase.PerformanceEvaluation sequentialWrite 1</p>
<p>从参数上看还是比较直接的。benchmark每个client通常对应10个mapper, 每个client操作<rows>个row,因此每个mapper操作<rows>/10个row,每个row大约1000bytes.</p>
<ul>
<li>filterScan 随机生成value，然后从头开始scan直到equal</li>
<li>randomRead 随机选取key读取</li>
<li>randomSeekScan 从某个随机位置开始scan最多100个</li>
<li>randomWrite 随即生成key写入</li>
<li>scan 每次scan 1个row，start随机</li>
<li>scan<num> 每次scan num个row，start随机</li>
<li>seqRead 顺序地读取每个key</li>
<li>seqWrite 顺序地写入每个key</li>
<li><strong>NOTE(dirlt):这里的key都非常简单，10个字符的数字，printf(&quot;%010d&quot;,row)</strong>
hdfs@hadoop1:~$ time hbase org.apache.hadoop.hbase.PerformanceEvaluation --rows=1000 sequentialWrite 2</li>
</ul>
<p>13/04/25 23:47:56 INFO mapred.JobClient:   HBase Performance Evaluation
13/04/25 23:47:56 INFO mapred.JobClient:     Row count=2000</p>
<p>13/04/25 23:47:56 INFO mapred.JobClient:     Elapsed time in milliseconds=258</p>
<p>输出结果是在counter里面，这里面row count = 2000, 占用时间为258 ms.
Date: 2013-12-15T10:28+0800</p>
<p><a href="http://orgmode.org/" target="_blank">Org</a> version 7.9.2 with <a href="http://www.gnu.org/software/emacs/" target="_blank">Emacs</a> version 24
<a href="http://validator.w3.org/check?uri=referer" target="_blank">Validate XHTML 1.0</a>
来源： <a href="[http://dirlt.com/hadoop.html/#sec-1-1-4](http://dirlt.com/hadoop.html#sec-1-1-4)">[http://dirlt.com/hadoop.html/#sec-1-1-4](http://dirlt.com/hadoop.html#sec-1-1-4)</a> </p>

      
    </div>
    
    
      
    
    
        <footer id="post-meta">
        <span class="categories">Posted in<span class="breadcrumb fa fa-folder"><li><a href="/categories/hadoop/">hadoop</a></li></span></span> | <span class="tags">Tagged <a href="/tags/hadoop/" class="label label-primary">hadoop</a></span> | <span class="time">recent updated:<time title="2014-03-07 09:54:36"datetime="2014-03-07 09:54:36"> mar. 7 2014</time></span> | <span class="comment-link">
<a href="http://itsolife.com/2014/02/02/2014-02-02-hadoop--hadoop/#comments" class="ds-thread-count comment-link" data-thread-key="2014-02-02-hadoop--hadoop" data-count-type="comments">暂无评论</a></span>
        </footer>
    
    <div class="clearfix"></div>
  </div>
</div>
</article>



  <article>
<div id="post" class="post well">
  <div class="post-content">
    <header class="well-sm">
      <i class="fa icon fa-5x pull-left"></i>
      <h1 class="title"><a href="/2014/02/02/2014-02-02-hadoop--Hadoop版本梳理/">Hadoop版本梳理</a></h1>
      
        <span>Posted on<time datetime="2014-02-02T01:54:36.000Z"> <a href="/2014/02/02/2014-02-02-hadoop--Hadoop版本梳理/">feb. 2 2014</a></time></span>      
    </header>
    
    
    <div class="entry">
      
        <h1 id="hadoop-">Hadoop版本梳理</h1>
<p>由于Hadoop版本混乱多变，因此，Hadoop的版本选择问题一直令很多初级用户苦恼。本文总结了Apache Hadoop和Cloudera Hadoop的版本衍化过程，并给出了选择Hadoop版本的一些建议。</p>
<h3 id="1-apache-hadoop">1. Apache Hadoop</h3>
<h3 id="1-1-apache-">1.1  Apache版本衍化</h3>
<p>截至目前（2012年12月23日），Apache Hadoop版本分为两代，我们将第一代Hadoop称为Hadoop 1.0，第二代Hadoop称为Hadoop 2.0。第一代Hadoop包含三个大版本，分别是0.20.x，0.21.x和0.22.x，其中，0.20.x最后演化成1.0.x，变成了稳定版，而0.21.x和0.22.x则NameNode HA等新的重大特性。第二代Hadoop包含两个版本，分别是0.23.x和2.x，它们完全不同于Hadoop 1.0，是一套全新的架构，均包含HDFS Federation和YARN两个系统，相比于0.23.x，2.x增加了NameNode HA和Wire-compatibility两个重大特性。</p>
<p>经过上面的大体解释，大家可能明白了Hadoop以重大特性区分各个版本的，总结起来，用于区分Hadoop版本的特性有以下几个：</p>
<p><strong>（1）Append</strong> 支持文件追加功能，如果想使用HBase，需要这个特性。</p>
<p><strong>（2）RAID </strong>在保证数据可靠的前提下，通过引入校验码较少数据块数目。详细链接：</p>
<p><a href="https://issues.apache.org/jira/browse/HDFS/component/12313080" target="_blank">https://issues.apache.org/jira/browse/HDFS/component/12313080</a></p>
<p><strong>（3）Symlink</strong> 支持HDFS文件链接，具体可参考： <a href="https://issues.apache.org/jira/browse/HDFS-245" target="_blank"><a href="https://issues.apache.org/jira/browse/HDFS-245">https://issues.apache.org/jira/browse/HDFS-245</a></a></p>
<p><strong>（4）Security</strong> Hadoop安全，具体可参考：<a href="https://issues.apache.org/jira/browse/HADOOP-4487" target="_blank"><a href="https://issues.apache.org/jira/browse/HADOOP-4487">https://issues.apache.org/jira/browse/HADOOP-4487</a></a></p>
<p><strong>（5） NameNode HA</strong> 具体可参考：<a href="https://issues.apache.org/jira/browse/HDFS-1064" target="_blank"><a href="https://issues.apache.org/jira/browse/HDFS-1064">https://issues.apache.org/jira/browse/HDFS-1064</a></a></p>
<p><strong>（6） HDFS Federation和YARN</strong></p>
<p><img src="&quot;apache-hadoop-versions&quot;" alt=""></p>
<p>需要注意的是，Hadoop 2.0主要由Yahoo独立出来的hortonworks公司主持开发。</p>
<h3 id="1-2-apache-">1.2  Apache版本下载</h3>
<p>（1） 各版本说明：<a href="http://hadoop.apache.org/releases.html" target="_blank"><a href="http://hadoop.apache.org/releases.html">http://hadoop.apache.org/releases.html</a></a>。</p>
<p>（2） 下载稳定版：找到一个镜像，下载stable文件夹下的版本。</p>
<p>（3） Hadoop最全版本：<a href="http://svn.apache.org/repos/asf/hadoop/common/branches/" target="_blank"><a href="http://svn.apache.org/repos/asf/hadoop/common/branches/">http://svn.apache.org/repos/asf/hadoop/common/branches/</a></a>，可直接导到eclipse中。</p>
<h3 id="2-cloudera-hadoop">2. Cloudera Hadoop</h3>
<h3 id="2-1-cdh-">2.1  CDH版本衍化</h3>
<p>Apache当前的版本管理是比较混乱的，各种版本层出不穷，让很多初学者不知所措，相比之下，Cloudera公司的Hadoop版本管理的要很多。</p>
<p>我们知道，Hadoop遵从Apache开源协议，用户可以免费地任意使用和修改Hadoop，也正因此，市面上出现了很多Hadoop版本，其中比较出名的一是Cloudera公司的发行版，我们将该版本称为CDH（Cloudera Distribution Hadoop）。截至目前为止，CDH共有4个版本，其中，前两个已经不再更新，最近的两个，分别是CDH3（在Apache Hadoop 0.20.2版本基础上演化而来的）和CDH4在Apache Hadoop 2.0.0版本基础上演化而来的），分别对应Apache的Hadoop 1.0和Hadoop 2.0，它们每隔一段时间便会更新一次。</p>
<p><img src="&quot;cloudera-hadoop-versions&quot;" alt=""></p>
<p>Cloudera以patch level划分小版本，比如patch level为923.142表示在原生态Apache Hadoop 0.20.2基础上添加了1065个patch（这些patch是各个公司或者个人贡献的，在Hadoop jira上均有记录），其中923个是最后一个beta版本添加的patch，而142个是稳定版发行后新添加的patch。由此可见，patch level越高，功能越完备且解决的bug越多。</p>
<p>Cloudera版本层次更加清晰，且它提供了适用于各种操作系统的Hadoop安装包，可直接使用apt-get或者yum命令进行安装，更加省事。</p>
<h3 id="2-2-cdh-">2.2 CDH版本下载</h3>
<p>（1） 版本含义介绍：</p>
<p><a href="https://ccp.cloudera.com/display/DOC/CDH+Version+and+Packaging+Information" target="_blank"><a href="https://ccp.cloudera.com/display/DOC/CDH+Version+and+Packaging+Information">https://ccp.cloudera.com/display/DOC/CDH+Version+and+Packaging+Information</a></a></p>
<p>（2）各版本特性查看：</p>
<p><a href="https://ccp.cloudera.com/display/DOC/CDH+Packaging+Information+for+Previous+Releases" target="_blank"><a href="https://ccp.cloudera.com/display/DOC/CDH+Packaging+Information+for+Previous+Releases">https://ccp.cloudera.com/display/DOC/CDH+Packaging+Information+for+Previous+Releases</a></a></p>
<p>（3）各版本下载：</p>
<p>CDH3：<a href="http://archive.cloudera.com/cdh/3/" target="_blank"><a href="http://archive.cloudera.com/cdh/3/">http://archive.cloudera.com/cdh/3/</a></a></p>
<p>CDH4：<a href="http://archive.cloudera.com/cdh4/cdh/4/" target="_blank"><a href="http://archive.cloudera.com/cdh4/cdh/4/">http://archive.cloudera.com/cdh4/cdh/4/</a></a></p>
<p>注意，Hadoop压缩包在这两个链接中的最上层目录中，不在某个文件夹里，很多人进到链接还找不到安装包！</p>
<h3 id="3-hadoop-">3. 如何选择Hadoop版本</h3>
<p>当前Hadoop版本比较混乱，让很多用户不知所措。实际上，当前Hadoop只有两个版本：Hadoop 1.0和Hadoop 2.0，其中，Hadoop 1.0由一个分布式文件系统HDFS和一个离线计算框架MapReduce组成，而Hadoop 2.0则包含一个支持NameNode横向扩展的HDFS，一个资源管理系统YARN和一个运行在YARN上的离线计算框架MapReduce。相比于Hadoop 1.0，Hadoop 2.0功能更加强大，且具有更好的扩展性、性能，并支持多种计算框架。</p>
<p>当我们决定是否采用某个软件用于开源环境时，通常需要考虑以下几个因素：</p>
<p>（1）是否为开源软件，即是否免费。</p>
<p>（2） 是否有稳定版，这个一般软件官方网站会给出说明。</p>
<p>（3） 是否经实践验证，这个可通过检查是否有一些大点的公司已经在生产环境中使用知道。</p>
<p>（4） 是否有强大的社区支持，当出现一个问题时，能够通过社区、论坛等网络资源快速获取解决方法。</p>
<p>如今Hadoop 2.0已经发布了最新的稳定版2.2.0，推荐使用该版本，具体介绍可阅读：“<a href="http://dongxicheng.org/mapreduce-nextgen/hadoop-2-2-0/" target="_blank">Hadoop 2.0稳定版本2.2.0新特性剖析</a>”，升级方法可参考：“<a href="http://dongxicheng.org/mapreduce-nextgen/hadoop-upgrade-to-version-2/" target="_blank">Hadoop升级方案（二）：从Hadoop 1.0升级到2.0（1）</a>”。
来源： <a href="[http://dongxicheng.org/mapreduce-nextgen/how-to-select-hadoop-versions/](http://dongxicheng.org/mapreduce-nextgen/how-to-select-hadoop-versions/)">[http://dongxicheng.org/mapreduce-nextgen/how-to-select-hadoop-versions/](http://dongxicheng.org/mapreduce-nextgen/how-to-select-hadoop-versions/)</a></p>
<p>另外一篇：</p>
<h1 id="-hadoop-http-www-cnblogs-com-xuxm2007-archive-2013-04-04-2999741-html-"><a href="http://www.cnblogs.com/xuxm2007/archive/2013/04/04/2999741.html" target="_blank">hadoop的版本问题</a></h1>
<p>现在hadoop的版本比较乱,常常搞不清楚版本之间的关系,下面简单的摘要了,apache hadoop和cloudera hadoop 的版本的演化.</p>
<p><strong>apache hadoop官方给出的版本说明是:</strong></p>
<p><strong>1.0.X -</strong>current stable version, 1.0 release</p>
<p><strong>1.1.X -</strong>current beta version, 1.1 release</p>
<p><strong>2.X.X -</strong>current alpha version</p>
<p><strong>0.23.X -</strong>simmilar to 2.X.X but missing NN HA.</p>
<p><strong>0.22.X -</strong>does not include security</p>
<p><strong>0.20.203.X -</strong>old legacy stable version</p>
<p><strong>0.20.X -</strong>old legacy version</p>
<p>下图来自<a href="http://blog.cloudera.com/blog/2012/01/an-update-on-apache-hadoop-1-0/" target="_blank"><a href="http://blog.cloudera.com/blog/2012/01/an-update-on-apache-hadoop-1-0/">http://blog.cloudera.com/blog/2012/01/an-update-on-apache-hadoop-1-0/</a></a></p>
<p>可以简单说明apache hadoop和cloudera hadoop版本之间的变化关系</p>
<p><a href="http://images.cnitblog.com/blog/73083/201304/04194843-67590ee16a15440497b1153b688fad40.png" target="_blank"><img src="&quot;diagram-3&quot;" alt="diagram-3"></a></p>
<p>0.20.x版本最后演化成了现在的1.0.x版本</p>
<p>0.23.x版本最后演化成了现在的2.x版本</p>
<p>hadoop 1.0 指的是1.x(0.20.x),0.21,0.22</p>
<p>hadoop 2.0 指的是2.x,0.23.x</p>
<p>CDH3,CDH4分别对应了hadoop1.0 hadoop2.0</p>
<p>董的博客有2篇文章也很清晰的解释了,hadoop版本以及各自的版本特性:</p>
<p><a href="http://dongxicheng.org/mapreduce-nextgen/how-to-select-hadoop-versions/" target="_blank"><a href="http://dongxicheng.org/mapreduce-nextgen/how-to-select-hadoop-versions/">http://dongxicheng.org/mapreduce-nextgen/how-to-select-hadoop-versions/</a></a></p>
<p><a href="http://dongxicheng.org/mapreduce-nextgen/hadoop-2-0-terms-explained/" target="_blank"><a href="http://dongxicheng.org/mapreduce-nextgen/hadoop-2-0-terms-explained/">http://dongxicheng.org/mapreduce-nextgen/hadoop-2-0-terms-explained/</a></a></p>
<p><a href="http://images.cnitblog.com/blog/73083/201304/04195633-b2c2d402daf94e2187a2130c24d441b1.jpg" target="_blank"><img src="&quot;apache-hadoop-versions&quot;" alt="apache-hadoop-versions"></a></p>
<p>最后给出常见的下载hadoop不同版本的地址:</p>
<p><a href="http://archive.apache.org/dist/hadoop/core/" target="_blank"><a href="http://archive.apache.org/dist/hadoop/core/">http://archive.apache.org/dist/hadoop/core/</a></a></p>
<p><a href="http://archive.cloudera.com/cdh/3/" target="_blank"><a href="http://archive.cloudera.com/cdh/3/">http://archive.cloudera.com/cdh/3/</a></a></p>
<p><a href="http://archive.cloudera.com/cdh4/cdh/4/" target="_blank"><a href="http://archive.cloudera.com/cdh4/cdh/4/">http://archive.cloudera.com/cdh4/cdh/4/</a></a></p>
<p>另外附注一个 hadoop各商业发行版的比较:</p>
<p><a href="http://www.xiaohui.org/archives/795.html" target="_blank"><a href="http://www.xiaohui.org/archives/795.html">http://www.xiaohui.org/archives/795.html</a></a>
来源： &lt;<a href="http://www.cnblogs.com/xuxm2007/archive/2013/04/04/2999741.html" target="_blank">hadoop的版本问题 - 阿笨猫 - 博客园</a>&gt;  </p>

      
    </div>
    
    
      
    
    
        <footer id="post-meta">
        <span class="categories">Posted in<span class="breadcrumb fa fa-folder"><li><a href="/categories/hadoop/">hadoop</a></li></span></span> | <span class="tags">Tagged <a href="/tags/hadoop/" class="label label-primary">hadoop</a></span> | <span class="time">recent updated:<time title="2014-03-07 09:54:36"datetime="2014-03-07 09:54:36"> mar. 7 2014</time></span> | <span class="comment-link">
<a href="http://itsolife.com/2014/02/02/2014-02-02-hadoop--Hadoop版本梳理/#comments" class="ds-thread-count comment-link" data-thread-key="2014-02-02-hadoop--Hadoop版本梳理" data-count-type="comments">暂无评论</a></span>
        </footer>
    
    <div class="clearfix"></div>
  </div>
</div>
</article>



  <article>
<div id="post" class="post well">
  <div class="post-content">
    <header class="well-sm">
      <i class="fa icon fa-5x pull-left"></i>
      <h1 class="title"><a href="/2014/02/02/2014-02-02-linux-ubuntu--Ubuntu1210软件更新源列表_Linux教程_Linux公社-Linux系统门户网站/">Ubuntu 12.10 软件更新源列表_Linux教程_Linux公社</a></h1>
      
        <span>Posted on<time datetime="2014-02-02T01:54:36.000Z"> <a href="/2014/02/02/2014-02-02-linux-ubuntu--Ubuntu1210软件更新源列表_Linux教程_Linux公社-Linux系统门户网站/">feb. 2 2014</a></time></span>      
    </header>
    
    
    <div class="entry">
      
        <h1 id="ubuntu-12-10-_linux-_linux-linux-">Ubuntu 12.10 软件更新源列表_Linux教程_Linux公社-Linux系统门户网站</h1>
<h3 id="-">分享到</h3>
<ul>
<li><a href="http://www.linuxidc.com/Linux/2012-10/73114.htm#" target="_blank">一键分享</a></li>
<li><a href="http://www.linuxidc.com/Linux/2012-10/73114.htm#" target="_blank">QQ空间</a></li>
<li><a href="http://www.linuxidc.com/Linux/2012-10/73114.htm#" target="_blank">新浪微博</a></li>
<li><a href="http://www.linuxidc.com/Linux/2012-10/73114.htm#" target="_blank">百度搜藏</a></li>
<li><a href="http://www.linuxidc.com/Linux/2012-10/73114.htm#" target="_blank">人人网</a></li>
<li><a href="http://www.linuxidc.com/Linux/2012-10/73114.htm#" target="_blank">腾讯微博</a></li>
<li><a href="http://www.linuxidc.com/Linux/2012-10/73114.htm#" target="_blank">百度相册</a></li>
<li><a href="http://www.linuxidc.com/Linux/2012-10/73114.htm#" target="_blank">开心网</a></li>
<li><a href="http://www.linuxidc.com/Linux/2012-10/73114.htm#" target="_blank">腾讯朋友</a></li>
<li><a href="http://www.linuxidc.com/Linux/2012-10/73114.htm#" target="_blank">百度贴吧</a></li>
<li><a href="http://www.linuxidc.com/Linux/2012-10/73114.htm#" target="_blank">豆瓣网</a></li>
<li><a href="http://www.linuxidc.com/Linux/2012-10/73114.htm#" target="_blank">搜狐微博</a></li>
<li><a href="http://www.linuxidc.com/Linux/2012-10/73114.htm#" target="_blank">百度新首页</a></li>
<li><a href="http://www.linuxidc.com/Linux/2012-10/73114.htm#" target="_blank">QQ</a></li>
<li><a href="http://www.linuxidc.com/Linux/2012-10/73114.htm#" target="_blank">和讯微博</a></li>
<li><a href="http://www.linuxidc.com/Linux/2012-10/73114.htm#" target="_blank">更多...</a></li>
</ul>
<p><a href="http://www.linuxidc.com/Linux/2012-10/73114.htm#" target="_blank">百度分享</a></p>
<p><a href="&quot;點擊以繁體中文方式浏覽&quot;">繁體</a></p>
<p>你好，游客 <a href="http://www.linuxidc.com/Linux/2012-10/73114.htm#" target="_blank">登录</a> <a href="http://www.linuxidc.com/memberreg.aspx" target="_blank">注册</a> <a href="http://www.linuxidc.com/membernewsadd.aspx" target="_blank">发布</a><a href="http://www.linuxidc.com/search.aspx" target="_blank">搜索</a>
<a href="http://www.linuxidc.com/" target="_blank"><img src="" alt="Linux公社"></a>
<a href="http://www.linuxidc.com/topicnews.aspx?tid=11" title="Android专题" target="_blank"><img src="" alt="Android专题"></a></p>
<p><a href="http://www.linuxidc.com/index.htm" target="_blank">首页</a><a href="http://www.linuxidc.com/it/" target="_blank">Linux新闻</a><a href="http://www.linuxidc.com/Linuxit/" target="_blank">Linux教程</a><a href="http://www.linuxidc.com/MySql/" target="_blank">数据库技术</a><a href="http://www.linuxidc.com/RedLinux/" target="_blank">Linux编程</a><a href="http://www.linuxidc.com/Apache/" target="_blank">服务器应用</a><a href="http://www.linuxidc.com/Unix/" target="_blank">Linux安全</a><a href="http://www.linuxidc.com/download/" target="_blank">Linux下载</a><a href="http://www.linuxidc.com/Linuxrz/" target="_blank">Linux认证</a><a href="http://www.linuxidc.com/theme/" target="_blank">Linux主题</a><a href="http://www.linuxidc.com/Linuxwallpaper/" target="_blank">Linux壁纸</a><a href="http://www.linuxidc.com/linuxsoft/" target="_blank">Linux软件</a><a href="http://www.linuxidc.com/digi/" target="_blank">数码</a><a href="http://www.linuxidc.com/mobile/" target="_blank">手机</a><a href="http://www.linuxidc.com/diannao/" target="_blank">电脑</a>
<a href="http://www.linuxidc.com/index.htm" target="_blank">首页</a> → <a href="http://www.linuxidc.com/Linuxit/" target="_blank">Linux教程</a></p>
<p><a href="http://www.yutianedu.com/list.asp?Unid=22239" target="_blank"><img src="" alt=""></a> <a href="http://www.boxue.com.cn/" target="_blank"><img src="" alt=""></a>
背景：<img src="" alt="#EDF0F5"> <img src="" alt="#FAFBE6"> <img src="" alt="#FFF2E2"> <img src="" alt="#FDE6E0"> <img src="" alt="#F3FFE1"> <img src="" alt="#DAFAF3"> <img src="" alt="#EAEAEF"> <img src="" alt="默认"> 阅读新闻</p>
<h1 id="ubuntu-12-10-">Ubuntu 12.10 软件更新源列表</h1>
<p> [日期：2012-10-28] 来源：Linux公社  作者：Linux [字体：<a href="">大</a> <a href="">中</a> <a href="">小</a>]</p>
<p><a href="http://www.upemb.com/page/uealinuxidc12531.php" title="尚观Linux" target="_blank"><img src="" alt=""></a>
<a href="http://www.linuxidc.com/topicnews.aspx?tid=2" title="Ubuntu" target="_blank">Ubuntu</a> 12.10也正式发布了， 安装好后第一件事就是更换源，Ubuntu网易的更新源速度很不错。</p>
<p>Ubuntu 12.10正式版发布下载  <a href="http://www.linuxidc.com/Linux/2012-10/72581.htm" target="_blank"><a href="http://www.linuxidc.com/Linux/2012-10/72581.htm">http://www.linuxidc.com/Linux/2012-10/72581.htm</a></a></p>
<p>废话少说， 上源：</p>
<p>首先，备份一下Ubuntu 12.10 原来的源地址列表文件</p>
<p>sudo cp /etc/apt/sources.list /etc/apt/sources.list.old</p>
<p>然后进行修改
sudo gedit /etc/apt/sources.list</p>
<p>可以在里面添加资源地址，我是直接覆盖掉原来的。</p>
<p>下面是网上找到的一些较好的源，有大型网站的，也有教育网的，可以根据自己的情况添加两三个即可。</p>
<p>/#网易的源（163源，无论是不是教育网，速度都很快）
deb <a href="http://mirrors.163.com/ubuntu/" target="_blank">http://mirrors.163.com/ubuntu/</a> quantal main universe restricted multiverse
deb-src <a href="http://mirrors.163.com/ubuntu/" target="_blank">http://mirrors.163.com/ubuntu/</a> quantal main universe restricted multiverse
deb <a href="http://mirrors.163.com/ubuntu/" target="_blank">http://mirrors.163.com/ubuntu/</a> quantal-security universe main multiverse restricted
deb-src <a href="http://mirrors.163.com/ubuntu/" target="_blank">http://mirrors.163.com/ubuntu/</a> quantal-security universe main multiverse restricted
deb <a href="http://mirrors.163.com/ubuntu/" target="_blank">http://mirrors.163.com/ubuntu/</a> quantal-updates universe main multiverse restricted
deb <a href="http://mirrors.163.com/ubuntu/" target="_blank">http://mirrors.163.com/ubuntu/</a> quantal-proposed universe main multiverse restricted
deb-src <a href="http://mirrors.163.com/ubuntu/" target="_blank">http://mirrors.163.com/ubuntu/</a> quantal-proposed universe main multiverse restricted
deb <a href="http://mirrors.163.com/ubuntu/" target="_blank">http://mirrors.163.com/ubuntu/</a> quantal-backports universe main multiverse restricted
deb-src <a href="http://mirrors.163.com/ubuntu/" target="_blank">http://mirrors.163.com/ubuntu/</a> quantal-backports universe main multiverse restricted
deb-src <a href="http://mirrors.163.com/ubuntu/" target="_blank">http://mirrors.163.com/ubuntu/</a> quantal-updates universe main multiverse restricted</p>
<p>/#搜狐的源（sohu 源今天还没有更新，不过应该快了）
deb <a href="http://mirrors.sohu.com/ubuntu/" target="_blank">http://mirrors.sohu.com/ubuntu/</a> quantal main restricted
deb-src <a href="http://mirrors.sohu.com/ubuntu/" target="_blank">http://mirrors.sohu.com/ubuntu/</a> quantal main restricted
deb <a href="http://mirrors.sohu.com/ubuntu/" target="_blank">http://mirrors.sohu.com/ubuntu/</a> quantal-updates main restricted
deb-src <a href="http://mirrors.sohu.com/ubuntu/" target="_blank">http://mirrors.sohu.com/ubuntu/</a> quantal-updates main restricted
deb <a href="http://mirrors.sohu.com/ubuntu/" target="_blank">http://mirrors.sohu.com/ubuntu/</a> quantal universe
deb-src <a href="http://mirrors.sohu.com/ubuntu/" target="_blank">http://mirrors.sohu.com/ubuntu/</a> quantal universe
deb <a href="http://mirrors.sohu.com/ubuntu/" target="_blank">http://mirrors.sohu.com/ubuntu/</a> quantal-updates universe
deb-src <a href="http://mirrors.sohu.com/ubuntu/" target="_blank">http://mirrors.sohu.com/ubuntu/</a> quantal-updates universe
deb <a href="http://mirrors.sohu.com/ubuntu/" target="_blank">http://mirrors.sohu.com/ubuntu/</a> quantal multiverse
deb-src <a href="http://mirrors.sohu.com/ubuntu/" target="_blank">http://mirrors.sohu.com/ubuntu/</a> quantal multiverse
deb <a href="http://mirrors.sohu.com/ubuntu/" target="_blank">http://mirrors.sohu.com/ubuntu/</a> quantal-updates multiverse
deb-src <a href="http://mirrors.sohu.com/ubuntu/" target="_blank">http://mirrors.sohu.com/ubuntu/</a> quantal-updates multiverse
deb <a href="http://mirrors.sohu.com/ubuntu/" target="_blank">http://mirrors.sohu.com/ubuntu/</a> quantal-backports main restricted universe multiverse
deb-src <a href="http://mirrors.sohu.com/ubuntu/" target="_blank">http://mirrors.sohu.com/ubuntu/</a> quantal-backports main restricted universe multiverse
deb <a href="http://mirrors.sohu.com/ubuntu/" target="_blank">http://mirrors.sohu.com/ubuntu/</a> quantal-security main restricted
deb-src <a href="http://mirrors.sohu.com/ubuntu/" target="_blank">http://mirrors.sohu.com/ubuntu/</a> quantal-security main restricted
deb <a href="http://mirrors.sohu.com/ubuntu/" target="_blank">http://mirrors.sohu.com/ubuntu/</a> quantal-security universe
deb-src <a href="http://mirrors.sohu.com/ubuntu/" target="_blank">http://mirrors.sohu.com/ubuntu/</a> quantal-security universe
deb <a href="http://mirrors.sohu.com/ubuntu/" target="_blank">http://mirrors.sohu.com/ubuntu/</a> quantal-security multiverse
deb-src <a href="http://mirrors.sohu.com/ubuntu/" target="_blank">http://mirrors.sohu.com/ubuntu/</a> quantal-security multiverse
deb <a href="http://extras.ubuntu.com/ubuntu" target="_blank">http://extras.ubuntu.com/ubuntu</a> quantal main
deb-src <a href="http://extras.ubuntu.com/ubuntu" target="_blank">http://extras.ubuntu.com/ubuntu</a> quantal main</p>
<p>/#台湾源（台湾的ubuntu 更新源还是很给力的）
deb <a href="http://tw.archive.ubuntu.com/ubuntu/" target="_blank">http://tw.archive.ubuntu.com/ubuntu/</a> quantal main universe restricted multiverse
deb-src <a href="http://tw.archive.ubuntu.com/ubuntu/" target="_blank">http://tw.archive.ubuntu.com/ubuntu/</a> quantal main universe restricted multiverse
deb <a href="http://tw.archive.ubuntu.com/ubuntu/" target="_blank">http://tw.archive.ubuntu.com/ubuntu/</a> quantal-security universe main multiverse restricted
deb-src <a href="http://tw.archive.ubuntu.com/ubuntu/" target="_blank">http://tw.archive.ubuntu.com/ubuntu/</a> quantal-security universe main multiverse restricted
deb <a href="http://tw.archive.ubuntu.com/ubuntu/" target="_blank">http://tw.archive.ubuntu.com/ubuntu/</a> quantal-updates universe main multiverse restricted
deb-src <a href="http://tw.archive.ubuntu.com/ubuntu/" target="_blank">http://tw.archive.ubuntu.com/ubuntu/</a> quantal-updates universe main multiverse restricted</p>
<p>/#骨头源，骨头源是bones7456架设的一个Ubuntu源 ，提供ubuntu,deepin
deb <a href="http://ubuntu.srt.cn/ubuntu/" target="_blank">http://ubuntu.srt.cn/ubuntu/</a> quantal main universe restricted multiverse
deb-src <a href="http://ubuntu.srt.cn/ubuntu/" target="_blank">http://ubuntu.srt.cn/ubuntu/</a> quantal main universe restricted multiverse
deb <a href="http://ubuntu.srt.cn/ubuntu/" target="_blank">http://ubuntu.srt.cn/ubuntu/</a> quantal-security universe main multiverse restricted
deb-src <a href="http://ubuntu.srt.cn/ubuntu/" target="_blank">http://ubuntu.srt.cn/ubuntu/</a> quantal-security universe main multiverse restricted
deb <a href="http://ubuntu.srt.cn/ubuntu/" target="_blank">http://ubuntu.srt.cn/ubuntu/</a> quantal-updates universe main multiverse restricted
deb <a href="http://ubuntu.srt.cn/ubuntu/" target="_blank">http://ubuntu.srt.cn/ubuntu/</a> quantal-proposed universe main multiverse restricted
deb-src <a href="http://ubuntu.srt.cn/ubuntu/" target="_blank">http://ubuntu.srt.cn/ubuntu/</a> quantal-proposed universe main multiverse restricted
deb <a href="http://ubuntu.srt.cn/ubuntu/" target="_blank">http://ubuntu.srt.cn/ubuntu/</a> quantal-backports universe main multiverse restricted
deb-src <a href="http://ubuntu.srt.cn/ubuntu/" target="_blank">http://ubuntu.srt.cn/ubuntu/</a> quantal-backports universe main multiverse restricted
deb-src <a href="http://ubuntu.srt.cn/ubuntu/" target="_blank">http://ubuntu.srt.cn/ubuntu/</a> quantal-updates universe main multiverse restricted</p>
<p>/#ubuntu.cn99.com源（推荐）:
deb <a href="http://ubuntu.cn99.com/ubuntu/" target="_blank">http://ubuntu.cn99.com/ubuntu/</a> quantal main restricted universe multiverse
deb <a href="http://ubuntu.cn99.com/ubuntu/" target="_blank">http://ubuntu.cn99.com/ubuntu/</a> quantal-updates main restricted universe multiverse
deb <a href="http://ubuntu.cn99.com/ubuntu/" target="_blank">http://ubuntu.cn99.com/ubuntu/</a> quantal-security main restricted universe multiverse
deb <a href="http://ubuntu.cn99.com/ubuntu/" target="_blank">http://ubuntu.cn99.com/ubuntu/</a> quantal-backports main restricted universe multiverse
deb <a href="http://ubuntu.cn99.com/ubuntu-cn/" target="_blank">http://ubuntu.cn99.com/ubuntu-cn/</a> quantal main restricted universe multiverse</p>
<p>/#教育网源
/#电子科技大学
deb <a href="http://ubuntu.uestc.edu.cn/ubuntu/" target="_blank">http://ubuntu.uestc.edu.cn/ubuntu/</a> quantal main restricted universe multiverse
deb <a href="http://ubuntu.uestc.edu.cn/ubuntu/" target="_blank">http://ubuntu.uestc.edu.cn/ubuntu/</a> quantal-backports main restricted universe multiverse
deb <a href="http://ubuntu.uestc.edu.cn/ubuntu/" target="_blank">http://ubuntu.uestc.edu.cn/ubuntu/</a> quantal-proposed main restricted universe multiverse
deb <a href="http://ubuntu.uestc.edu.cn/ubuntu/" target="_blank">http://ubuntu.uestc.edu.cn/ubuntu/</a> quantal-security main restricted universe multiverse
deb <a href="http://ubuntu.uestc.edu.cn/ubuntu/" target="_blank">http://ubuntu.uestc.edu.cn/ubuntu/</a> quantal-updates main restricted universe multiverse
deb-src <a href="http://ubuntu.uestc.edu.cn/ubuntu/" target="_blank">http://ubuntu.uestc.edu.cn/ubuntu/</a> quantal main restricted universe multiverse
deb-src <a href="http://ubuntu.uestc.edu.cn/ubuntu/" target="_blank">http://ubuntu.uestc.edu.cn/ubuntu/</a> quantal-backports main restricted universe multiverse
deb-src <a href="http://ubuntu.uestc.edu.cn/ubuntu/" target="_blank">http://ubuntu.uestc.edu.cn/ubuntu/</a> quantal-proposed main restricted universe multiverse
deb-src <a href="http://ubuntu.uestc.edu.cn/ubuntu/" target="_blank">http://ubuntu.uestc.edu.cn/ubuntu/</a> quantal-security main restricted universe multiverse
deb-src <a href="http://ubuntu.uestc.edu.cn/ubuntu/" target="_blank">http://ubuntu.uestc.edu.cn/ubuntu/</a> quantal-updates main restricted universe multiverse</p>
<p>/#中国科技大学
deb <a href="http://debian.ustc.edu.cn/ubuntu/" target="_blank">http://debian.ustc.edu.cn/ubuntu/</a> quantal main restricted universe multiverse
deb <a href="http://debian.ustc.edu.cn/ubuntu/" target="_blank">http://debian.ustc.edu.cn/ubuntu/</a> quantal-backports restricted universe multiverse
deb <a href="http://debian.ustc.edu.cn/ubuntu/" target="_blank">http://debian.ustc.edu.cn/ubuntu/</a> quantal-proposed main restricted universe multiverse
deb <a href="http://debian.ustc.edu.cn/ubuntu/" target="_blank">http://debian.ustc.edu.cn/ubuntu/</a> quantal-security main restricted universe multiverse
deb <a href="http://debian.ustc.edu.cn/ubuntu/" target="_blank">http://debian.ustc.edu.cn/ubuntu/</a> quantal-updates main restricted universe multiverse
deb-src <a href="http://debian.ustc.edu.cn/ubuntu/" target="_blank">http://debian.ustc.edu.cn/ubuntu/</a> quantal main restricted universe multiverse
deb-src <a href="http://debian.ustc.edu.cn/ubuntu/" target="_blank">http://debian.ustc.edu.cn/ubuntu/</a> quantal-backports main restricted universe multiverse
deb-src <a href="http://debian.ustc.edu.cn/ubuntu/" target="_blank">http://debian.ustc.edu.cn/ubuntu/</a> quantal-proposed main restricted universe multiverse
deb-src <a href="http://debian.ustc.edu.cn/ubuntu/" target="_blank">http://debian.ustc.edu.cn/ubuntu/</a> quantal-security main restricted universe multiverse
deb-src <a href="http://debian.ustc.edu.cn/ubuntu/" target="_blank">http://debian.ustc.edu.cn/ubuntu/</a> quantal-updates main restricted universe multiverse
/#北京理工大学
deb <a href="http://mirror.bjtu.edu.cn/ubuntu/" target="_blank">http://mirror.bjtu.edu.cn/ubuntu/</a> quantal main multiverse restricted universe
deb <a href="http://mirror.bjtu.edu.cn/ubuntu/" target="_blank">http://mirror.bjtu.edu.cn/ubuntu/</a> quantal-backports main multiverse restricted universe
deb <a href="http://mirror.bjtu.edu.cn/ubuntu/" target="_blank">http://mirror.bjtu.edu.cn/ubuntu/</a> quantal-proposed main multiverse restricted universe
deb <a href="http://mirror.bjtu.edu.cn/ubuntu/" target="_blank">http://mirror.bjtu.edu.cn/ubuntu/</a> quantal-security main multiverse restricted universe
deb <a href="http://mirror.bjtu.edu.cn/ubuntu/" target="_blank">http://mirror.bjtu.edu.cn/ubuntu/</a> quantal-updates main multiverse restricted universe
deb-src <a href="http://mirror.bjtu.edu.cn/ubuntu/" target="_blank">http://mirror.bjtu.edu.cn/ubuntu/</a> quantal main multiverse restricted universe
deb-src <a href="http://mirror.bjtu.edu.cn/ubuntu/" target="_blank">http://mirror.bjtu.edu.cn/ubuntu/</a> quantal-backports main multiverse restricted universe
deb-src <a href="http://mirror.bjtu.edu.cn/ubuntu/" target="_blank">http://mirror.bjtu.edu.cn/ubuntu/</a> quantal-proposed main multiverse restricted universe
deb-src <a href="http://mirror.bjtu.edu.cn/ubuntu/" target="_blank">http://mirror.bjtu.edu.cn/ubuntu/</a> quantal-security main multiverse restricted universe
deb-src <a href="http://mirror.bjtu.edu.cn/ubuntu/" target="_blank">http://mirror.bjtu.edu.cn/ubuntu/</a> quantal-updates main multiverse restricted universe</p>
<p>/#兰州大学
deb ftp://mirror.lzu.edu.cn/ubuntu/ quantal main multiverse restricted universe
deb ftp://mirror.lzu.edu.cn/ubuntu/ quantal-backports main multiverse restricted universe
deb ftp://mirror.lzu.edu.cn/ubuntu/ quantal-proposed main multiverse restricted universe
deb ftp://mirror.lzu.edu.cn/ubuntu/ quantal-security main multiverse restricted universe
deb ftp://mirror.lzu.edu.cn/ubuntu/ quantal-updates main multiverse restricted universe
deb ftp://mirror.lzu.edu.cn/ubuntu-cn/ quantal main multiverse restricted universe</p>
<p>/#上海交通大学（上海交大源，教育网的速度不用说了）
deb <a href="http://ftp.sjtu.edu.cn/ubuntu/" target="_blank">http://ftp.sjtu.edu.cn/ubuntu/</a> quantal main multiverse restricted universe
deb <a href="http://ftp.sjtu.edu.cn/ubuntu/" target="_blank">http://ftp.sjtu.edu.cn/ubuntu/</a> quantal-backports main multiverse restricted universe
deb <a href="http://ftp.sjtu.edu.cn/ubuntu/" target="_blank">http://ftp.sjtu.edu.cn/ubuntu/</a> quantal-proposed main multiverse restricted universe
deb <a href="http://ftp.sjtu.edu.cn/ubuntu/" target="_blank">http://ftp.sjtu.edu.cn/ubuntu/</a> quantal-security main multiverse restricted universe
deb <a href="http://ftp.sjtu.edu.cn/ubuntu/" target="_blank">http://ftp.sjtu.edu.cn/ubuntu/</a> quantal-updates main multiverse restricted universe
deb <a href="http://ftp.sjtu.edu.cn/ubuntu-cn/" target="_blank">http://ftp.sjtu.edu.cn/ubuntu-cn/</a> quantal main multiverse restricted universe
deb-src <a href="http://ftp.sjtu.edu.cn/ubuntu/" target="_blank">http://ftp.sjtu.edu.cn/ubuntu/</a> quantal main multiverse restricted universe
deb-src <a href="http://ftp.sjtu.edu.cn/ubuntu/" target="_blank">http://ftp.sjtu.edu.cn/ubuntu/</a> quantal-backports main multiverse restricted universe
deb-src <a href="http://ftp.sjtu.edu.cn/ubuntu/" target="_blank">http://ftp.sjtu.edu.cn/ubuntu/</a> quantal-proposed main multiverse restricted universe
deb-src <a href="http://ftp.sjtu.edu.cn/ubuntu/" target="_blank">http://ftp.sjtu.edu.cn/ubuntu/</a> quantal-security main multiverse restricted universe
deb-src <a href="http://ftp.sjtu.edu.cn/ubuntu/" target="_blank">http://ftp.sjtu.edu.cn/ubuntu/</a> quantal-updates main multiverse restricted universe</p>
<p>添加好后保存，再输入 sudo apt-get update 就可以更新了，等着慢慢下载东西吧。</p>
<ul>
<li>1</li>
<li><a href="http://www.linuxidc.com/Linux/2012-10/73114.htm###" target="_blank">顶一下</a>
<a href="http://www.linuxidc.com/Linux/2012-10/73114.htm#" title="分享到新浪微博" target="_blank"></a> <a href="http://www.linuxidc.com/Linux/2012-10/73114.htm#" title="分享到百度搜藏"></a> <a href="http://www.linuxidc.com/Linux/2012-10/73114.htm#" title="分享到QQ空间" target="_blank"></a> <a href="http://www.linuxidc.com/Linux/2012-10/73114.htm#" title="分享到腾讯微博"></a> <a href="http://www.linuxidc.com/Linux/2012-10/73114.htm#" title="分享到人人网" target="_blank"></a> <a href="http://www.linuxidc.com/Linux/2012-10/73114.htm#" title="分享到搜狐微博"></a> <a href="http://www.linuxidc.com/Linux/2012-10/73114.htm#" title="分享到有道云笔记" target="_blank"></a> <a href="http://www.linuxidc.com/Linux/2012-10/73114.htm#" title="累计分享2次">2</a>
<a href="http://www.linuxidc.com/Linux/2012-10/73111.htm" target="_blank">Ubuntu Grub2启动上一次正确启动的内核</a></li>
</ul>
<p><a href="http://www.linuxidc.com/Linux/2012-10/73115.htm" target="_blank">Linux PAM make err : undefine yywrap()问题</a></p>
<p>相关资讯       <a href="http://www.linuxidc.com/search.aspx?where=nkey&amp;keyword=3408" target="_blank">Ubuntu源</a>  <a href="http://www.linuxidc.com/search.aspx?where=nkey&amp;keyword=3835" target="_blank">ubuntu更新源</a>  <a href="http://www.linuxidc.com/search.aspx?where=nkey&amp;keyword=14209" target="_blank">Ubuntu 12.10更新源</a>  <a href="http://www.linuxidc.com/search.aspx?where=nkey&amp;keyword=14210" target="_blank">Ubuntu教育网更新源</a> </p>
<ul>
<li><a href="http://www.linuxidc.com/Linux/2012-07/64562.htm" target="_blank">Ubuntu 12.04 Server 用户更新源</a>  (07月07日)</li>
<li><a href="http://www.linuxidc.com/Linux/2010-12/30938.htm" target="_blank">Ubuntu 10.10更新源</a>  (12/29/2010 20:10:37)</li>
<li><p><a href="http://www.linuxidc.com/Linux/2009-09/21827.htm" target="_blank">架设Ubuntu源时的两个脚本</a>  (09/22/2009 05:48:40)</p>
</li>
<li><p><a href="http://www.linuxidc.com/Linux/2011-11/47624.htm" target="_blank">Ubuntu 11.10 教育网源</a>  (11/20/2011 04:08:48)</p>
</li>
<li><a href="http://www.linuxidc.com/Linux/2010-01/24170.htm" target="_blank">Ubuntu 9.10更新源的添加和更新</a>  (01/23/2010 12:09:55)</li>
<li><p><a href="http://www.linuxidc.com/Linux/2009-09/21812.htm" title="利用Nginx反向代理功能架设Ubuntu升级源" target="_blank">利用Nginx反向代理功能架设Ubuntu</a>  (09/20/2009 14:38:44)
图片资讯      </p>
</li>
<li><p><a href="http://www.linuxidc.com/Linux/2010-12/30938.htm" target="_blank"><img src="" alt="Ubuntu 10.10更新源">Ubuntu 10.10更新源</a></p>
</li>
<li><a href="http://www.linuxidc.com/Linux/2010-01/24170.htm" title="Ubuntu 9.10更新源的添加和更新" target="_blank"><img src="" alt="Ubuntu 9.10更新源的添加和更新">Ubuntu 9.10更新源的</a></li>
</ul>
<p>本文评论 　　<a href="http://www.linuxidc.com/remark.aspx?id=73114" target="_blank">查看全部评论</a> (0)</p>
<p>表情： <img src="" alt="表情"> 姓名：  匿名 字数
点评：
同意评论声明 　　　发表评论声明</p>
<ul>
<li>尊重网上道德，遵守中华人民共和国的各项有关法律法规</li>
<li>承担一切因您的行为而直接或间接导致的民事或刑事法律责任</li>
<li>本站管理人员有权保留或删除其管辖留言中的任意内容</li>
<li>本站有权在网站内转载或引用您的评论</li>
<li><p>参与本评论即表明您已经阅读并接受上述条款
Digg排行</p>
</li>
<li><p><a href="http://www.linuxidc.com/Linux/2012-05/59564.htm" target="_blank">10Ubuntu 12.04安装QQ2012</a></p>
</li>
<li><a href="http://www.linuxidc.com/Linux/2011-11/47705.htm" title="Linux下除了某个文件外的其他文件全部删除" target="_blank">6Linux下除了某个文件外的其他文</a></li>
<li><a href="http://www.linuxidc.com/Linux/2010-04/25829.htm" title="在VMware虚拟机上安装Ubuntu 10.04" target="_blank">4在VMware虚拟机上安装Ubuntu 10.</a></li>
<li><a href="http://www.linuxidc.com/Linux/2012-10/71796.htm" title="如何从CentOS 6.0升级到CentOS 6.2" target="_blank">3如何从CentOS 6.0升级到CentOS 6</a></li>
<li><a href="http://www.linuxidc.com/Linux/2012-06/63424.htm" target="_blank">3Win7用VMware安装Fedora 16</a></li>
<li><a href="http://www.linuxidc.com/Linux/2012-05/60806.htm" target="_blank">3Ubuntu 12.04 root用户登录设置</a></li>
<li><a href="http://www.linuxidc.com/Linux/2011-12/48609.htm" title="虚拟机下安装BackTrack5 (BT5)教程及BT5汉化" target="_blank">3虚拟机下安装BackTrack5 (BT5)教</a></li>
<li><a href="http://www.linuxidc.com/Linux/2012-10/71874.htm" title="新安装 Ubuntu 12.10 需要做的 10 件事" target="_blank">2新安装 Ubuntu 12.10 需要做的</a></li>
<li><a href="http://www.linuxidc.com/Linux/2012-09/71478.htm" title="Ubuntu使用conky美化监测系统状态" target="_blank">2Ubuntu使用conky美化监测系统状</a></li>
<li><a href="http://www.linuxidc.com/Linux/2012-09/71477.htm" title="Ubuntu在顶部面板显示系统负载、流量，磁盘I/O" target="_blank">2Ubuntu在顶部面板显示系统负载、</a></li>
</ul>
<p><a href="http://www.linuxidc.com/Linux/2012-07/66157.htm" title="Oracle" target="_blank"><img src="" alt=""></a> <a href="http://www.linuxidc.net/" target="_blank"><img src="" alt="LinuxIDC"></a> <a href="http://www.linuxidc.com/search.aspx?Where=Nkey&amp;Keyword=Ubuntu+11.10" target="_blank"><img src="" alt="Ubuntu"></a> <a href="http://www.linuxidc.com/topicnews.aspx?tid=2" target="_blank"><img src="" alt=""></a>
最新资讯</p>
<ul>
<li><a href="http://www.linuxidc.com/Linux/2012-10/73115.htm" target="_blank">Linux PAM make err : undefine yywrap()问题</a></li>
<li><a href="">Ubuntu 12.10 软件更新源列表</a></li>
<li><a href="http://www.linuxidc.com/Linux/2012-10/73113.htm" target="_blank">C/# 使用定时任务 之 Timer类</a></li>
<li><a href="http://www.linuxidc.com/Linux/2012-10/73112.htm" target="_blank">C/# 使用SQLite数据库</a></li>
<li><a href="http://www.linuxidc.com/Linux/2012-10/73111.htm" target="_blank">Ubuntu Grub2启动上一次正确启动的内核</a></li>
<li><a href="http://www.linuxidc.com/Linux/2012-10/73110.htm" target="_blank">Mozilla 向微软送蛋糕 祝贺其 IE10 发布</a></li>
<li><a href="http://www.linuxidc.com/Linux/2012-10/73109.htm" target="_blank">Wine 1.5.16 发布</a></li>
<li><a href="http://www.linuxidc.com/Linux/2012-10/73108.htm" target="_blank">FreeNAS 8.3.0 正式版发布</a></li>
<li><a href="http://www.linuxidc.com/Linux/2012-10/73107.htm" target="_blank">Debian 7.0 “Wheezy” Beta3 发布</a></li>
<li><a href="http://www.linuxidc.com/Linux/2012-10/73106.htm" target="_blank">Hadoop安装配置入门手册</a></li>
<li><a href="http://www.linuxidc.com/Linux/2012-10/73105.htm" target="_blank">配置Hive使用嵌入式derby或客服模式derby方法</a></li>
<li><a href="http://www.linuxidc.com/Linux/2012-10/73104.htm" title="Canonical发布Ubuntu Nexus 7 Desktop Installer" target="_blank">Canonical发布Ubuntu Nexus 7 Desktop</a></li>
</ul>
<p>本周热门</p>
<ul>
<li><a href="http://www.linuxidc.com/Linux/2010-04/25829.htm" target="_blank">在VMware虚拟机上安装Ubuntu 10.04</a></li>
<li><a href="http://www.linuxidc.com/Linux/2012-05/59564.htm" target="_blank">Ubuntu 12.04安装QQ2012</a></li>
<li><a href="http://www.linuxidc.com/Linux/2010-03/24993.htm" target="_blank">Fedora 12 LiveCD安装记录[图文]</a></li>
<li><a href="http://www.linuxidc.com/Linux/2010-09/28435.htm" title="Ubuntu中用VirtualBox虚拟机安装Windows XP完整图解" target="_blank">Ubuntu中用VirtualBox虚拟机安装Windows XP完整</a></li>
<li><a href="http://www.linuxidc.com/Linux/2012-05/59663.htm" target="_blank">Ubuntu 12.04和Windows 7双系统安装图解</a></li>
<li><a href="http://www.linuxidc.com/Linux/2011-12/48609.htm" target="_blank">虚拟机下安装BackTrack5 (BT5)教程及BT5汉化</a></li>
<li><a href="http://www.linuxidc.com/Linux/2012-04/59433.htm" target="_blank">Windows XP硬盘安装Ubuntu 12.04双系统图文详解</a></li>
<li><a href="http://www.linuxidc.com/Linux/2010-04/25573.htm" target="_blank">Virtualbox虚拟机安装Ubuntu图文教程</a></li>
<li><a href="http://www.linuxidc.com/Linux/2006-11/1006.htm" target="_blank">红旗Linux5.0安装教程</a></li>
<li><a href="http://www.linuxidc.com/Linux/2011-10/46327.htm" target="_blank">Windows XP硬盘安装Ubuntu 11.10双系统全程图解</a></li>
<li><a href="http://www.linuxidc.com/Linux/2012-05/60321.htm" target="_blank">Linux卷管理详解--VG LV PV</a></li>
<li><a href="http://www.linuxidc.com/Linux/2012-06/63424.htm" target="_blank">Win7用VMware安装Fedora 16</a></li>
</ul>
<p><a href="http://www.linuxidc.com/aboutus.htm" target="_blank">Linux公社简介</a> - <a href="http://www.linuxidc.com/adsense.htm" target="_blank">广告服务</a> - <a href="http://www.linuxidc.com/sitemap.aspx" target="_blank">网站地图</a> - <a href="http://www.linuxidc.com/help.htm" target="_blank">帮助信息</a> - <a href="http://www.linuxidc.com/contactus.htm" target="_blank">联系我们</a>
本站（LinuxIDC）所刊载文章不代表同意其说法或描述，仅为提供更多信息，也不构成任何建议。
主编：漏网的鱼 (QQ:3165270) 联系邮箱:<img src="" alt=""> (如有版权及广告合作请联系)
本站带宽由[<a href="http://www.6688.cc/" target="_blank">6688.CC</a>]友情提供
关注Linux，关注LinuxIDC.com，请向您的QQ好友宣传LinuxIDC.com，多谢支持！
Copyright © 2006-2011　<a href="http://www.linuxidc.com/" target="_blank">Linux公社</a>　All rights reserved 浙ICP备06018118号</p>
<p>成功接收数据</p>

      
    </div>
    
    
      
    
    
        <footer id="post-meta">
        <span class="categories">Posted in<span class="breadcrumb fa fa-folder"><li><a href="/categories/linux/">linux</a></li></span><span class="breadcrumb"><li><a href="/categories/linux/">linux</a></li><li><a href="/categories/linux/ubuntu/">ubuntu</a></li></span></span> | <span class="tags">Tagged <a href="/tags/linux/" class="label label-primary">linux</a><a href="/tags/ubuntu/" class="label label-success">ubuntu</a></span> | <span class="time">recent updated:<time title="2014-03-07 09:54:36"datetime="2014-03-07 09:54:36"> mar. 7 2014</time></span> | <span class="comment-link">
<a href="http://itsolife.com/2014/02/02/2014-02-02-linux-ubuntu--Ubuntu1210软件更新源列表_Linux教程_Linux公社-Linux系统门户网站/#comments" class="ds-thread-count comment-link" data-thread-key="2014-02-02-linux-ubuntu--Ubuntu1210软件更新源列表_Linux教程_Linux公社-Linux系统门户网站" data-count-type="comments">暂无评论</a></span>
        </footer>
    
    <div class="clearfix"></div>
  </div>
</div>
</article>





<ul id="pagination" class="pagination pagination-lg">
  <li><a class="extend prev" href="/page/107/">&laquo;</a></li><li><a class="page-number" href="/">1</a></li><li><a class="page-number" href="/page/2/">2</a></li><li><span class="space">&hellip;</span></li><li><a class="page-number" href="/page/105/">105</a></li><li><a class="page-number" href="/page/106/">106</a></li><li><a class="page-number" href="/page/107/">107</a></li><li class="active"><li><span class="page-number current">108</span></li><li><a class="page-number" href="/page/109/">109</a></li><li><a class="page-number" href="/page/110/">110</a></li><li><a class="page-number" href="/page/111/">111</a></li><li><span class="space">&hellip;</span></li></li><li><a class="page-number" href="/page/164/">164</a></li><li><a class="page-number" href="/page/165/">165</a></li><li><a class="extend next" href="/page/109/">&raquo;</a></li>
  <div class="clearfix"></div>
</ul></div><!--wapper-->
       </div><!-- ID main-col END -->
       <aside id="sidebar" class="alignright col-sx-6 col-sm-4 col-md-3 col-lg-3">
<div id="widget_search" class="widget panel panel-primary">
    <form action="//google.com/search" method="get" accept-charset="utf-8">
  <div class="input-group">
    <input class="form-control" id="searchbox" type="search" name="q" results="0" placeholder="search">
    <span class="input-group-btn">
      <button class="btn btn-default" type="submit">Go!</button>
    </span>
    <input type="hidden" name="q" value="site:itsolife.com">
  </div>
</form>
</div>

<div id="widget_category" class="widget panel panel-primary">
  <div class="panel-heading">category</div>  <div data-src='category' class='ajax_widgets'>正在加载...</div>
</div>

<div id="widget_recent_posts" class="widget panel panel-primary">
  <div class="panel-heading">recent posts</div>  <div data-src='recent_posts' class='ajax_widgets'>正在加载...</div></div>

<div id="widget_tagcloud" class="widget panel panel-primary">
  <div class="panel-heading">tagcloud</div>  <div data-src='tagcloud' class='ajax_widgets'>正在加载...</div></div>

<div id="widget_latest_update_posts" class="widget panel panel-primary">
  <div class="panel-heading">最近更新</div>  <div data-src='latest_update_posts' class='ajax_widgets'>正在加载...</div></div>

<div id="widget_recent_comments" class="widget panel panel-primary">
  <div class="panel-heading">recent comments</div>  

<div class="list-group-item ds-recent-comments" data-num-items="6" data-show-avatars="1" data-show-time="1" data-show-title="1" data-show-admin="1" data-excerpt-length="50"></div>



</div>

</aside>
       <div class="clearfix"></div>
     </div><!-- row END -->
  </div>
  <footer id="footer" class="container">
  <div class="panel panel-info">
  <section id='footer_widget'></section>  <div class="panel-footer">
  <div id="site-info">
    <span class='author'>
  
  &copy; 2014 RobinChia
  
    &nbsp;&nbsp;</span>
  
  <span id='analytics-51la'></span><span id='analytics-google'>
<script type="text/javascript">
  var _gaq = _gaq || [];
  _gaq.push(['_setAccount', 'UA-48559895-1']);
  _gaq.push(['_trackPageview']);
  _js2load.push({src:('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js'});
</script></span><span id='analytics-cnzz'>
<script type="text/javascript">var cnzz_protocol = (("https:" == document.location.protocol) ? " https://" : " http://");document.write(unescape("%3Cspan id='cnzz_stat_icon_5774006'%3E%3C/span%3E%3Cscript src='" + cnzz_protocol + "s17.cnzz.com/stat.php%3Fid%3D5774006%26show%3Dpic' type='text/javascript'%3E%3C/script%3E"));</script>
</span><span id='analytics-baidu'>
<script>
var _hmt = _hmt || [];
_js2load.push({src:"//hm.baidu.com/hm.js?1442f50724afc42380b51f097c43082c"});
</script>
</span>  </div>
  <div id="copyright">Blog powered by <a href='http://zespia.tw/hexo/'><strong>hexo</strong></a> Theme <strong><a href='https://github.com/chenall/hexo-theme-chenall'>chenall</a></strong>(Some change in it)<span class="pull-right"> 更新时间: <em>2014-03-15 17:28:50</em></span></div>
</div>
<div class="clearfix"></div>


  </div>
  </footer>
  
        <script src="http://cdn.bootcss.com/jquery/1.10.2/jquery.min.js"></script>        
        <script src="http://cdn.staticfile.org/twitter-bootstrap/3.1.0/js/bootstrap.min.js"></script>        
                
        <script src="http://cdn.bootcss.com/prettify/r298/prettify.min.js"></script>    <script type="text/javascript">
   var lang=["bsh", "c", "cc", "cpp", "cs", "csh", "cyc", "cv", "htm", "html",
    "java", "js", "m", "mxml", "perl", "pl", "pm", "py", "rb", "sh",
    "xhtml", "xml", "xsl"];
   var pretty_base='';
   $('script').each(function(){
	var c = $(this).attr('src');
	if (!c)
	    return;
	if (c.match(/(\/)?prettify(\.min)?\.js/i))
	{
	    var index = c.lastIndexOf('/');
	    if (index != -1)
		pretty_base = c.substr(0,index + 1);
	    return false;
	}
   })
   $('pre code').each(function(){
	var c = $(this).attr('class')
	if (!c)
	    return;
	c = c.match(/\s?(lang\-\w+)/i);
	if (c && lang.indexOf(c[1]) == -1)
	{
	    lang.push(c[1]);
	    $.getScript(pretty_base + c[1] + '.min.js');
	}
   })

    $(window).load(function(){
       $("pre").addClass("prettyprint");
       prettyPrint();
    })
</script>    
            <script type="text/javascript">
var duoshuoQuery = {short_name:"robinchia"};
_js2load.push({src:'http://static.duoshuo.com/embed.js',charset:'UTF-8'});
</script>
    
            <!--wumii_relatedItems-->
    
        <script src="http://cdn.bootcss.com/fancybox/2.1.5/jquery.fancybox.min.js"></script>    <script type="text/javascript">
(function($){
  $('.entry').each(function(i){
    $(this).find('img').each(function(){
      var alt = this.alt;

      if (alt){
        $(this).before('<span class="caption">' + alt + '</span>');
      }

      $(this).wrap('<a href="' + this.src + '" title="' + alt + '" class="fancybox" rel="fancybox' + i + '" />');
    });
  });

  $('.fancybox').fancybox();
})(jQuery);
</script>    
        <script src="http://cdn.bootcss.com/mathjax/2.3/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>    <script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      processEscapes: true
    }
  });
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>
    
<script type="text/javascript">
$('.ajax_widgets').each(function(){var src=$(this).attr('data-src');if(src)$(this).load('/widgets/'+src+'.html');});
$.each(_js2load,function(index,obj){loadjs(obj.src,obj.charset)});
</script>

<div id="scroll2top">
<img src="/scroll2top/arrow.png"/>
</div>
<script src="/scroll2top/scroll2top.min.js"></script>
<div id="winterland">
  <canvas></canvas>
</div>
<script src="/js/winterland.min.js"></script>

  </body>
</html>
