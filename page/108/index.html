
<!DOCTYPE HTML>
<html>
<head>
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<title>The 108 Page | It so life</title>
<meta name="author" content="RobinChia">

<meta name="description" content="It so life">


<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">


<meta property="og:site_name" content="It so life"/>

<!--[if IE]><style>.testIE.IE{display:inline;}</style><![endif]-->
<!--[if lte IE 7]><link rel="stylesheet" href="/css/ie7.css" type="text/css"><![endif]-->
<!--[if (lt IE 9)&(gt IE 7)]><style>.testIE.IE8{display:inline;}</style><![endif]-->
<!--[if gt IE 8]><style>.testIE.IE9{display:inline;}</style><![endif]-->

<link href="/favicon.png" rel="icon">
<link rel="alternate" href="/atom.xml" title="It so life Feed" type="application/atom+xml">

<link rel="stylesheet" href="http://cdn.staticfile.org/twitter-bootstrap/3.1.0/css/bootstrap.min.css" type="text/css">
<link rel="stylesheet" href="http://cdn.bootcss.com/font-awesome/4.0.3/css/font-awesome.min.css" type="text/css">
<link rel="stylesheet" href="http://cdn.bootcss.com/prettify/r298/prettify.min.css" type="text/css">
<link rel="stylesheet" href="http://cdn.bootcss.com/fancybox/2.1.5/jquery.fancybox.min.css" type="text/css">
<link rel="stylesheet" href="/css/style.css" type="text/css">
<!--[if lt IE 9]>
   <style>article,aside,dialog,footer,header,section,footer,nav,figure,menu{display:block}</style>
   <script src="http://cdn.staticfile.org/html5shiv/r29/html5.min.js"></script>
   <script src="http://cdn.staticfile.org/respond.js/1.4.2/respond.min.js"></script>
<link href="http://cdn.staticfile.org/respond.js/1.4.2/respond-proxy.html" id="respond-proxy" rel="respond-proxy" />
<link href="/js/respond.proxy.gif" id="respond-redirect" rel="respond-redirect" />
<script src="/js/respond.proxy.js"></script>
   <script src="http://cdn.bootcss.com/selectivizr/1.0.2/selectivizr-min.js"></script>
<![endif]-->
<script type="text/javascript">
function loadjs(c,d){var a=document.createElement("script");a.async=!0;a.type="text/javascript";a.src=c;a.charset=d||"gbk";(document.getElementsByTagName("HEAD")[0]||document.getElementsByTagName("BODY")[0]).appendChild(a)};
var _protocol = (("https:" == document.location.protocol) ? " https://" : " http://");
var _js2load = [];
</script>

</head>
<body>
      <header id="header" class="container"><nav id="main-nav" class="navbar navbar-default navbar-fixed-top " role="navigation">
  <div class="container">
    <div class="navbar-header">
    <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".navbar-collapse">
      <span class="sr-only">Toggle navigation</span>
      <span class="icon-bar"></span>
      <span class="icon-bar"></span>
      <span class="icon-bar"></span>
    </button>
    <a class="navbar-brand" href="/">It so life</a>
    </div>
    <div  class="collapse navbar-collapse">
      <ul  class="nav navbar-nav">
  
        <li><a href="/" title="Home">Home</a></li>      
        <li><a href="/about/" title="About">About</a></li>      
        <li><a href="/archives/" title="Archives">Archives</a></li>      
      <li class='dropdown'>
        <a class='dropdown-toggle' data-toggle='dropdown' href='#'>website<b class='caret'></b></a>
        <ul class='dropdown-menu pure-menu-selected'>
    
          <li><a href="//groups.google.com/forum/#!forum/pongba" title="TopLanguage">TopLanguage</a></li>    
          <li><a href="//itpub.net/" title="ITPub">ITPub</a></li>    
          <li><a href="//blog.jobbole.com/" title="Bole">Bole</a></li>    
          <li><a href="//nosql-database.org/" title="nosql">nosql</a></li>    
          <li><a href="//gitimmersion.googol.im/" title="Git">Git</a></li>    
        </ul>
      </li>
    
      </ul>
      <ul class="nav navbar-nav navbar-right">
      
        <li><a href="/atom.xml">RSS</a></li>
      
      
        <li><a href="https://twitter.com/robinchia">twitter</a></li>
      
      
      
      
        <li><a href="https://github.com/robinchia">github</a></li>
      
      </ul>
    </div>
  </div>
</nav>
<div class="clearfix"></div>
</header>
  <div id='content' class="container">
     <div class="page-header-wrapper">
      <!--[if lt IE 9]><div class="alert alert-warning alert-dismissable"><button type="button" class="close" data-dismiss="alert" aria-hidden="true">&times;</button><strong>提示:</strong>您的浏览器版本太低了,建议升级到 <strong><a href="http://windows.microsoft.com/zh-cn/internet-explorer/download-ie" title="IE9">IE9</a></strong> 以上,本站使用<a href="https://www.google.com/intl/zh-CN/chrome/">Chrome浏览器</a>可以获得最好的显示效果.</div><![endif]-->
      <div class="page-header"><h1 align="center"><big>It so life</big> </h1>
        <h5 align="center"><big>love as life</big></h5>
      </div>
     </div>
     <div class="row">
       <div id="main-col" class="alignleft col-sx-12 col-sm-8 col-md-9 col-lg-9">
      <section id='header_widget'></section>
          <div id="wrapper"><article>
<div id="post" class="post well">
  <div class="post-content">
    <header class="well-sm">
      <i class="fa icon fa-5x pull-left"></i>
      <h1 class="title"><a href="/2014/02/02/2014-02-02-linux--linux之sort用法/">linux之sort用法</a></h1>
      
        <span>Posted on<time datetime="2014-02-02T01:54:36.000Z"> <a href="/2014/02/02/2014-02-02-linux--linux之sort用法/">feb. 2 2014</a></time></span>      
    </header>
    
    
    <div class="entry">
      
        <h1 id="linux-sort-">linux之sort用法</h1>
<p>sort命令是帮我们依据不同的数据类型进行排序，其语法及常用参数格式：
sort [-bcfMnrtk][源文件][-o 输出文件]
补充说明：sort可针对文本文件的内容，以行为单位来排序。</p>
<p>参　　数：
  -b   忽略每行前面开始出的空格字符。
  -c   检查文件是否已经按照顺序排序。
  -f   排序时，忽略大小写字母。
  -M   将前面3个字母依照月份的缩写进行排序。
  -n   依照数值的大小排序。
  -o&lt;输出文件&gt;   将排序后的结果存入指定的文件。
  -r   以相反的顺序来排序。
  -t&lt;分隔字符&gt;   指定排序时所用的栏位分隔字符。
  -k  选择以哪个区间进行排序。
  <del>~</del><del>~</del><del>~</del><del>~</del><del>~</del><del>~</del>~~~~</p>
<p>下面通过几个例子来讲述Sort的使用。
（1）sort将文件的每一行作为一个单位，相互比较，比较原则是从首字符向后，依次按ASCII码值进行比较，最后将他们按升序输出。</p>
<p>[rocrocket@rocrocket programming]$ cat seq.txt
banana
apple
pear
orange
[rocrocket@rocrocket programming]$ sort seq.txt
apple
banana
orange
pear</p>
<p>用户可以保存排序后的文件内容，或把排序后的文件内容输出至打印机。下例中用户把排序后的文件内容保存到名为result的文件中。
$ Sort seq.txt &gt; result</p>
<p>（2）sort的-u选项</p>
<p>它的作用很简单，就是在输出行中去除重复行。</p>
<p>[rocrocket@rocrocket programming]$ cat seq.txt
banana
apple
pear
orange
pear
[rocrocket@rocrocket programming]$ sort seq.txt
apple
banana
orange
pear
pear
[rocrocket@rocrocket programming]$ sort -u seq.txt
apple
banana
orange
pear</p>
<p>pear由于重复被-u选项无情的删除了。</p>
<p>（3）sort的-r选项</p>
<p>sort默认的排序方式是升序，如果想改成降序，就加个-r就搞定了。</p>
<p>[rocrocket@rocrocket programming]$ cat number.txt
1
3
5
2
4
[rocrocket@rocrocket programming]$ sort number.txt
1
2
3
4
5
[rocrocket@rocrocket programming]$ sort -r number.txt
5
4
3
2
1
（5）sort的-o选项</p>
<p>由于sort默认是把结果输出到标准输出，所以需要用重定向才能将结果写入文件，形如sort filename &gt; newfile。</p>
<p>但是，如果你想把排序结果输出到原文件中，用重定向可就不行了。</p>
<p>[rocrocket@rocrocket programming]$ sort -r number.txt &gt; number.txt
[rocrocket@rocrocket programming]$ cat number.txt
[rocrocket@rocrocket programming]$
看，竟然将number清空了。</p>
<p>就在这个时候，-o选项出现了，它成功的解决了这个问题，让你放心的将结果写入原文件。这或许也是-o比重定向的唯一优势所在。</p>
<p>[rocrocket@rocrocket programming]$ cat number.txt
1
3
5
2
4
[rocrocket@rocrocket programming]$ sort -r number.txt -o number.txt
[rocrocket@rocrocket programming]$ cat number.txt
5
4
3
2
1</p>
<p>（6） sort的-n选项</p>
<p>你有没有遇到过10比2小的情况。我反正遇到过。出现这种情况是由于排序程序将这些数字按字符来排序了，排序程序会先比较1和2，显然1小，所以就将10放在2前面喽。这也是sort的一贯作风。</p>
<p>我们如果想改变这种现状，就要使用-n选项，来告诉sort，“要以数值来排序”！</p>
<p>[rocrocket@rocrocket programming]$ cat number.txt
1
10
19
11
2
5
[rocrocket@rocrocket programming]$ sort number.txt
1
10
11
19
2
5
[rocrocket@rocrocket programming]$ sort -n number.txt
1
2
5
10
11
19</p>
<p>（7） sort的-t选项和-k选项</p>
<p>如果有一个文件的内容是这样：</p>
<p>[rocrocket@rocrocket programming]$ cat facebook.txt
banana:30:5.5
apple:10:2.5
pear:90:2.3
orange:20:3.4</p>
<p>这个文件有三列，列与列之间用冒号隔开了，第一列表示水果类型，第二列表示水果数量，第三列表示水果价格。那么我想以水果数量来排序，也就是以第二列来排序，如何利用sort实现？幸好，sort提供了-t选项，后面可以设定间隔符。指定了间隔符之后，就可以用-k来指定列数了。</p>
<p>[rocrocket@rocrocket programming]$ sort -n -k 2 -t ‘:’ facebook.txt
apple:10:2.5
orange:20:3.4
banana:30:5.5
pear:90:2.3</p>
<p>（8） 其他的sort常用选项</p>
<p>-f会将小写字母都转换为大写字母来进行比较，亦即忽略大小写</p>
<p>-c会检查文件是否已排好序，如果乱序，则输出第一个乱序的行的相关信息，最后返回1</p>
<p>-C会检查文件是否已排好序，如果乱序，不输出内容，仅返回1</p>
<p>-M会以月份来排序，比如JAN小于FEB等等</p>
<p>-b会忽略每一行前面的所有空白部分，从第一个可见字符开始比较。
来源： <a href="[http://www.cnblogs.com/dong008259/archive/2011/12/08/2281214.html](http://www.cnblogs.com/dong008259/archive/2011/12/08/2281214.html)">[http://www.cnblogs.com/dong008259/archive/2011/12/08/2281214.html](http://www.cnblogs.com/dong008259/archive/2011/12/08/2281214.html)</a> </p>

      
    </div>
    
    
      
    
    
        <footer id="post-meta">
        <span class="categories">Posted in<span class="breadcrumb fa fa-folder"><li><a href="/categories/linux/">linux</a></li></span></span> | <span class="tags">Tagged <a href="/tags/linux/" class="label label-primary">linux</a></span> | <span class="time">recent updated:<time title="2014-03-07 09:54:36"datetime="2014-03-07 09:54:36"> mar. 7 2014</time></span> | <span class="comment-link">
<a href="http://itsolife.com/2014/02/02/2014-02-02-linux--linux之sort用法/#comments" class="ds-thread-count comment-link" data-thread-key="2014-02-02-linux--linux之sort用法" data-count-type="comments">暂无评论</a></span>
        </footer>
    
    <div class="clearfix"></div>
  </div>
</div>
</article>



  <article>
<div id="post" class="post well">
  <div class="post-content">
    <header class="well-sm">
      <i class="fa icon fa-5x pull-left"></i>
      <h1 class="title"><a href="/2014/02/02/2014-02-02-hadoop--Hadoop集群_Hadoop安装配置/">Hadoop集群_Hadoop安装配置</a></h1>
      
        <span>Posted on<time datetime="2014-02-02T01:54:36.000Z"> <a href="/2014/02/02/2014-02-02-hadoop--Hadoop集群_Hadoop安装配置/">feb. 2 2014</a></time></span>      
    </header>
    
    
    <div class="entry">
      
        <h1 id="hadoop-_hadoop-">Hadoop集群_Hadoop安装配置</h1>
<h1 id="-hadoop-5-_hadoop-http-www-cnblogs-com-xia520pi-archive-2012-05-16-2503949-html-"><a href="http://www.cnblogs.com/xia520pi/archive/2012/05/16/2503949.html" target="_blank">Hadoop集群（第5期）_Hadoop安装配置</a></h1>
<h2 id="1-">1、集群部署介绍</h2>
<h3 id="1-1-hadoop-">1.1 Hadoop简介</h3>
<p><img src="" alt="">　　Hadoop是Apache软件基金会旗下的一个开源分布式计算平台。以Hadoop分布式文件系统（HDFS，Hadoop Distributed Filesystem）和MapReduce（Google MapReduce的开源实现）为<strong>核心</strong>的Hadoop为用户提供了系统底层细节透明的分布式基础架构。</p>
<p>对于Hadoop的集群来讲，可以分成两大类角色：Master和Salve。一个<strong>HDFS</strong>集群是由一个NameNode和若干个DataNode组成的。其中NameNode作为主服务器，管理文件系统的命名空间和客户端对文件系统的访问操作；集群中的DataNode管理存储的数据。<strong>MapReduce</strong>框架是由一个单独运行在主节点上的JobTracker和运行在每个集群从节点的TaskTracker共同组成的。主节点负责调度构成一个作业的所有任务，这些任务分布在不同的从节点上。主节点监控它们的执行情况，并且重新执行之前的失败任务；从节点仅负责由主节点指派的任务。当一个Job被提交时，JobTracker接收到提交作业和配置信息之后，就会将配置信息等分发给从节点，同时调度任务并监控TaskTracker的执行。</p>
<p>从上面的介绍可以看出，HDFS和MapReduce共同组成了Hadoop分布式系统体系结构的核心。<strong>HDFS</strong>在集群上实现分布式文件系统，<strong>MapReduce</strong>在集群上实现了分布式计算和任务处理。HDFS在MapReduce任务处理过程中提供了文件操作和存储等支持，MapReduce在HDFS的基础上实现了任务的分发、跟踪、执行等工作，并收集结果，二者相互作用，完成了Hadoop分布式集群的主要任务。</p>
<h3 id="1-2-">1.2 环境说明</h3>
<p>集群中包括4个节点：1个Master，3个Salve，节点之间局域网连接，可以相互ping通，具体集群信息可以查看&quot;<strong>Hadoop集群（第2期）</strong>&quot;。节点IP地址分布如下：</p>
<p><strong>机器名称</strong></p>
<p><strong>IP地址</strong>Master.Hadoop</p>
<p>192.168.1.2Salve1.Hadoop</p>
<p>192.168.1.3Salve2.Hadoop</p>
<p>192.168.1.4Salve3.Hadoop</p>
<p>192.168.1.5</p>
<p>四个节点上均是CentOS6.0系统，并且有一个相同的用户<strong>hadoop</strong>。Master机器主要配置NameNode和JobTracker的角色，负责总管分布式数据和分解任务的执行；3个Salve机器配置DataNode和TaskTracker的角色，负责分布式数据存储以及任务的执行。其实应该还应该有1个Master机器，用来作为<strong>备用</strong>，以防止Master服务器<strong>宕机</strong>，还有一个备用马上启用。后续经验积累一定阶段后<strong>补上</strong>一台备用Master机器。</p>
<h3 id="1-3-">1.3 网络配置</h3>
<p>Hadoop集群要按照<strong>1.2小节</strong>表格所示进行配置，我们在&quot;<strong>Hadoop集群（第1期）</strong>&quot;的CentOS6.0安装过程就按照提前规划好的主机名进行安装和配置。如果实验室后来人在安装系统时，没有配置好，不要紧，没有必要重新安装，在安装完系统之后仍然可以根据后来的规划对机器的主机名进行修改。</p>
<p>下面的例子我们将以Master机器为例，即主机名为&quot;Master.Hadoop&quot;，IP为&quot;192.168.1.2&quot;进行一些主机名配置的相关操作。其他的Slave机器以此为依据进行修改。</p>
<p><strong>1）查看当前机器名称</strong></p>
<p>用下面命令进行显示机器名称，如果跟规划的不一致，要按照下面进行修改。</p>
<p>hostname</p>
<p><img src="" alt=""></p>
<p>上图中，用&quot;hostname&quot;查&quot;Master&quot;机器的名字为&quot;Master.Hadoop&quot;，与我们预先规划的一致。</p>
<p><strong>2）修改当前机器名称</strong></p>
<p><strong>假定</strong>我们发现我们的机器的主机名不是我们想要的，通过对&quot;<strong>/etc/sysconfig/network</strong>&quot;文件修改其中&quot;<strong>HOSTNAME</strong>&quot;后面的值，改成我们规划的名称。</p>
<p>这个&quot;<strong>/etc/sysconfig/network</strong>&quot;文件是定义hostname和是否利用网络的不接触网络设备的对系统全体定义的文件。</p>
<p><strong>设定形式</strong>：设定值=值</p>
<p>&quot;/etc/sysconfig/network&quot;的<strong>设定项目</strong>如下：</p>
<p>NETWORKING 是否利用网络</p>
<p>GATEWAY 默认网关</p>
<p>IPGATEWAYDEV 默认网关的接口名</p>
<p>HOSTNAME 主机名</p>
<p>DOMAIN 域名</p>
<p>用下面命令进行修改当前机器的主机名（<strong>备注：</strong>修改系统文件一般用<strong>root</strong>用户）</p>
<p>vim /etc/sysconfig/network</p>
<p><img src="" alt=""></p>
<p>通过上面的命令我们从&quot;/etc/sysconfig/network&quot;中找到&quot;HOSTNAME&quot;进行修改，查看内容如下：</p>
<p><img src="" alt=""></p>
<p><strong>3）修改当前机器IP</strong></p>
<p><strong>假定</strong>我们的机器连IP在当时安装机器时都没有配置好，那此时我们需要对&quot;<strong>ifcfg-eth0</strong>&quot;文件进行配置，该文件位于&quot;<strong>/etc/sysconfig/network-scripts</strong>&quot;文件夹下。</p>
<p>在这个目录下面，存放的是网络接口（网卡）的制御脚本文件（控制文件），ifcfg- eth0是默认的第一个网络接口，如果机器中有多个网络接口，那么名字就将依此类推ifcfg-eth1，ifcfg-eth2，ifcfg- eth3，……。</p>
<p>这里面的文件是相当重要的，涉及到网络能否正常工作。</p>
<p>设定形式：设定值=值</p>
<p>设定项目项目如下：</p>
<p>DEVICE 接口名（设备,网卡）</p>
<p>BOOTPROTO IP的配置方法（static:固定IP， dhcpHCP， none:手动）</p>
<p>HWADDR MAC地址</p>
<p>ONBOOT 系统启动的时候网络接口是否有效（yes/no）</p>
<p>TYPE 网络类型（通常是Ethemet）</p>
<p>NETMASK 网络掩码</p>
<p><strong>IPADDR</strong> IP地址</p>
<p>IPV6INIT IPV6是否有效（yes/no）</p>
<p>GATEWAY 默认网关IP地址</p>
<p>查看&quot;/etc/sysconfig/network-scripts/ifcfg-eth0&quot;内容，如果IP不复核，就行修改。</p>
<p><img src="" alt=""></p>
<p>如果上图中IP与规划不相符，用下面命令进行修改：</p>
<p>vim /etc/sysconfig/network-scripts/ifcgf-eth0</p>
<p>修改完之后可以用&quot;ifconfig&quot;进行查看。</p>
<p><img src="" alt=""></p>
<p><strong>4）配置hosts文件（必须）</strong></p>
<p>&quot;<strong>/etc/hosts</strong>&quot;这个文件是用来配置主机将用的<strong>DNS</strong>服务器信息，是记载LAN内接续的各主机的对应[HostName和IP]用的。当用户在进行网络连接时，首先查找该文件，寻找对应主机名（或域名）对应的IP地址。</p>
<p>我们要测试两台机器之间知否连通，一般用&quot;ping 机器的IP&quot;，如果想用&quot;ping 机器的主机名&quot;发现找不见该名称的机器，解决的办法就是修改&quot;<strong>/etc/hosts</strong>&quot;这个文件，通过把LAN内的各主机的IP地址和HostName的<strong>一一对应</strong>写入这个文件的时候，就可以解决问题。</p>
<p>例如：机器为&quot;Master.Hadoop:192.168.1.2&quot;对机器为&quot;Salve1.Hadoop:192.168.1.3&quot;用命令&quot;ping&quot;记性连接测试。测试结果如下：</p>
<p><img src="" alt=""></p>
<p>从上图中的值，直接对IP地址进行测试，能够ping通，但是对主机名进行测试，发现没有ping通，提示&quot;unknown host——未知主机&quot;，这时查看&quot;Master.Hadoop&quot;的&quot;/etc/hosts&quot;文件内容。</p>
<p><img src="" alt=""></p>
<p>发现里面没有&quot;192.168.1.3 Slave1.Hadoop&quot;内容，故而本机器是无法对机器的主机名为&quot;Slave1.Hadoop&quot; 解析。</p>
<p>在进行<strong>Hadoop集群</strong>配置中，需要在&quot;/etc/hosts&quot;文件中添加集群中所有机器的IP与主机名，这样Master与所有的Slave机器之间不仅可以通过IP进行通信，而且还可以通过主机名进行通信。所以在所有的机器上的&quot;/etc/hosts&quot;文件<strong>末尾</strong>中都要添加如下内容：</p>
<p>192.168.1.2 Master.Hadoop</p>
<p>192.168.1.3 Slave1.Hadoop</p>
<p>192.168.1.4 Slave2.Hadoop</p>
<p>192.168.1.5 Slave3.Hadoop</p>
<p>用以下命令进行添加：</p>
<p>vim /etc/hosts</p>
<p><img src="" alt=""></p>
<p>添加结果如下：</p>
<p><img src="" alt=""></p>
<p>现在我们在进行对机器为&quot;Slave1.Hadoop&quot;的主机名进行ping通测试，看是否能测试成功。</p>
<p><img src="" alt=""></p>
<p>从上图中我们已经能用主机名进行ping通了，说明我们刚才添加的内容，在局域网内能进行DNS解析了，那么现在剩下的事儿就是在其余的Slave机器上进行相同的配置。然后进行测试。（<strong>备注：</strong>当设置SSH无密码验证后，可以&quot;scp&quot;进行复制，然后把原来的&quot;hosts&quot;文件执行覆盖即可。）</p>
<h3 id="1-4-">1.4 所需软件</h3>
<p><strong>1）JDK软件</strong></p>
<p>下载地址：<a href="http://www.oracle.com/technetwork/java/javase/index.html" target="_blank"><a href="http://www.oracle.com/technetwork/java/javase/index.html">http://www.oracle.com/technetwork/java/javase/index.html</a></a></p>
<p>JDK版本：jdk-6u31-linux-i586.bin</p>
<p><strong>2）Hadoop软件</strong></p>
<p>下载地址：<a href="http://hadoop.apache.org/common/releases.html" target="_blank"><a href="http://hadoop.apache.org/common/releases.html">http://hadoop.apache.org/common/releases.html</a></a></p>
<p>Hadoop版本：hadoop-1.0.0.tar.gz</p>
<h3 id="1-5-vsftp-">1.5 VSFTP上传</h3>
<p>在&quot;<strong>Hadoop集群（第3期）</strong>&quot;讲了VSFTP的安装及配置，如果没有安装VSFTP可以按照该文档进行安装。如果安装好了，就可以通过<strong>FlashFXP.exe</strong>软件把我们下载的JDK6.0和Hadoop1.0软件上传到&quot;<strong>Master.Hadoop:192.168.1.2</strong>&quot;服务器上。</p>
<p><img src="" alt=""></p>
<p>刚才我们用一般用户（hadoop）通过FlashFXP软件把所需的两个软件上传了跟目下，我们通过命令查看下一下是否已经上传了。</p>
<p><img src="" alt=""></p>
<p>从图中，我们的所需软件已经准备好了。</p>
<h2 id="2-ssh-">2、SSH无密码验证配置</h2>
<p>Hadoop运行过程中需要管理远端Hadoop守护进程，在Hadoop启动以后，NameNode是通过SSH（Secure Shell）来启动和停止各个DataNode上的各种守护进程的。这就必须在节点之间执行指令的时候是不需要输入密码的形式，故我们需要配置SSH运用无密码公钥认证的形式，这样NameNode使用SSH无密码登录并启动DataName进程，同样原理，DataNode上也能使用SSH无密码登录到NameNode。</p>
<h3 id="2-1-ssh-">2.1 安装和启动SSH协议</h3>
<p>在&quot;Hadoop集群（第1期）&quot;安装CentOS6.0时，我们选择了一些基本安装包，所以我们需要两个服务：ssh和rsync已经安装了。可以通过下面命令查看结果显示如下：</p>
<p>rpm –qa | grep openssh</p>
<p>rpm –qa | grep rsync</p>
<p><img src="" alt=""></p>
<p><strong>假设</strong>没有安装ssh和rsync，可以通过下面命令进行安装。</p>
<p>yum install ssh 安装SSH协议</p>
<p>yum install rsync （rsync是一个远程数据同步工具，可通过LAN/WAN快速同步多台主机间的文件）</p>
<p>service sshd restart 启动服务</p>
<p>确保所有的服务器都安装，上面命令执行完毕，各台机器之间可以通过密码验证相互登。</p>
<h3 id="2-2-master-salve">2.2 配置Master无密码登录所有Salve</h3>
<p><strong>1）SSH无密码原理</strong></p>
<p>Master（NameNode | JobTracker）作为客户端，要实现无密码公钥认证，连接到服务器Salve（DataNode | Tasktracker）上时，需要在Master上生成一个密钥对，包括一个公钥和一个私钥，而后将公钥复制到所有的Slave上。当Master通过SSH连接Salve时，Salve就会生成一个随机数并用Master的公钥对随机数进行加密，并发送给Master。Master收到加密数之后再用私钥解密，并将解密数回传给Slave，Slave确认解密数无误之后就允许Master进行连接了。这就是一个公钥认证过程，其间不需要用户手工输入密码。重要过程是将客户端Master复制到Slave上。</p>
<p><strong>2）Master机器上生成密码对</strong></p>
<p>在Master节点上执行以下命令：</p>
<p>ssh-keygen –t rsa –P &#39;&#39;</p>
<p>这条命是生成其<strong>无密码密钥对</strong>，询问其保存路径时<strong>直接回车</strong>采用默认路径。生成的密钥对：id_rsa和id_rsa.pub，默认存储在&quot;<strong>/home/hadoop/.ssh</strong>&quot;目录下。</p>
<p><img src="" alt=""></p>
<p>查看&quot;/home/hadoop/&quot;下是否有&quot;.ssh&quot;文件夹，且&quot;.ssh&quot;文件下是否有两个刚生产的无密码密钥对。</p>
<p><img src="" alt=""></p>
<p>接着在Master节点上做如下配置，把id_rsa.pub追加到授权的key里面去。</p>
<p>cat ~/.ssh/id_rsa.pub &gt;&gt; ~/.ssh/authorized_keys</p>
<p><img src="" alt=""></p>
<p>在验证前，需要做两件事儿。第一件事儿是修改文件&quot;<strong>authorized_keys</strong>&quot;权限（<strong>权限的设置非常重要，因为不安全的设置安全设置，会让你不能使用RSA功能</strong>），另一件事儿是用root用户设置&quot;<strong>/etc/ssh/sshd_config</strong>&quot;的内容。使其无密码登录有效。</p>
<p><strong>1）修改文件&quot;authorized_keys&quot;</strong></p>
<p>chmod 600 ~/.ssh/authorized_keys</p>
<p><img src="" alt=""></p>
<p><strong>备注：</strong>如果不进行设置，在验证时，扔提示你输入密码，在这里花费了将近半天时间来查找原因。在网上查到了几篇不错的文章，把作为&quot;<strong>Hadoop集群_第5期副刊_JDK和SSH无密码配置</strong>&quot;来帮助额外学习之用。</p>
<p><strong>2）设置SSH配置</strong></p>
<p>用<strong>root</strong>用户登录服务器修改SSH配置文件&quot;/etc/ssh/sshd_config&quot;的下列内容。</p>
<p><img src="" alt=""></p>
<p>RSAAuthentication yes /# 启用 RSA 认证</p>
<p>PubkeyAuthentication yes /# 启用公钥私钥配对认证方式</p>
<p>AuthorizedKeysFile .ssh/authorized_keys /# 公钥文件路径（和上面生成的文件同）</p>
<p>设置完之后记得<strong>重启SSH服务</strong>，才能使刚才设置有效。</p>
<p>service sshd restart</p>
<p><strong>退出root登录</strong>，使用<strong>hadoop</strong>普通用户验证是否成功。</p>
<p>ssh localhost</p>
<p><img src="" alt=""></p>
<p>从上图中得知无密码登录本级已经设置完毕，接下来的事儿是把<strong>公钥</strong>复制<strong>所有</strong>的Slave机器上。使用下面的命令格式进行复制公钥：</p>
<p>scp ~/.ssh/id_rsa.pub 远程用户名@远程服务器IP:~/</p>
<p>例如：</p>
<p>scp ~/.ssh/id_rsa.pub hadoop@192.168.1.3:~/</p>
<p>上面的命令是<strong>复制</strong>文件&quot;<strong>id_rsa.pub</strong>&quot;到服务器IP为&quot;<strong>192.168.1.3</strong>&quot;的用户为&quot;<strong>hadoop</strong>&quot;的&quot;<strong>/home/hadoop/</strong>&quot;下面。</p>
<p>下面就针对IP为&quot;192.168.1.3&quot;的Slave1.Hadoop的节点进行配置。</p>
<p><strong>1）把Master.Hadoop上的公钥复制到Slave1.Hadoop上</strong></p>
<hr>
<p><img src="" alt=""></p>
<p>从上图中我们得知，已经把文件&quot;id_rsa.pub&quot;传过去了，因为并没有建立起无密码连接，所以在连接时，仍然要提示输入输入Slave1.Hadoop服务器用户hadoop的密码。为了确保确实已经把文件传过去了，用SecureCRT登录Slave1.Hadoop:192.168.1.3服务器，查看&quot;/home/hadoop/&quot;下是否存在这个文件。</p>
<p><img src="" alt=""></p>
<p>从上面得知我们已经成功把公钥复制过去了。</p>
<p><strong>2）在&quot;/home/hadoop/&quot;下创建&quot;.ssh&quot;文件夹</strong></p>
<p>这一步<strong>并不是必须</strong>的，如果在Slave1.Hadoop的&quot;/home/hadoop&quot;<strong>已经存在</strong>就不需要创建了，因为我们之前并没有对Slave机器做过无密码登录配置，所以该文件是不存在的。用下面命令进行创建。（<strong>备注：</strong>用hadoop登录系统，如果不涉及系统文件修改，一般情况下都是用我们之前建立的普通用户hadoop进行执行命令。）</p>
<p>mkdir ~/.ssh</p>
<p>然后是修改文件夹&quot;<strong>.ssh</strong>&quot;的用户权限，把他的权限修改为&quot;<strong>700</strong>&quot;，用下面命令执行：</p>
<p>chmod 700 ~/.ssh</p>
<p><strong>备注：</strong>如果不进行，即使你按照前面的操作设置了&quot;authorized_keys&quot;权限，并配置了&quot;/etc/ssh/sshd_config&quot;，还重启了sshd服务，在Master能用&quot;ssh localhost&quot;进行无密码登录，但是对Slave1.Hadoop进行登录仍然需要输入密码，就是因为&quot;.ssh&quot;文件夹的权限设置不对。这个文件夹&quot;.ssh&quot;在配置SSH无密码登录时系统自动生成时，权限自动为&quot;700&quot;，如果是自己手动创建，它的组权限和其他权限都有，这样就会导致RSA无密码远程登录失败。</p>
<p><img src="" alt=""></p>
<p><img src="" alt=""></p>
<p>对比上面两张图，发现文件夹&quot;.ssh&quot;权限已经变了。</p>
<p><strong>3）追加到授权文件&quot;authorized_keys&quot;</strong></p>
<p>到目前为止Master.Hadoop的公钥也有了，文件夹&quot;.ssh&quot;也有了，且权限也修改了。这一步就是把Master.Hadoop的公钥<strong>追加</strong>到Slave1.Hadoop的授权文件&quot;authorized_keys&quot;中去。使用下面命令进行追加并修改&quot;authorized_keys&quot;文件权限：</p>
<p>cat ~/id_rsa.pub &gt;&gt; ~/.ssh/authorized_keys</p>
<p>chmod 600 ~/.ssh/authorized_keys</p>
<p><img src="" alt=""></p>
<p><strong>4）用root用户修改&quot;/etc/ssh/sshd_config&quot;</strong></p>
<p><strong>**具体步骤参考前面Master.Hadoop的&quot;</strong>设置SSH配置**&quot;，具体分为两步：第1是修改配置文件；第2是重启SSH服务。</p>
<p><strong>5）用Master.Hadoop使用SSH无密码登录Slave1.Hadoop</strong></p>
<p>当前面的步骤设置完毕，就可以使用下面命令格式进行SSH无密码登录了。</p>
<p>ssh 远程服务器IP</p>
<p><img src="" alt=""></p>
<p>从上图我们主要3个地方，第1个就是SSH无密码登录命令，第2、3个就是登录前后&quot;<strong>@</strong>&quot;后面的<strong>机器名</strong>变了，由&quot;<strong>Master</strong>&quot;变为了&quot;<strong>Slave1</strong>&quot;，这就说明我们已经成功实现了SSH无密码登录了。</p>
<p>最后记得把&quot;/home/hadoop/&quot;目录下的&quot;id_rsa.pub&quot;文件删除掉。</p>
<p>rm –r ~/id_rsa.pub</p>
<p><img src="" alt=""></p>
<p>到此为止，我们经过前5步已经实现了从&quot;Master.Hadoop&quot;到&quot;Slave1.Hadoop&quot;SSH无密码登录，下面就是重复上面的步骤把剩余的两台（Slave2.Hadoop和Slave3.Hadoop）Slave服务器进行配置。<strong>这样</strong>，我们就完成了&quot;配置Master无密码登录所有的Slave服务器&quot;。</p>
<h3 id="2-3-slave-master">2.3 配置所有Slave无密码登录Master</h3>
<p>和Master无密码登录所有Slave原理一样，就是把Slave的公钥<strong>追加</strong>到Master的&quot;.ssh&quot;文件夹下的&quot;authorized_keys&quot;中，记得是<strong>追加（&gt;&gt;）</strong>。</p>
<p>为了说明情况，我们现在就以&quot;Slave1.Hadoop&quot;无密码登录&quot;Master.Hadoop&quot;为例，进行一遍操作，也算是<strong>巩固</strong>一下前面所学知识，剩余的&quot;Slave2.Hadoop&quot;和&quot;Slave3.Hadoop&quot;就按照这个示例进行就可以了。</p>
<p>首先创建&quot;Slave1.Hadoop&quot;自己的公钥和私钥，并把自己的公钥追加到&quot;authorized_keys&quot;文件中。用到的命令如下：</p>
<p>ssh-keygen –t rsa –P &#39;&#39;</p>
<p>cat ~/.ssh/id_rsa.pub &gt;&gt; ~/.ssh/authorized_keys</p>
<p><img src="" alt=""></p>
<p>接着是用命令&quot;<strong>scp</strong>&quot;复制&quot;Slave1.Hadoop&quot;的公钥&quot;id_rsa.pub&quot;到&quot;Master.Hadoop&quot;的&quot;/home/hadoop/&quot;目录下，并<strong>追加</strong>到&quot;Master.Hadoop&quot;的&quot;authorized_keys&quot;中。</p>
<p><strong>1）在&quot;Slave1.Hadoop&quot;服务器的操作</strong></p>
<p>用到的命令如下：</p>
<p>scp ~/.ssh/id_rsa.pub hadoop@192.168.1.2:~/</p>
<p><img src="" alt=""></p>
<hr>
<p><strong>2）在&quot;Master.Hadoop&quot;服务器的操作</strong></p>
<p>用到的命令如下：</p>
<p>cat ~/id_rsa.pub &gt;&gt; ~/.ssh/authorized_keys</p>
<p><img src="" alt=""></p>
<p>然后删除掉刚才复制过来的&quot;id_rsa.pub&quot;文件。</p>
<p><img src="" alt=""></p>
<p>最后是测试从&quot;Slave1.Hadoop&quot;到&quot;Master.Hadoop&quot;无密码登录。</p>
<p><img src="" alt=""></p>
<p>从上面结果中可以看到已经成功实现了，再试下从&quot;Master.Hadoop&quot;到&quot;Slave1.Hadoop&quot;无密码登录。</p>
<p><img src="" alt=""></p>
<p>至此&quot;Master.Hadoop&quot;与&quot;Slave1.Hadoop&quot;之间可以互相无密码登录了，剩下的就是按照上面的步骤把剩余的&quot;Slave2.Hadoop&quot;和&quot;Slave3.Hadoop&quot;与&quot;Master.Hadoop&quot;之间建立起无密码登录。这样，Master能无密码验证登录每个Slave，每个Slave也能无密码验证登录到Master。</p>
<h2 id="3-java-">3、Java环境安装</h2>
<p>所有的机器上都要安装JDK，现在就先在Master服务器安装，然后其他服务器按照步骤重复进行即可。安装JDK以及配置环境变量，需要以&quot;<strong>root</strong>&quot;的身份进行。</p>
<h3 id="3-1-jdk">3.1 安装JDK</h3>
<p>首先用<strong>root</strong>身份登录&quot;Master.Hadoop&quot;后在&quot;/usr&quot;下创建&quot;java&quot;文件夹，再把用FTP上传到&quot;/home/hadoop/&quot;下的&quot;jdk-6u31-linux-i586.bin&quot;复制到&quot;/usr/java&quot;文件夹中。</p>
<p>mkdir /usr/java</p>
<p>cp /home/hadoop/ jdk-6u31-linux-i586.bin /usr/java</p>
<p><img src="" alt=""></p>
<p>接着<strong>进入</strong>&quot;<strong>/usr/java</strong>&quot;目录<strong>下</strong>通过下面命令使其JDK获得可执行权限，并安装JDK。</p>
<p>chmod +x jdk-6u31-linux-i586.bin</p>
<p>./jdk-6u31-linux-i586.bin</p>
<p><img src="" alt=""></p>
<p>按照上面几步进行操作，最后点击&quot;<strong>Enter</strong>&quot;键开始安装，安装完会提示你按&quot;<strong>Enter</strong>&quot;键退出，然后查看&quot;<strong>/usr/java</strong>&quot;下面会发现多了一个名为&quot;<strong>jdk1.6.0_31</strong>&quot;文件夹，说明我们的JDK安装结束，删除&quot;jdk-6u31-linux-i586.bin&quot;文件，进入下一个&quot;配置环境变量&quot;环节。</p>
<p><img src="" alt=""></p>
<h3 id="3-2-">3.2 配置环境变量</h3>
<p>编辑&quot;/etc/profile&quot;文件，在后面添加Java的&quot;JAVA_HOME&quot;、&quot;CLASSPATH&quot;以及&quot;PATH&quot;内容。</p>
<p><strong>1）编辑&quot;/etc/profile&quot;文件</strong></p>
<p>vim /etc/profile</p>
<p><img src="" alt=""></p>
<p><strong>2）添加Java环境变量</strong></p>
<p>在&quot;<strong>/etc/profile</strong>&quot;文件的<strong>尾部</strong>添加以下内容：</p>
<p>/# set java environment</p>
<p>export JAVA_HOME=/usr/java/jdk1.6.0_31/</p>
<p>export JRE_HOME=/usr/java/jdk1.6.0_31/jre</p>
<p>export CLASSPATH=.:$CLASSPATH:$JAVA_HOME/lib:$JRE_HOME/lib</p>
<p>export PATH=$PATH:$JAVA_HOME/bin:$JRE_HOME/bin</p>
<p>或者</p>
<p>/# set java environment</p>
<p>export JAVA_HOME=/usr/java/jdk1.6.0_31</p>
<p>export CLASSPATH=.:$CLASSPATH:$JAVA_HOME/lib:$JAVA_HOME/jre/lib</p>
<p>export PATH=$PATH:$JAVA_HOME/bin:$JAVA_HOME/jre/bin</p>
<p>以上两种意思一样，那么我们就选择<strong>第2种</strong>来进行设置。</p>
<p><img src="" alt=""></p>
<p><strong>3）使配置生效</strong></p>
<p>保存并退出，执行下面命令使其配置立即生效。</p>
<p>source /etc/profile</p>
<p><img src="" alt=""></p>
<h3 id="3-3-">3.3 验证安装成功</h3>
<p>配置完毕并生效后，用下面命令判断是否成功。</p>
<p>java -version</p>
<p><img src="" alt=""></p>
<p>从上图中得知，我们以确定JDK已经安装成功。</p>
<h3 id="3-4-">3.4 安装剩余机器</h3>
<p>这时用<strong>普通用户hadoop</strong>通过下面命令格式把&quot;Master.Hadoop&quot;文件夹&quot;/home/hadoop/&quot;的JDK复制到其他Slave的&quot;/home/hadoop/&quot;下面，剩下的事儿就是在其余的Slave服务器上按照上图的步骤安装JDK。</p>
<p>scp /home/hadoop/jdk-6u31-linux-i586.bin 远程用户名@远程服务器IP:~/</p>
<p>或者</p>
<p>scp ~/jdk-6u31-linux-i586.bin 远程用户名@远程服务器IP:~/</p>
<p><strong>备注：</strong>&quot;<strong>~</strong>&quot;代表<strong>当前</strong>用户的主目录，当<strong>前用户为hadoop</strong>，所以&quot;<strong>~</strong>&quot;代表&quot;<strong>/home/hadoop</strong>&quot;。</p>
<p><strong>例如：</strong>把JDK从&quot;Master.Hadoop&quot;复制到&quot;Slave1.Hadoop&quot;的命令如下。</p>
<p>scp ~/jdk-6u31-linux-i586 hadoop@192.168.1.3:~/</p>
<p><img src="" alt=""></p>
<p>然后查看&quot;Slave1.Hadoop&quot;的&quot;/home/hadoop&quot;查看是否已经复制成功了。</p>
<p><img src="" alt=""></p>
<p>从上图中得知，我们已经成功复制了，现在我们就用<strong>最高权限用户root</strong>进行安装了。其他的与这个一样。</p>
<h2 id="4-hadoop-">4、Hadoop集群安装</h2>
<p>所有的机器上都要安装hadoop，现在就先在Master服务器安装，然后其他服务器按照步骤重复进行即可。安装和配置hadoop需要以&quot;<strong>root</strong>&quot;的身份进行。</p>
<h3 id="4-1-hadoop">4.1 安装hadoop</h3>
<p>首先用<strong>root</strong>用户登录&quot;Master.Hadoop&quot;机器，查看我们之前用FTP上传至&quot;/home/Hadoop&quot;上传的&quot;<strong>hadoop-1.0.0.tar.gz</strong>&quot;。</p>
<p><img src="" alt=""></p>
<p>接着把&quot;<strong>hadoop-1.0.0.tar.gz</strong>&quot;复制到&quot;/usr&quot;目录下面。</p>
<p>cp /home/hadoop/hadoop-1.0.0.tar.gz /usr</p>
<p><img src="" alt=""></p>
<p>下一步进入&quot;/usr&quot;目录下，用下面命令把&quot;<strong>hadoop-1.0.0.tar.gz</strong>&quot;进行解压，并将其命名为&quot;hadoop&quot;，把该文件夹的<strong>读权限</strong>分配给普通用户<strong>hadoop</strong>，然后删除&quot;<strong>hadoop-1.0.0.tar.gz</strong>&quot;安装包。</p>
<p>cd /usr /#进入&quot;/usr&quot;目录</p>
<p>tar –zxvf hadoop-1.0.0.tar.gz /#解压&quot;<strong>hadoop-1.0.0.tar.gz</strong>&quot;安装包</p>
<p>mv hadoop-1.0.0 hadoop /#将&quot;hadoop-1.0.0&quot;文件夹<strong>重命名</strong>&quot;hadoop&quot;</p>
<p>chown <strong>–R</strong> hadoop:hadoop hadoop /#<strong>将文件夹&quot;hadoop&quot;读权限分配给hadoop用户</strong></p>
<p>rm –rf hadoop-1.0.0.tar.gz /#删除&quot;<strong>hadoop-1.0.0.tar.gz</strong>&quot;安装包</p>
<p><img src="" alt=""></p>
<p>解压后，并重命名。</p>
<p><img src="" alt=""></p>
<p>把&quot;/usr/hadoop&quot;<strong>读权</strong>限分配给<strong>hadoop</strong>用户（<strong>非常重要</strong>）</p>
<p><img src="" alt=""></p>
<p>删除&quot;<strong>hadoop-1.0.0.tar.gz</strong>&quot;安装包</p>
<p><img src="" alt=""></p>
<p>最后在&quot;<strong>/usr/hadoop</strong>&quot;下面创建<strong>tmp</strong>文件夹，把Hadoop的安装路径添加到&quot;<strong>/etc/profile</strong>&quot;中，修改&quot;/etc/profile&quot;文件（配置java环境变量的文件），将以下语句添加到<strong>末尾</strong>，并使其有效：</p>
<p>/# set hadoop path</p>
<p>export HADOOP_HOME=/usr/hadoop</p>
<p>export PATH=$PATH :$HADOOP_HOME/bin</p>
<p><strong>1）在&quot;/usr/hadoop&quot;创建&quot;tmp&quot;文件夹</strong></p>
<p>mkdir /usr/hadoop/tmp</p>
<p><img src="" alt=""></p>
<p><strong>2）配置&quot;/etc/profile&quot;</strong></p>
<p>vim /etc/profile</p>
<p><img src="" alt=""></p>
<p>配置后的文件如下：</p>
<p><img src="" alt=""></p>
<p><strong>3）重启&quot;/etc/profile&quot;</strong></p>
<p>source /etc/profile</p>
<p><img src="" alt=""></p>
<h3 id="4-2-hadoop">4.2 配置hadoop</h3>
<p><strong>1）配置hadoop-env.sh</strong></p>
<p>该&quot;<strong>hadoop-env.sh</strong>&quot;文件位于&quot;<strong>/usr/hadoop/conf</strong>&quot;目录下。</p>
<p><img src="" alt=""></p>
<p>在文件的末尾添加下面内容。</p>
<p>/# set java environment</p>
<p>export JAVA_HOME=/usr/java/jdk1.6.0_31</p>
<p><img src="" alt=""></p>
<p>Hadoop配置文件在conf目录下，之前的版本的配置文件主要是Hadoop-default.xml和Hadoop-site.xml。由于Hadoop发展迅速，代码量急剧增加，代码开发分为了core，hdfs和map/reduce三部分，配置文件也被分成了三个core-site.xml、hdfs-site.xml、mapred-site.xml。core-site.xml和hdfs-site.xml是站在HDFS角度上配置文件；core-site.xml和mapred-site.xml是站在MapReduce角度上配置文件。</p>
<p><strong>2）配置core-site.xml文件</strong></p>
<p>修改Hadoop核心配置文件core-site.xml，这里配置的是HDFS的地址和端口号。</p>
<configuration>

<property>

<name>hadoop.tmp.dir</name>

<value>/usr/hadoop/tmp</value>

（<strong>备注：</strong>请先在 /usr/hadoop 目录下建立 tmp 文件夹）

<description>A base for other temporary directories.</description>

</property>

<!-- file system properties -->

<property>

<name>fs.default.name</name>

<value>hdfs://<strong>192.168.1.2</strong>:<strong>9000</strong></value>

</property>

</configuration>

<p><strong>备注：</strong>如没有配置hadoop.tmp.dir参数，此时系统默认的临时目录为：/tmp/hadoo-hadoop。而这个目录在每次重启后都会被干掉，必须重新执行format才行，否则会出错。</p>
<p>用下面命令进行编辑：</p>
<p><img src="" alt=""></p>
<p>编辑结果显示如下：</p>
<p><img src="" alt=""></p>
<p><strong>3）配置hdfs-site.xml文件</strong></p>
<p>修改Hadoop中HDFS的配置，配置的备份方式默认为3。</p>
<configuration>

<property>

<name>dfs.replication</name>

<value><strong>1</strong></value>

(<strong>备注：</strong>replication 是数据副本数量，默认为3，salve少于3台就会报错)

</property>

<configuration>

用下面命令进行编辑：

<img src="" alt="">

编辑结果显示如下：

<img src="" alt="">

<strong>4）配置mapred-site.xml文件</strong>

修改Hadoop中MapReduce的配置文件，配置的是JobTracker的地址和端口。

<configuration>

<property>

<name>mapred.job.tracker</name>

<value><a href="http://**192.168.1.2**:**9001**" target="_blank">http://**192.168.1.2**:**9001**</a></value>

</property>

</configuration>

<p>用下面命令进行编辑：</p>
<p><img src="" alt=""></p>
<p>编辑结果显示如下：</p>
<p><img src="" alt=""></p>
<p><strong>5）配置masters文件</strong></p>
<p>有两种方案：</p>
<p>（1）第一种</p>
<p>修改localhost为Master.Hadoop</p>
<p>（2）第二种</p>
<p>去掉&quot;localhost&quot;，加入Master机器的IP：192.168.1.2</p>
<p>为保险起见，启用第二种，因为万一忘记配置&quot;/etc/hosts&quot;局域网的DNS失效，这样就会出现意想不到的错误，但是一旦IP配对，网络畅通，就能通过IP找到相应主机。</p>
<p>用下面命令进行修改：</p>
<p><img src="" alt=""></p>
<p>编辑结果显示如下：</p>
<p><img src="" alt=""></p>
<p><strong>6）配置slaves文件（Master主机特有</strong>）</p>
<p>有两种方案：</p>
<p>（1）第一种</p>
<p>去掉&quot;localhost&quot;，每行只添加一个主机名，把剩余的Slave主机名都填上。</p>
<p>例如：添加形式如下</p>
<p>Slave1.Hadoop</p>
<p>Slave2.Hadoop</p>
<p>Slave3.Hadoop</p>
<p>（2）第二种</p>
<p>去掉&quot;localhost&quot;，加入集群中所有Slave机器的IP，也是每行一个。</p>
<p>例如：添加形式如下</p>
<p>192.168.1.3</p>
<p>192.168.1.4</p>
<p>192.168.1.5</p>
<p>原因和添加&quot;masters&quot;文件一样，选择第二种方式。</p>
<p>用下面命令进行修改：</p>
<p><img src="" alt=""></p>
<p>编辑结果如下：</p>
<p><img src="" alt=""></p>
<p>现在在Master机器上的Hadoop配置就结束了，剩下的就是配置Slave机器上的Hadoop。</p>
<p><strong>一种方式</strong>是按照上面的步骤，把Hadoop的安装包在用<strong>普通用户hadoop</strong>通过&quot;<strong>scp</strong>&quot;复制到其他机器的&quot;/home/hadoop&quot;目录下，然后根据实际情况进行安装配置，<strong>除了第6步，那是Master特有的</strong>。用下面命令格式进行。（<strong>备注：</strong>此时切换到普通用户hadoop）</p>
<p>scp ~/hadoop-1.0.0.tar.gz hadoop@服务器IP:~/</p>
<p>例如：从&quot;Master.Hadoop&quot;到&quot;Slave1.Hadoop&quot;复制Hadoop的安装包。</p>
<p><img src="" alt=""></p>
<p><strong>另一种方式</strong>是将 Master上配置好的hadoop所在文件夹&quot;<strong>/usr/hadoop</strong>&quot;复制到所有的Slave的&quot;/usr&quot;目录下（实际上Slave机器上的slavers文件是不必要的， 复制了也没问题）。用下面命令格式进行。（<strong>备注：</strong>此时用户可以为hadoop也可以为root）</p>
<p>scp <strong>-r</strong> /usr/hadoop <strong>root</strong>@服务器IP:/usr/</p>
<p>例如：从&quot;Master.Hadoop&quot;到&quot;Slave1.Hadoop&quot;复制配置Hadoop的文件。</p>
<p><img src="" alt=""></p>
<p>上图中以root用户进行复制，当然不管是用户root还是hadoop，虽然Master机器上的&quot;/usr/hadoop&quot;文件夹用户hadoop有权限，但是Slave1上的hadoop用户却没有&quot;/usr&quot;权限，所以没有创建文件夹的权限。所以无论是哪个用户进行拷贝，右面都是&quot;root@机器IP&quot;格式。因为我们只是建立起了hadoop用户的SSH无密码连接，所以用root进行&quot;scp&quot;时，扔提示让你输入&quot;Slave1.Hadoop&quot;服务器用户root的密码。</p>
<p>查看&quot;Slave1.Hadoop&quot;服务器的&quot;/usr&quot;目录下是否已经存在&quot;hadoop&quot;文件夹，确认已经复制成功。查看结果如下：</p>
<p><img src="" alt=""></p>
<p>从上图中知道，hadoop文件夹确实已经复制了，但是我们发现hadoop权限是root，所以我们现在要给&quot;Slave1.Hadoop&quot;服务器上的用户hadoop添加对&quot;/usr/hadoop&quot;读权限。</p>
<p>以<strong>root</strong>用户登录&quot;Slave1.Hadoop&quot;，执行下面命令。</p>
<p>chown <strong>-R</strong> hadoop:hadoop（<strong>用户名：用户组</strong>） hadoop（<strong>文件夹</strong>）</p>
<p><img src="" alt=""></p>
<p>接着在&quot;Slave1 .Hadoop&quot;上修改&quot;/etc/profile&quot;文件（配置 java 环境变量的文件），将以下语句添加到末尾，并使其有效（source /etc/profile）：</p>
<p>/# set hadoop environment</p>
<p>export HADOOP_HOME=/usr/hadoop</p>
<p>export PATH=$PATH :$HADOOP_HOME/bin</p>
<p>如果不知道怎么设置，可以查看前面&quot;Master.Hadoop&quot;机器的&quot;/etc/profile&quot;文件的配置，到此为此在一台Slave机器上的Hadoop配置就结束了。剩下的事儿就是照葫芦画瓢把剩余的几台Slave机器按照《<strong>从&quot;Master.Hadoop&quot;到&quot;Slave1.Hadoop&quot;复制Hadoop的安装包。</strong>》这个例子进行部署Hadoop。</p>
<h3 id="4-3-">4.3 启动及验证</h3>
<p><strong>1）格式化HDFS文件系统</strong></p>
<p>在&quot;Master.Hadoop&quot;上使用<strong>普通</strong>用户<strong>hadoop</strong>进行操作。（<strong>备注：</strong>只需一次，下次启动不再需要格式化，只需 start-all.sh）</p>
<p>hadoop namenode -format</p>
<p>某些书上和网上的某些资料中用下面命令执行。</p>
<p><img src="" alt=""></p>
<p>我们在看好多文档包括有些书上，按照他们的hadoop环境变量进行配置后，并立即使其生效，但是执行发现没有找见&quot;bin/hadoop&quot;这个命令。</p>
<p><img src="" alt=""></p>
<p>其实我们会发现我们的环境变量配置的是&quot;<strong>$HADOOP_HOME/bin</strong>&quot;，我们已经把bin包含进入了，所以执行时，加上&quot;bin&quot;反而找不到该命令，除非我们的hadoop坏境变量如下设置。</p>
<p>/# set hadoop path</p>
<p>export HADOOP_HOME=/usr/hadoop</p>
<p>export PATH=$PATH :<strong>$HADOOP_HOME</strong>:<strong>$HADOOP_HOME/bin</strong></p>
<p>这样就能直接使用&quot;bin/hadoop&quot;也可以直接使用&quot;hadoop&quot;，现在不管哪种情况，hadoop命令都能找见了。我们也没有必要重新在设置hadoop环境变量了，只需要记住执行Hadoop命令时不需要在前面加&quot;bin&quot;就可以了。</p>
<p><img src="" alt=""></p>
<p><img src="" alt=""></p>
<p>从上图中知道我们已经成功格式话了，但是美中不足就是出现了一个<strong>警告</strong>，从网上的得知这个警告并不影响hadoop执行，但是也有办法解决，详情看后面的&quot;常见问题FAQ&quot;。</p>
<p><strong>2）启动hadoop</strong></p>
<p>在启动前关闭集群中所有机器的防火墙，不然会出现datanode开后又自动关闭。</p>
<p>service iptables stop</p>
<p>使用下面命令启动。</p>
<p>start-all.sh</p>
<p><img src="" alt=""></p>
<p>执行结果如下：</p>
<p><img src="" alt=""></p>
<p>可以通过以下启动日志看出，首先启动namenode 接着启动datanode1，datanode2，…，然后启动secondarynamenode。再启动jobtracker，然后启动tasktracker1，tasktracker2，…。</p>
<p>启动 hadoop成功后，在 Master 中的 tmp 文件夹中生成了 dfs 文件夹，在Slave 中的 tmp 文件夹中均生成了 dfs 文件夹和 mapred 文件夹。</p>
<p>查看Master中&quot;/usr/hadoop/tmp&quot;文件夹内容</p>
<p><img src="" alt=""></p>
<p>查看Slave1中&quot;/usr/hadoop/tmp&quot;文件夹内容。</p>
<p><img src="" alt=""></p>
<p><strong>3）验证hadoop</strong></p>
<p>（1）验证方法一：用&quot;jps&quot;命令</p>
<p>在Master上用 java自带的小工具<strong>jps</strong>查看进程。</p>
<p><img src="" alt=""></p>
<p>在Slave1上用jps查看进程。</p>
<p><img src="" alt=""></p>
<p>如果在查看Slave机器中发现&quot;DataNode&quot;和&quot;TaskTracker&quot;没有起来时，先查看一下日志的，如果是&quot;namespaceID&quot;不一致问题，采用&quot;常见问题FAQ6.2&quot;进行解决，如果是&quot;No route to host&quot;问题，采用&quot;常见问题FAQ6.3&quot;进行解决。</p>
<p>（2）验证方式二：用&quot;hadoop dfsadmin -report&quot;</p>
<p>用这个命令可以查看Hadoop集群的状态。</p>
<p>Master服务器的状态：</p>
<p><img src="" alt=""></p>
<p>Slave服务器的状态</p>
<p><img src="" alt=""></p>
<h3 id="4-4-">4.4 网页查看集群</h3>
<p><strong>1）访问&quot;http:192.168.1.2:50030&quot;</strong></p>
<p><img src="" alt=""></p>
<p>2）访问&quot;<strong>http:192.168.1.2:50070</strong>&quot;</p>
<p><img src="" alt=""></p>
<h2 id="5-faq">5、常见问题FAQ</h2>
<h3 id="5-1-warning-hadoop_home-is-deprecated-">5.1 关于 Warning: $HADOOP_HOME is deprecated.</h3>
<p>hadoop 1.0.0版本，安装完之后敲入hadoop命令时，<strong>老</strong>是提示这个警告：</p>
<p>Warning: $HADOOP_HOME is deprecated.</p>
<p>经查hadoop-1.0.0/bin/hadoop脚本和&quot;hadoop-config.sh&quot;脚本，发现脚本中对HADOOP_HOME的环境变量设置做了判断，笔者的环境根本不需要设置HADOOP_HOME环境变量。</p>
<p>解决方案一：编辑&quot;/etc/profile&quot;文件，去掉HADOOP_HOME的变量设定，重新输入hadoop fs命令，警告消失。</p>
<p>解决方案二：编辑&quot;/etc/profile&quot;文件，添加一个环境变量，之后警告消失：</p>
<p>export HADOOP_HOME_WARN_SUPPRESS=1</p>
<p>解决方案三：编辑&quot;hadoop-config.sh&quot;文件，把下面的&quot;if - fi&quot;功能注释掉。</p>
<p><img src="" alt=""></p>
<p>我们这里本着不动Hadoop原配置文件的前提下，采用&quot;<strong>方案二</strong>&quot;，在&quot;/etc/profile&quot;文件添加上面内容，并用命令&quot;source /etc/profile&quot;使之有效。</p>
<p><strong>1）切换至root用户</strong></p>
<p><img src="" alt=""></p>
<p><strong>2）添加内容</strong></p>
<p><img src="" alt=""></p>
<p><strong>3）重新生效</strong></p>
<p><img src="" alt=""></p>
<h3 id="5-2-no-datanode-to-stop-">5.2 解决&quot;no datanode to stop&quot;问题</h3>
<p>当我停止Hadoop时发现如下信息：</p>
<p><img src="" alt=""></p>
<p>原因：每次namenode format会重新创建一个namenodeId，而tmp/dfs/data下包含了上次format下的id，namenode format清空了namenode下的数据，但是没有清空datanode下的数据，导致启动时失败，所要做的就是每次fotmat前，清空tmp一下的所有目录。</p>
<p><strong>第一种解决方案如下：</strong></p>
<p><strong>1）先删除&quot;/usr/hadoop/tmp&quot;</strong></p>
<p>rm -rf /usr/hadoop/tmp</p>
<p><strong>2）创建&quot;/usr/hadoop/tmp&quot;文件夹</strong></p>
<p>mkdir /usr/hadoop/tmp</p>
<p><strong>3）删除&quot;/tmp&quot;下以&quot;hadoop&quot;开头文件</strong></p>
<p>rm -rf /tmp/hadoop/*</p>
<p><strong>4）重新格式化hadoop</strong></p>
<p>hadoop namenode -format</p>
<p><strong>5）启动hadoop</strong></p>
<p>start-all.sh</p>
<p>使用第一种方案，有种不好处就是原来集群上的重要数据全没有了。假如说Hadoop集群已经运行了一段时间。建议采用第二种。</p>
<p><strong>第二种方案如下：</strong></p>
<p>1）修改每个Slave的namespaceID使其与Master的namespaceID一致。</p>
<p>或者</p>
<p>2）修改Master的namespaceID使其与Slave的namespaceID一致。</p>
<p>该&quot;namespaceID&quot;位于&quot;<strong>/usr/hadoop/tmp/dfs/data/current/VERSION</strong>&quot;文件中，前面<strong>蓝色</strong>的可能根据实际情况变化，但后面<strong>红色</strong>是不变的。</p>
<p>例如：查看&quot;Master&quot;下的&quot;<strong>VERSION</strong>&quot;文件</p>
<p><img src="" alt=""></p>
<p>本人建议采用<strong>第二种</strong>，这样方便快捷，而且还能防止误删。</p>
<h3 id="5-3-slave-datanode-">5.3 Slave服务器中datanode启动后又自动关闭</h3>
<p>查看日志发下如下错误。</p>
<p><strong>ERROR</strong> org.apache.hadoop.hdfs.server.datanode.DataNode: java.io.IOException: Call to ... failed on local exception: java.net.NoRouteToHostException: <strong>No route to host</strong></p>
<p>解决方案是：关闭防火墙</p>
<p>service iptables stop</p>
<h3 id="5-4-hdfs-">5.4 从本地往hdfs文件系统上传文件</h3>
<p>出现如下错误：</p>
<p>INFO hdfs.DFSClient: Exception in createBlockOutputStream java.io.IOException: <strong>Bad connect ack with firstBadLink</strong></p>
<p>INFO hdfs.DFSClient: Abandoning block blk_-1300529705803292651_37023</p>
<p>WARN hdfs.DFSClient: DataStreamer Exception: java.io.IOException: <strong>Unable to create new block.</strong></p>
<p>解决方案是：</p>
<p><strong>1）关闭防火墙</strong></p>
<p>service iptables stop</p>
<p><strong>2）禁用selinux</strong></p>
<p>编辑 &quot;<strong>/etc/selinux/config</strong>&quot;文件，设置&quot;<strong>SELINUX</strong>=<strong>disabled</strong>&quot;</p>
<h3 id="5-5-">5.5 安全模式导致的错误</h3>
<p>出现如下错误：</p>
<p>org.apache.hadoop.dfs.SafeModeException: <strong>Cannot delete ..., Name node is in safe mode</strong></p>
<p>在分布式文件系统启动的时候，开始的时候会有安全模式，当分布式文件系统处于安全模式的情况下，文件系统中的内容不允许修改也不允许删除，直到安全模式结束。安全模式主要是为了系统启动的时候检查各个DataNode上数据块的有效性，同时根据策略必要的复制或者删除部分数据块。运行期通过命令也可以进入安全模式。在实践过程中，系统启动的时候去修改和删除文件也会有安全模式不允许修改的出错提示，只需要等待一会儿即可。</p>
<p>解决方案是：关闭安全模式</p>
<p>hadoop dfsadmin -safemode leave</p>
<h3 id="5-6-exceeded-max_failed_unique_fetches">5.6 解决Exceeded MAX_FAILED_UNIQUE_FETCHES</h3>
<p>出现错误如下：</p>
<p>Shuffle Error: Exceeded MAX_FAILED_UNIQUE_FETCHES; bailing-out</p>
<p>程序里面需要打开多个文件，进行分析，系统一般默认数量是1024，（用ulimit -a可以看到）对于正常使用是够了，但是对于程序来讲，就太少了。</p>
<p>解决方案是：修改2个文件。</p>
<p><strong>1）&quot;/etc/security/limits.conf&quot;</strong></p>
<p>vim /etc/security/limits.conf</p>
<p>加上：</p>
<p>soft nofile 102400</p>
<p>hard nofile 409600</p>
<p><strong>2）&quot;/etc/pam.d/login&quot;</strong></p>
<p>vim /etc/pam.d/login</p>
<p>添加：</p>
<p>session required /lib/security/pam_limits.so</p>
<p>针对第一个问题我纠正下答案：</p>
<p>这是reduce预处理阶段shuffle时获取已完成的map的输出失败次数超过上限造成的，上限默认为5。引起此问题的方式可能会有很多种，比如网络连接不正常，连接超时，带宽较差以及端口阻塞等。通常框架内网络情况较好是不会出现此错误的。</p>
<h3 id="5-7-too-many-fetch-failures-">5.7 解决&quot;Too many fetch-failures&quot;</h3>
<p>出现这个问题主要是结点间的连通不够全面。</p>
<p>解决方案是：</p>
<p><strong>1）检查&quot;/etc/hosts&quot;</strong></p>
<p>要求本机ip 对应 服务器名</p>
<p>要求要包含所有的服务器ip +服务器名</p>
<p><strong>2）检查&quot;.ssh/authorized_keys&quot;</strong></p>
<p>要求包含所有服务器（包括其自身）的public key</p>
<h3 id="5-8-">5.8 处理速度特别的慢</h3>
<p>出现<strong>map</strong>很<strong>快</strong>，但是<strong>reduce</strong>很<strong>慢</strong>，而且反复出现&quot;<strong>reduce=0%</strong>&quot;。</p>
<p>解决方案如下：</p>
<p>结合解决方案5.7，然后修改&quot;conf/hadoop-env.sh&quot;中的&quot;export HADOOP_HEAPSIZE=4000&quot;</p>
<h3 id="5-9-hadoop-outofmemoryerror-">5.9解决hadoop OutOfMemoryError问题</h3>
<p>出现这种异常，明显是jvm内存不够得原因。</p>
<p>解决方案如下：要修改所有的datanode的jvm内存大小。</p>
<p>Java –Xms 1024m -Xmx 4096m</p>
<p>一般jvm的最大内存使用应该为总内存大小的一半，我们使用的8G内存，所以设置为4096m，这一值可能依旧不是最优的值。</p>
<h3 id="5-10-namenode-in-safe-mode">5.10 Namenode in safe mode</h3>
<p>解决方案如下：</p>
<p>bin/hadoop dfsadmin -safemode leave</p>
<h3 id="5-11-io-">5.11 IO写操作出现问题</h3>
<p>0-1246359584298, infoPort=50075, ipcPort=50020):Got exception while serving blk_-5911099437886836280_1292 to /172.16.100.165:</p>
<p>java.net.SocketTimeoutException: 480000 millis timeout while waiting for channel to be ready for write. ch : java.nio.channels.SocketChannel[connected local=/</p>
<p>172.16.100.165:50010 remote=/172.16.100.165:50930]</p>
<p>at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:185)</p>
<p>at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:159)</p>
<p>……</p>
<p>It seems there are many reasons that it can timeout, the example given in HADOOP-3831 is a slow reading client.</p>
<p>解决方案如下：</p>
<p>在hadoop-site.xml中设置dfs.datanode.socket.write.timeout=0</p>
<h3 id="5-12-status-of-255-error">5.12 status of 255 error</h3>
<p>错误类型：</p>
<p>java.io.IOException: Task process exit with nonzero status of 255.</p>
<p>at org.apache.hadoop.mapred.TaskRunner.run(TaskRunner.java:424)</p>
<p>错误原因：</p>
<p>Set mapred.jobtracker.retirejob.interval and mapred.userlog.retain.hours to higher value. By default, their values are 24 hours. These might be the reason for failure, though I&#39;m not sure restart.</p>
<p>解决方案如下：单个datanode</p>
<p>如果一个datanode 出现问题，解决之后需要重新加入cluster而不重启cluster，方法如下：</p>
<p>bin/hadoop-daemon.sh start datanode</p>
<p>bin/hadoop-daemon.sh start jobtracker</p>
<h2 id="6-linux-">6、用到的Linux命令</h2>
<h3 id="6-1-chmod-">6.1 chmod命令详解</h3>
<p><strong>使用权限：</strong>所有使用者</p>
<p><strong>使用方式：</strong>chmod [-cfvR] [--help] [--version] mode file...</p>
<p><strong>说明：</strong></p>
<p>Linux/Unix 的档案存取权限分为三级 : 档案拥有者、群组、其他。利用 chmod 可以藉以控制档案如何被他人所存取。</p>
<p>mode ：权限设定字串，格式如下 ：[ugoa...][[+-=][rwxX]...][,...]，其中u 表示该档案的拥有者，g 表示与该档案的拥有者属于同一个群体(group)者，o 表示其他以外的人，a 表示这三者皆是。</p>
<ul>
<li>表示增加权限、- 表示取消权限、= 表示唯一设定权限。</li>
</ul>
<p>r 表示可读取，w 表示可写入，x 表示可执行，X 表示只有当该档案是个子目录或者该档案已经被设定过为可执行。</p>
<p>-c : 若该档案权限确实已经更改，才显示其更改动作</p>
<p>-f : 若该档案权限无法被更改也不要显示错误讯息</p>
<p>-v : 显示权限变更的详细资料</p>
<p>-R : 对目前目录下的所有档案与子目录进行相同的权限变更(即以递回的方式逐个变更)</p>
<p>--help : 显示辅助说明</p>
<p>--version : 显示版本</p>
<p><strong>范例：</strong></p>
<p>将档案 file1.txt 设为所有人皆可读取</p>
<p>chmod ugo+r file1.txt</p>
<p>将档案 file1.txt 设为所有人皆可读取</p>
<p>chmod a+r file1.txt</p>
<p>将档案 file1.txt 与 file2.txt 设为该档案拥有者，与其所属同一个群体者可写入，但其他以外的人则不可写入</p>
<p>chmod ug+w,o-w file1.txt file2.txt</p>
<p>将 ex1.py 设定为只有该档案拥有者可以执行</p>
<p>chmod u+x ex1.py</p>
<p>将目前目录下的所有档案与子目录皆设为任何人可读取</p>
<p>chmod -R a+r /*</p>
<p>此外chmod也可以用数字来表示权限如 chmod 777 file</p>
<p><strong>语法为：</strong>chmod abc file</p>
<p>其中a,b,c各为一个数字，分别表示User、Group、及Other的权限。</p>
<p>r=4，w=2，x=1</p>
<p>若要rwx属性则4+2+1=7；</p>
<p>若要rw-属性则4+2=6；</p>
<p>若要r-x属性则4+1=7。</p>
<p><strong>范例：</strong></p>
<p>chmod a=rwx file 和 chmod 777 file 效果相同</p>
<p>chmod ug=rwx,o=x file 和 chmod 771 file 效果相同</p>
<p>若用chmod 4755 filename可使此程式具有<strong>root</strong>的权限</p>
<h3 id="6-2-chown-">6.2 chown命令详解</h3>
<p><strong>使用权限：</strong>root</p>
<p><strong>使用方式：</strong>chown [-cfhvR] [--help] [--version] user[:group] file...</p>
<p><strong>说明：</strong></p>
<p>Linux/Unix 是多人多工作业系统，所有的档案皆有拥有者。利用 chown 可以将档案的拥有者加以改变。一般来说，这个指令只有是由系统管理者(root)所使用，一般使用者没有权限可以改变别人的档案拥有者，也没有权限可以自己的档案拥有者改设为别人。只有系统管理者(root)才有这样的权限。</p>
<p>user : 新的档案拥有者的使用者</p>
<p>IDgroup : 新的档案拥有者的使用者群体(group)</p>
<p>-c : 若该档案拥有者确实已经更改，才显示其更改动作</p>
<p>-f : 若该档案拥有者无法被更改也不要显示错误讯息</p>
<p>-h : 只对于连结(link)进行变更，而非该 link 真正指向的档案</p>
<p>-v : 显示拥有者变更的详细资料</p>
<p>-R : 对目前目录下的所有档案与子目录进行相同的拥有者变更(即以递回的方式逐个变更)</p>
<p>--help : 显示辅助说明</p>
<p>--version : 显示版本</p>
<p><strong>范例：</strong></p>
<p>将档案 file1.txt 的拥有者设为 users 群体的使用者 jessie</p>
<p>chown jessie:users file1.txt</p>
<p>将目前目录下的所有档案与子目录的拥有者皆设为 users 群体的使用者 lamport</p>
<p>chown -R lamport:users /*</p>
<p>-rw------- (600) -- 只有属主有读写权限。</p>
<p>-rw-r--r-- (644) -- 只有属主有读写权限；而属组用户和其他用户只有读权限。</p>
<p>-rwx------ (700) -- 只有属主有读、写、执行权限。</p>
<p>-rwxr-xr-x (755) -- 属主有读、写、执行权限；而属组用户和其他用户只有读、执行权限。</p>
<p>-rwx--x--x (711) -- 属主有读、写、执行权限；而属组用户和其他用户只有执行权限。</p>
<p>-rw-rw-rw- (666) -- 所有用户都有文件读、写权限。这种做法不可取。</p>
<p>-rwxrwxrwx (777) -- 所有用户都有读、写、执行权限。更不可取的做法。</p>
<p>以下是对目录的两个普通设定：</p>
<p>drwx------ (700) - 只有属主可在目录中读、写。</p>
<p>drwxr-xr-x (755) - 所有用户可读该目录，但只有属主才能改变目录中的内容</p>
<p>suid的代表数字是4，比如4755的结果是-rwsr-xr-x</p>
<p>sgid的代表数字是2，比如6755的结果是-rwsr-sr-x</p>
<p>sticky位代表数字是1，比如7755的结果是-rwsr-sr-t</p>
<h3 id="6-3-scp-">6.3 scp命令详解</h3>
<p>scp是 secure copy的缩写，scp是linux系统下基于ssh登陆进行安全的远程文件拷贝命令。linux的scp命令可以在linux服务器之间复制文件和目录。</p>
<p><strong>scp命令的用处：</strong></p>
<p>scp在网络上不同的主机之间复制文件，它使用ssh安全协议传输数据，具有和ssh一样的验证机制，从而安全的远程拷贝文件。</p>
<p><strong>scp命令基本格式：</strong></p>
<p>scp [-1246BCpqrv] [-c cipher] [-F ssh_config] [-i identity_file]</p>
<p>[-l limit] [-o ssh_option] [-P port] [-S program]</p>
<p>[[user@]host1:]file1 [...] [[user@]host2:]file2</p>
<p>scp命令的参数说明：</p>
<p>-1 强制scp命令使用协议ssh1</p>
<p>-2 强制scp命令使用协议ssh2</p>
<p>-4 强制scp命令只使用IPv4寻址</p>
<p>-6 强制scp命令只使用IPv6寻址</p>
<p>-B 使用批处理模式（传输过程中不询问传输口令或短语）</p>
<p>-C 允许压缩。（将-C标志传递给ssh，从而打开压缩功能）</p>
<p>-p 保留原文件的修改时间，访问时间和访问权限。</p>
<p>-q 不显示传输进度条。</p>
<p>-r 递归复制整个目录。</p>
<p>-v 详细方式显示输出。scp和ssh(1)会显示出整个过程的调试信息。这些信息用于调试连接，验证和配置问题。</p>
<p>-c cipher 以cipher将数据传输进行加密，这个选项将直接传递给ssh。</p>
<p>-F ssh_config 指定一个替代的ssh配置文件，此参数直接传递给ssh。</p>
<p>-i identity_file 从指定文件中读取传输时使用的密钥文件，此参数直接传递给ssh。</p>
<p>-l limit 限定用户所能使用的带宽，以Kbit/s为单位。</p>
<p>-o ssh_option 如果习惯于使用ssh_config(5)中的参数传递方式，</p>
<p>-P port 注意是大写的P, port是指定数据传输用到的端口号</p>
<p>-S program 指定加密传输时所使用的程序。此程序必须能够理解ssh(1)的选项。</p>
<p><strong>scp命令的实际应用</strong></p>
<p><strong>1）从本地服务器复制到远程服务器</strong></p>
<p><strong>(1) 复制文件：</strong></p>
<p>命令格式：</p>
<p>scp local_file remote_username@remote_ip:remote_folder</p>
<p>或者</p>
<p>scp local_file remote_username@remote_ip:remote_file</p>
<p>或者</p>
<p>scp local_file remote_ip:remote_folder</p>
<p>或者</p>
<p>scp local_file remote_ip:remote_file</p>
<p>第1,2个指定了用户名，命令执行后需要输入用户密码，第1个仅指定了远程的目录，文件名字不变，第2个指定了文件名</p>
<p>第3,4个没有指定用户名，命令执行后需要输入用户名和密码，第3个仅指定了远程的目录，文件名字不变，第4个指定了文件名</p>
<p><strong>实例：</strong></p>
<p>scp /home/linux/soft/scp.zip root@www.mydomain.com:/home/linux/others/soft</p>
<p>scp /home/linux/soft/scp.zip root@www.mydomain.com:/home/linux/others/soft/scp2.zip</p>
<p>scp /home/linux/soft/scp.zip www.mydomain.com:/home/linux/others/soft</p>
<p>scp /home/linux/soft/scp.zip www.mydomain.com:/home/linux/others/soft/scp2.zip</p>
<p><strong>(2) 复制目录：</strong></p>
<p>命令格式：</p>
<p>scp -r local_folder remote_username@remote_ip:remote_folder</p>
<p>或者</p>
<p>scp -r local_folder remote_ip:remote_folder</p>
<p>第1个指定了用户名，命令执行后需要输入用户密码；</p>
<p>第2个没有指定用户名，命令执行后需要输入用户名和密码；</p>
<p><strong>例子：</strong></p>
<p>scp -r /home/linux/soft/ root@www.mydomain.com:/home/linux/others/</p>
<p>scp -r /home/linux/soft/ www.mydomain.com:/home/linux/others/</p>
<p>上面 命令 将 本地 soft 目录 复制 到 远程 others 目录下，即复制后远程服务器上会有/home/linux/others/soft/ 目录。</p>
<p><strong>2）从远程服务器复制到本地服务器</strong></p>
<p>从远程复制到本地的scp命令与上面的命令雷同，只要将从本地复制到远程的命令后面2个参数互换顺序就行了。</p>
<p><strong>例如：</strong></p>
<p>scp root@www.mydomain.com:/home/linux/soft/scp.zip /home/linux/others/scp.zip</p>
<p>scp www.mydomain.com:/home/linux/soft/ -r /home/linux/others/</p>
<p>linux系统下scp命令中很多参数都和ssh1有关，还需要看到更原汁原味的参数信息，可以运行man scp 看到更细致的英文说明。</p>
<p>文章下载地址：<a href="http://files.cnblogs.com/xia520pi/HadoopCluster_Vol.5.rar" target="_blank"><a href="http://files.cnblogs.com/xia520pi/HadoopCluster_Vol.5.rar">http://files.cnblogs.com/xia520pi/HadoopCluster_Vol.5.rar</a></a>
来源： &lt;<a href="http://www.cnblogs.com/xia520pi/archive/2012/05/16/2503949.html" target="_blank">Hadoop集群（第5期）_Hadoop安装配置 - 虾皮 - 博客园</a>&gt; </p>

      
    </div>
    
    
      
    
    
        <footer id="post-meta">
        <span class="categories">Posted in<span class="breadcrumb fa fa-folder"><li><a href="/categories/hadoop/">hadoop</a></li></span></span> | <span class="tags">Tagged <a href="/tags/hadoop/" class="label label-primary">hadoop</a></span> | <span class="time">recent updated:<time title="2014-03-07 09:54:36"datetime="2014-03-07 09:54:36"> mar. 7 2014</time></span> | <span class="comment-link">
<a href="http://itsolife.com/2014/02/02/2014-02-02-hadoop--Hadoop集群_Hadoop安装配置/#comments" class="ds-thread-count comment-link" data-thread-key="2014-02-02-hadoop--Hadoop集群_Hadoop安装配置" data-count-type="comments">暂无评论</a></span>
        </footer>
    
    <div class="clearfix"></div>
  </div>
</div>
</article>



  <article>
<div id="post" class="post well">
  <div class="post-content">
    <header class="well-sm">
      <i class="fa icon fa-5x pull-left"></i>
      <h1 class="title"><a href="/2014/02/02/2014-02-02-linux--linux之cut用法/">linux之cut用法</a></h1>
      
        <span>Posted on<time datetime="2014-02-02T01:54:36.000Z"> <a href="/2014/02/02/2014-02-02-linux--linux之cut用法/">feb. 2 2014</a></time></span>      
    </header>
    
    
    <div class="entry">
      
        <h1 id="linux-cut-">linux之cut用法</h1>
<p>cut是一个选取命令，就是将一段数据经过分析，取出我们想要的。一般来说，选取信息通常是针对“行”来进行分析的，并不是整篇信息分析的。</p>
<p>（1）其语法格式为：
cut  [-bn] [file] 或 cut [-c] [file]  或  cut [-df] [file]</p>
<p>使用说明
cut 命令从文件的每一行剪切字节、字符和字段并将这些字节、字符和字段写至标准输出。
如果不指定 File 参数，cut 命令将读取标准输入。必须指定 -b、-c 或 -f 标志之一。</p>
<p>主要参数
-b ：以字节为单位进行分割。这些字节位置将忽略多字节字符边界，除非也指定了 -n 标志。
-c ：以字符为单位进行分割。
-d ：自定义分隔符，默认为制表符。
-f  ：与-d一起使用，指定显示哪个区域。
-n ：取消分割多字节字符。仅和 -b 标志一起使用。如果字符的最后一个字节落在由 -b 标志的 List 参数指示的<br />范围之内，该字符将被写出；否则，该字符将被排除。</p>
<p>（2）cut一般以什么为依据呢? 也就是说，我怎么告诉cut我想定位到的剪切内容呢?</p>
<p>cut命令主要是接受三个定位方法：</p>
<p>第一，字节（bytes），用选项-b</p>
<p>第二，字符（characters），用选项-c</p>
<p>第三，域（fields），用选项-f</p>
<p>（3）以“字节”定位</p>
<p>举个例子吧，当你执行ps命令时，会输出类似如下的内容：</p>
<p>[rocrocket@rocrocket programming]$ who
rocrocket :0           2009-01-08 11:07
rocrocket pts/0        2009-01-08 11:23 (:0.0)
rocrocket pts/1        2009-01-08 14:15 (:0.0)
如果我们想提取每一行的第3个字节，就这样：</p>
<p>[rocrocket@rocrocket programming]$ who|cut -b 3
c
c
c</p>
<p>（4） 如果“字节”定位中，我想提取第3，第4、第5和第8个字节，怎么办?</p>
<p>-b支持形如3-5的写法，而且多个定位之间用逗号隔开就成了。看看例子吧：</p>
<p>[rocrocket@rocrocket programming]$ who|cut -b 3-5,8
croe
croe
croe
但有一点要注意，cut命令如果使用了-b选项，那么执行此命令时，cut会先把-b后面所有的定位进行从小到大排序，然后再提取。可不能颠倒定位的顺序哦。这个例子就可以说明这个问题：</p>
<p>[rocrocket@rocrocket programming]$ who|cut -b 8,3-5
croe
croe
croe
（5） 还有哪些类似“3-5”这样的小技巧，列举一下吧!</p>
<p>[rocrocket@rocrocket programming]$ who
rocrocket :0           2009-01-08 11:07
rocrocket pts/0        2009-01-08 11:23 (:0.0)
rocrocket pts/1        2009-01-08 14:15 (:0.0)
[rocrocket@rocrocket programming]$ who|cut -b -3
roc
roc
roc
[rocrocket@rocrocket programming]$ who|cut -b 3-
crocket :0           2009-01-08 11:07
crocket pts/0        2009-01-08 11:23 (:0.0)
crocket pts/1        2009-01-08 14:15 (:0.0)
想必你也看到了，-3表示从第一个字节到第三个字节，而3-表示从第三个字节到行尾。如果你细心，你可以看到这两种情况下，都包括了第三个字节“c”。
如果我执行who|cut -b -3,3-，你觉得会如何呢？答案是输出整行，不会出现连续两个重叠的c的。看：</p>
<p>[rocrocket@rocrocket programming]$ who|cut -b -3,3-
rocrocket :0           2009-01-08 11:07
rocrocket pts/0        2009-01-08 11:23 (:0.0)
rocrocket pts/1        2009-01-08 14:15 (:0.0)
（6）给个以字符为定位标志的最简单的例子吧!</p>
<p>下面例子你似曾相识，提取第3，第4，第5和第8个字符：</p>
<p>[rocrocket@rocrocket programming]$ who|cut -c 3-5,8
croe
croe
croe
不过，看着怎么和-b没有什么区别啊？莫非-b和-c作用一样? 其实不然，看似相同，只是因为这个例子举的不好，who输出的都是单字节字符，所以用-b和-c没有区别，如果你提取中文，区别就看出来了，来，看看中文提取的情况：</p>
<p>[rocrocket@rocrocket programming]$ cat cut_ch.txt
星期一
星期二
星期三
星期四
[rocrocket@rocrocket programming]$ cut -b 3 cut_ch.txt
�
�
�
�
[rocrocket@rocrocket programming]$ cut -c 3 cut_ch.txt
一
二
三
四
看到了吧，用-c则会以字符为单位，输出正常；而-b只会傻傻的以字节（8位二进制位）来计算，输出就是乱码。
既然提到了这个知识点，就再补充一句，如果你学有余力，就提高一下。
当遇到多字节字符时，可以使用-n选项，-n用于告诉cut不要将多字节字符拆开。例子如下：</p>
<p>[rocrocket@rocrocket programming]$ cat cut_ch.txt |cut -b 2
�
�
�
�
[rocrocket@rocrocket programming]$ cat cut_ch.txt |cut -nb 2
[rocrocket@rocrocket programming]$ cat cut_ch.txt |cut -nb 1,2,3
星
星
星
星
（7）域是怎么回事呢？解释解释:)</p>
<p>为什么会有“域”的提取呢，因为刚才提到的-b和-c只能在固定格式的文档中提取信息，而对于非固定格式的信息则束手无策。这时候“域”就派上用场了。如果你观察过/etc/passwd文件，你会发现，它并不像who的输出信息那样具有固定格式，而是比较零散的排放。但是，冒号在这个文件的每一行中都起到了非常重要的作用，冒号用来隔开每一个项。</p>
<p>我们很幸运，cut命令提供了这样的提取方式，具体的说就是设置“间隔符”，再设置“提取第几个域”，就OK了！</p>
<p>以/etc/passwd的前五行内容为例：</p>
<p>[rocrocket@rocrocket programming]$ cat /etc/passwd|head -n 5
root:x:0:0:root:/root:/bin/bash
bin:x:1:1:bin:/bin:/sbin/nologin
daemon:x:2:2:daemon:/sbin:/sbin/nologin
adm:x:3:4:adm:/var/adm:/sbin/nologin
lp:x:4:7:lp:/var/spool/lpd:/sbin/nologin
[rocrocket@rocrocket programming]$ cat /etc/passwd|head -n 5|cut -d : -f 1
root
bin
daemon
adm
lp
看到了吧，用-d来设置间隔符为冒号，然后用-f来设置我要取的是第一个域，再按回车，所有的用户名就都列出来了！呵呵 有成就感吧！
当然，在设定-f时，也可以使用例如3-5或者4-类似的格式：</p>
<p>[rocrocket@rocrocket programming]$ cat /etc/passwd|head -n 5|cut -d : -f 1,3-5
root:0:0:root
bin:1:1:bin
daemon:2:2:daemon
adm:3:4:adm
lp:4:7:lp
[rocrocket@rocrocket programming]$ cat /etc/passwd|head -n 5|cut -d : -f 1,3-5,7
root:0:0:root:/bin/bash
bin:1:1:bin:/sbin/nologin
daemon:2:2:daemon:/sbin/nologin
adm:3:4:adm:/sbin/nologin
lp:4:7:lp:/sbin/nologin
[rocrocket@rocrocket programming]$ cat /etc/passwd|head -n 5|cut -d : -f -2
root:x
bin:x
daemon:x
adm:x
lp:x
（8）如果遇到空格和制表符时，怎么分辨呢？我觉得有点乱，怎么办？</p>
<p>有时候制表符确实很难辨认，有一个方法可以看出一段空格到底是由若干个空格组成的还是由一个制表符组成的。</p>
<p>[rocrocket@rocrocket programming]$ cat tab_space.txt
this is tab finish.
this is several space      finish.
[rocrocket@rocrocket programming]$ sed -n l tab_space.txt
this is tab\tfinish.$
this is several space      finish.$
看到了吧，如果是制表符（TAB），那么会显示为\t符号，如果是空格，就会原样显示。
通过此方法即可以判断制表符和空格了。
注意，上面sed -n后面的字符是L的小写字母哦，不要看错。</p>
<p>（9）我应该在cut -d中用什么符号来设定制表符或空格呢?</p>
<p>其实cut的-d选项的默认间隔符就是制表符，所以当你就是要使用制表符的时候，完全就可以省略-d选项，而直接用－f来取域就可以了。</p>
<p>如果你设定一个空格为间隔符，那么就这样：</p>
<p>[rocrocket@rocrocket programming]$ cat tab_space.txt |cut -d &#39; &#39; -f 1
this
this
注意，两个单引号之间可确实要有一个空格哦，不能偷懒。
而且，你只能在-d后面设置一个空格，可不许设置多个空格，因为cut只允许间隔符是一个字符。</p>
<p>[rocrocket@rocrocket programming]$ cat tab_space.txt |cut -d &#39; &#39; -f 1
cut: the delimiter must be a single character
Try `cut --help&#39; for more information.</p>
<p>（10）cut有哪些缺陷和不足？</p>
<p>猜出来了吧？对，就是在处理多空格时。
如果文件里面的某些域是由若干个空格来间隔的，那么用cut就有点麻烦了，因为cut只擅长处理“以一个字符间隔”的文本内容
来源： <a href="[http://www.cnblogs.com/dong008259/archive/2011/12/09/2282679.html](http://www.cnblogs.com/dong008259/archive/2011/12/09/2282679.html)">[http://www.cnblogs.com/dong008259/archive/2011/12/09/2282679.html](http://www.cnblogs.com/dong008259/archive/2011/12/09/2282679.html)</a> </p>

      
    </div>
    
    
      
    
    
        <footer id="post-meta">
        <span class="categories">Posted in<span class="breadcrumb fa fa-folder"><li><a href="/categories/linux/">linux</a></li></span></span> | <span class="tags">Tagged <a href="/tags/linux/" class="label label-primary">linux</a></span> | <span class="time">recent updated:<time title="2014-03-07 09:54:36"datetime="2014-03-07 09:54:36"> mar. 7 2014</time></span> | <span class="comment-link">
<a href="http://itsolife.com/2014/02/02/2014-02-02-linux--linux之cut用法/#comments" class="ds-thread-count comment-link" data-thread-key="2014-02-02-linux--linux之cut用法" data-count-type="comments">暂无评论</a></span>
        </footer>
    
    <div class="clearfix"></div>
  </div>
</div>
</article>



  <article>
<div id="post" class="post well">
  <div class="post-content">
    <header class="well-sm">
      <i class="fa icon fa-5x pull-left"></i>
      <h1 class="title"><a href="/2014/02/02/2014-02-02-linux-android--adb命令拷贝文件/">adb命令拷贝文件</a></h1>
      
        <span>Posted on<time datetime="2014-02-02T01:54:36.000Z"> <a href="/2014/02/02/2014-02-02-linux-android--adb命令拷贝文件/">feb. 2 2014</a></time></span>      
    </header>
    
    
    <div class="entry">
      
        <h1 id="adb-">adb命令拷贝文件</h1>
<p>To copy dirs, it seems you can use    </p>
<p>adb pull <remote> <local>
if you    want to copy file/dir from device,    and</p>
<p>adb push <local> <remote>
to    copy file/dir to device.    Alternatively, just to copy a file, you can use a simple    trick:</p>
<p>cat source_file &gt;    dest_file
. </p>
<p>Note that this does not work for user-inaccessible paths.</p>
<p>To edit files, I have not found a    simple solution, just some possible    workarounds. Try <a href="http://benno.id.au/blog/2007/11/14/android-busybox" target="_blank">this</a>, it seems    you can (after the setup) use it to    edit files like</p>
<p>busybox vi    <filename>
. Nano seems to be <a href="http://forum.xda-developers.com/showthread.php?t=556944" target="_blank">possible to use</a> to.</p>

      
    </div>
    
    
      
    
    
        <footer id="post-meta">
        <span class="categories">Posted in<span class="breadcrumb fa fa-folder"><li><a href="/categories/linux/">linux</a></li></span><span class="breadcrumb"><li><a href="/categories/linux/">linux</a></li><li><a href="/categories/linux/android/">android</a></li></span></span> | <span class="tags">Tagged <a href="/tags/android/" class="label label-primary">android</a><a href="/tags/linux/" class="label label-success">linux</a></span> | <span class="time">recent updated:<time title="2014-03-07 09:54:36"datetime="2014-03-07 09:54:36"> mar. 7 2014</time></span> | <span class="comment-link">
<a href="http://itsolife.com/2014/02/02/2014-02-02-linux-android--adb命令拷贝文件/#comments" class="ds-thread-count comment-link" data-thread-key="2014-02-02-linux-android--adb命令拷贝文件" data-count-type="comments">暂无评论</a></span>
        </footer>
    
    <div class="clearfix"></div>
  </div>
</div>
</article>



  <article>
<div id="post" class="post well">
  <div class="post-content">
    <header class="well-sm">
      <i class="fa icon fa-5x pull-left"></i>
      <h1 class="title"><a href="/2014/02/02/2014-02-02-hadoop--JAVA线程池管理及分布式HADOOP调度框架搭建/">JAVA线程池管理及分布式HADOOP调度框架搭建</a></h1>
      
        <span>Posted on<time datetime="2014-02-02T01:54:36.000Z"> <a href="/2014/02/02/2014-02-02-hadoop--JAVA线程池管理及分布式HADOOP调度框架搭建/">feb. 2 2014</a></time></span>      
    </header>
    
    
    <div class="entry">
      
        <h1 id="java-hadoop-">JAVA线程池管理及分布式HADOOP调度框架搭建</h1>
<p>平时的开发中线程是个少不了的东西，比如tomcat里的servlet就是线程，没有线程我们如何提供多用户访问呢？不过很多刚开始接触线程的开发攻城师却在这个上面吃了不少苦头。怎么做一套简便的线程开发模式框架让大家从单线程开发快速转入多线程开发，这确实是个比较难搞的工程。</p>
<p>那具体什么是线程呢？首先看看进程是什么，进程就是系统中执行的一个程序，这个程序可以使用内存、处理器、文件系统等相关资源。例如 QQ软件、eclipse、tomcat等就是一个exe程序，运行启动起来就是一个进程。为什么需要多线程？如果每个进程都是单独处理一件事情不能多个任务同时处理，比如我们打开qq只能和一个人聊天，我们用eclipse开发代码的时候不能编译代码，我们请求tomcat服务时只能服务一个用户请求，那我想我们还在原始社会。多线程的目的就是让一个进程能够同时处理多件事情或者请求。比如现在我们使用的QQ软件可以同时和多个人聊天，我们用eclipse开发代码时还可以编译代码，tomcat可以同时服务多个用户请求。</p>
<p>线程这么多好处，怎么把单进程程序变成多线程程序呢？不同的语言有不同的实现，这里说下java语言的实现多线程的两种方式：扩展java.lang.Thread类、实现java.lang.Runnable接口。
先看个例子，假设有100个数据需要分发并且计算。看下单线程的处理速度：
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26 package thread;
import java.util.Vector;
public class OneMain {
       public static void main(<a href="http://www.google.com/search?hl=en&amp;q=allinurl%3Astring+java.sun.com&amp;btnI=I%27m%20Feeling%20Lucky" target="_blank">String</a>[] args) throws <a href="http://www.google.com/search?hl=en&amp;q=allinurl%3Ainterruptedexception+java.sun.com&amp;btnI=I%27m%20Feeling%20Lucky" target="_blank">InterruptedException</a> {
            Vector<Integer> list = new Vector<Integer>(100);
             for (int i = 0; i &lt; 100; i++) {
                  list.add(i);
            }
             long start = <a href="http://www.google.com/search?hl=en&amp;q=allinurl%3Asystem+java.sun.com&amp;btnI=I%27m%20Feeling%20Lucky" target="_blank">System</a>.currentTimeMillis();
             while (list.size() &gt; 0) {
                   int val = list.remove(0);
                  <a href="http://www.google.com/search?hl=en&amp;q=allinurl%3Athread+java.sun.com&amp;btnI=I%27m%20Feeling%20Lucky" target="_blank">Thread</a>. sleep(100);//模拟处理
                  <a href="http://www.google.com/search?hl=en&amp;q=allinurl%3Asystem+java.sun.com&amp;btnI=I%27m%20Feeling%20Lucky" target="_blank">System</a>. out.println(val);
            }
             long end = <a href="http://www.google.com/search?hl=en&amp;q=allinurl%3Asystem+java.sun.com&amp;btnI=I%27m%20Feeling%20Lucky" target="_blank">System</a>.currentTimeMillis();
            <a href="http://www.google.com/search?hl=en&amp;q=allinurl%3Asystem+java.sun.com&amp;btnI=I%27m%20Feeling%20Lucky" target="_blank">System</a>. out.println(&quot;消耗 &quot; + (end - start) + &quot; ms&quot;);
      }
       // 消耗 10063 ms
}</p>
<p>再看一下多线程的处理速度，采用了10个线程分别处理：</p>
<p>1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48 package thread;
import java.util.Vector;
import java.util.concurrent.CountDownLatch;
public class MultiThread extends <a href="http://www.google.com/search?hl=en&amp;q=allinurl%3Athread+java.sun.com&amp;btnI=I%27m%20Feeling%20Lucky" target="_blank">Thread</a> {
     static Vector<Integer> list = new Vector<Integer>(100);
     static CountDownLatch count = new CountDownLatch(10);
     public void run() {
          while (list.size() &gt; 0) {
               try {
                    int val = list.remove(0);
                    <a href="http://www.google.com/search?hl=en&amp;q=allinurl%3Asystem+java.sun.com&amp;btnI=I%27m%20Feeling%20Lucky" target="_blank">System</a>.out.println(val);
                    <a href="http://www.google.com/search?hl=en&amp;q=allinurl%3Athread+java.sun.com&amp;btnI=I%27m%20Feeling%20Lucky" target="_blank">Thread</a>.sleep(100);//模拟处理
               } catch (<a href="http://www.google.com/search?hl=en&amp;q=allinurl%3Aexception+java.sun.com&amp;btnI=I%27m%20Feeling%20Lucky" target="_blank">Exception</a> e) {
                    // 可能数组越界，这个地方只是为了说明问题，忽略错误
               }
          }</p>
<pre><code>      count.countDown(); // 删除成功减一
 }
 public static void main([String](http://www.google.com/search?hl=en&amp;q=allinurl%3Astring+java.sun.com&amp;btnI=I%27m%20Feeling%20Lucky)[] args) throws [InterruptedException](http://www.google.com/search?hl=en&amp;q=allinurl%3Ainterruptedexception+java.sun.com&amp;btnI=I%27m%20Feeling%20Lucky) {

      for (int i = 0; i &lt; 100; i++) {
           list.add(i);
      }

      long start = [System](http://www.google.com/search?hl=en&amp;q=allinurl%3Asystem+java.sun.com&amp;btnI=I%27m%20Feeling%20Lucky).currentTimeMillis();
      for (int i = 0; i &lt; 10; i++) {
           new MultiThread().start();
      }

      count.await();
      long end = [System](http://www.google.com/search?hl=en&amp;q=allinurl%3Asystem+java.sun.com&amp;btnI=I%27m%20Feeling%20Lucky).currentTimeMillis();
      [System](http://www.google.com/search?hl=en&amp;q=allinurl%3Asystem+java.sun.com&amp;btnI=I%27m%20Feeling%20Lucky).out.println(&quot;消耗 &quot; + (end - start) + &quot; ms&quot;);
 }
 // 消耗 1001 ms
</code></pre><p>}</p>
<p>大家看到了线程的好处了吧！单线程需要10S，10个线程只需要1S。充分利用了系统资源实现并行计算。也许这里会产生一个误解，是不是增加的线程个数越多效率越高。线程越多处理性能越高这个是错误的，范式都要合适，过了就不好了。需要普及一下计算机硬件的一些知识。我们的cpu是个运算器，线程执行就需要这个运算器来运行。不过这个资源只有一个，大家就会争抢。一般通过以下几种算法实现争抢cpu的调度：</p>
<p>1、队列方式，先来先服务。不管是什么任务来了都要按照队列排队先来后到。
2、时间片轮转，这也是最古老的cpu调度算法。设定一个时间片，每个任务使用cpu的时间不能超过这个时间。如果超过了这个时间就把任务暂停保存状态，放到队列尾部继续等待执行。
3、优先级方式：给任务设定优先级，有优先级的先执行，没有优先级的就等待执行。</p>
<p>这三种算法都有优缺点，实际操作系统是结合多种算法，保证优先级的能够先处理，但是也不能一直处理优先级的任务。硬件方面为了提高效率也有多核cpu、多线程cpu等解决方案。目前看得出来线程增多了会带来cpu调度的负载增加，cpu需要调度大量的线程，包括创建线程、销毁线程、线程是否需要换出cpu、是否需要分配到cpu。这些都是需要消耗系统资源的，由此，我们需要一个机制来统一管理这一堆线程资源。线程池的理念提出解决了频繁创建、销毁线程的代价。线程池指预先创建好一定大小的线程等待随时服务用户的任务处理，不必等到用户需要的时候再去创建。特别是在java开发中，尽量减少垃圾回收机制的消耗就要减少对象的频繁创建和销毁。</p>
<p>之前我们都是自己实现的线程池，不过随之jdk1.5的推出，jdk自带了 java.util.concurrent并发开发框架，解决了我们大部分线程池框架的重复工作。可以使用Executors来建立线程池，列出以下大概的，后面再介绍。
newCachedThreadPool 建立具有缓存功能线程池
newFixedThreadPool 建立固定数量的线程
newScheduledThreadPool 建立具有时间调度的线程</p>
<p>有了线程池后有以下几个问题需要考虑：
1、线程怎么管理，比如新建任务线程。
2、线程如何停止、启动。
3、线程除了scheduled模式的间隔时间定时外能否实现精确时间启动。比如晚上1点启动。
4、线程如何监控，如果线程执行过程中死掉了，异常终止我们怎么知道。</p>
<p>考虑到这几点，我们需要把线程集中管理起来，用java.util.concurrent是做不到的。需要做以下几点：
1、将线程和业务分离，业务的配置单独做成一个表。
2、构建基于concurrent的线程调度框架，包括可以管理线程的状态、停止线程的接口、线程存活心跳机制、线程异常日志记录模块。
3、构建灵活的timer组件，添加quartz定时组件实现精准定时系统。
4、和业务配置信息结合构建线程池任务调度系统。可以通过配置管理、添加线程任务、监控、定时、管理等操作。
组件图为：
<a href="http://www.lanceyan.com/wp-content/uploads/2013/05/task1.png" target="_blank"><img src="" alt="分布式调度框架-lanceyan.com"></a></p>
<p>构建好线程调度框架是不是就可以应对大量计算的需求了呢?答案是否定的。因为一个机器的资源是有限的，上面也提到了cpu是时间周期的，任务一多了也会排队，就算增加cpu，一个机器能承载的cpu也是有限的。所以需要把整个线程池框架做成分布式的任务调度框架才能应对横向扩展，比如一个机器上的资源呢达到瓶颈了，马上增加一台机器部署调度框架和业务就可以增加计算能力了。好了，如何搭建？如下图：
<a href="http://www.lanceyan.com/wp-content/uploads/2013/05/task2.png" target="_blank"><img src="" alt="分布式调度框架-lanceyan.com"></a></p>
<p>基于jeeframework我们封装spring、ibatis、数据库等操作，并且可以调用业务方法完成业务处理。主要组件为：
1、任务集中存储到数据库服务器
2、控制中心负责管理集群中的节点状态，任务分发
3、线程池调度集群负责控制中心分发的任务执行
4、web服务器通过可视化操作任务的分派、管理、监控。</p>
<p>一般这个架构可以应对常用的分布式处理需求了，不过有个缺陷就是随着开发人员的增多和业务模型的增多，单线程的编程模型也会变得复杂。比如需要对1000w数据进行分词，如果这个放到一个线程里来执行，不算计算时间消耗光是查询数据库就需要耗费不少时间。有人说，那我把1000w数据打散放到不同机器去运算，然后再合并不就行了吗？因为这是个特例的模式，专为了这个需求去开发相应的程序没有问题，但是以后又有其他的海量需求如何办？比如把倒退3年的所有用户发的帖子中发帖子最多的粉丝转发的最高的用户作息时间取出来。又得编一套程序实现，太麻烦！分布式云计算架构要解决的就是这些问题，减少开发复杂度并且要高性能，大家会不会想到一个最近很热的一个框架，hadoop，没错就是这个玩意。hadoop解决的就是这个问题，把大的计算任务分解、计算、合并，这不就是我们要的东西吗？不过玩过这个的人都知道他是一个单独的进程。不是！他是一堆进程，怎么和我们的调度框架结合起来？看图说话：
<a href="http://www.lanceyan.com/wp-content/uploads/2013/05/task31.png" target="_blank"><img src="" alt="task31"></a></p>
<p>基本前面的分布式调度框架组件不变，增加如下组件和功能：
1、改造分布式调度框架，可以把本身线程任务变成mapreduce任务并提交到hadoop集群。
2、hadoop集群能够调用业务接口的spring、ibatis处理业务逻辑访问数据库。
3、hadoop需要的数据能够通过hive查询。
4、hadoop可以访问hdfs/hbase读写操作。
5、业务数据要及时加入hive仓库。
6、hive处理离线型数据、hbase处理经常更新的数据、hdfs是hive和hbase的底层结构也可以存放常规文件。</p>
<p>这样，整个改造基本完成。不过需要注意的是架构设计一定要减少开发程序的复杂度。这里虽然引入了hadoop模型，但是框架上开发者还是隐藏的。业务处理类既可以在单机模式下运行也可以在hadoop上运行，并且可以调用spring、ibatis。减少了开发的学习成本，在实战中慢慢体会就学会了一项新技能。</p>
<p>界面截图：
<a href="http://www.lanceyan.com/wp-content/uploads/2013/05/task4.png" target="_blank"><img src="" alt="task4"></a>
来源： <a href="[http://www.lanceyan.com/category/tech/hadoop](http://www.lanceyan.com/category/tech/hadoop)">[http://www.lanceyan.com/category/tech/hadoop](http://www.lanceyan.com/category/tech/hadoop)</a> </p>

      
    </div>
    
    
      
    
    
        <footer id="post-meta">
        <span class="categories">Posted in<span class="breadcrumb fa fa-folder"><li><a href="/categories/hadoop/">hadoop</a></li></span></span> | <span class="tags">Tagged <a href="/tags/hadoop/" class="label label-primary">hadoop</a></span> | <span class="time">recent updated:<time title="2014-03-07 09:54:36"datetime="2014-03-07 09:54:36"> mar. 7 2014</time></span> | <span class="comment-link">
<a href="http://itsolife.com/2014/02/02/2014-02-02-hadoop--JAVA线程池管理及分布式HADOOP调度框架搭建/#comments" class="ds-thread-count comment-link" data-thread-key="2014-02-02-hadoop--JAVA线程池管理及分布式HADOOP调度框架搭建" data-count-type="comments">暂无评论</a></span>
        </footer>
    
    <div class="clearfix"></div>
  </div>
</div>
</article>





<ul id="pagination" class="pagination pagination-lg">
  <li><a class="extend prev" href="/page/107/">&laquo;</a></li><li><a class="page-number" href="/">1</a></li><li><a class="page-number" href="/page/2/">2</a></li><li><span class="space">&hellip;</span></li><li><a class="page-number" href="/page/105/">105</a></li><li><a class="page-number" href="/page/106/">106</a></li><li><a class="page-number" href="/page/107/">107</a></li><li class="active"><li><span class="page-number current">108</span></li><li><a class="page-number" href="/page/109/">109</a></li><li><a class="page-number" href="/page/110/">110</a></li><li><a class="page-number" href="/page/111/">111</a></li><li><span class="space">&hellip;</span></li></li><li><a class="page-number" href="/page/164/">164</a></li><li><a class="page-number" href="/page/165/">165</a></li><li><a class="extend next" href="/page/109/">&raquo;</a></li>
  <div class="clearfix"></div>
</ul></div><!--wapper-->
       </div><!-- ID main-col END -->
       <aside id="sidebar" class="alignright col-sx-6 col-sm-4 col-md-3 col-lg-3">
<div id="widget_search" class="widget panel panel-primary">
    <form action="//google.com/search" method="get" accept-charset="utf-8">
  <div class="input-group">
    <input class="form-control" id="searchbox" type="search" name="q" results="0" placeholder="search">
    <span class="input-group-btn">
      <button class="btn btn-default" type="submit">Go!</button>
    </span>
    <input type="hidden" name="q" value="site:itsolife.com">
  </div>
</form>
</div>

<div id="widget_category" class="widget panel panel-primary">
  <div class="panel-heading">category</div>  <div data-src='category' class='ajax_widgets'>正在加载...</div>
</div>

<div id="widget_recent_posts" class="widget panel panel-primary">
  <div class="panel-heading">recent posts</div>  <div data-src='recent_posts' class='ajax_widgets'>正在加载...</div></div>

<div id="widget_tagcloud" class="widget panel panel-primary">
  <div class="panel-heading">tagcloud</div>  <div data-src='tagcloud' class='ajax_widgets'>正在加载...</div></div>

<div id="widget_latest_update_posts" class="widget panel panel-primary">
  <div class="panel-heading">最近更新</div>  <div data-src='latest_update_posts' class='ajax_widgets'>正在加载...</div></div>

<div id="widget_recent_comments" class="widget panel panel-primary">
  <div class="panel-heading">recent comments</div>  

<div class="list-group-item ds-recent-comments" data-num-items="6" data-show-avatars="1" data-show-time="1" data-show-title="1" data-show-admin="1" data-excerpt-length="50"></div>



</div>

</aside>
       <div class="clearfix"></div>
     </div><!-- row END -->
  </div>
  <footer id="footer" class="container">
  <div class="panel panel-info">
  <section id='footer_widget'></section>  <div class="panel-footer">
  <div id="site-info">
    <span class='author'>
  
  &copy; 2014 RobinChia
  
    &nbsp;&nbsp;</span>
  
  <span id='analytics-51la'></span><span id='analytics-google'>
<script type="text/javascript">
  var _gaq = _gaq || [];
  _gaq.push(['_setAccount', 'UA-48559895-1']);
  _gaq.push(['_trackPageview']);
  _js2load.push({src:('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js'});
</script></span><span id='analytics-cnzz'>
<script type="text/javascript">var cnzz_protocol = (("https:" == document.location.protocol) ? " https://" : " http://");document.write(unescape("%3Cspan id='cnzz_stat_icon_5774006'%3E%3C/span%3E%3Cscript src='" + cnzz_protocol + "s17.cnzz.com/stat.php%3Fid%3D5774006%26show%3Dpic' type='text/javascript'%3E%3C/script%3E"));</script>
</span><span id='analytics-baidu'>
<script>
var _hmt = _hmt || [];
_js2load.push({src:"//hm.baidu.com/hm.js?1442f50724afc42380b51f097c43082c"});
</script>
</span>  </div>
  <div id="copyright">Blog powered by <a href='http://zespia.tw/hexo/'><strong>hexo</strong></a> Theme <strong><a href='https://github.com/chenall/hexo-theme-chenall'>chenall</a></strong>(Some change in it)<span class="pull-right"> 更新时间: <em>2014-03-15 16:43:39</em></span></div>
</div>
<div class="clearfix"></div>


  </div>
  </footer>
  
        <script src="http://cdn.bootcss.com/jquery/1.10.2/jquery.min.js"></script>        
        <script src="http://cdn.staticfile.org/twitter-bootstrap/3.1.0/js/bootstrap.min.js"></script>        
                
        <script src="http://cdn.bootcss.com/prettify/r298/prettify.min.js"></script>    <script type="text/javascript">
   var lang=["bsh", "c", "cc", "cpp", "cs", "csh", "cyc", "cv", "htm", "html",
    "java", "js", "m", "mxml", "perl", "pl", "pm", "py", "rb", "sh",
    "xhtml", "xml", "xsl"];
   var pretty_base='';
   $('script').each(function(){
	var c = $(this).attr('src');
	if (!c)
	    return;
	if (c.match(/(\/)?prettify(\.min)?\.js/i))
	{
	    var index = c.lastIndexOf('/');
	    if (index != -1)
		pretty_base = c.substr(0,index + 1);
	    return false;
	}
   })
   $('pre code').each(function(){
	var c = $(this).attr('class')
	if (!c)
	    return;
	c = c.match(/\s?(lang\-\w+)/i);
	if (c && lang.indexOf(c[1]) == -1)
	{
	    lang.push(c[1]);
	    $.getScript(pretty_base + c[1] + '.min.js');
	}
   })

    $(window).load(function(){
       $("pre").addClass("prettyprint");
       prettyPrint();
    })
</script>    
            <script type="text/javascript">
var duoshuoQuery = {short_name:"robinchia"};
_js2load.push({src:'http://static.duoshuo.com/embed.js',charset:'UTF-8'});
</script>
    
            <!--wumii_relatedItems-->
    
        <script src="http://cdn.bootcss.com/fancybox/2.1.5/jquery.fancybox.min.js"></script>    <script type="text/javascript">
(function($){
  $('.entry').each(function(i){
    $(this).find('img').each(function(){
      var alt = this.alt;

      if (alt){
        $(this).before('<span class="caption">' + alt + '</span>');
      }

      $(this).wrap('<a href="' + this.src + '" title="' + alt + '" class="fancybox" rel="fancybox' + i + '" />');
    });
  });

  $('.fancybox').fancybox();
})(jQuery);
</script>    
        <script src="http://cdn.bootcss.com/mathjax/2.3/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>    <script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      processEscapes: true
    }
  });
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>
    
<script type="text/javascript">
$('.ajax_widgets').each(function(){var src=$(this).attr('data-src');if(src)$(this).load('/widgets/'+src+'.html');});
$.each(_js2load,function(index,obj){loadjs(obj.src,obj.charset)});
</script>

<div id="scroll2top">
<img src="/scroll2top/arrow.png"/>
</div>
<script src="/scroll2top/scroll2top.min.js"></script>
<div id="winterland">
  <canvas></canvas>
</div>
<script src="/js/winterland.min.js"></script>

  </body>
</html>
