
<!DOCTYPE HTML>
<html>
<head>
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<title>The 106 Page | It so life</title>
<meta name="author" content="RobinChia">

<meta name="description" content="It so life">


<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">


<meta property="og:site_name" content="It so life"/>

<!--[if IE]><style>.testIE.IE{display:inline;}</style><![endif]-->
<!--[if lte IE 7]><link rel="stylesheet" href="/css/ie7.css" type="text/css"><![endif]-->
<!--[if (lt IE 9)&(gt IE 7)]><style>.testIE.IE8{display:inline;}</style><![endif]-->
<!--[if gt IE 8]><style>.testIE.IE9{display:inline;}</style><![endif]-->

<link href="/favicon.png" rel="icon">
<link rel="alternate" href="/atom.xml" title="It so life Feed" type="application/atom+xml">

<link rel="stylesheet" href="http://cdn.staticfile.org/twitter-bootstrap/3.1.0/css/bootstrap.min.css" type="text/css">
<link rel="stylesheet" href="http://cdn.bootcss.com/font-awesome/4.0.3/css/font-awesome.min.css" type="text/css">
<link rel="stylesheet" href="http://cdn.bootcss.com/prettify/r298/prettify.min.css" type="text/css">
<link rel="stylesheet" href="http://cdn.bootcss.com/fancybox/2.1.5/jquery.fancybox.min.css" type="text/css">
<link rel="stylesheet" href="/css/style.css" type="text/css">
<!--[if lt IE 9]>
   <style>article,aside,dialog,footer,header,section,footer,nav,figure,menu{display:block}</style>
   <script src="http://cdn.staticfile.org/html5shiv/r29/html5.min.js"></script>
   <script src="http://cdn.staticfile.org/respond.js/1.4.2/respond.min.js"></script>
<link href="http://cdn.staticfile.org/respond.js/1.4.2/respond-proxy.html" id="respond-proxy" rel="respond-proxy" />
<link href="/js/respond.proxy.gif" id="respond-redirect" rel="respond-redirect" />
<script src="/js/respond.proxy.js"></script>
   <script src="http://cdn.bootcss.com/selectivizr/1.0.2/selectivizr-min.js"></script>
<![endif]-->
<script type="text/javascript">
function loadjs(c,d){var a=document.createElement("script");a.async=!0;a.type="text/javascript";a.src=c;a.charset=d||"gbk";(document.getElementsByTagName("HEAD")[0]||document.getElementsByTagName("BODY")[0]).appendChild(a)};
var _protocol = (("https:" == document.location.protocol) ? " https://" : " http://");
var _js2load = [];
</script>

</head>
<body>
      <header id="header" class="container"><nav id="main-nav" class="navbar navbar-default navbar-fixed-top " role="navigation">
  <div class="container">
    <div class="navbar-header">
    <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".navbar-collapse">
      <span class="sr-only">Toggle navigation</span>
      <span class="icon-bar"></span>
      <span class="icon-bar"></span>
      <span class="icon-bar"></span>
    </button>
    <a class="navbar-brand" href="/">It so life</a>
    </div>
    <div  class="collapse navbar-collapse">
      <ul  class="nav navbar-nav">
  
        <li><a href="/" title="Home">Home</a></li>      
        <li><a href="/about/" title="About">About</a></li>      
        <li><a href="/archives/" title="Archives">Archives</a></li>      
      <li class='dropdown'>
        <a class='dropdown-toggle' data-toggle='dropdown' href='#'>website<b class='caret'></b></a>
        <ul class='dropdown-menu pure-menu-selected'>
    
          <li><a href="//groups.google.com/forum/#!forum/pongba" title="TopLanguage">TopLanguage</a></li>    
          <li><a href="//itpub.net/" title="ITPub">ITPub</a></li>    
          <li><a href="//blog.jobbole.com/" title="Bole">Bole</a></li>    
          <li><a href="//nosql-database.org/" title="nosql">nosql</a></li>    
          <li><a href="//gitimmersion.googol.im/" title="Git">Git</a></li>    
        </ul>
      </li>
    
      </ul>
      <ul class="nav navbar-nav navbar-right">
      
        <li><a href="/atom.xml">RSS</a></li>
      
      
        <li><a href="https://twitter.com/robinchia">twitter</a></li>
      
      
      
      
        <li><a href="https://github.com/robinchia">github</a></li>
      
      </ul>
    </div>
  </div>
</nav>
<div class="clearfix"></div>
</header>
  <div id='content' class="container">
     <div class="page-header-wrapper">
      <!--[if lt IE 9]><div class="alert alert-warning alert-dismissable"><button type="button" class="close" data-dismiss="alert" aria-hidden="true">&times;</button><strong>提示:</strong>您的浏览器版本太低了,建议升级到 <strong><a href="http://windows.microsoft.com/zh-cn/internet-explorer/download-ie" title="IE9">IE9</a></strong> 以上,本站使用<a href="https://www.google.com/intl/zh-CN/chrome/">Chrome浏览器</a>可以获得最好的显示效果.</div><![endif]-->
      <div class="page-header"><h1 align="center"><big>It so life</big> </h1>
        <h5 align="center"><big>love as life</big></h5>
      </div>
     </div>
     <div class="row">
       <div id="main-col" class="alignleft col-sx-12 col-sm-8 col-md-9 col-lg-9">
      <section id='header_widget'></section>
          <div id="wrapper"><article>
<div id="post" class="post well">
  <div class="post-content">
    <header class="well-sm">
      <i class="fa icon fa-5x pull-left"></i>
      <h1 class="title"><a href="/2014/02/02/2014-02-02-hadoop--hdfs_design/">hdfs_design</a></h1>
      
        <span>Posted on<time datetime="2014-02-02T01:54:36.000Z"> <a href="/2014/02/02/2014-02-02-hadoop--hdfs_design/">feb. 2 2014</a></time></span>      
    </header>
    
    
    <div class="entry">
      
        <h1 id="hdfs_design">hdfs_design</h1>
<p>HDFS Architecture
by Dhruba Borthakur
Table of contents
1 2
Introduction .......................................................................................................................3 Assumptions and Goals .....................................................................................................3
2.1 2.2 2.3 2.4 2.5 2.6
Hardware Failure .......................................................................................................... 3 Streaming Data Access .................................................................................................3 Large Data Sets .............................................................................................................3 Simple Coherency Model ............................................................................................. 4 “Moving Computation is Cheaper than Moving Data” ................................................4 Portability Across Heterogeneous Hardware and Software Platforms .........................4
3 4 5
NameNode and DataNodes ...............................................................................................4 The File System Namespace ............................................................................................. 5 Data Replication ................................................................................................................6
5.1 5.2 5.3
Replica Placement: The First Baby Steps .................................................................... 7 Replica Selection .......................................................................................................... 8 Safemode ...................................................................................................................... 8
6 7 8
The Persistence of File System Metadata ......................................................................... 8 The Communication Protocols ......................................................................................... 9 Robustness ........................................................................................................................ 9
8.1 8.2 8.3 8.4 8.5
Data Disk Failure, Heartbeats and Re-Replication .....................................................10 Cluster Rebalancing ....................................................................................................10 Data Integrity ..............................................................................................................10 Metadata Disk Failure ................................................................................................ 10 Snapshots ....................................................................................................................11
Copyright © 2008 The Apache Software Foundation. All rights reserved.
HDFS Architecture
9
Data Organization ........................................................................................................... 11
9.1 9.2 9.3
Data Blocks ................................................................................................................ 11 Staging ........................................................................................................................11 Replication Pipelining ................................................................................................ 12 FS Shell .....................................................................................................................12 DFSAdmin ................................................................................................................ 13 Browser Interface ......................................................................................................13 File Deletes and Undeletes ....................................................................................... 13 Decrease Replication Factor ..................................................................................... 14
10
Accessibility .................................................................................................................. 12
10.1 10.2 10.3 11
Space Reclamation ........................................................................................................ 13
11.1 11.2 12
References ..................................................................................................................... 14
Page 2
Copyright © 2008 The Apache Software Foundation. All rights reserved.
HDFS Architecture</p>
<ol>
<li>Introduction
The Hadoop Distributed File System (HDFS) is a distributed file system designed to run on commodity hardware. It has many similarities with existing distributed file systems. However, the differences from other distributed file systems are significant. HDFS is highly fault-tolerant and is designed to be deployed on low-cost hardware. HDFS provides high throughput access to application data and is suitable for applications that have large data sets. HDFS relaxes a few POSIX requirements to enable streaming access to file system data. HDFS was originally built as infrastructure for the Apache Nutch web search engine project. HDFS is part of the Apache Hadoop Core project. The project URL is <a href="http://hadoop.apache.org/core/" target="_blank">http://hadoop.apache.org/core/</a>.</li>
<li>Assumptions and Goals
2.1. Hardware Failure
Hardware failure is the norm rather than the exception. An HDFS instance may consist of hundreds or thousands of server machines, each storing part of the file system’s data. The fact that there are a huge number of components and that each component has a non-trivial probability of failure means that some component of HDFS is always non-functional. Therefore, detection of faults and quick, automatic recovery from them is a core architectural goal of HDFS.
2.2. Streaming Data Access
Applications that run on HDFS need streaming access to their data sets. They are not general purpose applications that typically run on general purpose file systems. HDFS is designed more for batch processing rather than interactive use by users. The emphasis is on high throughput of data access rather than low latency of data access. POSIX imposes many hard requirements that are not needed for applications that are targeted for HDFS. POSIX semantics in a few key areas has been traded to increase data throughput rates.
2.3. Large Data Sets
Applications that run on HDFS have large data sets. A typical file in HDFS is gigabytes to terabytes in size. Thus, HDFS is tuned to support large files. It should provide high aggregate data bandwidth and scale to hundreds of nodes in a single cluster. It should support tens of millions of files in a single instance.
Page 3
Copyright © 2008 The Apache Software Foundation. All rights reserved.
HDFS Architecture
2.4. Simple Coherency Model
HDFS applications need a write-once-read-many access model for files. A file once created, written, and closed need not be changed. This assumption simplifies data coherency issues and enables high throughput data access. A Map/Reduce application or a web crawler application fits perfectly with this model. There is a plan to support appending-writes to files in the future.
2.5. “Moving Computation is Cheaper than Moving Data”
A computation requested by an application is much more efficient if it is executed near the data it operates on. This is especially true when the size of the data set is huge. This minimizes network congestion and increases the overall throughput of the system. The assumption is that it is often better to migrate the computation closer to where the data is located rather than moving the data to where the application is running. HDFS provides interfaces for applications to move themselves closer to where the data is located.
2.6. Portability Across Heterogeneous Hardware and Software Platforms
HDFS has been designed to be easily portable from one platform to another. This facilitates widespread adoption of HDFS as a platform of choice for a large set of applications.</li>
<li>NameNode and DataNodes
HDFS has a master/slave architecture. An HDFS cluster consists of a single NameNode, a master server that manages the file system namespace and regulates access to files by clients. In addition, there are a number of DataNodes, usually one per node in the cluster, which manage storage attached to the nodes that they run on. HDFS exposes a file system namespace and allows user data to be stored in files. Internally, a file is split into one or more blocks and these blocks are stored in a set of DataNodes. The NameNode executes file system namespace operations like opening, closing, and renaming files and directories. It also determines the mapping of blocks to DataNodes. The DataNodes are responsible for serving read and write requests from the file system’s clients. The DataNodes also perform block creation, deletion, and replication upon instruction from the NameNode.
Page 4
Copyright © 2008 The Apache Software Foundation. All rights reserved.
<img src="" alt=""> HDFS Architecture
The NameNode and DataNode are pieces of software designed to run on commodity machines. These machines typically run a GNU/Linux operating system (OS). HDFS is built using the Java language; any machine that supports Java can run the NameNode or the DataNode software. Usage of the highly portable Java language means that HDFS can be deployed on a wide range of machines. A typical deployment has a dedicated machine that runs only the NameNode software. Each of the other machines in the cluster runs one instance of the DataNode software. The architecture does not preclude running multiple DataNodes on the same machine but in a real deployment that is rarely the case. The existence of a single NameNode in a cluster greatly simplifies the architecture of the system. The NameNode is the arbitrator and repository for all HDFS metadata. The system is designed in such a way that user data never flows through the NameNode.</li>
<li>The File System Namespace
HDFS supports a traditional hierarchical file organization. A user or an application can create directories and store files inside these directories. The file system namespace hierarchy is
Page 5
Copyright © 2008 The Apache Software Foundation. All rights reserved.
HDFS Architecture
similar to most other existing file systems; one can create and remove files, move a file from one directory to another, or rename a file. HDFS does not yet implement user quotas or access permissions. HDFS does not support hard links or soft links. However, the HDFS architecture does not preclude implementing these features. The NameNode maintains the file system namespace. Any change to the file system namespace or its properties is recorded by the NameNode. An application can specify the number of replicas of a file that should be maintained by HDFS. The number of copies of a file is called the replication factor of that file. This information is stored by the NameNode.</li>
<li>Data Replication
HDFS is designed to reliably store very large files across machines in a large cluster. It stores each file as a sequence of blocks; all blocks in a file except the last block are the same size. The blocks of a file are replicated for fault tolerance. The block size and replication factor are configurable per file. An application can specify the number of replicas of a file. The replication factor can be specified at file creation time and can be changed later. Files in HDFS are write-once and have strictly one writer at any time. The NameNode makes all decisions regarding replication of blocks. It periodically receives a Heartbeat and a Blockreport from each of the DataNodes in the cluster. Receipt of a Heartbeat implies that the DataNode is functioning properly. A Blockreport contains a list of all blocks on a DataNode.
Page 6
Copyright © 2008 The Apache Software Foundation. All rights reserved.
<img src="" alt=""> HDFS Architecture
5.1. Replica Placement: The First Baby Steps
The placement of replicas is critical to HDFS reliability and performance. Optimizing replica placement distinguishes HDFS from most other distributed file systems. This is a feature that needs lots of tuning and experience. The purpose of a rack-aware replica placement policy is to improve data reliability, availability, and network bandwidth utilization. The current implementation for the replica placement policy is a first effort in this direction. The short-term goals of implementing this policy are to validate it on production systems, learn more about its behavior, and build a foundation to test and research more sophisticated policies. Large HDFS instances run on a cluster of computers that commonly spread across many racks. Communication between two nodes in different racks has to go through switches. In most cases, network bandwidth between machines in the same rack is greater than network bandwidth between machines in different racks. The NameNode determines the rack id each DataNode belongs to via the process outlined in Rack Awareness. A simple but non-optimal policy is to place replicas on unique racks. This prevents losing data when an entire rack fails and allows use of bandwidth from multiple
Page 7
Copyright © 2008 The Apache Software Foundation. All rights reserved.
HDFS Architecture
racks when reading data. This policy evenly distributes replicas in the cluster which makes it easy to balance load on component failure. However, this policy increases the cost of writes because a write needs to transfer blocks to multiple racks. For the common case, when the replication factor is three, HDFS’s placement policy is to put one replica on one node in the local rack, another on a different node in the local rack, and the last on a different node in a different rack. This policy cuts the inter-rack write traffic which generally improves write performance. The chance of rack failure is far less than that of node failure; this policy does not impact data reliability and availability guarantees. However, it does reduce the aggregate network bandwidth used when reading data since a block is placed in only two unique racks rather than three. With this policy, the replicas of a file do not evenly distribute across the racks. One third of replicas are on one node, two thirds of replicas are on one rack, and the other third are evenly distributed across the remaining racks. This policy improves write performance without compromising data reliability or read performance. The current, default replica placement policy described here is a work in progress.
5.2. Replica Selection
To minimize global bandwidth consumption and read latency, HDFS tries to satisfy a read request from a replica that is closest to the reader. If there exists a replica on the same rack as the reader node, then that replica is preferred to satisfy the read request. If angg/ HDFS cluster spans multiple data centers, then a replica that is resident in the local data center is preferred over any remote replica.
5.3. Safemode
On startup, the NameNode enters a special state called Safemode. Replication of data blocks does not occur when the NameNode is in the Safemode state. The NameNode receives Heartbeat and Blockreport messages from the DataNodes. A Blockreport contains the list of data blocks that a DataNode is hosting. Each block has a specified minimum number of replicas. A block is considered safely replicated when the minimum number of replicas of that data block has checked in with the NameNode. After a configurable percentage of safely replicated data blocks checks in with the NameNode (plus an additional 30 seconds), the NameNode exits the Safemode state. It then determines the list of data blocks (if any) that still have fewer than the specified number of replicas. The NameNode then replicates these blocks to other DataNodes.</li>
<li>The Persistence of File System Metadata
Page 8
Copyright © 2008 The Apache Software Foundation. All rights reserved.
HDFS Architecture
The HDFS namespace is stored by the NameNode. The NameNode uses a transaction log called the EditLog to persistently record every change that occurs to file system metadata. For example, creating a new file in HDFS causes the NameNode to insert a record into the EditLog indicating this. Similarly, changing the replication factor of a file causes a new record to be inserted into the EditLog. The NameNode uses a file in its local host OS file system to store the EditLog. The entire file system namespace, including the mapping of blocks to files and file system properties, is stored in a file called the FsImage. The FsImage is stored as a file in the NameNode’s local file system too. The NameNode keeps an image of the entire file system namespace and file Blockmap in memory. This key metadata item is designed to be compact, such that a NameNode with 4 GB of RAM is plenty to support a huge number of files and directories. When the NameNode starts up, it reads the FsImage and EditLog from disk, applies all the transactions from the EditLog to the in-memory representation of the FsImage, and flushes out this new version into a new FsImage on disk. It can then truncate the old EditLog because its transactions have been applied to the persistent FsImage. This process is called a checkpoint. In the current implementation, a checkpoint only occurs when the NameNode starts up. Work is in progress to support periodic checkpointing in the near future. The DataNode stores HDFS data in files in its local file system. The DataNode has no knowledge about HDFS files. It stores each block of HDFS data in a separate file in its local file system. The DataNode does not create all files in the same directory. Instead, it uses a heuristic to determine the optimal number of files per directory and creates subdirectories appropriately. It is not optimal to create all local files in the same directory because the local file system might not be able to efficiently support a huge number of files in a single directory. When a DataNode starts up, it scans through its local file system, generates a list of all HDFS data blocks that correspond to each of these local files and sends this report to the NameNode: this is the Blockreport.</li>
<li>The Communication Protocols
All HDFS communication protocols are layered on top of the TCP/IP protocol. A client establishes a connection to a configurable TCP port on the NameNode machine. It talks the ClientProtocol with the NameNode. The DataNodes talk to the NameNode using the DataNode Protocol. A Remote Procedure Call (RPC) abstraction wraps both the Client Protocol and the DataNode Protocol. By design, the NameNode never initiates any RPCs. Instead, it only responds to RPC requests issued by DataNodes or clients.</li>
<li>Robustness
The primary objective of HDFS is to store data reliably even in the presence of failures. The
Page 9
Copyright © 2008 The Apache Software Foundation. All rights reserved.
HDFS Architecture
three common types of failures are NameNode failures, DataNode failures and network partitions.
8.1. Data Disk Failure, Heartbeats and Re-Replication
Each DataNode sends a Heartbeat message to the NameNode periodically. A network partition can cause a subset of DataNodes to lose connectivity with the NameNode. The NameNode detects this condition by the absence of a Heartbeat message. The NameNode marks DataNodes without recent Heartbeats as dead and does not forward any new IO requests to them. Any data that was registered to a dead DataNode is not available to HDFS any more. DataNode death may cause the replication factor of some blocks to fall below their specified value. The NameNode constantly tracks which blocks need to be replicated and initiates replication whenever necessary. The necessity for re-replication may arise due to many reasons: a DataNode may become unavailable, a replica may become corrupted, a hard disk on a DataNode may fail, or the replication factor of a file may be increased.
8.2. Cluster Rebalancing
The HDFS architecture is compatible with data rebalancing schemes. A scheme might automatically move data from one DataNode to another if the free space on a DataNode falls below a certain threshold. In the event of a sudden high demand for a particular file, a scheme might dynamically create additional replicas and rebalance other data in the cluster. These types of data rebalancing schemes are not yet implemented.
8.3. Data Integrity
It is possible that a block of data fetched from a DataNode arrives corrupted. This corruption can occur because of faults in a storage device, network faults, or buggy software. The HDFS client software implements checksum checking on the contents of HDFS files. When a client creates an HDFS file, it computes a checksum of each block of the file and stores these checksums in a separate hidden file in the same HDFS namespace. When a client retrieves file contents it verifies that the data it received from each DataNode matches the checksum stored in the associated checksum file. If not, then the client can opt to retrieve that block from another DataNode that has a replica of that block.
8.4. Metadata Disk Failure
The FsImage and the EditLog are central data structures of HDFS. A corruption of these files can cause the HDFS instance to be non-functional. For this reason, the NameNode can be configured to support maintaining multiple copies of the FsImage and EditLog. Any update
Page 10
Copyright © 2008 The Apache Software Foundation. All rights reserved.
HDFS Architecture
to either the FsImage or EditLog causes each of the FsImages and EditLogs to get updated synchronously. This synchronous updating of multiple copies of the FsImage and EditLog may degrade the rate of namespace transactions per second that a NameNode can support. However, this degradation is acceptable because even though HDFS applications are very data intensive in nature, they are not metadata intensive. When a NameNode restarts, it selects the latest consistent FsImage and EditLog to use. The NameNode machine is a single point of failure for an HDFS cluster. If the NameNode machine fails, manual intervention is necessary. Currently, automatic restart and failover of the NameNode software to another machine is not supported.
8.5. Snapshots
Snapshots support storing a copy of data at a particular instant of time. One usage of the snapshot feature may be to roll back a corrupted HDFS instance to a previously known good point in time. HDFS does not currently support snapshots but will in a future release.</li>
<li>Data Organization
9.1. Data Blocks
HDFS is designed to support very large files. Applications that are compatible with HDFS are those that deal with large data sets. These applications write their data only once but they read it one or more times and require these reads to be satisfied at streaming speeds. HDFS supports write-once-read-many semantics on files. A typical block size used by HDFS is 64 MB. Thus, an HDFS file is chopped up into 64 MB chunks, and if possible, each chunk will reside on a different DataNode.
9.2. Staging
A client request to create a file does not reach the NameNode immediately. In fact, initially the HDFS client caches the file data into a temporary local file. Application writes are transparently redirected to this temporary local file. When the local file accumulates data worth over one HDFS block size, the client contacts the NameNode. The NameNode inserts the file name into the file system hierarchy and allocates a data block for it. The NameNode responds to the client request with the identity of the DataNode and the destination data block. Then the client flushes the block of data from the local temporary file to the specified DataNode. When a file is closed, the remaining un-flushed data in the temporary local file is transferred to the DataNode. The client then tells the NameNode that the file is closed. At this point, the NameNode commits the file creation operation into a persistent store. If the
Page 11
Copyright © 2008 The Apache Software Foundation. All rights reserved.
HDFS Architecture
NameNode dies before the file is closed, the file is lost. The above approach has been adopted after careful consideration of target applications that run on HDFS. These applications need streaming writes to files. If a client writes to a remote file directly without any client side buffering, the network speed and the congestion in the network impacts throughput considerably. This approach is not without precedent. Earlier distributed file systems, e.g. AFS, have used client side caching to improve performance. A POSIX requirement has been relaxed to achieve higher performance of data uploads.
9.3. Replication Pipelining
When a client is writing data to an HDFS file, its data is first written to a local file as explained in the previous section. Suppose the HDFS file has a replication factor of three. When the local file accumulates a full block of user data, the client retrieves a list of DataNodes from the NameNode. This list contains the DataNodes that will host a replica of that block. The client then flushes the data block to the first DataNode. The first DataNode starts receiving the data in small portions (4 KB), writes each portion to its local repository and transfers that portion to the second DataNode in the list. The second DataNode, in turn starts receiving each portion of the data block, writes that portion to its repository and then flushes that portion to the third DataNode. Finally, the third DataNode writes the data to its local repository. Thus, a DataNode can be receiving data from the previous one in the pipeline and at the same time forwarding data to the next one in the pipeline. Thus, the data is pipelined from one DataNode to the next.</li>
<li>Accessibility
HDFS can be accessed from applications in many different ways. Natively, HDFS provides a FileSystem Java API for applications to use. A C language wrapper for this Java API is also available. In addition, an HTTP browser can also be used to browse the files of an HDFS instance. Work is in progress to expose HDFS through the WebDAV protocol.
10.1. FS Shell
HDFS allows user data to be organized in the form of files and directories. It provides a commandline interface called FS shell that lets a user interact with the data in HDFS. The syntax of this command set is similar to other shells (e.g. bash, csh) that users are already familiar with. Here are some sample action/command pairs:
Action Create a directory named /foodir Command bin/hadoop dfs -mkdir /foodir
Page 12
Copyright © 2008 The Apache Software Foundation. All rights reserved.
HDFS Architecture
Remove a directory named /foodir View the contents of a file named /foodir/myfile.txt
bin/hadoop dfs -rmr /foodir bin/hadoop dfs -cat /foodir/myfile.txt
FS shell is targeted for applications that need a scripting language to interact with the stored data.
10.2. DFSAdmin
The DFSAdmin command set is used for administering an HDFS cluster. These are commands that are used only by an HDFS administrator. Here are some sample action/command pairs:
Action Put the cluster in Safemode Generate a list of DataNodes Recommission or decommission DataNode(s) Command bin/hadoop dfsadmin -safemode enter bin/hadoop dfsadmin -report bin/hadoop dfsadmin -refreshNodes
10.3. Browser Interface
A typical HDFS install configures a web server to expose the HDFS namespace through a configurable TCP port. This allows a user to navigate the HDFS namespace and view the contents of its files using a web browser.</li>
<li>Space Reclamation
11.1. File Deletes and Undeletes
When a file is deleted by a user or an application, it is not immediately removed from HDFS. Instead, HDFS first renames it to a file in the /trash directory. The file can be restored quickly as long as it remains in /trash. A file remains in /trash for a configurable amount of time. After the expiry of its life in /trash, the NameNode deletes the file from the HDFS namespace. The deletion of a file causes the blocks associated with the file to be freed. Note that there could be an appreciable time delay between the time a file is deleted by a user and the time of the corresponding increase in free space in HDFS. A user can Undelete a file after deleting it as long as it remains in the /trash directory. If a user wants to undelete a file that he/she has deleted, he/she can navigate the /trash directory and retrieve the file. The /trash directory contains only the latest copy of the file
Page 13
Copyright © 2008 The Apache Software Foundation. All rights reserved.
HDFS Architecture
that was deleted. The /trash directory is just like any other directory with one special feature: HDFS applies specified policies to automatically delete files from this directory. The current default policy is to delete files from /trash that are more than 6 hours old. In the future, this policy will be configurable through a well defined interface.
11.2. Decrease Replication Factor
When the replication factor of a file is reduced, the NameNode selects excess replicas that can be deleted. The next Heartbeat transfers this information to the DataNode. The DataNode then removes the corresponding blocks and the corresponding free space appears in the cluster. Once again, there might be a time delay between the completion of the setReplication API call and the appearance of free space in the cluster.</li>
<li>References
Hadoop JavaDoc API. HDFS source code: <a href="http://hadoop.apache.org/core/version_control.html" target="_blank">http://hadoop.apache.org/core/version_control.html</a>
Page 14
Copyright © 2008 The Apache Software Foundation. All rights reserved.</li>
</ol>

      
    </div>
    
    
      
    
    
        <footer id="post-meta">
        <span class="categories">Posted in<span class="breadcrumb fa fa-folder"><li><a href="/categories/hadoop/">hadoop</a></li></span></span> | <span class="tags">Tagged <a href="/tags/hadoop/" class="label label-primary">hadoop</a></span> | <span class="time">recent updated:<time title="2014-03-07 09:54:36"datetime="2014-03-07 09:54:36"> mar. 7 2014</time></span> | <span class="comment-link">
<a href="http://itsolife.com/2014/02/02/2014-02-02-hadoop--hdfs_design/#comments" class="ds-thread-count comment-link" data-thread-key="2014-02-02-hadoop--hdfs_design" data-count-type="comments">暂无评论</a></span>
        </footer>
    
    <div class="clearfix"></div>
  </div>
</div>
</article>



  <article>
<div id="post" class="post well">
  <div class="post-content">
    <header class="well-sm">
      <i class="fa icon fa-5x pull-left"></i>
      <h1 class="title"><a href="/2014/02/02/2014-02-02-hadoop--NotesforHadoopthedefinitiveguide/">Notes for Hadoop the definitive guide</a></h1>
      
        <span>Posted on<time datetime="2014-02-02T01:54:36.000Z"> <a href="/2014/02/02/2014-02-02-hadoop--NotesforHadoopthedefinitiveguide/">feb. 2 2014</a></time></span>      
    </header>
    
    
    <div class="entry">
      
        <h1 id="notes-for-hadoop-the-definitive-guide">Notes for Hadoop the definitive guide</h1>
<h1 id="1-introduction-to-hdfs">1. Introduction to HDFS</h1>
<h2 id="1-1-hdfs-concepts">1.1. HDFS Concepts</h2>
<h3 id="1-1-1-blocks">1.1.1. Blocks</h3>
<p>l HDFS too has the concept of a block, but it is a much larger unit 64 MB by default.</p>
<p>l Like in a filesystem for a single disk, files in HDFS are broken into block-sized chunks, which are stored as independent units.</p>
<p>l Unlike a filesystem for a single disk, a file in HDFS that is smaller than a single block does not occupy a full block’s worth of underlying storage.</p>
<h3 id="1-1-2-namenodes-and-datanodes">1.1.2. Namenodes and Datanodes</h3>
<p>l The namenode manages the filesystem namespace.</p>
<p>n It maintains the filesystem tree and the metadata for all the files and directories in the tree.</p>
<p>n This information is stored persistently on the local disk in the form of two files: the namespace image and the edit log.</p>
<p>n The namenode also knows the datanodes on which all the blocks for a given file are located, however, it does not store block locations persistently, since this information is reconstructed from datanodes when the system starts.</p>
<p>l Datanodes are the work horses of the filesystem.</p>
<p>n They store and retrieve blocks when they are told to (by clients or the namenode)</p>
<p>n They report back to the namenode periodically with lists of blocks that they are storing.</p>
<p>l secondary namenode</p>
<p>n It does not act as a namenode.</p>
<p>n Its main role is to periodically merge the namespace image with the edit log to prevent the edit log from becoming too large.</p>
<p>n It keeps a copy of the merged name space image, which can be used in the event of the namenode failing.</p>
<h3 id="namenode-directory-structure">Namenode directory structure</h3>
<p><a href="http://images.cnblogs.com/cnblogs_com/forfuture1978/WindowsLiveWriter/NotesforHadoopthedefinitiveguide_14109/clip_image002_2.jpg" target="_blank"><img src="&quot;clip_image002&quot;" alt="clip_image002"></a></p>
<p>l The VERSION file is a Java properties file that contains information about the version of HDFS that is running</p>
<p>n The layoutVersion is a negative integer that defines the version of HDFS’s persistent data structures.</p>
<p>n The namespaceID is a unique identifier for the filesystem, which is created when the filesystem is first formatted.</p>
<p>n The cTime property marks the creation time of the namenode’s storage.</p>
<p>n The storageType indicates that this storage directory contains data structures for a namenode.</p>
<p><a href="http://images.cnblogs.com/cnblogs_com/forfuture1978/WindowsLiveWriter/NotesforHadoopthedefinitiveguide_14109/clip_image004_2.jpg" target="_blank"><img src="&quot;clip_image004&quot;" alt="clip_image004"></a></p>
<h3 id="the-filesystem-image-and-edit-log">The filesystem image and edit log</h3>
<p>l When a filesystem client performs a write operation, it is first recorded in the edit log.</p>
<p>l The namenode also has an in-memory representation of the filesystem metadata, which it updates after the edit log has been modified.</p>
<p>l The edit log is flushed and synced after every write before a success code is returned to the client.</p>
<p>l The fsimage file is a persistent checkpoint of the filesystem metadata. it is not updated for every filesystem write operation.</p>
<p>l If the namenode fails, then the latest state of its metadata can be reconstructed by loading the fsimage from disk into memory, then applying each of the operations in the edit log.</p>
<p>l This is precisely what the namenode does when it starts up.</p>
<p>l The fsimage file contains a serialized form of all the directory and file inodes in the filesystem.</p>
<p>l The secondary namenode is to produce checkpoints of the primary’s in-memory filesystem metadata.</p>
<p>l The checkpointing process proceeds as follows :</p>
<p>n The secondary asks the primary to roll its edits file, so new edits go to a new file.</p>
<p>n The secondary retrieves fsimage and edits from the primary (using HTTP GET).</p>
<p>n The secondary loads fsimage into memory, applies each operation from edits, then creates a new consolidated fsimage file.</p>
<p>n The secondary sends the new fsimage back to the primary (using HTTP POST).</p>
<p>n The primary replaces the old fsimage with the new one from the secondary, and the old edits file with the new one it started in step 1. It also updates the fstime file to record the time that the checkpoint was taken.</p>
<p>n At the end of the process, the primary has an up-to-date fsimage file, and a shorter edits file.</p>
<p><a href="http://images.cnblogs.com/cnblogs_com/forfuture1978/WindowsLiveWriter/NotesforHadoopthedefinitiveguide_14109/clip_image006_2.jpg" target="_blank"><img src="&quot;clip_image006&quot;" alt="clip_image006"></a></p>
<h3 id="secondary-namenode-directory-structure">Secondary namenode directory structure</h3>
<p><a href="http://images.cnblogs.com/cnblogs_com/forfuture1978/WindowsLiveWriter/NotesforHadoopthedefinitiveguide_14109/clip_image008_2.jpg" target="_blank"><img src="&quot;clip_image008&quot;" alt="clip_image008"></a></p>
<h3 id="datanode-directory-structure">Datanode directory structure</h3>
<p><a href="http://images.cnblogs.com/cnblogs_com/forfuture1978/WindowsLiveWriter/NotesforHadoopthedefinitiveguide_14109/clip_image010_2.jpg" target="_blank"><img src="&quot;clip_image010&quot;" alt="clip_image010"></a></p>
<p>l A datanode’s VERSION file</p>
<p><a href="http://images.cnblogs.com/cnblogs_com/forfuture1978/WindowsLiveWriter/NotesforHadoopthedefinitiveguide_14109/clip_image012_2.jpg" target="_blank"><img src="&quot;clip_image012&quot;" alt="clip_image012"></a></p>
<p>l The other files in the datanode’s current storage directory are the files with the blk_ prefix.</p>
<p>n There are two types: the HDFS blocks themselves (which just consist of the file’s raw bytes) and the metadata for a block (with a .meta suffix).</p>
<p>n A block file just consists of the raw bytes of a portion of the file being stored;</p>
<p>n the metadata file is made up of a header with version and type information, followed by a series of checksums for sections of the block.</p>
<p>l When the number of blocks in a directory grows to a certain size, the datanode creates a new subdirectory in which to place new blocks and their accompanying metadata.</p>
<h2 id="1-2-data-flow">1.2. Data Flow</h2>
<h3 id="1-2-1-anatomy-of-a-file-read">1.2.1. Anatomy of a File Read</h3>
<p><a href="http://images.cnblogs.com/cnblogs_com/forfuture1978/WindowsLiveWriter/NotesforHadoopthedefinitiveguide_14109/clip_image014_2.jpg" target="_blank"><img src="&quot;clip_image014&quot;" alt="clip_image014"></a></p>
<p>l The client opens the file it wishes to read by calling open() on the FileSystem object (step 1).</p>
<p>l DistributedFileSystem calls the namenode, using RPC, to determine the locations of the blocks for the first few blocks in the file (step 2).</p>
<p>l For each block, the namenode returns the addresses of the datanodes that have a copy of that block.</p>
<p>l The datanodes are sorted according to their proximity to the client.</p>
<p>l The DistributedFileSystem returns a FSDataInputStream to the client for it to read data from.</p>
<p>l The client then calls read() on the stream (step 3).</p>
<p>l DFSInputStream connects to the first (closest) datanode for the first block in the file.</p>
<p>l Data is streamed from the datanode back to the client (step 4).</p>
<p>l When the end of the block is reached, DFSInputStream will close the connection to the datanode, then find the best datanode for the next block (step 5).</p>
<p>l When the client has finished reading, it calls close() on the FSDataInputStream (step 6).</p>
<p>l During reading, if the client encounters an error while communicating with a datanode, then it will try the next closest one for that block.</p>
<p>l It will also remember datanodes that have failed so that it doesn’t needlessly retry them for later blocks.</p>
<p>l The client also verifies checksums for the data transferred to it from the datanode. If a corrupted block is found, it is reported to the namenode.</p>
<h3 id="1-2-2-anatomy-of-a-file-write">1.2.2. Anatomy of a File Write</h3>
<p><a href="http://images.cnblogs.com/cnblogs_com/forfuture1978/WindowsLiveWriter/NotesforHadoopthedefinitiveguide_14109/clip_image016_2.jpg" target="_blank"><img src="&quot;clip_image016&quot;" alt="clip_image016"></a></p>
<p>l The client creates the file by calling create() (step 1).</p>
<p>l DistributedFileSystem makes an RPC call to the namenode to create a new file in the filesystem’s namespace, with no blocks associated with it (step 2).</p>
<p>l The namenode performs various checks to make sure the file doesn’t already exist, and that the client has the right permissions to create the file. If these checks pass, the namenode makes a record of the new file; otherwise, file creation fails and the client is thrown an IOException.</p>
<p>l The DistributedFileSystem returns a FSDataOutputStream for the client to start writing data to.</p>
<p>l As the client writes data (step 3), DFSOutputStream splits it into packets, which it writes to an internal queue, called the data queue.</p>
<p>l The data queue is consumed by the Data Streamer, whose responsibility it is to ask the namenode to allocate new blocks by picking a list of suitable datanodes to store the replicas. The list of datanodes forms apipeline.</p>
<p>l The DataStreamer streams the packets to the first datanode in the pipeline, which stores the packet and forwards it to the second datanode in the pipeline. Similarly, the second datanode stores the packet and forwards it to the third (and last) datanode in the pipe line (step 4).</p>
<p>l DFSOutputStream also maintains an internal queue of packets that are waiting to be acknowledged by datanodes, called the ack queue. A packet is removed from the ack queue only when it has been acknowledged by all the datanodes in the pipeline (step 5).</p>
<p>l If a datanode fails while data is being written to it,</p>
<p>n First the pipeline is closed, and any packets in the ack queue are added to the front of the data queue.</p>
<p>n The current block on the good datanodes is given a new identity by the namenode, so that the partial block on the failed datanode will be deleted if the failed data node recovers later on.</p>
<p>n The failed datanode is removed from the pipeline and the remainder of the block’s data is written to the two good datanodes in the pipeline.</p>
<p>n The namenode notices that the block is under-replicated, and it arranges for a further replica to be created on another node.</p>
<p>l When the client has finished writing data it calls close() on the stream (step 6). This action flushes all the remaining packets to the datanode pipeline and waits for acknowledgments before contacting the namenode to signal that the file is complete (step7).</p>
<h1 id="2-meet-map-reduce">2. Meet Map/Reduce</h1>
<p>l MapReduce has two phases: the map phase and the reduce phase.</p>
<p>l Each phase has key-value pairs as input and output (the types can be specified).</p>
<p>n The input key-value types of the map phase is determined by the input format</p>
<p>n The output key-value types of the map phase should match the input key value types of the reduce phase</p>
<p>n The output key-value types of the reduce phase can be set in the JobConf interface.</p>
<p>l The programmer specifies two functions: the map function and the reduce function.</p>
<h2 id="2-1-mapreduce-logical-data-flow">2.1. MapReduce logical data flow</h2>
<p><a href="http://images.cnblogs.com/cnblogs_com/forfuture1978/WindowsLiveWriter/NotesforHadoopthedefinitiveguide_14109/clip_image018_2.jpg" target="_blank"><img src="&quot;clip_image018&quot;" alt="clip_image018"></a></p>
<h2 id="2-2-mapreduce-code">2.2. MapReduce Code</h2>
<h3 id="2-2-1-the-map-function-is-represented-by-an-implementation-of-the-mapper-interface-which-declares-a-map-method-">2.2.1. The map function is represented by an implementation of the Mapper interface, which declares a map() method.</h3>
<p><a href="http://images.cnblogs.com/cnblogs_com/forfuture1978/WindowsLiveWriter/NotesforHadoopthedefinitiveguide_14109/clip_image020_2.jpg" target="_blank"><img src="&quot;clip_image020&quot;" alt="clip_image020"></a></p>
<h3 id="2-2-2-the-reduce-function-is-defined-using-a-reducer">2.2.2. The reduce function is defined using a Reducer</h3>
<p>l The input types of the reduce function must match the output type of the map function.</p>
<p><a href="http://images.cnblogs.com/cnblogs_com/forfuture1978/WindowsLiveWriter/NotesforHadoopthedefinitiveguide_14109/clip_image022_2.jpg" target="_blank"><img src="&quot;clip_image022&quot;" alt="clip_image022"></a></p>
<h3 id="2-2-3-the-code-runs-the-mapreduce-job">2.2.3. The code runs the MapReduce job</h3>
<p>l An input path is specified by calling the static addInputPath() method on FileInputFormat</p>
<p>n It can be a single file, a directory, or a file pattern.</p>
<p>n addInputPath() can be called more than once to use input from multiple paths.</p>
<p>l The output path is specified by the static setOutputPath() method on FileOutputFormat.</p>
<p>n It specifies a directory where the output files from the reducer functions are written.</p>
<p>n The directory shouldn’t exist before running the job</p>
<p>l The map and reduce types can be specified via the setMapperClass() and setReducerClass() methods.</p>
<p>l The setOutputKeyClass() and setOutputValueClass() methods control the output types for the map and the reduce functions, which are often the same.</p>
<p>n If they are different, then the map output types can be set using the methods setMapOutputKeyClass() and setMapOutputValueClass().</p>
<p>l The input types are controlled via the input format, which we have not explicitly set since we are using the default TextInputFormat.</p>
<p><a href="http://images.cnblogs.com/cnblogs_com/forfuture1978/WindowsLiveWriter/NotesforHadoopthedefinitiveguide_14109/clip_image024_2.jpg" target="_blank"><img src="&quot;clip_image024&quot;" alt="clip_image024"></a></p>
<h2 id="2-3-scaling-out">2.3. Scaling Out</h2>
<h3 id="2-3-1-mapreduce-data-flow-with-a-single-reduce-task">2.3.1. MapReduce data flow with a single reduce task</h3>
<p>l A MapReduce job is a unit of work that the client wants to be performed: it consists of the input data, the MapReduce program, and configuration information.</p>
<p>l Hadoop runs the job by dividing it into tasks, of which there are two types: map tasks and reduce tasks.</p>
<p>l There are two types of nodes that control the job execution process: a jobtracker and a number of tasktrackers.</p>
<p>n The jobtracker coordinates all the jobs run on the system by scheduling tasks to run on tasktrackers.</p>
<p>n Tasktrackers run tasks and send progress reports to the jobtracker, which keeps a record of the overall progress of each job.</p>
<p>n If a tasks fails, the jobtracker can reschedule it on a different tasktracker.</p>
<p>l Hadoop divides the input to a MapReduce job into fixed-size input splits.</p>
<p>l Hadoop creates one map task for each split, which runs the user defined map function for each record in the split.</p>
<p>l Hadoop does its best to run the map task on a node where the input data resides in HDFS.</p>
<p>n This is called the data locality optimization.</p>
<p>n This is why the optimal split size is the same as the block size: it is the largest size of input that can be guaranteed to be stored on a single node.</p>
<p>l Reduce tasks don’t have the advantage of data locality</p>
<p>n The input to a single reduce task is normally the output from all mappers.</p>
<p>n The output of the reduce is normally stored in HDFS for reliability.</p>
<p><a href="http://images.cnblogs.com/cnblogs_com/forfuture1978/WindowsLiveWriter/NotesforHadoopthedefinitiveguide_14109/clip_image026_2.jpg" target="_blank"><img src="&quot;clip_image026&quot;" alt="clip_image026"></a></p>
<h3 id="2-3-2-mapreduce-data-flow-with-multiple-reduce-tasks">2.3.2. MapReduce data flow with multiple reduce tasks</h3>
<p>The number of reduce tasks is not governed by the size of the input, but is specified independently.</p>
<p>l When there are multiple reducers, the map tasks partition their output, each creating one partition for each reduce task.</p>
<p>l There can be many keys (and their associated values) in each partition, but the records for every key are all in a single partition.</p>
<p>l The partitioning can be controlled by a user-defined partitioning function</p>
<p>n Normally the default partitioner which buckets keys using a hash function.</p>
<p>n conf.setPartitionerClass(HashPartitioner.class);</p>
<p>n conf.setNumReduceTasks(1);</p>
<p>l The data flow between map and reduce tasks is “the shuffle,” as each reduce task is fed by many map tasks.</p>
<p><a href="http://images.cnblogs.com/cnblogs_com/forfuture1978/WindowsLiveWriter/NotesforHadoopthedefinitiveguide_14109/clip_image028_2.jpg" target="_blank"><img src="&quot;clip_image028&quot;" alt="clip_image028"></a></p>
<p>l It’s also possible to have zero reduce tasks. This can be appropriate when you don’t need the shuffle since the processing can be carried out entirely in parallel</p>
<h1 id="3-mapreduce-types-and-formats">3. MapReduce Types and Formats</h1>
<h2 id="3-1-mapreduce-types">3.1. MapReduce Types</h2>
<p>l The map and reduce functions in Hadoop MapReduce have the following general form:</p>
<p><a href="http://images.cnblogs.com/cnblogs_com/forfuture1978/WindowsLiveWriter/NotesforHadoopthedefinitiveguide_14109/clip_image030_2.jpg" target="_blank"><img src="&quot;clip_image030&quot;" alt="clip_image030"></a></p>
<p><a href="http://images.cnblogs.com/cnblogs_com/forfuture1978/WindowsLiveWriter/NotesforHadoopthedefinitiveguide_14109/clip_image032_2.jpg" target="_blank"><img src="&quot;clip_image032&quot;" alt="clip_image032"></a></p>
<p>l The partition function operates on the intermediate key and value types (K2 and V2), and returns the partition index.</p>
<p><a href="http://images.cnblogs.com/cnblogs_com/forfuture1978/WindowsLiveWriter/NotesforHadoopthedefinitiveguide_14109/clip_image034_2.jpg" target="_blank"><img src="&quot;clip_image034&quot;" alt="clip_image034"></a></p>
<p><a href="http://images.cnblogs.com/cnblogs_com/forfuture1978/WindowsLiveWriter/NotesforHadoopthedefinitiveguide_14109/clip_image036_2.jpg" target="_blank"><img src="&quot;clip_image036&quot;" alt="clip_image036"></a></p>
<h3 id="3-1-1-configuration-of-mapreduce-types">3.1.1. Configuration of MapReduce types</h3>
<p><a href="http://images.cnblogs.com/cnblogs_com/forfuture1978/WindowsLiveWriter/NotesforHadoopthedefinitiveguide_14109/clip_image038_2.jpg" target="_blank"><img src="&quot;clip_image038&quot;" alt="clip_image038"></a></p>
<p>l Input types are set by the input format.</p>
<p>n For instance, a TextInputFormat generates keys of type LongWritable and values of type Text.</p>
<p>l A minimal MapReduce driver, with the defaults explicitly set</p>
<p><a href="http://images.cnblogs.com/cnblogs_com/forfuture1978/WindowsLiveWriter/NotesforHadoopthedefinitiveguide_14109/clip_image040_2.jpg" target="_blank"><img src="&quot;clip_image040&quot;" alt="clip_image040"></a></p>
<p>l The default input format is TextInputFormat, which produces keys of type LongWritable (the offset of the beginning of the line in the file) and values of type Text (the line of text).</p>
<p>l The setNumMapTasks() call does not necessarily set the number of map tasks to one</p>
<p>n The actual number of map tasks depends on the size of the input</p>
<p>l The default mapper is IdentityMapper</p>
<p><a href="http://images.cnblogs.com/cnblogs_com/forfuture1978/WindowsLiveWriter/NotesforHadoopthedefinitiveguide_14109/clip_image042_2.jpg" target="_blank"><img src="&quot;clip_image042&quot;" alt="clip_image042"></a></p>
<p>l Map tasks are run by MapRunner, the default implementation of MapRunnable that calls the Mapper’s map() method sequentially with each record.</p>
<p>l The default partitioner is HashPartitioner, which hashes a record’s key to determine which partition the record belongs in.</p>
<p>n Each partition is processed by a reduce task, so the number of partitions is equal to the number of reduce tasks for the job</p>
<p><a href="http://images.cnblogs.com/cnblogs_com/forfuture1978/WindowsLiveWriter/NotesforHadoopthedefinitiveguide_14109/clip_image044_2.jpg" target="_blank"><img src="&quot;clip_image044&quot;" alt="clip_image044"></a></p>
<p>l The default reducer is IdentityReducer</p>
<p><a href="http://images.cnblogs.com/cnblogs_com/forfuture1978/WindowsLiveWriter/NotesforHadoopthedefinitiveguide_14109/clip_image046_2.jpg" target="_blank"><img src="&quot;clip_image046&quot;" alt="clip_image046"></a></p>
<p>l Records are sorted by the MapReduce system before being presented to the reducer.</p>
<p>l The default output format is TextOutputFormat, which writes out records, one per line, by converting keys and values to strings and separating them with a tab character.</p>
<h2 id="3-2-input-formats">3.2. Input Formats</h2>
<h3 id="3-2-1-input-splits-and-records">3.2.1. Input Splits and Records</h3>
<p>l An input split is a chunk of the input that is processed by a single map.</p>
<p>l Each split is divided into records, and the map processes each record—a key-value pair—in turn.</p>
<p><a href="http://images.cnblogs.com/cnblogs_com/forfuture1978/WindowsLiveWriter/NotesforHadoopthedefinitiveguide_14109/clip_image048_2.jpg" target="_blank"><img src="&quot;clip_image048&quot;" alt="clip_image048"></a></p>
<p>l An InputSplit has a length in bytes, and a set of storage locations, which are just hostname strings.</p>
<p>l A split doesn’t contain the input data; it is just a reference to the data.</p>
<p>l The storage locations are used by the MapReduce system to place map tasks as close to the split’s data as possible</p>
<p>l The size is used to order the splits so that the largest get processed first</p>
<p>l An InputFormat is responsible for creating the input splits, and dividing them into records.</p>
<p><a href="http://images.cnblogs.com/cnblogs_com/forfuture1978/WindowsLiveWriter/NotesforHadoopthedefinitiveguide_14109/clip_image050_2.jpg" target="_blank"><img src="&quot;clip_image050&quot;" alt="clip_image050"></a></p>
<p>l The JobClient calls the getSplits() method, passing the desired number of map tasks as the numSplits argument.</p>
<p>l Having calculated the splits, the client sends them to the jobtracker, which uses their storage locations to schedule map tasks to process them on the tasktrackers.</p>
<p>l On a tasktracker, the map task passes the split to the getRecordReader() method on InputFormat to obtain a RecordReader for that split.</p>
<p>l A RecordReader is little more than an iterator over records, and the map task uses one to generate record key-value pairs, which it passes to the map function.</p>
<p><a href="http://images.cnblogs.com/cnblogs_com/forfuture1978/WindowsLiveWriter/NotesforHadoopthedefinitiveguide_14109/clip_image052_2.jpg" target="_blank"><img src="&quot;clip_image052&quot;" alt="clip_image052"></a></p>
<p>l The same key and value objects are used on each invocation of the map() method—only their contents are changed. If you need to change the value out of map, make a copy of the object you want to hold on to.</p>
<h3 id="3-2-2-fileinputformat">3.2.2. FileInputFormat</h3>
<p>l FileInputFormat is the base class for all implementations of InputFormat that use files as their data source.</p>
<p>l It provides two things: a place to define which files are included as the input to a job, and an implementation for generating splits for the input files.</p>
<p><a href="http://images.cnblogs.com/cnblogs_com/forfuture1978/WindowsLiveWriter/NotesforHadoopthedefinitiveguide_14109/clip_image054_2.jpg" target="_blank"><img src="&quot;clip_image054&quot;" alt="clip_image054"></a></p>
<p>l FileInputFormat input paths may represent a file, a directory, or, by using a glob, a collection of files and directories.</p>
<p><a href="http://images.cnblogs.com/cnblogs_com/forfuture1978/WindowsLiveWriter/NotesforHadoopthedefinitiveguide_14109/clip_image056_2.jpg" target="_blank"><img src="&quot;clip_image056&quot;" alt="clip_image056"></a></p>
<p>l To exclude certain files from the input, you can set a filter using the setInputPathFilter() method on FileInputFormat</p>
<p><a href="http://images.cnblogs.com/cnblogs_com/forfuture1978/WindowsLiveWriter/NotesforHadoopthedefinitiveguide_14109/clip_image058_2.jpg" target="_blank"><img src="&quot;clip_image058&quot;" alt="clip_image058"></a></p>
<p>l FileInputFormat splits only large files. Here “large” means larger than an HDFS block.</p>
<p>l Properties for controlling split size</p>
<p>n The minimum split size is usually 1 byte, by setting this to a value larger than the block size, they can force splits to be larger than a block.</p>
<p>n The maximum split size defaults to the maximum value that can be represented by a Java long type. It has an effect only when it is less than the block size, forcing splits to be smaller than a block.</p>
<h3 id="small-files-and-combinefileinputformat">Small files and CombineFileInputFormat</h3>
<p>l Hadoop works better with a small number of large files than a large number of small files.</p>
<p>l Where FileInputFormat creates a split per file, CombineFileInputFormat packs many files into each split so that each mapper has more to process.</p>
<p>l One technique for avoiding the many small files case is to merge small files into larger files by using a SequenceFile: the keys can act as filenames and the values as file contents.</p>
<h3 id="3-2-3-text-input">3.2.3. Text Input</h3>
<p>l TextInputFormat is the default InputFormat.</p>
<p>n Each record is a line of input.</p>
<p>n The key, a LongWritable, is the byte offset within the file of the beginning of the line.</p>
<p>n The value is the contents of the line, excluding any line terminators, and is packaged as a Text object.</p>
<p>l The logical records that FileInputFormats define do not usually fit neatly into HDFS blocks.</p>
<p>l A single file is broken into lines, and the line boundaries do not correspond with the HDFS block boundaries.</p>
<p>l Splits honor logical record boundaries</p>
<p>n The first split contains line 5, even though it spans the first and second block.</p>
<p>n The second split starts at line 6.</p>
<p>l Data-local maps will perform some remote reads.</p>
<p><a href="http://images.cnblogs.com/cnblogs_com/forfuture1978/WindowsLiveWriter/NotesforHadoopthedefinitiveguide_14109/clip_image060_2.jpg" target="_blank"><img src="&quot;clip_image060&quot;" alt="clip_image060"></a></p>
<h3 id="keyvaluetextinputformat">KeyValueTextInputFormat</h3>
<p>l It is common for each line in a file to be a key-value pair, separated by a delimiter such as a tab character.</p>
<p>l You can specify the separator via the key.value.separator.in.input.line property.</p>
<h3 id="nlineinputformat">NLineInputFormat</h3>
<p>l If you want your mappers to receive a fixed number of lines of input, then NLineInputFormat is the InputFormat to use.</p>
<p>l Like TextInputFormat, the keys are the byte offsets within the file and the values are the lines themselves.</p>
<p>l N refers to the number of lines of input that each mapper receives.</p>
<h3 id="3-2-4-binary-input">3.2.4. Binary Input</h3>
<h3 id="sequencefileinputformat">SequenceFileInputFormat</h3>
<p>l Hadoop’s sequence file format stores sequences of binary key-value pairs.</p>
<p>l To use data from sequence files as the input to MapReduce, you use SequenceFileInputFormat.</p>
<p>l The keys and values are determined by the sequence file, and you need to make sure that your map input types correspond.</p>
<p>l For example, if your sequence file has IntWritable keys and Text values, then the map signature would be Mapper<IntWritable, Text, K, V>.</p>
<h3 id="sequencefileastextinputformat">SequenceFileAsTextInputFormat</h3>
<p>l SequenceFileAsTextInputFormat is a variant of SequenceFileInputFormat that converts the sequence file’s keys and values to Text objects.</p>
<h3 id="sequencefileasbinaryinputformat">SequenceFileAsBinaryInputFormat</h3>
<p>l SequenceFileAsBinaryInputFormat is a variant of SequenceFileInputFormat that retrieves the sequence file’s keys and values as opaque binary objects.</p>
<p>l They are encapsulated as BytesWritable objects</p>
<h3 id="sequencefile">SequenceFile</h3>
<p>l Writing a SequenceFile</p>
<p>n To create a SequenceFile, use one of its createWriter() static methods, which returns a SequenceFile.Writer instance.</p>
<p>n specify a stream to write to (either a FSDataOutputStream or a FileSystem and Path pairing), a Configuration object, and the key and value types.</p>
<p>n Once you have a SequenceFile.Writer, you then write key-value pairs, using the append() method.</p>
<p>n Then when you’ve finished you call the close() method</p>
<p><a href="http://images.cnblogs.com/cnblogs_com/forfuture1978/WindowsLiveWriter/NotesforHadoopthedefinitiveguide_14109/clip_image062_2.jpg" target="_blank"><img src="&quot;clip_image062&quot;" alt="clip_image062"></a></p>
<p>l Reading a SequenceFile</p>
<p>n Reading sequence files from beginning to end is a matter of creating an instance of SequenceFile.Reader, and iterating over records by repeatedly invoking one of the next() methods.</p>
<p><a href="http://images.cnblogs.com/cnblogs_com/forfuture1978/WindowsLiveWriter/NotesforHadoopthedefinitiveguide_14109/clip_image064_2.jpg" target="_blank"><img src="&quot;clip_image064&quot;" alt="clip_image064"></a></p>
<p>l The SequenceFile Format</p>
<p>n A sequence file consists of a header followed by one or more records.</p>
<p>n The first three bytes of a sequence file are the bytes SEQ, which acts a magic number, followed by a single byte representing the version number.</p>
<p>n The header contains other fields including the names of the key and value classes, compression details, user-defined metadata, and the sync marker.</p>
<p>n The sync marker is used to allow a reader to synchronize to a record boundary from any position in the file.</p>
<p><a href="http://images.cnblogs.com/cnblogs_com/forfuture1978/WindowsLiveWriter/NotesforHadoopthedefinitiveguide_14109/clip_image066_2.jpg" target="_blank"><img src="&quot;clip_image066&quot;" alt="clip_image066"></a></p>
<h3 id="3-2-5-multiple-inputs">3.2.5. Multiple Inputs</h3>
<p>l The MultipleInputs class allows you to specify the InputFormat and Mapper to use on a per-path basis.</p>
<p><a href="http://images.cnblogs.com/cnblogs_com/forfuture1978/WindowsLiveWriter/NotesforHadoopthedefinitiveguide_14109/clip_image068_2.jpg" target="_blank"><img src="&quot;clip_image068&quot;" alt="clip_image068"></a></p>
<h2 id="3-3-output-formats">3.3. Output Formats</h2>
<h3 id="3-3-1-text-output">3.3.1. Text Output</h3>
<p>l The default output format, TextOutputFormat, writes records as lines of text.</p>
<p>l Its keys and values may be of any type, since TextOutputFormat turns them to strings by calling toString() on them.</p>
<p>l Each key-value pair is separated by a tab character, although that may be changed using the mapred.textoutputformat.separator property.</p>
<h3 id="3-3-2-binary-output">3.3.2. Binary Output</h3>
<p>l SequenceFileOutputFormat</p>
<p>l SequenceFileAsBinaryOutputFormat</p>
<p>l MapFileOutputFormat</p>
<h3 id="writing-a-mapfile">Writing a MapFile</h3>
<p>l You create an instance of MapFile.Writer, then call the append() method to add entries in order.</p>
<p>l Keys must be instances of WritableComparable, and values must be Writable</p>
<p><a href="http://images.cnblogs.com/cnblogs_com/forfuture1978/WindowsLiveWriter/NotesforHadoopthedefinitiveguide_14109/clip_image070_2.jpg" target="_blank"><img src="&quot;clip_image070&quot;" alt="clip_image070"></a></p>
<p>l If we look at the MapFile, we see it’s actually a directory containing two files called data and index:</p>
<p><a href="http://images.cnblogs.com/cnblogs_com/forfuture1978/WindowsLiveWriter/NotesforHadoopthedefinitiveguide_14109/clip_image072_2.jpg" target="_blank"><img src="&quot;clip_image072&quot;" alt="clip_image072"></a></p>
<p>l Both files are SequenceFiles. The data file contains all of the entries, in order:</p>
<p><a href="http://images.cnblogs.com/cnblogs_com/forfuture1978/WindowsLiveWriter/NotesforHadoopthedefinitiveguide_14109/clip_image074_2.jpg" target="_blank"><img src="&quot;clip_image074&quot;" alt="clip_image074"></a></p>
<p>l The index file contains a fraction of the keys, and contains a mapping from the key to that key’s offset in the data file:</p>
<p><a href="http://images.cnblogs.com/cnblogs_com/forfuture1978/WindowsLiveWriter/NotesforHadoopthedefinitiveguide_14109/clip_image076_2.jpg" target="_blank"><img src="&quot;clip_image076&quot;" alt="clip_image076"></a></p>
<h3 id="reading-a-mapfile">Reading a MapFile</h3>
<p>l you create a MapFile.Reader, then call the next() method until it returns false</p>
<h3 id="3-3-3-multiple-outputs">3.3.3. Multiple Outputs</h3>
<h3 id="multipleoutputformat">MultipleOutputFormat</h3>
<p>l MultipleOutputFormat allows you to write data to multiple files whose names are derived from the output keys and values.</p>
<p>n conf.setOutputFormat(StationNameMultipleTextOutputFormat.class);</p>
<p><a href="http://images.cnblogs.com/cnblogs_com/forfuture1978/WindowsLiveWriter/NotesforHadoopthedefinitiveguide_14109/clip_image078_2.jpg" target="_blank"><img src="&quot;clip_image078&quot;" alt="clip_image078"></a></p>
<h3 id="multipleoutputs">MultipleOutputs</h3>
<p>l MultipleOutputs can emit different types for each output.</p>
<p><a href="http://images.cnblogs.com/cnblogs_com/forfuture1978/WindowsLiveWriter/NotesforHadoopthedefinitiveguide_14109/clip_image080_2.jpg" target="_blank"><img src="&quot;clip_image080&quot;" alt="clip_image080"></a></p>
<h1 id="4-developing-a-mapreduce-application">4. Developing a MapReduce Application</h1>
<h2 id="4-1-the-configuration-api">4.1. The Configuration API</h2>
<p>l An instance of the Configuration class (found in the org.apache.hadoop.conf package) represents a collection of configuration properties and their values.</p>
<p>l Configurations read their properties from resources—XML files</p>
<p><a href="http://images.cnblogs.com/cnblogs_com/forfuture1978/WindowsLiveWriter/NotesforHadoopthedefinitiveguide_14109/clip_image082_2.jpg" target="_blank"><img src="&quot;clip_image082&quot;" alt="clip_image082"></a></p>
<p>l we can access its properties using a piece of code like this:</p>
<p><a href="http://images.cnblogs.com/cnblogs_com/forfuture1978/WindowsLiveWriter/NotesforHadoopthedefinitiveguide_14109/clip_image084_2.jpg" target="_blank"><img src="&quot;clip_image084&quot;" alt="clip_image084"></a></p>
<h2 id="4-2-configuring-the-development-environment">4.2. Configuring the Development Environment</h2>
<h3 id="4-2-1-managing-configuration">4.2.1. Managing Configuration</h3>
<p>l When developing Hadoop applications, it is common to switch between running the application locally and running it on a cluster.</p>
<p>l hadoop-local.xml</p>
<p><a href="http://images.cnblogs.com/cnblogs_com/forfuture1978/WindowsLiveWriter/NotesforHadoopthedefinitiveguide_14109/clip_image086_2.jpg" target="_blank"><img src="&quot;clip_image086&quot;" alt="clip_image086"></a></p>
<p>l hadoop-localhost.xml</p>
<p><a href="http://images.cnblogs.com/cnblogs_com/forfuture1978/WindowsLiveWriter/NotesforHadoopthedefinitiveguide_14109/clip_image088_2.jpg" target="_blank"><img src="&quot;clip_image088&quot;" alt="clip_image088"></a></p>
<p>l hadoop-cluster.xml</p>
<p><a href="http://images.cnblogs.com/cnblogs_com/forfuture1978/WindowsLiveWriter/NotesforHadoopthedefinitiveguide_14109/clip_image090_2.jpg" target="_blank"><img src="&quot;clip_image090&quot;" alt="clip_image090"></a></p>
<p><a href="http://images.cnblogs.com/cnblogs_com/forfuture1978/WindowsLiveWriter/NotesforHadoopthedefinitiveguide_14109/clip_image092_2.jpg" target="_blank"><img src="&quot;clip_image092&quot;" alt="clip_image092"></a></p>
<p>l With this setup, it is easy to use any configuration with the -conf command-line switch.</p>
<p>l For example, the following command shows a directory listing on the HDFS server running in pseudo-distributed mode on localhost:</p>
<p><a href="http://images.cnblogs.com/cnblogs_com/forfuture1978/WindowsLiveWriter/NotesforHadoopthedefinitiveguide_14109/clip_image094_2.jpg" target="_blank"><img src="&quot;clip_image094&quot;" alt="clip_image094"></a></p>
<h3 id="4-2-2-genericoptionsparser-tool-and-toolrunner">4.2.2. GenericOptionsParser, Tool, and ToolRunner</h3>
<p><a href="http://images.cnblogs.com/cnblogs_com/forfuture1978/WindowsLiveWriter/NotesforHadoopthedefinitiveguide_14109/clip_image096_2.jpg" target="_blank"><img src="&quot;clip_image096&quot;" alt="clip_image096"></a></p>
<p><a href="http://images.cnblogs.com/cnblogs_com/forfuture1978/WindowsLiveWriter/NotesforHadoopthedefinitiveguide_14109/clip_image098_2.jpg" target="_blank"><img src="&quot;clip_image098&quot;" alt="clip_image098"></a></p>
<h1 id="5-how-mapreduce-works">5. How MapReduce Works</h1>
<h2 id="5-1-anatomy-of-a-mapreduce-job-run">5.1. Anatomy of a MapReduce Job Run</h2>
<p>l There are four independent entities:</p>
<p>n The client, which submits the MapReduce job.</p>
<p>n The jobtracker, which coordinates the job run. The jobtracker is a Java application whose main class is JobTracker.</p>
<p>n The tasktrackers, which run the tasks that the job has been split into. Tasktrackers are Java applications whose main class is TaskTracker.</p>
<p>n The distributed filesystem, which is used for sharing job files between the other entities.</p>
<p><a href="http://images.cnblogs.com/cnblogs_com/forfuture1978/WindowsLiveWriter/NotesforHadoopthedefinitiveguide_14109/clip_image100_2.jpg" target="_blank"><img src="&quot;clip_image100&quot;" alt="clip_image100"></a></p>
<h3 id="5-1-1-job-submission">5.1.1. Job Submission</h3>
<p>l The runJob() method on JobClient creates a new JobClient instance and calls submitJob() on it.</p>
<p>l Having submitted the job, runJob() polls the job’s progress once a second, and reports the progress to the console if it has changed since the last report.</p>
<p>l When the job is complete, if it was successful, the job counters are displayed. Otherwise, the error that caused the job to fail is logged to the console.</p>
<h3 id="the-job-submission-process">The job submission process</h3>
<p>l Asks the jobtracker for a new job ID (by calling getNewJobId() on JobTracker)</p>
<p>l Checks the output specification of the job.</p>
<p>l Computes the input splits for the job.</p>
<p>l Copies the resources needed to run the job, including the job JAR file, the configuration file and the computed input splits, to the jobtracker’s filesystem in a directory named after the job ID.</p>
<p>l Tells the jobtracker that the job is ready for execution (by calling submitJob() on JobTracker)</p>
<h3 id="5-1-2-job-initialization">5.1.2. Job Initialization</h3>
<p>l When the JobTracker receives a call to its submitJob() method, it puts it into an internal queue from where the job scheduler will pick it up and initialize it.</p>
<p>l Initialization involves creating an object to represent the job being run, which encapsulates its tasks, and bookkeeping information to keep track of the tasks’ status and progress.</p>
<p>l To create the list of tasks to run, the job scheduler first retrieves the input splits computed by the JobClient from the shared filesystem.</p>
<p>l It then creates one map task for each split.</p>
<p>l Tasks are given IDs at this point.</p>
<h3 id="5-1-3-task-assignment">5.1.3. Task Assignment</h3>
<p>l Tasktrackers run a simple loop that periodically sends heartbeat method calls to the jobtracker.</p>
<p>l As a part of the heartbeat, a tasktracker will indicate whether it is ready to run a new task, and if it is, the jobtracker will allocate it a task, which it communicates to the tasktracker using the heartbeat return value</p>
<p>l Before it can choose a task for the tasktracker, the jobtracker must choose a job to select the task from according to priority.(setJobPriority() and FIFO)</p>
<p>l Tasktrackers have a fixed number of slots for map tasks and for reduce tasks.</p>
<p>l The default scheduler fills empty map task slots before reduce task slots</p>
<p>l To choose a reduce task the jobtracker simply takes the next in its list of yet-to-be-run reduce tasks, since there are no data locality considerations.</p>
<h3 id="5-1-4-task-execution">5.1.4. Task Execution</h3>
<p>l Now the tasktracker has been assigned a task, the next step is for it to run the task.</p>
<p>l First, it localizes the job JAR by copying it from the shared filesystem to the tasktracker’s filesystem.</p>
<p>l It also copies any files needed from the distributed cache by the application to the local disk</p>
<p>l Second, it creates a local working directory for the task, and un-jars the contents of the JAR into this directory.</p>
<p>l Third, it creates an instance of TaskRunner to run the task.</p>
<p>l TaskRunner launches a new Java Virtual Machine to run each task in</p>
<p>l It is however possible to reuse the JVM between tasks;</p>
<p>l The child process communicates with its parent through the umbilical interface.</p>
<h3 id="5-1-5-job-completion">5.1.5. Job Completion</h3>
<p>l When the jobtracker receives a notification that the last task for a job is complete, it changes the status for the job to “successful.” T</p>
<p>l hen, when the JobClient polls for status, it learns that the job has completed successfully, so it prints a message to tell the user, and then returns from the runJob() method.</p>
<p><a href="http://images.cnblogs.com/cnblogs_com/forfuture1978/WindowsLiveWriter/NotesforHadoopthedefinitiveguide_14109/clip_image102_2.jpg" target="_blank"><img src="&quot;clip_image102&quot;" alt="clip_image102"></a></p>
<h2 id="5-2-failures">5.2. Failures</h2>
<h3 id="5-2-1-task-failure">5.2.1. Task Failure</h3>
<p>l The most common way is when user code in the map or reduce task throws a runtime exception.</p>
<p>n the child JVM reports the error back to its parent tasktracker, before it exits.</p>
<p>n The error ultimately makes it into the user logs.</p>
<p>n The tasktracker marks the task attempt as failed, freeing up a slot to run another task.</p>
<p>l Another failure mode is the sudden exit of the child JVM</p>
<p>n the tasktracker notices that the process has exited, and marks the attempt as failed.</p>
<p>l Hanging tasks are dealt with differently.</p>
<p>n The tasktracker notices that it hasn’t received a progress update for a while, and proceeds to mark the task as failed.</p>
<p>n The child JVM process will be automatically killed after this period</p>
<p>l When the jobtracker is notified of a task attempt that has failed (by the tasktracker’s heartbeat call) it will reschedule execution of the task.</p>
<p>n The jobtracker will try to avoid rescheduling the task on a tasktracker where it has previously failed.</p>
<p>n If a task fails more than four times, it will not be retried further.</p>
<h3 id="5-2-2-tasktracker-failure">5.2.2. Tasktracker Failure</h3>
<p>l If a tasktracker fails by crashing, or running very slowly, it will stop sending heartbeats to the jobtracker (or send them very infrequently).</p>
<p>l The jobtracker will notice a tasktracker that has stopped sending heartbeats and remove it from its pool of tasktrackers to schedule tasks on.</p>
<p>l The jobtracker arranges for map tasks that were run and completed successfully on that tasktracker to be rerun if they belong to incomplete jobs, since their intermediate output residing on the failed tasktracker’s local filesystem may not be accessible to the reduce task. Any tasks in progress are also rescheduled.</p>
<h3 id="5-2-3-jobtracker-failure">5.2.3. Jobtracker Failure</h3>
<h2 id="5-3-shuffle-and-sort">5.3. Shuffle and Sort</h2>
<p><a href="http://images.cnblogs.com/cnblogs_com/forfuture1978/WindowsLiveWriter/NotesforHadoopthedefinitiveguide_14109/clip_image104_2.jpg" target="_blank"><img src="&quot;clip_image104&quot;" alt="clip_image104"></a></p>
<h3 id="5-3-1-the-map-side">5.3.1. The Map Side</h3>
<p>l When the map function starts producing output, it is not simply written to disk.</p>
<p>l Each map task has a circular memory buffer that it writes the output to.</p>
<p>l When the contents of the buffer reach a certain threshold size, a background thread will start to spill the contents to disk.</p>
<p>l Spills are written in round-robin fashion to the directories specified by the mapred.local.dir property</p>
<p>l Before it writes to disk, the thread first divides the data into partitions corresponding to the reducers that they will ultimately be sent to.</p>
<p>l Within each partition, the background thread performs an in-memory sort by key.</p>
<p>l Each time the memory buffer reaches the spill threshold, a new spill file is created, so after the map task has written its last output record there could be several spill files.</p>
<p>l Before the task is finished, the spill files are merged into a single partitioned and sorted output file.</p>
<p>l The output file’s partitions are made available to the reducers over HTTP.</p>
<p>l The number of worker threads used to serve the file partitions is controlled by the task tracker.http.threads property</p>
<h3 id="5-3-2-the-reduce-side">5.3.2. The Reduce Side</h3>
<p>l As map tasks complete successfully, they notify their parent tasktracker of the status update, which in turn notifies the jobtracker.</p>
<p>l for a given job, the jobtracker knows the mapping between map outputs and tasktrackers.</p>
<p>l A thread in the reducer periodically asks the jobtracker for map output locations until it has retrieved them all.</p>
<p>l The reduce task needs the map output for its particular partition from several map tasks across the cluster.</p>
<p>l The map tasks may finish at different times, so the reduce task starts copying their outputs as soon as each completes. This is known as the copy phase of the reduce task.</p>
<p>l The reduce task has a small number of copier threads so that it can fetch map outputs in parallel.</p>
<p>l As the copies accumulate on disk, a background thread merges them into larger, sorted files.</p>
<p>l When all the map outputs have been copied, the reduce task moves into the sort phase (which should properly be called the merge phase, as the sorting was carried out on the map side), which merges the map outputs, maintaining their sort ordering.</p>
<p>l During the reduce phase the reduce function is invoked for each key in the sorted output. The output of this phase is written directly to the output filesystem, typically HDFS.
来源： &lt;<a href="http://www.cnblogs.com/forfuture1978/archive/2010/02/27/1674955.html" target="_blank">Notes for Hadoop the definitive guide - 觉先 - 博客园</a>&gt; </p>

      
    </div>
    
    
      
    
    
        <footer id="post-meta">
        <span class="categories">Posted in<span class="breadcrumb fa fa-folder"><li><a href="/categories/hadoop/">hadoop</a></li></span></span> | <span class="tags">Tagged <a href="/tags/hadoop/" class="label label-primary">hadoop</a></span> | <span class="time">recent updated:<time title="2014-03-07 09:54:36"datetime="2014-03-07 09:54:36"> mar. 7 2014</time></span> | <span class="comment-link">
<a href="http://itsolife.com/2014/02/02/2014-02-02-hadoop--NotesforHadoopthedefinitiveguide/#comments" class="ds-thread-count comment-link" data-thread-key="2014-02-02-hadoop--NotesforHadoopthedefinitiveguide" data-count-type="comments">暂无评论</a></span>
        </footer>
    
    <div class="clearfix"></div>
  </div>
</div>
</article>



  <article>
<div id="post" class="post well">
  <div class="post-content">
    <header class="well-sm">
      <i class="fa icon fa-5x pull-left"></i>
      <h1 class="title"><a href="/2014/02/02/2014-02-02-hadoop--Hadoop集群_Hadoop安装配置/">Hadoop集群_Hadoop安装配置</a></h1>
      
        <span>Posted on<time datetime="2014-02-02T01:54:36.000Z"> <a href="/2014/02/02/2014-02-02-hadoop--Hadoop集群_Hadoop安装配置/">feb. 2 2014</a></time></span>      
    </header>
    
    
    <div class="entry">
      
        <h1 id="hadoop-_hadoop-">Hadoop集群_Hadoop安装配置</h1>
<h1 id="-hadoop-5-_hadoop-http-www-cnblogs-com-xia520pi-archive-2012-05-16-2503949-html-"><a href="http://www.cnblogs.com/xia520pi/archive/2012/05/16/2503949.html" target="_blank">Hadoop集群（第5期）_Hadoop安装配置</a></h1>
<h2 id="1-">1、集群部署介绍</h2>
<h3 id="1-1-hadoop-">1.1 Hadoop简介</h3>
<p><img src="" alt="">　　Hadoop是Apache软件基金会旗下的一个开源分布式计算平台。以Hadoop分布式文件系统（HDFS，Hadoop Distributed Filesystem）和MapReduce（Google MapReduce的开源实现）为<strong>核心</strong>的Hadoop为用户提供了系统底层细节透明的分布式基础架构。</p>
<p>对于Hadoop的集群来讲，可以分成两大类角色：Master和Salve。一个<strong>HDFS</strong>集群是由一个NameNode和若干个DataNode组成的。其中NameNode作为主服务器，管理文件系统的命名空间和客户端对文件系统的访问操作；集群中的DataNode管理存储的数据。<strong>MapReduce</strong>框架是由一个单独运行在主节点上的JobTracker和运行在每个集群从节点的TaskTracker共同组成的。主节点负责调度构成一个作业的所有任务，这些任务分布在不同的从节点上。主节点监控它们的执行情况，并且重新执行之前的失败任务；从节点仅负责由主节点指派的任务。当一个Job被提交时，JobTracker接收到提交作业和配置信息之后，就会将配置信息等分发给从节点，同时调度任务并监控TaskTracker的执行。</p>
<p>从上面的介绍可以看出，HDFS和MapReduce共同组成了Hadoop分布式系统体系结构的核心。<strong>HDFS</strong>在集群上实现分布式文件系统，<strong>MapReduce</strong>在集群上实现了分布式计算和任务处理。HDFS在MapReduce任务处理过程中提供了文件操作和存储等支持，MapReduce在HDFS的基础上实现了任务的分发、跟踪、执行等工作，并收集结果，二者相互作用，完成了Hadoop分布式集群的主要任务。</p>
<h3 id="1-2-">1.2 环境说明</h3>
<p>集群中包括4个节点：1个Master，3个Salve，节点之间局域网连接，可以相互ping通，具体集群信息可以查看&quot;<strong>Hadoop集群（第2期）</strong>&quot;。节点IP地址分布如下：</p>
<p><strong>机器名称</strong></p>
<p><strong>IP地址</strong>Master.Hadoop</p>
<p>192.168.1.2Salve1.Hadoop</p>
<p>192.168.1.3Salve2.Hadoop</p>
<p>192.168.1.4Salve3.Hadoop</p>
<p>192.168.1.5</p>
<p>四个节点上均是CentOS6.0系统，并且有一个相同的用户<strong>hadoop</strong>。Master机器主要配置NameNode和JobTracker的角色，负责总管分布式数据和分解任务的执行；3个Salve机器配置DataNode和TaskTracker的角色，负责分布式数据存储以及任务的执行。其实应该还应该有1个Master机器，用来作为<strong>备用</strong>，以防止Master服务器<strong>宕机</strong>，还有一个备用马上启用。后续经验积累一定阶段后<strong>补上</strong>一台备用Master机器。</p>
<h3 id="1-3-">1.3 网络配置</h3>
<p>Hadoop集群要按照<strong>1.2小节</strong>表格所示进行配置，我们在&quot;<strong>Hadoop集群（第1期）</strong>&quot;的CentOS6.0安装过程就按照提前规划好的主机名进行安装和配置。如果实验室后来人在安装系统时，没有配置好，不要紧，没有必要重新安装，在安装完系统之后仍然可以根据后来的规划对机器的主机名进行修改。</p>
<p>下面的例子我们将以Master机器为例，即主机名为&quot;Master.Hadoop&quot;，IP为&quot;192.168.1.2&quot;进行一些主机名配置的相关操作。其他的Slave机器以此为依据进行修改。</p>
<p><strong>1）查看当前机器名称</strong></p>
<p>用下面命令进行显示机器名称，如果跟规划的不一致，要按照下面进行修改。</p>
<p>hostname</p>
<p><img src="" alt=""></p>
<p>上图中，用&quot;hostname&quot;查&quot;Master&quot;机器的名字为&quot;Master.Hadoop&quot;，与我们预先规划的一致。</p>
<p><strong>2）修改当前机器名称</strong></p>
<p><strong>假定</strong>我们发现我们的机器的主机名不是我们想要的，通过对&quot;<strong>/etc/sysconfig/network</strong>&quot;文件修改其中&quot;<strong>HOSTNAME</strong>&quot;后面的值，改成我们规划的名称。</p>
<p>这个&quot;<strong>/etc/sysconfig/network</strong>&quot;文件是定义hostname和是否利用网络的不接触网络设备的对系统全体定义的文件。</p>
<p><strong>设定形式</strong>：设定值=值</p>
<p>&quot;/etc/sysconfig/network&quot;的<strong>设定项目</strong>如下：</p>
<p>NETWORKING 是否利用网络</p>
<p>GATEWAY 默认网关</p>
<p>IPGATEWAYDEV 默认网关的接口名</p>
<p>HOSTNAME 主机名</p>
<p>DOMAIN 域名</p>
<p>用下面命令进行修改当前机器的主机名（<strong>备注：</strong>修改系统文件一般用<strong>root</strong>用户）</p>
<p>vim /etc/sysconfig/network</p>
<p><img src="" alt=""></p>
<p>通过上面的命令我们从&quot;/etc/sysconfig/network&quot;中找到&quot;HOSTNAME&quot;进行修改，查看内容如下：</p>
<p><img src="" alt=""></p>
<p><strong>3）修改当前机器IP</strong></p>
<p><strong>假定</strong>我们的机器连IP在当时安装机器时都没有配置好，那此时我们需要对&quot;<strong>ifcfg-eth0</strong>&quot;文件进行配置，该文件位于&quot;<strong>/etc/sysconfig/network-scripts</strong>&quot;文件夹下。</p>
<p>在这个目录下面，存放的是网络接口（网卡）的制御脚本文件（控制文件），ifcfg- eth0是默认的第一个网络接口，如果机器中有多个网络接口，那么名字就将依此类推ifcfg-eth1，ifcfg-eth2，ifcfg- eth3，……。</p>
<p>这里面的文件是相当重要的，涉及到网络能否正常工作。</p>
<p>设定形式：设定值=值</p>
<p>设定项目项目如下：</p>
<p>DEVICE 接口名（设备,网卡）</p>
<p>BOOTPROTO IP的配置方法（static:固定IP， dhcpHCP， none:手动）</p>
<p>HWADDR MAC地址</p>
<p>ONBOOT 系统启动的时候网络接口是否有效（yes/no）</p>
<p>TYPE 网络类型（通常是Ethemet）</p>
<p>NETMASK 网络掩码</p>
<p><strong>IPADDR</strong> IP地址</p>
<p>IPV6INIT IPV6是否有效（yes/no）</p>
<p>GATEWAY 默认网关IP地址</p>
<p>查看&quot;/etc/sysconfig/network-scripts/ifcfg-eth0&quot;内容，如果IP不复核，就行修改。</p>
<p><img src="" alt=""></p>
<p>如果上图中IP与规划不相符，用下面命令进行修改：</p>
<p>vim /etc/sysconfig/network-scripts/ifcgf-eth0</p>
<p>修改完之后可以用&quot;ifconfig&quot;进行查看。</p>
<p><img src="" alt=""></p>
<p><strong>4）配置hosts文件（必须）</strong></p>
<p>&quot;<strong>/etc/hosts</strong>&quot;这个文件是用来配置主机将用的<strong>DNS</strong>服务器信息，是记载LAN内接续的各主机的对应[HostName和IP]用的。当用户在进行网络连接时，首先查找该文件，寻找对应主机名（或域名）对应的IP地址。</p>
<p>我们要测试两台机器之间知否连通，一般用&quot;ping 机器的IP&quot;，如果想用&quot;ping 机器的主机名&quot;发现找不见该名称的机器，解决的办法就是修改&quot;<strong>/etc/hosts</strong>&quot;这个文件，通过把LAN内的各主机的IP地址和HostName的<strong>一一对应</strong>写入这个文件的时候，就可以解决问题。</p>
<p>例如：机器为&quot;Master.Hadoop:192.168.1.2&quot;对机器为&quot;Salve1.Hadoop:192.168.1.3&quot;用命令&quot;ping&quot;记性连接测试。测试结果如下：</p>
<p><img src="" alt=""></p>
<p>从上图中的值，直接对IP地址进行测试，能够ping通，但是对主机名进行测试，发现没有ping通，提示&quot;unknown host——未知主机&quot;，这时查看&quot;Master.Hadoop&quot;的&quot;/etc/hosts&quot;文件内容。</p>
<p><img src="" alt=""></p>
<p>发现里面没有&quot;192.168.1.3 Slave1.Hadoop&quot;内容，故而本机器是无法对机器的主机名为&quot;Slave1.Hadoop&quot; 解析。</p>
<p>在进行<strong>Hadoop集群</strong>配置中，需要在&quot;/etc/hosts&quot;文件中添加集群中所有机器的IP与主机名，这样Master与所有的Slave机器之间不仅可以通过IP进行通信，而且还可以通过主机名进行通信。所以在所有的机器上的&quot;/etc/hosts&quot;文件<strong>末尾</strong>中都要添加如下内容：</p>
<p>192.168.1.2 Master.Hadoop</p>
<p>192.168.1.3 Slave1.Hadoop</p>
<p>192.168.1.4 Slave2.Hadoop</p>
<p>192.168.1.5 Slave3.Hadoop</p>
<p>用以下命令进行添加：</p>
<p>vim /etc/hosts</p>
<p><img src="" alt=""></p>
<p>添加结果如下：</p>
<p><img src="" alt=""></p>
<p>现在我们在进行对机器为&quot;Slave1.Hadoop&quot;的主机名进行ping通测试，看是否能测试成功。</p>
<p><img src="" alt=""></p>
<p>从上图中我们已经能用主机名进行ping通了，说明我们刚才添加的内容，在局域网内能进行DNS解析了，那么现在剩下的事儿就是在其余的Slave机器上进行相同的配置。然后进行测试。（<strong>备注：</strong>当设置SSH无密码验证后，可以&quot;scp&quot;进行复制，然后把原来的&quot;hosts&quot;文件执行覆盖即可。）</p>
<h3 id="1-4-">1.4 所需软件</h3>
<p><strong>1）JDK软件</strong></p>
<p>下载地址：<a href="http://www.oracle.com/technetwork/java/javase/index.html" target="_blank"><a href="http://www.oracle.com/technetwork/java/javase/index.html">http://www.oracle.com/technetwork/java/javase/index.html</a></a></p>
<p>JDK版本：jdk-6u31-linux-i586.bin</p>
<p><strong>2）Hadoop软件</strong></p>
<p>下载地址：<a href="http://hadoop.apache.org/common/releases.html" target="_blank"><a href="http://hadoop.apache.org/common/releases.html">http://hadoop.apache.org/common/releases.html</a></a></p>
<p>Hadoop版本：hadoop-1.0.0.tar.gz</p>
<h3 id="1-5-vsftp-">1.5 VSFTP上传</h3>
<p>在&quot;<strong>Hadoop集群（第3期）</strong>&quot;讲了VSFTP的安装及配置，如果没有安装VSFTP可以按照该文档进行安装。如果安装好了，就可以通过<strong>FlashFXP.exe</strong>软件把我们下载的JDK6.0和Hadoop1.0软件上传到&quot;<strong>Master.Hadoop:192.168.1.2</strong>&quot;服务器上。</p>
<p><img src="" alt=""></p>
<p>刚才我们用一般用户（hadoop）通过FlashFXP软件把所需的两个软件上传了跟目下，我们通过命令查看下一下是否已经上传了。</p>
<p><img src="" alt=""></p>
<p>从图中，我们的所需软件已经准备好了。</p>
<h2 id="2-ssh-">2、SSH无密码验证配置</h2>
<p>Hadoop运行过程中需要管理远端Hadoop守护进程，在Hadoop启动以后，NameNode是通过SSH（Secure Shell）来启动和停止各个DataNode上的各种守护进程的。这就必须在节点之间执行指令的时候是不需要输入密码的形式，故我们需要配置SSH运用无密码公钥认证的形式，这样NameNode使用SSH无密码登录并启动DataName进程，同样原理，DataNode上也能使用SSH无密码登录到NameNode。</p>
<h3 id="2-1-ssh-">2.1 安装和启动SSH协议</h3>
<p>在&quot;Hadoop集群（第1期）&quot;安装CentOS6.0时，我们选择了一些基本安装包，所以我们需要两个服务：ssh和rsync已经安装了。可以通过下面命令查看结果显示如下：</p>
<p>rpm –qa | grep openssh</p>
<p>rpm –qa | grep rsync</p>
<p><img src="" alt=""></p>
<p><strong>假设</strong>没有安装ssh和rsync，可以通过下面命令进行安装。</p>
<p>yum install ssh 安装SSH协议</p>
<p>yum install rsync （rsync是一个远程数据同步工具，可通过LAN/WAN快速同步多台主机间的文件）</p>
<p>service sshd restart 启动服务</p>
<p>确保所有的服务器都安装，上面命令执行完毕，各台机器之间可以通过密码验证相互登。</p>
<h3 id="2-2-master-salve">2.2 配置Master无密码登录所有Salve</h3>
<p><strong>1）SSH无密码原理</strong></p>
<p>Master（NameNode | JobTracker）作为客户端，要实现无密码公钥认证，连接到服务器Salve（DataNode | Tasktracker）上时，需要在Master上生成一个密钥对，包括一个公钥和一个私钥，而后将公钥复制到所有的Slave上。当Master通过SSH连接Salve时，Salve就会生成一个随机数并用Master的公钥对随机数进行加密，并发送给Master。Master收到加密数之后再用私钥解密，并将解密数回传给Slave，Slave确认解密数无误之后就允许Master进行连接了。这就是一个公钥认证过程，其间不需要用户手工输入密码。重要过程是将客户端Master复制到Slave上。</p>
<p><strong>2）Master机器上生成密码对</strong></p>
<p>在Master节点上执行以下命令：</p>
<p>ssh-keygen –t rsa –P &#39;&#39;</p>
<p>这条命是生成其<strong>无密码密钥对</strong>，询问其保存路径时<strong>直接回车</strong>采用默认路径。生成的密钥对：id_rsa和id_rsa.pub，默认存储在&quot;<strong>/home/hadoop/.ssh</strong>&quot;目录下。</p>
<p><img src="" alt=""></p>
<p>查看&quot;/home/hadoop/&quot;下是否有&quot;.ssh&quot;文件夹，且&quot;.ssh&quot;文件下是否有两个刚生产的无密码密钥对。</p>
<p><img src="" alt=""></p>
<p>接着在Master节点上做如下配置，把id_rsa.pub追加到授权的key里面去。</p>
<p>cat ~/.ssh/id_rsa.pub &gt;&gt; ~/.ssh/authorized_keys</p>
<p><img src="" alt=""></p>
<p>在验证前，需要做两件事儿。第一件事儿是修改文件&quot;<strong>authorized_keys</strong>&quot;权限（<strong>权限的设置非常重要，因为不安全的设置安全设置，会让你不能使用RSA功能</strong>），另一件事儿是用root用户设置&quot;<strong>/etc/ssh/sshd_config</strong>&quot;的内容。使其无密码登录有效。</p>
<p><strong>1）修改文件&quot;authorized_keys&quot;</strong></p>
<p>chmod 600 ~/.ssh/authorized_keys</p>
<p><img src="" alt=""></p>
<p><strong>备注：</strong>如果不进行设置，在验证时，扔提示你输入密码，在这里花费了将近半天时间来查找原因。在网上查到了几篇不错的文章，把作为&quot;<strong>Hadoop集群_第5期副刊_JDK和SSH无密码配置</strong>&quot;来帮助额外学习之用。</p>
<p><strong>2）设置SSH配置</strong></p>
<p>用<strong>root</strong>用户登录服务器修改SSH配置文件&quot;/etc/ssh/sshd_config&quot;的下列内容。</p>
<p><img src="" alt=""></p>
<p>RSAAuthentication yes /# 启用 RSA 认证</p>
<p>PubkeyAuthentication yes /# 启用公钥私钥配对认证方式</p>
<p>AuthorizedKeysFile .ssh/authorized_keys /# 公钥文件路径（和上面生成的文件同）</p>
<p>设置完之后记得<strong>重启SSH服务</strong>，才能使刚才设置有效。</p>
<p>service sshd restart</p>
<p><strong>退出root登录</strong>，使用<strong>hadoop</strong>普通用户验证是否成功。</p>
<p>ssh localhost</p>
<p><img src="" alt=""></p>
<p>从上图中得知无密码登录本级已经设置完毕，接下来的事儿是把<strong>公钥</strong>复制<strong>所有</strong>的Slave机器上。使用下面的命令格式进行复制公钥：</p>
<p>scp ~/.ssh/id_rsa.pub 远程用户名@远程服务器IP:~/</p>
<p>例如：</p>
<p>scp ~/.ssh/id_rsa.pub hadoop@192.168.1.3:~/</p>
<p>上面的命令是<strong>复制</strong>文件&quot;<strong>id_rsa.pub</strong>&quot;到服务器IP为&quot;<strong>192.168.1.3</strong>&quot;的用户为&quot;<strong>hadoop</strong>&quot;的&quot;<strong>/home/hadoop/</strong>&quot;下面。</p>
<p>下面就针对IP为&quot;192.168.1.3&quot;的Slave1.Hadoop的节点进行配置。</p>
<p><strong>1）把Master.Hadoop上的公钥复制到Slave1.Hadoop上</strong></p>
<hr>
<p><img src="" alt=""></p>
<p>从上图中我们得知，已经把文件&quot;id_rsa.pub&quot;传过去了，因为并没有建立起无密码连接，所以在连接时，仍然要提示输入输入Slave1.Hadoop服务器用户hadoop的密码。为了确保确实已经把文件传过去了，用SecureCRT登录Slave1.Hadoop:192.168.1.3服务器，查看&quot;/home/hadoop/&quot;下是否存在这个文件。</p>
<p><img src="" alt=""></p>
<p>从上面得知我们已经成功把公钥复制过去了。</p>
<p><strong>2）在&quot;/home/hadoop/&quot;下创建&quot;.ssh&quot;文件夹</strong></p>
<p>这一步<strong>并不是必须</strong>的，如果在Slave1.Hadoop的&quot;/home/hadoop&quot;<strong>已经存在</strong>就不需要创建了，因为我们之前并没有对Slave机器做过无密码登录配置，所以该文件是不存在的。用下面命令进行创建。（<strong>备注：</strong>用hadoop登录系统，如果不涉及系统文件修改，一般情况下都是用我们之前建立的普通用户hadoop进行执行命令。）</p>
<p>mkdir ~/.ssh</p>
<p>然后是修改文件夹&quot;<strong>.ssh</strong>&quot;的用户权限，把他的权限修改为&quot;<strong>700</strong>&quot;，用下面命令执行：</p>
<p>chmod 700 ~/.ssh</p>
<p><strong>备注：</strong>如果不进行，即使你按照前面的操作设置了&quot;authorized_keys&quot;权限，并配置了&quot;/etc/ssh/sshd_config&quot;，还重启了sshd服务，在Master能用&quot;ssh localhost&quot;进行无密码登录，但是对Slave1.Hadoop进行登录仍然需要输入密码，就是因为&quot;.ssh&quot;文件夹的权限设置不对。这个文件夹&quot;.ssh&quot;在配置SSH无密码登录时系统自动生成时，权限自动为&quot;700&quot;，如果是自己手动创建，它的组权限和其他权限都有，这样就会导致RSA无密码远程登录失败。</p>
<p><img src="" alt=""></p>
<p><img src="" alt=""></p>
<p>对比上面两张图，发现文件夹&quot;.ssh&quot;权限已经变了。</p>
<p><strong>3）追加到授权文件&quot;authorized_keys&quot;</strong></p>
<p>到目前为止Master.Hadoop的公钥也有了，文件夹&quot;.ssh&quot;也有了，且权限也修改了。这一步就是把Master.Hadoop的公钥<strong>追加</strong>到Slave1.Hadoop的授权文件&quot;authorized_keys&quot;中去。使用下面命令进行追加并修改&quot;authorized_keys&quot;文件权限：</p>
<p>cat ~/id_rsa.pub &gt;&gt; ~/.ssh/authorized_keys</p>
<p>chmod 600 ~/.ssh/authorized_keys</p>
<p><img src="" alt=""></p>
<p><strong>4）用root用户修改&quot;/etc/ssh/sshd_config&quot;</strong></p>
<p><strong>**具体步骤参考前面Master.Hadoop的&quot;</strong>设置SSH配置**&quot;，具体分为两步：第1是修改配置文件；第2是重启SSH服务。</p>
<p><strong>5）用Master.Hadoop使用SSH无密码登录Slave1.Hadoop</strong></p>
<p>当前面的步骤设置完毕，就可以使用下面命令格式进行SSH无密码登录了。</p>
<p>ssh 远程服务器IP</p>
<p><img src="" alt=""></p>
<p>从上图我们主要3个地方，第1个就是SSH无密码登录命令，第2、3个就是登录前后&quot;<strong>@</strong>&quot;后面的<strong>机器名</strong>变了，由&quot;<strong>Master</strong>&quot;变为了&quot;<strong>Slave1</strong>&quot;，这就说明我们已经成功实现了SSH无密码登录了。</p>
<p>最后记得把&quot;/home/hadoop/&quot;目录下的&quot;id_rsa.pub&quot;文件删除掉。</p>
<p>rm –r ~/id_rsa.pub</p>
<p><img src="" alt=""></p>
<p>到此为止，我们经过前5步已经实现了从&quot;Master.Hadoop&quot;到&quot;Slave1.Hadoop&quot;SSH无密码登录，下面就是重复上面的步骤把剩余的两台（Slave2.Hadoop和Slave3.Hadoop）Slave服务器进行配置。<strong>这样</strong>，我们就完成了&quot;配置Master无密码登录所有的Slave服务器&quot;。</p>
<h3 id="2-3-slave-master">2.3 配置所有Slave无密码登录Master</h3>
<p>和Master无密码登录所有Slave原理一样，就是把Slave的公钥<strong>追加</strong>到Master的&quot;.ssh&quot;文件夹下的&quot;authorized_keys&quot;中，记得是<strong>追加（&gt;&gt;）</strong>。</p>
<p>为了说明情况，我们现在就以&quot;Slave1.Hadoop&quot;无密码登录&quot;Master.Hadoop&quot;为例，进行一遍操作，也算是<strong>巩固</strong>一下前面所学知识，剩余的&quot;Slave2.Hadoop&quot;和&quot;Slave3.Hadoop&quot;就按照这个示例进行就可以了。</p>
<p>首先创建&quot;Slave1.Hadoop&quot;自己的公钥和私钥，并把自己的公钥追加到&quot;authorized_keys&quot;文件中。用到的命令如下：</p>
<p>ssh-keygen –t rsa –P &#39;&#39;</p>
<p>cat ~/.ssh/id_rsa.pub &gt;&gt; ~/.ssh/authorized_keys</p>
<p><img src="" alt=""></p>
<p>接着是用命令&quot;<strong>scp</strong>&quot;复制&quot;Slave1.Hadoop&quot;的公钥&quot;id_rsa.pub&quot;到&quot;Master.Hadoop&quot;的&quot;/home/hadoop/&quot;目录下，并<strong>追加</strong>到&quot;Master.Hadoop&quot;的&quot;authorized_keys&quot;中。</p>
<p><strong>1）在&quot;Slave1.Hadoop&quot;服务器的操作</strong></p>
<p>用到的命令如下：</p>
<p>scp ~/.ssh/id_rsa.pub hadoop@192.168.1.2:~/</p>
<p><img src="" alt=""></p>
<hr>
<p><strong>2）在&quot;Master.Hadoop&quot;服务器的操作</strong></p>
<p>用到的命令如下：</p>
<p>cat ~/id_rsa.pub &gt;&gt; ~/.ssh/authorized_keys</p>
<p><img src="" alt=""></p>
<p>然后删除掉刚才复制过来的&quot;id_rsa.pub&quot;文件。</p>
<p><img src="" alt=""></p>
<p>最后是测试从&quot;Slave1.Hadoop&quot;到&quot;Master.Hadoop&quot;无密码登录。</p>
<p><img src="" alt=""></p>
<p>从上面结果中可以看到已经成功实现了，再试下从&quot;Master.Hadoop&quot;到&quot;Slave1.Hadoop&quot;无密码登录。</p>
<p><img src="" alt=""></p>
<p>至此&quot;Master.Hadoop&quot;与&quot;Slave1.Hadoop&quot;之间可以互相无密码登录了，剩下的就是按照上面的步骤把剩余的&quot;Slave2.Hadoop&quot;和&quot;Slave3.Hadoop&quot;与&quot;Master.Hadoop&quot;之间建立起无密码登录。这样，Master能无密码验证登录每个Slave，每个Slave也能无密码验证登录到Master。</p>
<h2 id="3-java-">3、Java环境安装</h2>
<p>所有的机器上都要安装JDK，现在就先在Master服务器安装，然后其他服务器按照步骤重复进行即可。安装JDK以及配置环境变量，需要以&quot;<strong>root</strong>&quot;的身份进行。</p>
<h3 id="3-1-jdk">3.1 安装JDK</h3>
<p>首先用<strong>root</strong>身份登录&quot;Master.Hadoop&quot;后在&quot;/usr&quot;下创建&quot;java&quot;文件夹，再把用FTP上传到&quot;/home/hadoop/&quot;下的&quot;jdk-6u31-linux-i586.bin&quot;复制到&quot;/usr/java&quot;文件夹中。</p>
<p>mkdir /usr/java</p>
<p>cp /home/hadoop/ jdk-6u31-linux-i586.bin /usr/java</p>
<p><img src="" alt=""></p>
<p>接着<strong>进入</strong>&quot;<strong>/usr/java</strong>&quot;目录<strong>下</strong>通过下面命令使其JDK获得可执行权限，并安装JDK。</p>
<p>chmod +x jdk-6u31-linux-i586.bin</p>
<p>./jdk-6u31-linux-i586.bin</p>
<p><img src="" alt=""></p>
<p>按照上面几步进行操作，最后点击&quot;<strong>Enter</strong>&quot;键开始安装，安装完会提示你按&quot;<strong>Enter</strong>&quot;键退出，然后查看&quot;<strong>/usr/java</strong>&quot;下面会发现多了一个名为&quot;<strong>jdk1.6.0_31</strong>&quot;文件夹，说明我们的JDK安装结束，删除&quot;jdk-6u31-linux-i586.bin&quot;文件，进入下一个&quot;配置环境变量&quot;环节。</p>
<p><img src="" alt=""></p>
<h3 id="3-2-">3.2 配置环境变量</h3>
<p>编辑&quot;/etc/profile&quot;文件，在后面添加Java的&quot;JAVA_HOME&quot;、&quot;CLASSPATH&quot;以及&quot;PATH&quot;内容。</p>
<p><strong>1）编辑&quot;/etc/profile&quot;文件</strong></p>
<p>vim /etc/profile</p>
<p><img src="" alt=""></p>
<p><strong>2）添加Java环境变量</strong></p>
<p>在&quot;<strong>/etc/profile</strong>&quot;文件的<strong>尾部</strong>添加以下内容：</p>
<p>/# set java environment</p>
<p>export JAVA_HOME=/usr/java/jdk1.6.0_31/</p>
<p>export JRE_HOME=/usr/java/jdk1.6.0_31/jre</p>
<p>export CLASSPATH=.:$CLASSPATH:$JAVA_HOME/lib:$JRE_HOME/lib</p>
<p>export PATH=$PATH:$JAVA_HOME/bin:$JRE_HOME/bin</p>
<p>或者</p>
<p>/# set java environment</p>
<p>export JAVA_HOME=/usr/java/jdk1.6.0_31</p>
<p>export CLASSPATH=.:$CLASSPATH:$JAVA_HOME/lib:$JAVA_HOME/jre/lib</p>
<p>export PATH=$PATH:$JAVA_HOME/bin:$JAVA_HOME/jre/bin</p>
<p>以上两种意思一样，那么我们就选择<strong>第2种</strong>来进行设置。</p>
<p><img src="" alt=""></p>
<p><strong>3）使配置生效</strong></p>
<p>保存并退出，执行下面命令使其配置立即生效。</p>
<p>source /etc/profile</p>
<p><img src="" alt=""></p>
<h3 id="3-3-">3.3 验证安装成功</h3>
<p>配置完毕并生效后，用下面命令判断是否成功。</p>
<p>java -version</p>
<p><img src="" alt=""></p>
<p>从上图中得知，我们以确定JDK已经安装成功。</p>
<h3 id="3-4-">3.4 安装剩余机器</h3>
<p>这时用<strong>普通用户hadoop</strong>通过下面命令格式把&quot;Master.Hadoop&quot;文件夹&quot;/home/hadoop/&quot;的JDK复制到其他Slave的&quot;/home/hadoop/&quot;下面，剩下的事儿就是在其余的Slave服务器上按照上图的步骤安装JDK。</p>
<p>scp /home/hadoop/jdk-6u31-linux-i586.bin 远程用户名@远程服务器IP:~/</p>
<p>或者</p>
<p>scp ~/jdk-6u31-linux-i586.bin 远程用户名@远程服务器IP:~/</p>
<p><strong>备注：</strong>&quot;<strong>~</strong>&quot;代表<strong>当前</strong>用户的主目录，当<strong>前用户为hadoop</strong>，所以&quot;<strong>~</strong>&quot;代表&quot;<strong>/home/hadoop</strong>&quot;。</p>
<p><strong>例如：</strong>把JDK从&quot;Master.Hadoop&quot;复制到&quot;Slave1.Hadoop&quot;的命令如下。</p>
<p>scp ~/jdk-6u31-linux-i586 hadoop@192.168.1.3:~/</p>
<p><img src="" alt=""></p>
<p>然后查看&quot;Slave1.Hadoop&quot;的&quot;/home/hadoop&quot;查看是否已经复制成功了。</p>
<p><img src="" alt=""></p>
<p>从上图中得知，我们已经成功复制了，现在我们就用<strong>最高权限用户root</strong>进行安装了。其他的与这个一样。</p>
<h2 id="4-hadoop-">4、Hadoop集群安装</h2>
<p>所有的机器上都要安装hadoop，现在就先在Master服务器安装，然后其他服务器按照步骤重复进行即可。安装和配置hadoop需要以&quot;<strong>root</strong>&quot;的身份进行。</p>
<h3 id="4-1-hadoop">4.1 安装hadoop</h3>
<p>首先用<strong>root</strong>用户登录&quot;Master.Hadoop&quot;机器，查看我们之前用FTP上传至&quot;/home/Hadoop&quot;上传的&quot;<strong>hadoop-1.0.0.tar.gz</strong>&quot;。</p>
<p><img src="" alt=""></p>
<p>接着把&quot;<strong>hadoop-1.0.0.tar.gz</strong>&quot;复制到&quot;/usr&quot;目录下面。</p>
<p>cp /home/hadoop/hadoop-1.0.0.tar.gz /usr</p>
<p><img src="" alt=""></p>
<p>下一步进入&quot;/usr&quot;目录下，用下面命令把&quot;<strong>hadoop-1.0.0.tar.gz</strong>&quot;进行解压，并将其命名为&quot;hadoop&quot;，把该文件夹的<strong>读权限</strong>分配给普通用户<strong>hadoop</strong>，然后删除&quot;<strong>hadoop-1.0.0.tar.gz</strong>&quot;安装包。</p>
<p>cd /usr /#进入&quot;/usr&quot;目录</p>
<p>tar –zxvf hadoop-1.0.0.tar.gz /#解压&quot;<strong>hadoop-1.0.0.tar.gz</strong>&quot;安装包</p>
<p>mv hadoop-1.0.0 hadoop /#将&quot;hadoop-1.0.0&quot;文件夹<strong>重命名</strong>&quot;hadoop&quot;</p>
<p>chown <strong>–R</strong> hadoop:hadoop hadoop /#<strong>将文件夹&quot;hadoop&quot;读权限分配给hadoop用户</strong></p>
<p>rm –rf hadoop-1.0.0.tar.gz /#删除&quot;<strong>hadoop-1.0.0.tar.gz</strong>&quot;安装包</p>
<p><img src="" alt=""></p>
<p>解压后，并重命名。</p>
<p><img src="" alt=""></p>
<p>把&quot;/usr/hadoop&quot;<strong>读权</strong>限分配给<strong>hadoop</strong>用户（<strong>非常重要</strong>）</p>
<p><img src="" alt=""></p>
<p>删除&quot;<strong>hadoop-1.0.0.tar.gz</strong>&quot;安装包</p>
<p><img src="" alt=""></p>
<p>最后在&quot;<strong>/usr/hadoop</strong>&quot;下面创建<strong>tmp</strong>文件夹，把Hadoop的安装路径添加到&quot;<strong>/etc/profile</strong>&quot;中，修改&quot;/etc/profile&quot;文件（配置java环境变量的文件），将以下语句添加到<strong>末尾</strong>，并使其有效：</p>
<p>/# set hadoop path</p>
<p>export HADOOP_HOME=/usr/hadoop</p>
<p>export PATH=$PATH :$HADOOP_HOME/bin</p>
<p><strong>1）在&quot;/usr/hadoop&quot;创建&quot;tmp&quot;文件夹</strong></p>
<p>mkdir /usr/hadoop/tmp</p>
<p><img src="" alt=""></p>
<p><strong>2）配置&quot;/etc/profile&quot;</strong></p>
<p>vim /etc/profile</p>
<p><img src="" alt=""></p>
<p>配置后的文件如下：</p>
<p><img src="" alt=""></p>
<p><strong>3）重启&quot;/etc/profile&quot;</strong></p>
<p>source /etc/profile</p>
<p><img src="" alt=""></p>
<h3 id="4-2-hadoop">4.2 配置hadoop</h3>
<p><strong>1）配置hadoop-env.sh</strong></p>
<p>该&quot;<strong>hadoop-env.sh</strong>&quot;文件位于&quot;<strong>/usr/hadoop/conf</strong>&quot;目录下。</p>
<p><img src="" alt=""></p>
<p>在文件的末尾添加下面内容。</p>
<p>/# set java environment</p>
<p>export JAVA_HOME=/usr/java/jdk1.6.0_31</p>
<p><img src="" alt=""></p>
<p>Hadoop配置文件在conf目录下，之前的版本的配置文件主要是Hadoop-default.xml和Hadoop-site.xml。由于Hadoop发展迅速，代码量急剧增加，代码开发分为了core，hdfs和map/reduce三部分，配置文件也被分成了三个core-site.xml、hdfs-site.xml、mapred-site.xml。core-site.xml和hdfs-site.xml是站在HDFS角度上配置文件；core-site.xml和mapred-site.xml是站在MapReduce角度上配置文件。</p>
<p><strong>2）配置core-site.xml文件</strong></p>
<p>修改Hadoop核心配置文件core-site.xml，这里配置的是HDFS的地址和端口号。</p>
<configuration>

<property>

<name>hadoop.tmp.dir</name>

<value>/usr/hadoop/tmp</value>

（<strong>备注：</strong>请先在 /usr/hadoop 目录下建立 tmp 文件夹）

<description>A base for other temporary directories.</description>

</property>

<!-- file system properties -->

<property>

<name>fs.default.name</name>

<value>hdfs://<strong>192.168.1.2</strong>:<strong>9000</strong></value>

</property>

</configuration>

<p><strong>备注：</strong>如没有配置hadoop.tmp.dir参数，此时系统默认的临时目录为：/tmp/hadoo-hadoop。而这个目录在每次重启后都会被干掉，必须重新执行format才行，否则会出错。</p>
<p>用下面命令进行编辑：</p>
<p><img src="" alt=""></p>
<p>编辑结果显示如下：</p>
<p><img src="" alt=""></p>
<p><strong>3）配置hdfs-site.xml文件</strong></p>
<p>修改Hadoop中HDFS的配置，配置的备份方式默认为3。</p>
<configuration>

<property>

<name>dfs.replication</name>

<value><strong>1</strong></value>

(<strong>备注：</strong>replication 是数据副本数量，默认为3，salve少于3台就会报错)

</property>

<configuration>

用下面命令进行编辑：

<img src="" alt="">

编辑结果显示如下：

<img src="" alt="">

<strong>4）配置mapred-site.xml文件</strong>

修改Hadoop中MapReduce的配置文件，配置的是JobTracker的地址和端口。

<configuration>

<property>

<name>mapred.job.tracker</name>

<value><a href="http://**192.168.1.2**:**9001**" target="_blank">http://**192.168.1.2**:**9001**</a></value>

</property>

</configuration>

<p>用下面命令进行编辑：</p>
<p><img src="" alt=""></p>
<p>编辑结果显示如下：</p>
<p><img src="" alt=""></p>
<p><strong>5）配置masters文件</strong></p>
<p>有两种方案：</p>
<p>（1）第一种</p>
<p>修改localhost为Master.Hadoop</p>
<p>（2）第二种</p>
<p>去掉&quot;localhost&quot;，加入Master机器的IP：192.168.1.2</p>
<p>为保险起见，启用第二种，因为万一忘记配置&quot;/etc/hosts&quot;局域网的DNS失效，这样就会出现意想不到的错误，但是一旦IP配对，网络畅通，就能通过IP找到相应主机。</p>
<p>用下面命令进行修改：</p>
<p><img src="" alt=""></p>
<p>编辑结果显示如下：</p>
<p><img src="" alt=""></p>
<p><strong>6）配置slaves文件（Master主机特有</strong>）</p>
<p>有两种方案：</p>
<p>（1）第一种</p>
<p>去掉&quot;localhost&quot;，每行只添加一个主机名，把剩余的Slave主机名都填上。</p>
<p>例如：添加形式如下</p>
<p>Slave1.Hadoop</p>
<p>Slave2.Hadoop</p>
<p>Slave3.Hadoop</p>
<p>（2）第二种</p>
<p>去掉&quot;localhost&quot;，加入集群中所有Slave机器的IP，也是每行一个。</p>
<p>例如：添加形式如下</p>
<p>192.168.1.3</p>
<p>192.168.1.4</p>
<p>192.168.1.5</p>
<p>原因和添加&quot;masters&quot;文件一样，选择第二种方式。</p>
<p>用下面命令进行修改：</p>
<p><img src="" alt=""></p>
<p>编辑结果如下：</p>
<p><img src="" alt=""></p>
<p>现在在Master机器上的Hadoop配置就结束了，剩下的就是配置Slave机器上的Hadoop。</p>
<p><strong>一种方式</strong>是按照上面的步骤，把Hadoop的安装包在用<strong>普通用户hadoop</strong>通过&quot;<strong>scp</strong>&quot;复制到其他机器的&quot;/home/hadoop&quot;目录下，然后根据实际情况进行安装配置，<strong>除了第6步，那是Master特有的</strong>。用下面命令格式进行。（<strong>备注：</strong>此时切换到普通用户hadoop）</p>
<p>scp ~/hadoop-1.0.0.tar.gz hadoop@服务器IP:~/</p>
<p>例如：从&quot;Master.Hadoop&quot;到&quot;Slave1.Hadoop&quot;复制Hadoop的安装包。</p>
<p><img src="" alt=""></p>
<p><strong>另一种方式</strong>是将 Master上配置好的hadoop所在文件夹&quot;<strong>/usr/hadoop</strong>&quot;复制到所有的Slave的&quot;/usr&quot;目录下（实际上Slave机器上的slavers文件是不必要的， 复制了也没问题）。用下面命令格式进行。（<strong>备注：</strong>此时用户可以为hadoop也可以为root）</p>
<p>scp <strong>-r</strong> /usr/hadoop <strong>root</strong>@服务器IP:/usr/</p>
<p>例如：从&quot;Master.Hadoop&quot;到&quot;Slave1.Hadoop&quot;复制配置Hadoop的文件。</p>
<p><img src="" alt=""></p>
<p>上图中以root用户进行复制，当然不管是用户root还是hadoop，虽然Master机器上的&quot;/usr/hadoop&quot;文件夹用户hadoop有权限，但是Slave1上的hadoop用户却没有&quot;/usr&quot;权限，所以没有创建文件夹的权限。所以无论是哪个用户进行拷贝，右面都是&quot;root@机器IP&quot;格式。因为我们只是建立起了hadoop用户的SSH无密码连接，所以用root进行&quot;scp&quot;时，扔提示让你输入&quot;Slave1.Hadoop&quot;服务器用户root的密码。</p>
<p>查看&quot;Slave1.Hadoop&quot;服务器的&quot;/usr&quot;目录下是否已经存在&quot;hadoop&quot;文件夹，确认已经复制成功。查看结果如下：</p>
<p><img src="" alt=""></p>
<p>从上图中知道，hadoop文件夹确实已经复制了，但是我们发现hadoop权限是root，所以我们现在要给&quot;Slave1.Hadoop&quot;服务器上的用户hadoop添加对&quot;/usr/hadoop&quot;读权限。</p>
<p>以<strong>root</strong>用户登录&quot;Slave1.Hadoop&quot;，执行下面命令。</p>
<p>chown <strong>-R</strong> hadoop:hadoop（<strong>用户名：用户组</strong>） hadoop（<strong>文件夹</strong>）</p>
<p><img src="" alt=""></p>
<p>接着在&quot;Slave1 .Hadoop&quot;上修改&quot;/etc/profile&quot;文件（配置 java 环境变量的文件），将以下语句添加到末尾，并使其有效（source /etc/profile）：</p>
<p>/# set hadoop environment</p>
<p>export HADOOP_HOME=/usr/hadoop</p>
<p>export PATH=$PATH :$HADOOP_HOME/bin</p>
<p>如果不知道怎么设置，可以查看前面&quot;Master.Hadoop&quot;机器的&quot;/etc/profile&quot;文件的配置，到此为此在一台Slave机器上的Hadoop配置就结束了。剩下的事儿就是照葫芦画瓢把剩余的几台Slave机器按照《<strong>从&quot;Master.Hadoop&quot;到&quot;Slave1.Hadoop&quot;复制Hadoop的安装包。</strong>》这个例子进行部署Hadoop。</p>
<h3 id="4-3-">4.3 启动及验证</h3>
<p><strong>1）格式化HDFS文件系统</strong></p>
<p>在&quot;Master.Hadoop&quot;上使用<strong>普通</strong>用户<strong>hadoop</strong>进行操作。（<strong>备注：</strong>只需一次，下次启动不再需要格式化，只需 start-all.sh）</p>
<p>hadoop namenode -format</p>
<p>某些书上和网上的某些资料中用下面命令执行。</p>
<p><img src="" alt=""></p>
<p>我们在看好多文档包括有些书上，按照他们的hadoop环境变量进行配置后，并立即使其生效，但是执行发现没有找见&quot;bin/hadoop&quot;这个命令。</p>
<p><img src="" alt=""></p>
<p>其实我们会发现我们的环境变量配置的是&quot;<strong>$HADOOP_HOME/bin</strong>&quot;，我们已经把bin包含进入了，所以执行时，加上&quot;bin&quot;反而找不到该命令，除非我们的hadoop坏境变量如下设置。</p>
<p>/# set hadoop path</p>
<p>export HADOOP_HOME=/usr/hadoop</p>
<p>export PATH=$PATH :<strong>$HADOOP_HOME</strong>:<strong>$HADOOP_HOME/bin</strong></p>
<p>这样就能直接使用&quot;bin/hadoop&quot;也可以直接使用&quot;hadoop&quot;，现在不管哪种情况，hadoop命令都能找见了。我们也没有必要重新在设置hadoop环境变量了，只需要记住执行Hadoop命令时不需要在前面加&quot;bin&quot;就可以了。</p>
<p><img src="" alt=""></p>
<p><img src="" alt=""></p>
<p>从上图中知道我们已经成功格式话了，但是美中不足就是出现了一个<strong>警告</strong>，从网上的得知这个警告并不影响hadoop执行，但是也有办法解决，详情看后面的&quot;常见问题FAQ&quot;。</p>
<p><strong>2）启动hadoop</strong></p>
<p>在启动前关闭集群中所有机器的防火墙，不然会出现datanode开后又自动关闭。</p>
<p>service iptables stop</p>
<p>使用下面命令启动。</p>
<p>start-all.sh</p>
<p><img src="" alt=""></p>
<p>执行结果如下：</p>
<p><img src="" alt=""></p>
<p>可以通过以下启动日志看出，首先启动namenode 接着启动datanode1，datanode2，…，然后启动secondarynamenode。再启动jobtracker，然后启动tasktracker1，tasktracker2，…。</p>
<p>启动 hadoop成功后，在 Master 中的 tmp 文件夹中生成了 dfs 文件夹，在Slave 中的 tmp 文件夹中均生成了 dfs 文件夹和 mapred 文件夹。</p>
<p>查看Master中&quot;/usr/hadoop/tmp&quot;文件夹内容</p>
<p><img src="" alt=""></p>
<p>查看Slave1中&quot;/usr/hadoop/tmp&quot;文件夹内容。</p>
<p><img src="" alt=""></p>
<p><strong>3）验证hadoop</strong></p>
<p>（1）验证方法一：用&quot;jps&quot;命令</p>
<p>在Master上用 java自带的小工具<strong>jps</strong>查看进程。</p>
<p><img src="" alt=""></p>
<p>在Slave1上用jps查看进程。</p>
<p><img src="" alt=""></p>
<p>如果在查看Slave机器中发现&quot;DataNode&quot;和&quot;TaskTracker&quot;没有起来时，先查看一下日志的，如果是&quot;namespaceID&quot;不一致问题，采用&quot;常见问题FAQ6.2&quot;进行解决，如果是&quot;No route to host&quot;问题，采用&quot;常见问题FAQ6.3&quot;进行解决。</p>
<p>（2）验证方式二：用&quot;hadoop dfsadmin -report&quot;</p>
<p>用这个命令可以查看Hadoop集群的状态。</p>
<p>Master服务器的状态：</p>
<p><img src="" alt=""></p>
<p>Slave服务器的状态</p>
<p><img src="" alt=""></p>
<h3 id="4-4-">4.4 网页查看集群</h3>
<p><strong>1）访问&quot;http:192.168.1.2:50030&quot;</strong></p>
<p><img src="" alt=""></p>
<p>2）访问&quot;<strong>http:192.168.1.2:50070</strong>&quot;</p>
<p><img src="" alt=""></p>
<h2 id="5-faq">5、常见问题FAQ</h2>
<h3 id="5-1-warning-hadoop_home-is-deprecated-">5.1 关于 Warning: $HADOOP_HOME is deprecated.</h3>
<p>hadoop 1.0.0版本，安装完之后敲入hadoop命令时，<strong>老</strong>是提示这个警告：</p>
<p>Warning: $HADOOP_HOME is deprecated.</p>
<p>经查hadoop-1.0.0/bin/hadoop脚本和&quot;hadoop-config.sh&quot;脚本，发现脚本中对HADOOP_HOME的环境变量设置做了判断，笔者的环境根本不需要设置HADOOP_HOME环境变量。</p>
<p>解决方案一：编辑&quot;/etc/profile&quot;文件，去掉HADOOP_HOME的变量设定，重新输入hadoop fs命令，警告消失。</p>
<p>解决方案二：编辑&quot;/etc/profile&quot;文件，添加一个环境变量，之后警告消失：</p>
<p>export HADOOP_HOME_WARN_SUPPRESS=1</p>
<p>解决方案三：编辑&quot;hadoop-config.sh&quot;文件，把下面的&quot;if - fi&quot;功能注释掉。</p>
<p><img src="" alt=""></p>
<p>我们这里本着不动Hadoop原配置文件的前提下，采用&quot;<strong>方案二</strong>&quot;，在&quot;/etc/profile&quot;文件添加上面内容，并用命令&quot;source /etc/profile&quot;使之有效。</p>
<p><strong>1）切换至root用户</strong></p>
<p><img src="" alt=""></p>
<p><strong>2）添加内容</strong></p>
<p><img src="" alt=""></p>
<p><strong>3）重新生效</strong></p>
<p><img src="" alt=""></p>
<h3 id="5-2-no-datanode-to-stop-">5.2 解决&quot;no datanode to stop&quot;问题</h3>
<p>当我停止Hadoop时发现如下信息：</p>
<p><img src="" alt=""></p>
<p>原因：每次namenode format会重新创建一个namenodeId，而tmp/dfs/data下包含了上次format下的id，namenode format清空了namenode下的数据，但是没有清空datanode下的数据，导致启动时失败，所要做的就是每次fotmat前，清空tmp一下的所有目录。</p>
<p><strong>第一种解决方案如下：</strong></p>
<p><strong>1）先删除&quot;/usr/hadoop/tmp&quot;</strong></p>
<p>rm -rf /usr/hadoop/tmp</p>
<p><strong>2）创建&quot;/usr/hadoop/tmp&quot;文件夹</strong></p>
<p>mkdir /usr/hadoop/tmp</p>
<p><strong>3）删除&quot;/tmp&quot;下以&quot;hadoop&quot;开头文件</strong></p>
<p>rm -rf /tmp/hadoop/*</p>
<p><strong>4）重新格式化hadoop</strong></p>
<p>hadoop namenode -format</p>
<p><strong>5）启动hadoop</strong></p>
<p>start-all.sh</p>
<p>使用第一种方案，有种不好处就是原来集群上的重要数据全没有了。假如说Hadoop集群已经运行了一段时间。建议采用第二种。</p>
<p><strong>第二种方案如下：</strong></p>
<p>1）修改每个Slave的namespaceID使其与Master的namespaceID一致。</p>
<p>或者</p>
<p>2）修改Master的namespaceID使其与Slave的namespaceID一致。</p>
<p>该&quot;namespaceID&quot;位于&quot;<strong>/usr/hadoop/tmp/dfs/data/current/VERSION</strong>&quot;文件中，前面<strong>蓝色</strong>的可能根据实际情况变化，但后面<strong>红色</strong>是不变的。</p>
<p>例如：查看&quot;Master&quot;下的&quot;<strong>VERSION</strong>&quot;文件</p>
<p><img src="" alt=""></p>
<p>本人建议采用<strong>第二种</strong>，这样方便快捷，而且还能防止误删。</p>
<h3 id="5-3-slave-datanode-">5.3 Slave服务器中datanode启动后又自动关闭</h3>
<p>查看日志发下如下错误。</p>
<p><strong>ERROR</strong> org.apache.hadoop.hdfs.server.datanode.DataNode: java.io.IOException: Call to ... failed on local exception: java.net.NoRouteToHostException: <strong>No route to host</strong></p>
<p>解决方案是：关闭防火墙</p>
<p>service iptables stop</p>
<h3 id="5-4-hdfs-">5.4 从本地往hdfs文件系统上传文件</h3>
<p>出现如下错误：</p>
<p>INFO hdfs.DFSClient: Exception in createBlockOutputStream java.io.IOException: <strong>Bad connect ack with firstBadLink</strong></p>
<p>INFO hdfs.DFSClient: Abandoning block blk_-1300529705803292651_37023</p>
<p>WARN hdfs.DFSClient: DataStreamer Exception: java.io.IOException: <strong>Unable to create new block.</strong></p>
<p>解决方案是：</p>
<p><strong>1）关闭防火墙</strong></p>
<p>service iptables stop</p>
<p><strong>2）禁用selinux</strong></p>
<p>编辑 &quot;<strong>/etc/selinux/config</strong>&quot;文件，设置&quot;<strong>SELINUX</strong>=<strong>disabled</strong>&quot;</p>
<h3 id="5-5-">5.5 安全模式导致的错误</h3>
<p>出现如下错误：</p>
<p>org.apache.hadoop.dfs.SafeModeException: <strong>Cannot delete ..., Name node is in safe mode</strong></p>
<p>在分布式文件系统启动的时候，开始的时候会有安全模式，当分布式文件系统处于安全模式的情况下，文件系统中的内容不允许修改也不允许删除，直到安全模式结束。安全模式主要是为了系统启动的时候检查各个DataNode上数据块的有效性，同时根据策略必要的复制或者删除部分数据块。运行期通过命令也可以进入安全模式。在实践过程中，系统启动的时候去修改和删除文件也会有安全模式不允许修改的出错提示，只需要等待一会儿即可。</p>
<p>解决方案是：关闭安全模式</p>
<p>hadoop dfsadmin -safemode leave</p>
<h3 id="5-6-exceeded-max_failed_unique_fetches">5.6 解决Exceeded MAX_FAILED_UNIQUE_FETCHES</h3>
<p>出现错误如下：</p>
<p>Shuffle Error: Exceeded MAX_FAILED_UNIQUE_FETCHES; bailing-out</p>
<p>程序里面需要打开多个文件，进行分析，系统一般默认数量是1024，（用ulimit -a可以看到）对于正常使用是够了，但是对于程序来讲，就太少了。</p>
<p>解决方案是：修改2个文件。</p>
<p><strong>1）&quot;/etc/security/limits.conf&quot;</strong></p>
<p>vim /etc/security/limits.conf</p>
<p>加上：</p>
<p>soft nofile 102400</p>
<p>hard nofile 409600</p>
<p><strong>2）&quot;/etc/pam.d/login&quot;</strong></p>
<p>vim /etc/pam.d/login</p>
<p>添加：</p>
<p>session required /lib/security/pam_limits.so</p>
<p>针对第一个问题我纠正下答案：</p>
<p>这是reduce预处理阶段shuffle时获取已完成的map的输出失败次数超过上限造成的，上限默认为5。引起此问题的方式可能会有很多种，比如网络连接不正常，连接超时，带宽较差以及端口阻塞等。通常框架内网络情况较好是不会出现此错误的。</p>
<h3 id="5-7-too-many-fetch-failures-">5.7 解决&quot;Too many fetch-failures&quot;</h3>
<p>出现这个问题主要是结点间的连通不够全面。</p>
<p>解决方案是：</p>
<p><strong>1）检查&quot;/etc/hosts&quot;</strong></p>
<p>要求本机ip 对应 服务器名</p>
<p>要求要包含所有的服务器ip +服务器名</p>
<p><strong>2）检查&quot;.ssh/authorized_keys&quot;</strong></p>
<p>要求包含所有服务器（包括其自身）的public key</p>
<h3 id="5-8-">5.8 处理速度特别的慢</h3>
<p>出现<strong>map</strong>很<strong>快</strong>，但是<strong>reduce</strong>很<strong>慢</strong>，而且反复出现&quot;<strong>reduce=0%</strong>&quot;。</p>
<p>解决方案如下：</p>
<p>结合解决方案5.7，然后修改&quot;conf/hadoop-env.sh&quot;中的&quot;export HADOOP_HEAPSIZE=4000&quot;</p>
<h3 id="5-9-hadoop-outofmemoryerror-">5.9解决hadoop OutOfMemoryError问题</h3>
<p>出现这种异常，明显是jvm内存不够得原因。</p>
<p>解决方案如下：要修改所有的datanode的jvm内存大小。</p>
<p>Java –Xms 1024m -Xmx 4096m</p>
<p>一般jvm的最大内存使用应该为总内存大小的一半，我们使用的8G内存，所以设置为4096m，这一值可能依旧不是最优的值。</p>
<h3 id="5-10-namenode-in-safe-mode">5.10 Namenode in safe mode</h3>
<p>解决方案如下：</p>
<p>bin/hadoop dfsadmin -safemode leave</p>
<h3 id="5-11-io-">5.11 IO写操作出现问题</h3>
<p>0-1246359584298, infoPort=50075, ipcPort=50020):Got exception while serving blk_-5911099437886836280_1292 to /172.16.100.165:</p>
<p>java.net.SocketTimeoutException: 480000 millis timeout while waiting for channel to be ready for write. ch : java.nio.channels.SocketChannel[connected local=/</p>
<p>172.16.100.165:50010 remote=/172.16.100.165:50930]</p>
<p>at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:185)</p>
<p>at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:159)</p>
<p>……</p>
<p>It seems there are many reasons that it can timeout, the example given in HADOOP-3831 is a slow reading client.</p>
<p>解决方案如下：</p>
<p>在hadoop-site.xml中设置dfs.datanode.socket.write.timeout=0</p>
<h3 id="5-12-status-of-255-error">5.12 status of 255 error</h3>
<p>错误类型：</p>
<p>java.io.IOException: Task process exit with nonzero status of 255.</p>
<p>at org.apache.hadoop.mapred.TaskRunner.run(TaskRunner.java:424)</p>
<p>错误原因：</p>
<p>Set mapred.jobtracker.retirejob.interval and mapred.userlog.retain.hours to higher value. By default, their values are 24 hours. These might be the reason for failure, though I&#39;m not sure restart.</p>
<p>解决方案如下：单个datanode</p>
<p>如果一个datanode 出现问题，解决之后需要重新加入cluster而不重启cluster，方法如下：</p>
<p>bin/hadoop-daemon.sh start datanode</p>
<p>bin/hadoop-daemon.sh start jobtracker</p>
<h2 id="6-linux-">6、用到的Linux命令</h2>
<h3 id="6-1-chmod-">6.1 chmod命令详解</h3>
<p><strong>使用权限：</strong>所有使用者</p>
<p><strong>使用方式：</strong>chmod [-cfvR] [--help] [--version] mode file...</p>
<p><strong>说明：</strong></p>
<p>Linux/Unix 的档案存取权限分为三级 : 档案拥有者、群组、其他。利用 chmod 可以藉以控制档案如何被他人所存取。</p>
<p>mode ：权限设定字串，格式如下 ：[ugoa...][[+-=][rwxX]...][,...]，其中u 表示该档案的拥有者，g 表示与该档案的拥有者属于同一个群体(group)者，o 表示其他以外的人，a 表示这三者皆是。</p>
<ul>
<li>表示增加权限、- 表示取消权限、= 表示唯一设定权限。</li>
</ul>
<p>r 表示可读取，w 表示可写入，x 表示可执行，X 表示只有当该档案是个子目录或者该档案已经被设定过为可执行。</p>
<p>-c : 若该档案权限确实已经更改，才显示其更改动作</p>
<p>-f : 若该档案权限无法被更改也不要显示错误讯息</p>
<p>-v : 显示权限变更的详细资料</p>
<p>-R : 对目前目录下的所有档案与子目录进行相同的权限变更(即以递回的方式逐个变更)</p>
<p>--help : 显示辅助说明</p>
<p>--version : 显示版本</p>
<p><strong>范例：</strong></p>
<p>将档案 file1.txt 设为所有人皆可读取</p>
<p>chmod ugo+r file1.txt</p>
<p>将档案 file1.txt 设为所有人皆可读取</p>
<p>chmod a+r file1.txt</p>
<p>将档案 file1.txt 与 file2.txt 设为该档案拥有者，与其所属同一个群体者可写入，但其他以外的人则不可写入</p>
<p>chmod ug+w,o-w file1.txt file2.txt</p>
<p>将 ex1.py 设定为只有该档案拥有者可以执行</p>
<p>chmod u+x ex1.py</p>
<p>将目前目录下的所有档案与子目录皆设为任何人可读取</p>
<p>chmod -R a+r /*</p>
<p>此外chmod也可以用数字来表示权限如 chmod 777 file</p>
<p><strong>语法为：</strong>chmod abc file</p>
<p>其中a,b,c各为一个数字，分别表示User、Group、及Other的权限。</p>
<p>r=4，w=2，x=1</p>
<p>若要rwx属性则4+2+1=7；</p>
<p>若要rw-属性则4+2=6；</p>
<p>若要r-x属性则4+1=7。</p>
<p><strong>范例：</strong></p>
<p>chmod a=rwx file 和 chmod 777 file 效果相同</p>
<p>chmod ug=rwx,o=x file 和 chmod 771 file 效果相同</p>
<p>若用chmod 4755 filename可使此程式具有<strong>root</strong>的权限</p>
<h3 id="6-2-chown-">6.2 chown命令详解</h3>
<p><strong>使用权限：</strong>root</p>
<p><strong>使用方式：</strong>chown [-cfhvR] [--help] [--version] user[:group] file...</p>
<p><strong>说明：</strong></p>
<p>Linux/Unix 是多人多工作业系统，所有的档案皆有拥有者。利用 chown 可以将档案的拥有者加以改变。一般来说，这个指令只有是由系统管理者(root)所使用，一般使用者没有权限可以改变别人的档案拥有者，也没有权限可以自己的档案拥有者改设为别人。只有系统管理者(root)才有这样的权限。</p>
<p>user : 新的档案拥有者的使用者</p>
<p>IDgroup : 新的档案拥有者的使用者群体(group)</p>
<p>-c : 若该档案拥有者确实已经更改，才显示其更改动作</p>
<p>-f : 若该档案拥有者无法被更改也不要显示错误讯息</p>
<p>-h : 只对于连结(link)进行变更，而非该 link 真正指向的档案</p>
<p>-v : 显示拥有者变更的详细资料</p>
<p>-R : 对目前目录下的所有档案与子目录进行相同的拥有者变更(即以递回的方式逐个变更)</p>
<p>--help : 显示辅助说明</p>
<p>--version : 显示版本</p>
<p><strong>范例：</strong></p>
<p>将档案 file1.txt 的拥有者设为 users 群体的使用者 jessie</p>
<p>chown jessie:users file1.txt</p>
<p>将目前目录下的所有档案与子目录的拥有者皆设为 users 群体的使用者 lamport</p>
<p>chown -R lamport:users /*</p>
<p>-rw------- (600) -- 只有属主有读写权限。</p>
<p>-rw-r--r-- (644) -- 只有属主有读写权限；而属组用户和其他用户只有读权限。</p>
<p>-rwx------ (700) -- 只有属主有读、写、执行权限。</p>
<p>-rwxr-xr-x (755) -- 属主有读、写、执行权限；而属组用户和其他用户只有读、执行权限。</p>
<p>-rwx--x--x (711) -- 属主有读、写、执行权限；而属组用户和其他用户只有执行权限。</p>
<p>-rw-rw-rw- (666) -- 所有用户都有文件读、写权限。这种做法不可取。</p>
<p>-rwxrwxrwx (777) -- 所有用户都有读、写、执行权限。更不可取的做法。</p>
<p>以下是对目录的两个普通设定：</p>
<p>drwx------ (700) - 只有属主可在目录中读、写。</p>
<p>drwxr-xr-x (755) - 所有用户可读该目录，但只有属主才能改变目录中的内容</p>
<p>suid的代表数字是4，比如4755的结果是-rwsr-xr-x</p>
<p>sgid的代表数字是2，比如6755的结果是-rwsr-sr-x</p>
<p>sticky位代表数字是1，比如7755的结果是-rwsr-sr-t</p>
<h3 id="6-3-scp-">6.3 scp命令详解</h3>
<p>scp是 secure copy的缩写，scp是linux系统下基于ssh登陆进行安全的远程文件拷贝命令。linux的scp命令可以在linux服务器之间复制文件和目录。</p>
<p><strong>scp命令的用处：</strong></p>
<p>scp在网络上不同的主机之间复制文件，它使用ssh安全协议传输数据，具有和ssh一样的验证机制，从而安全的远程拷贝文件。</p>
<p><strong>scp命令基本格式：</strong></p>
<p>scp [-1246BCpqrv] [-c cipher] [-F ssh_config] [-i identity_file]</p>
<p>[-l limit] [-o ssh_option] [-P port] [-S program]</p>
<p>[[user@]host1:]file1 [...] [[user@]host2:]file2</p>
<p>scp命令的参数说明：</p>
<p>-1 强制scp命令使用协议ssh1</p>
<p>-2 强制scp命令使用协议ssh2</p>
<p>-4 强制scp命令只使用IPv4寻址</p>
<p>-6 强制scp命令只使用IPv6寻址</p>
<p>-B 使用批处理模式（传输过程中不询问传输口令或短语）</p>
<p>-C 允许压缩。（将-C标志传递给ssh，从而打开压缩功能）</p>
<p>-p 保留原文件的修改时间，访问时间和访问权限。</p>
<p>-q 不显示传输进度条。</p>
<p>-r 递归复制整个目录。</p>
<p>-v 详细方式显示输出。scp和ssh(1)会显示出整个过程的调试信息。这些信息用于调试连接，验证和配置问题。</p>
<p>-c cipher 以cipher将数据传输进行加密，这个选项将直接传递给ssh。</p>
<p>-F ssh_config 指定一个替代的ssh配置文件，此参数直接传递给ssh。</p>
<p>-i identity_file 从指定文件中读取传输时使用的密钥文件，此参数直接传递给ssh。</p>
<p>-l limit 限定用户所能使用的带宽，以Kbit/s为单位。</p>
<p>-o ssh_option 如果习惯于使用ssh_config(5)中的参数传递方式，</p>
<p>-P port 注意是大写的P, port是指定数据传输用到的端口号</p>
<p>-S program 指定加密传输时所使用的程序。此程序必须能够理解ssh(1)的选项。</p>
<p><strong>scp命令的实际应用</strong></p>
<p><strong>1）从本地服务器复制到远程服务器</strong></p>
<p><strong>(1) 复制文件：</strong></p>
<p>命令格式：</p>
<p>scp local_file remote_username@remote_ip:remote_folder</p>
<p>或者</p>
<p>scp local_file remote_username@remote_ip:remote_file</p>
<p>或者</p>
<p>scp local_file remote_ip:remote_folder</p>
<p>或者</p>
<p>scp local_file remote_ip:remote_file</p>
<p>第1,2个指定了用户名，命令执行后需要输入用户密码，第1个仅指定了远程的目录，文件名字不变，第2个指定了文件名</p>
<p>第3,4个没有指定用户名，命令执行后需要输入用户名和密码，第3个仅指定了远程的目录，文件名字不变，第4个指定了文件名</p>
<p><strong>实例：</strong></p>
<p>scp /home/linux/soft/scp.zip root@www.mydomain.com:/home/linux/others/soft</p>
<p>scp /home/linux/soft/scp.zip root@www.mydomain.com:/home/linux/others/soft/scp2.zip</p>
<p>scp /home/linux/soft/scp.zip www.mydomain.com:/home/linux/others/soft</p>
<p>scp /home/linux/soft/scp.zip www.mydomain.com:/home/linux/others/soft/scp2.zip</p>
<p><strong>(2) 复制目录：</strong></p>
<p>命令格式：</p>
<p>scp -r local_folder remote_username@remote_ip:remote_folder</p>
<p>或者</p>
<p>scp -r local_folder remote_ip:remote_folder</p>
<p>第1个指定了用户名，命令执行后需要输入用户密码；</p>
<p>第2个没有指定用户名，命令执行后需要输入用户名和密码；</p>
<p><strong>例子：</strong></p>
<p>scp -r /home/linux/soft/ root@www.mydomain.com:/home/linux/others/</p>
<p>scp -r /home/linux/soft/ www.mydomain.com:/home/linux/others/</p>
<p>上面 命令 将 本地 soft 目录 复制 到 远程 others 目录下，即复制后远程服务器上会有/home/linux/others/soft/ 目录。</p>
<p><strong>2）从远程服务器复制到本地服务器</strong></p>
<p>从远程复制到本地的scp命令与上面的命令雷同，只要将从本地复制到远程的命令后面2个参数互换顺序就行了。</p>
<p><strong>例如：</strong></p>
<p>scp root@www.mydomain.com:/home/linux/soft/scp.zip /home/linux/others/scp.zip</p>
<p>scp www.mydomain.com:/home/linux/soft/ -r /home/linux/others/</p>
<p>linux系统下scp命令中很多参数都和ssh1有关，还需要看到更原汁原味的参数信息，可以运行man scp 看到更细致的英文说明。</p>
<p>文章下载地址：<a href="http://files.cnblogs.com/xia520pi/HadoopCluster_Vol.5.rar" target="_blank"><a href="http://files.cnblogs.com/xia520pi/HadoopCluster_Vol.5.rar">http://files.cnblogs.com/xia520pi/HadoopCluster_Vol.5.rar</a></a>
来源： &lt;<a href="http://www.cnblogs.com/xia520pi/archive/2012/05/16/2503949.html" target="_blank">Hadoop集群（第5期）_Hadoop安装配置 - 虾皮 - 博客园</a>&gt; </p>

      
    </div>
    
    
      
    
    
        <footer id="post-meta">
        <span class="categories">Posted in<span class="breadcrumb fa fa-folder"><li><a href="/categories/hadoop/">hadoop</a></li></span></span> | <span class="tags">Tagged <a href="/tags/hadoop/" class="label label-primary">hadoop</a></span> | <span class="time">recent updated:<time title="2014-03-07 09:54:36"datetime="2014-03-07 09:54:36"> mar. 7 2014</time></span> | <span class="comment-link">
<a href="http://itsolife.com/2014/02/02/2014-02-02-hadoop--Hadoop集群_Hadoop安装配置/#comments" class="ds-thread-count comment-link" data-thread-key="2014-02-02-hadoop--Hadoop集群_Hadoop安装配置" data-count-type="comments">暂无评论</a></span>
        </footer>
    
    <div class="clearfix"></div>
  </div>
</div>
</article>



  <article>
<div id="post" class="post well">
  <div class="post-content">
    <header class="well-sm">
      <i class="fa icon fa-5x pull-left"></i>
      <h1 class="title"><a href="/2014/02/02/2014-02-02-hadoop--OpenStack_Hadoop/">OpenStack_Hadoop</a></h1>
      
        <span>Posted on<time datetime="2014-02-02T01:54:36.000Z"> <a href="/2014/02/02/2014-02-02-hadoop--OpenStack_Hadoop/">feb. 2 2014</a></time></span>      
    </header>
    
    
    <div class="entry">
      
        <h1 id="openstack_hadoop">OpenStack_Hadoop</h1>
<p>针对OpenStack、Hadoop种不同领域软件的分析</p>
<p>1 <img src="" alt=""> OpenStack</p>
<p>1.1 简介</p>
<p>一款管理分布在多台物理机器上的多台虚拟机的开源虚拟机管理软件。</p>
<p>虚拟化是指在同一台物理机器上提供多台虚拟机器的技术。</p>
<p>其可以在多台计算机（PC或者小型机）组成的网络集群上不跨物理机器的前提下自由调配单机资源以虚拟化成一台或者多台个人PC机器、局域网供用户使用和服务。</p>
<p>优点：可自由调配、定制以几乎单机全部的有限资源合理应对模拟几乎无限种可能的低运算量业务处理环境。</p>
<p>缺点：当全部资源都被使用后，实际资源有效利用率低（考虑多个虚拟机重复的操作系统环境、分布式虚拟机镜像文件存储的冗余备份、虚拟机技术本身的VM运行机制）。</p>
<p>其更适用于办公、开发、测试环境使用。</p>
<p>1.2 网文：虚拟化的误区</p>
<p>服务器虚拟化技术之十大误区</p>
<p>2008-09-08 10:20 摘自<a href="http://datacenter.chinabyte.com/280/8297280.shtml" target="_blank">http://datacenter.chinabyte.com/280/8297280.shtml</a></p>
<p>[导读]服务器虚拟化技术之十大误区</p>
<p>误区1：虚拟化技术可以实现多台物理服务器资源整合，从而实现单个应用通过虚拟化技术而运行在多台物理硬件上</p>
<p>实际上，虚拟化技术不能将一个应用分布运行在多台物理硬件上，那是分布式计算要去解决的问题。分布式计算环境和虚拟化环境是两种不同的资源整合方式。当然，如果想通过虚拟化技术实现一个应用跨物理平台运行技术上来说是可行的，只是为了解决不同硬件之间的CPU和内存级指令、数据的同步，需要使用一些特别的技术，比如Infiniband等，这会极大地增加系统的复杂性和成本。实际上，基于这种理念的虚拟化产品曾在实验室实现，但是由于成本等因素无法投入市场。今天能看到的所有服务器虚拟化技术解决方案都不提供一个应用跨物理服务器运行，也就是说，虚拟化环境下一个应用能使用的最大资源就是一台独立的物理服务器。</p>
<p>误区2：服务器虚拟化技术就会陷入将多个鸡蛋放到一个篮子的尴尬</p>
<p>通过虚拟化技术，提高了服务器的利用效率和灵活性。但同时也使得单台服务器上运行了多个独立的虚拟机，也就是多个不同的应用。我们原来在一台服务器上只运行一个应用，服务器维护和升级时只会影响单个应用。通过运行虚拟化技术，我们在维护和升级服务器时会影响该服务器上运行的所有虚拟机和应用。这导致很多人认为的问题：多个虚拟机放置在一台服务器上的“鸡蛋和篮子”问题。</p>
<p>实际上，VMware很早就意识到了这个问题，这个问题可以通过两个方面的能力去解决。一是怎么保证虚拟化后的服务器物理硬件维护和升级的问题。二是物理服务器故障时如何保护这些虚拟机的安全。</p>
<p>首先，VMware创造性的发明了VMotion的技术，解决了虚拟化后物理服务器的升级和维护问题。通过VMotion，VMware可以在服务器需要维护升级时动态将虚拟机迁移到其他的物理服务器，通过内存复制技术，确保每台虚拟机任何对外的服务都不发生中断，从而实现了：停物理硬件、不停应用。下图是VMotion的具体实现，已经有超过50%的VMware客户部署了VMotion技术。</p>
<p><img src="" alt=""></p>
<p>其次，VMware推出了VMware HA的功能来保护物理服务器的安全。一旦发生物理服务器故障，VMware HA可以智能检测到这一事件，及时快速地在其他物理服务器上重新启用这些虚拟机，从而保证虚拟机的安全性和可靠性。</p>
<p><img src="" alt=""></p>
<p>误区3：动态在线虚拟机迁移可以跨越任何硬件进行</p>
<p>目前VMware在业界推出了标志性的创新产品功能VMotion，可以实现虚拟机动态在线跨越硬件服务器进行迁移。但是这是有一个兼容前提，也就是两台物理服务器要达到CPU指令级的兼容，或者是完全一样的CPU，或者是同一家族的CPU。如果CPU指令不兼容，进行内存复制后新机器CPU不能识别这些指令就会导致系统崩溃。当然，具体CPU指令级是否兼容，VMotion会自动进行判定。</p>
<p>当然，如果您可以离线进行虚拟机的迁移，就可以跨越任何ESX兼容的硬件进行迁移，就没有CPU型号等的制约。</p>
<p>误区4：数据中心虚拟化后可以节约虚拟机里运行软件许可证的成本</p>
<p>虚拟化技术并未改变软件许可证的发放方式，因此虚拟化技术并不意味着操作系统或应用软件许可证成本的节约，除非操作系统、应用软件厂商重新调整了软件许可证策略。因此，想通过使用虚拟化技术来减少应用软件许可成本的想法是错误的。当然，实施虚拟化技术也不会增加操作系统或应用软件的许可证成本。</p>
<p>误区5：数据中心虚拟化只使用于边缘应用，对关键应用或资源消耗较大的应用目前还不能虚拟化</p>
<p>PC服务器的虚拟化技术已经相当成熟，在美国和欧洲已经获得了广泛应用。实际上，很多关键的业务应用已经运行在虚拟化的平台上。对于资源消耗比较高的应用，需要进行合理的规划才能迁移到虚拟化上来，即使某个机器的资源消耗特别巨大，仍然可以通过升级服务器的内存、CPU来使它顺利迁移到高端PC服务器上来。当然，某个虚拟机能够支持的最大资源仍然是有限制的，比如运行在VMware的ESX Server 3.0上的虚拟机，最多可以支持16GB内存和4颗虚拟CPU。如果这些资源仍然无法满足某个应用的需求，该应用还是不能运行在虚拟化的平台上。基于一般考虑，大多数资源消耗较大的应用仍然能够安全运行到虚拟化平台上。</p>
<p>误区6：英特尔和AMD都开始在CPU级支持虚拟化技术，已不需要再购买虚拟化软件了</p>
<p>CPU的厂商英特尔和AMD都在推行基于CPU的虚拟化，实际上CPU级的虚拟化就是在CPU指令级增加了许多虚拟化的指令而已，这并非说用户可以不需要购买虚拟化软件了，CPU级的虚拟化需要虚拟化软件才能使用起来。目前所有的常用操作系统都不支持CPU级的虚拟化。而VMware提供的虚拟化平台正是通过利用英特尔和AMD提供的CPU指令的虚拟化，进而提高了虚拟化的效率，有效提高了虚拟机的性能，降低了虚拟化带来的损耗，大大加速数据中心虚拟化的进程。所以说，CPU的虚拟化是对服务器虚拟化的极大推动，而不是限制VMware这样的虚拟化产品的推广。</p>
<p>误区7：数据中心虚拟化会极大地降低服务器的性能</p>
<p>虚拟化有两种基本架构：寄居架构和裸金属架构，两种架构如下图所示。寄居架构由于基于传统的操作系统之上，所以性能消耗大，往往会对服务器性能影响很大。而裸金属架构基于专门为虚拟化而设计的虚拟化层而实现，大大降低了虚拟化引入的损耗，可以极大改善虚拟机的性能，是企业级数据中心进行虚拟化的首选架构。</p>
<p>因此，对用户来说，为了满足应用对性能的追求，建议采用企业级虚拟化架构――裸金属架构，这可以尽可能降低数据中心虚拟化对服务器性能的影响，一般影响可以降到10%以下。</p>
<p><img src="" alt=""></p>
<p>下图是采用裸金属架构虚拟化对应用性能的影响情况，这是VMware在中国某个用户现场的实测结果，已经很好说明了虚拟化带来的消耗是很低的。</p>
<p><img src="" alt=""></p>
<p>误区8：虚拟化技术仍然不成熟，数据中心虚拟化还不能提上议事日程</p>
<p>虚拟化技术已经获得了广泛地应用，财富100强的所有用户都已经部署了VMware的虚拟化解决方案，财富1000强中超过800家都是VMware的用户。实际上，VMware的企业级用户数量已经超过20000家，而所有用户的数量已经超过四百万家。VMware的服务器虚拟化方案已经久经考验，成为整个IT业界津津乐道的热点，虚拟化已经成为企业级用户构建新型数据中心的利器，成为值得信赖的可靠、稳定的企业级解决方案。</p>
<p>误区9：虚拟化由于引入了新的层次，会增加数据中心的管理难度</p>
<p>在数据中心引入虚拟化确实增加了一个虚拟化层，但并非因此而增加了管理难度。由于虚拟化的管理软件能够很好的管理控制虚拟平台的同时，简化了杂乱的服务器的管理，从而大大降低了大型数据中心的管理复杂性。如VMware  VirtualCenter就是很好的例证，Virtual Center提供了直观的管理界面，提供了丰富的资料和数据来监控整合虚拟化中心，为数据中心高效管理提供了强大的手段，成为新型虚拟化数据中心的必备工具。下图是Virtual center对虚拟机的管理界面。</p>
<p><img src="" alt=""></p>
<p>误区10：服务器虚拟化技术很美好，从原来架构迁移到虚拟架构耗时费力，而且可能风险巨大</p>
<p>如果迁移到虚拟化平台是很多用户的顾虑之一，因为虚拟化是一种架构决策。VMware已经进行了大量工作来简化从物理架构向虚拟架构的迁移，VMware Converter可以让用户不需要重新安装操作系统和应用，通过打包方式，将原来的物理服务器轻松迁移到虚拟平台上来。这不仅简化了流程，也降低了整个的迁移风险，目前很多企业级的用户都在享受VMware Converter所带来的好处。下图是VMware Converter的一个操作主界面，用户可以从VMware的网站免费下载VMware Converter的试用版来进行迁移试验。</p>
<p>1.3 网文：同类系统及其原理</p>
<p>最近笼统地学习和试用了几款比较有名的虚拟化管理软件。学习的内容包括Eucalyptus,  OpenNebula, OpenStack,  OpenQRM, XenServer, Oracle VM, CloudStack, ConVirt。借这一系列文章，对过去一个月的学习内容作一个阶段性的总结。</p>
<p>摘自<a href="http://zhumeng8337797.blog.163.com/blog/static/1007689142011112035330566/" target="_blank"><a href="http://zhumeng8337797.blog.163.com/blog/static/1007689142011112035330566/">http://zhumeng8337797.blog.163.com/blog/static/1007689142011112035330566/</a></a></p>
<p>（1）授权协议、许可证管理、购买价格等方面的比较</p>
<p><strong>授权协议</strong></p>
<p><strong>许可证管理</strong></p>
<p><strong>商业模式</strong> <strong>Eucalyptus</strong></p>
<p>社区版采用GPLv3授权协议</p>
<p>企业版使用自定义的商业授权协议</p>
<p>社区版不需要安装许可证</p>
<p>企业版需要在云控制器（CLC）节点上安装许可证</p>
<p>社区版免费使用</p>
<p>企业版按处理器核心总数收费，用户购买的许可证针对特定版本永久有效。 <strong>OpenStack</strong></p>
<p>Apache 2.0授权协议</p>
<p>不需要许可证</p>
<p>免费使用 <strong>OpenNebula</strong></p>
<p>Apache 2.0授权协议</p>
<p>不需要许可证</p>
<p>社区版免费使用</p>
<p>企业版将社区版重新打包，提供补丁等程序的访问权限，使得用户能够更容易的安装、配置和管理，以订阅的模式提供服务。</p>
<p>企业版按物理服务器总数收费，每台物理服务器器的服务价格为250欧元每年。 <strong>OpenQRM</strong></p>
<p>社区版使用GPLv2授权协议</p>
<p>企业版使用自定义的商业授权协议</p>
<p>不需要许可证</p>
<p>社区版免费使用</p>
<p>企业版将社区版重新打包，提供补丁等程序的访问权限，使得用户能够更容易的安装、配置和管理，以订阅的模式提供服务。基本、标准和高级服务的价格分别为480、960、1920欧元每月。 <strong>XenServer</strong></p>
<p>Citrix  XenServer系列产品均使用自定义的商业授权协议</p>
<p>基于XenServer的Xen Cloud  Platform使用GPLv2授权协议</p>
<p>不管是XenServer还是Xen Cloud  Platform都需要在每台服务器安装许可证</p>
<p>许可证每年更新一次</p>
<p>XenServer免费版本和开源版本的Xen Cloud Platform可以免费使用</p>
<p>XenServer高级版、企业版和白金版按物理服务器数量收费，分别是1000、2500和5000美元。购买的许可证针对特定版本永久有效 <strong>Oracle VM</strong></p>
<p>Oracle VM  Server是基于Xen开发的，使用GPLv2协议发布，从Oracle的网站可以下载到源代码，但是Oracle并不宣传这一点。</p>
<p>Oracle VM  Manager使用自定义的商业授权协议。</p>
<p>Oracle VM  VirtualBox的二进制版本使用自定义的商业授权协议，源代码使用GPLv2授权协议。</p>
<p>不需要许可证</p>
<p>免费使用，可以购买技术支持。技术支持的费用为每台物理服务器8184人民币每年。 <strong>CloudStack</strong></p>
<p>社区版采用GPLv3授权协议企业版使用自定义的商业授权协议</p>
<p>社区版不需要安装许可证</p>
<p>企业版需要在管理服务器上安装许可证</p>
<p>社区版免费使用企业版提供增强功能和技术支持，收费模式不详。 <strong>ConVirt</strong></p>
<p>社区版使用GPLv2授权协议</p>
<p>企业版使用自定义的商业授权协议</p>
<p>社区版不需要安装许可证</p>
<p>企业版需要在管理服务器上安装许可证</p>
<p>社区版免费使用</p>
<p>企业版提供增强功能和技术支持，按物理服务器数量收费，每个节点费用1090美元。购买的许可证针对特定版本永久有效。</p>
<p>（2）项目历史与运营团队、社区规模和活跃程度、沟通交流等方面的比较</p>
<p>   项目历史与运营团队社区规模和活跃程度沟通交流</p>
<p><strong>项目历史与运营团队</strong></p>
<p><strong>社区规模和活跃程度</strong></p>
<p><strong>沟通交流</strong> <strong>Eucalyptus</strong></p>
<p>最 初是UCSB的HPC研究项目，2009年初成立公司来支持该项目的商业化运营。现任CEO是曾担任MySQL CEO的Marten Mickos，现任工程部门SVP的Tim Cramerc曾担任  Sun公司NetBeans和OpenSolaris项目的执行总监。整个管理团队对开放源代码项目的管理和运营方面具有丰富的经验。</p>
<p>在 同类开放源代码项目当中，Eucalyptus的社区规模最大，活跃程度也最高。主要原因是该项目起源于大学研究项目，次要原因是管理团队对开放源代码理 念的高度认同。Ubuntu 10.04服务器版选择Eucalyptus作为UEC的基础构架，大大地促进了Eucalyptu的推广。</p>
<p>社区发表在论坛上的问题通常在48小时内得到回应，通过技术支持电子邮件提出的问题通常在24小时内得到回应。</p>
<p>Eucalyptus在北京和深圳设有办事处，在中国有工程师提供支持团队。 <strong>OpenStack</strong></p>
<p>OpenStack 是服务器托管公司RackSpace与NASA共同发起的开放源代码项目。在开放源代码项目的管理和运营方面，RackSpace和NASA显然缺乏足够  的经验。针对OpenStack项目的批评集中在（1）RackSpace对项目有过于强烈的控制欲，（2）OpenStack项目的运作对于社区成员来 说基本上是不透明的，（3）OpenStack项目对同类开放源代码项目的攻击性过強。</p>
<p>社 区规模较小，主要参与者为支持／参与该项目的公司人员。有几个公开的邮件列表，流量很小。由于该项目比较新，在网络上可以参考的安装与配置方面的文章不 多。Ubuntu 11.04服务器版同时支持Eucalyptus和OpenStack作为UEC的基础构架，将有助于OpenStack的推广。</p>
<p>通过邮件列表进行技术方面的沟通，通常在48小时内得到回应。商务方面的邮件沟通，没有得到回应。 <strong>OpenNebula</strong></p>
<p>2005年启动的研究性项目，2008年初发布第一个开放源代码版本，2010年初大力推进开源社区的建设。</p>
<p>社区规模较小，主要参与者为支持／参与该项目的公司人员，以及少量的用户。有几个公开的邮件列表，流量比OpenStack项目的流量稍大。在网络上搜索到一些中文版安装和配置方面的文章，基本上是以讹传讹，缺乏可操作性。英文版的相关文章也不多，可操作的更少。</p>
<p>通过邮件列表进行技术方面的沟通，通常在48小时内得到回应。 <strong>OpenQRM</strong></p>
<p>起源于集群管理方面的软件，2006年公开源代码，2008年免费发布，目前版本为4.8。</p>
<p>项目的运营团队较小，似乎只有Matt  Rechenburg一个人。</p>
<p>有一些零星的用户，基本上没有形成社区。虽然功能还在不断更新，但是用户文档的日期是2008年的。相关论坛的活跃程度比OpenStack和OpenNebula更差。</p>
<p>在论坛发布的问题，大约有50％左右没有得到回应。通过电子邮件进行商务沟通，反应迅速，在24小时以内得到回应。 <strong>XenServer</strong></p>
<p>Citrix公司的产品，与Xen项目的发展基本同步。</p>
<p>围绕Xen Cloud Platform有一些开放源代码的项目，用于替代XenCentor提供基于桌面或者是浏览器的管理功能。</p>
<p>初期商务沟通的速度比较快。 <strong>Oracle  VM</strong></p>
<p>Oracle公司的产品，用户量较小。Oracle  VM仅仅是Oracle用户生态系统中的一部分，不是Oracle的关键业务。</p>
<p>有一定数量的用户，但是没有形成社区。在网络上缺少与Oracle相关的讨论与交流。Oracle VM团队有一个博客网站，但是最近两篇文章的日期分别是2010年11月和2008年1 月。产品下载的速度很慢。</p>
<p>初期商务沟通的速度比较快。在技术方面的沟通，Oracle在国内没有相应的技术人员提供支持。 <strong>CloudStack</strong></p>
<p>源于2008年成立的VMOps公司，2010年五月启用cloud.com域名，2010年6 月共同启动OpenStack项目。</p>
<p>用户数量较少，论坛不是很活跃。官方文档非常完备，按照文档操作至少能够顺利地完成安装和配置过程。网络上可以搜索到一些可操作的安装和配置文档（得益于CloudStack的安装和配置比较简单）。</p>
<p>商务沟通比较困难，通过社区论坛和电子邮件提出的问题都没有得到回应。 <strong>ConVirt</strong></p>
<p>起源于2006年发起的XenMan项目，与Xen项目的发展基本同步。目前的版本为ConVirt 2.0。现任CEO和工程部门EVP均来自Oracle。</p>
<p>用户规模与Eucalyptus相当，论坛的活跃程度很高。官方文档非常完备，按照文档操作至少能够顺利地完成安装和配置过程。在网络上搜索到的中英文的安装配置教程也基本可用。</p>
<p>商务沟通非常顺畅，社区发表在论坛上的问题通常在48小时内得到回应，通过技术支持电子邮件提出的问题通常在24小时内得到回应。</p>
<p>（3）综合评估</p>
<p>总 的来说，虚拟化管理软件的用户还不是很多。大部分虚拟化管理软件的社区规模较小，活跃程度也不高。除了Eucalyptus积极地鼓励社区用户参与项目的 开发与测试之外，其他项目选择开放源代码只是一种营销策略。如果排除技术和价格方面的因素，最值得选择的软件无疑是Eucalyptus和  ConVirt。这两个项目拥有最大和最活跃的用户社区，其开发／运营团队与潜在客户之间的沟通最为顺畅。XenServer也是一个值得考虑的对象，但 是XenServer社区版要求对每台物理服务器都要每年更新一次许可证。对于拥有大量物理服务器的公司来说，管理和维护成千上百个许可证将是一个令人头 疼的问题。</p>
<p>架构篇：</p>
<p>（1）系统构架比较</p>
<p><strong>系统构架</strong> <strong>Eucalyptus</strong></p>
<p>Eucalyptus 是一个与Amazon EC2兼容的IaaS系统。Eucalyptus包括云控制器（CLC）、Walrus、集群控制器（CC）、存储控制器（SC）和节点控制器（NC）。 CLC是整个Eucalyptu系统的核心，负责高层次的资源调度，例如向CC请求计算资源。Walrus是 一个与Amazon S3类似的存储服务，主要用于存储虚拟机映像和用户数据。CC是一个集群的前端，负责协调一个集群内的计算资源，并且管理集群内的网络流量。SC是一个与 Amazon EBS类似的存储块设备服务，可以用来存储业务数据。NC是最终的计算节点，通过调用操作系统层的虚拟化技术来启动和关闭虚拟机。在同一个集群（CC）内 的所有计算节点（NC）必须在同一个子网内。 在一个集群（CC）内通常需要部署一台存储服务器（SC），为该集群内的计算节点提供数据存储服务。</p>
<p>Eucalyptus 通过Agent的方式来管理计算资源。在每一个计算节点上，都需要运行一个eucalyptus-nc的服务。该服务在集群控制器（CC）上注册后，云控 制器（CLC）即可通过集群控制器（CLC）将需要运行的虚拟机映像文件（EMI）拷贝到该计算节点上运行。</p>
<p>Eucalyptus 将虚拟机映像文件存储在Walrus上。当用户启动一个虚拟机实例的时候，Eucalyptus首先将相应的虚拟机映像（EMI）从Walrus拷贝到将 要运行该实例的计算节点（NC）上。当用户关闭（或者是由于意外而重启）一个虚拟机实例的时候，对虚拟机所做的修改并不会被写回到Walrus上原来的虚 拟机映像（EMI）上，所有对该虚拟机的修改都会丢失。如果用户需要保存修改过的虚拟机，就需要利用工具（euca2ools）将该虚拟机实例保存为新的  虚拟机映像（EMI）。如果用户需要保存数据，则需要利用存储服务器（SC）所提供的弹性块设备来完成。</p>
<p><a href="http://www.qyjohn.net/wp-content/uploads/2011/05/eee_arch.jpg" target="_blank"><img src="" alt=""></a> <strong>OpenStack</strong></p>
<p>OpenStack是一个与Amazon EC2兼容的IaaS系统。OpenStack包括OpenStack  Compute和OpenStack Object Storage两个部分。</p>
<p>OpenStack Compute又包含Web前端、计算服务、存储服务、身份认证服务、存储块设备（卷）服务、网络服务、任务调度等多个模块。OpenStack Compute的不同模块之间不共享任何信息，通过消息传递进行通讯。因此，不同的模块可以运行在不同的服务器上，也可以运行在同一台服务器上。
<a href="http://www.qyjohn.net/wp-content/uploads/2011/05/NOVA_ARCH.png" target="_blank"><img src="" alt=""></a></p>
<p>OpenStack Object Store可以利用通用服务器搭建可扩展的海量数据仓库，并且通过冗余来保证数据的安全性。同一份数据的在多台服务器上都有副本，将出现故障的服务器从集 群中撤除不会影响数据的完整性，加入新的服务器后系统会自动地在新的服务器上为相应的文件创建新的副本。从功能上讲，OpenStack Object Store同时具备Eucalyptus中的Walrus服务和弹性块设备（SC）服务。不过OpenStack Object Store不是一个文件系统，不能够保证数据的实时性。从这个方面来考虑，OpenStack Object Store更适合用于存储需要长期保存的静态数据，例如操作系统映像文件和多媒体数据。</p>
<p><a href="http://www.qyjohn.net/wp-content/uploads/2011/05/os-os.png" target="_blank"><img src="" alt=""></a></p>
<p>OpenStack通过Agent的方式来管理计算资源。在每一个计算节点上，都需要运行nova- network服务和nova-compute服务。这些服务启动之后，就可以通过消息队列来与云控制器进行交互。</p>
<p>  <strong>OpenNebula</strong></p>
<p>OpenNebula 的构架包括三个部分：驱动层、核心层、工具层。驱动层直接与操作系统打交道，负责虚拟机的创建、启动和关闭，为虚拟机分配存储，监控物理机和虚拟机的运行 状况。核心层负责对虚拟机、存储设备、虚拟网络等进行管理。工具层通过命令行界面／浏览器界面方式提供用户交互接口，通过API方式提供程序调用接口。</p>
<p><a href="http://www.qyjohn.net/wp-content/uploads/2011/05/one-architecture.png" target="_blank"><img src="" alt=""></a></p>
<p>OpenNebula 使用共享存储设备（例如NFS）来提供虚拟机映像服务，使得每一个计算节点都能够访问到相同的虚拟机映像资源。当用户需要启动或者是关闭某个虚拟机 时，OpenNebula通过SSH登陆到计算节点，在计算节点上直接运行相对应的虚拟化管理命令。这种模式也称为无代理模式，由于不需要在计算节点上安 装额外的软件（或者服务），系统的复杂度也相对降低了。</p>
<p><a href="http://www.qyjohn.net/wp-content/uploads/2011/05/one-sample-arch2.png" target="_blank"><img src="" alt=""></a> <strong>OpenQRM</strong></p>
<p>OpenQRM 是为了管理混合虚拟化环境而开发的一个虚拟化管理框架，包括基础层（框架层）和插件。基础层（框架）的作用是管理不同的插件，而对虚拟资源的管理（计算资 源，存储资源，映像资源）都是通过插件来实现的。OpenQRM的框架类似于Java语言中的Interface，定义了一系列虚拟机资源生命周期管理的 方法，例如创建、启动、关闭虚拟机等等。在个框架的基础上，OpenQRM针对不同的虚拟化平台（Xen、KVM)实现了不同的插件，用来管理不同的物理 和虚拟资源。当出现新的资源需要支持的时候，只需要为OpenQRM编写新的插件，就可以无缝地整合到原来的环境中去。</p>
<p><a href="http://www.qyjohn.net/wp-content/uploads/2011/05/openqrm-architecture.png" target="_blank"><img src="" alt=""></a></p>
<p>OpenQRM 插件也是使用无代理模式工作的。当需要管理的目标节点提供SSH登录方式时，OpenQRM插件通过SSH登陆到计算节点，在计算节点上直接运行相对应的 虚拟化管理命令。当需要管理的目标节点提供HTTP／HTTPS／XML－RPC远程调用接口时，OpenQRM插件通过目标节点所提供的远程调用接口实 现对目标平台的管理。</p>
<p>OpenQRM是一个虚拟化管理平台，不提供与Amazon EC2兼容的云管理接口。</p>
<p>  <strong>XenServer</strong></p>
<p>XenServer 是对Xen虚拟化技术的进一步封装，在Dom0上提供一系列命令行和远程调用接口，独立的管理软件XenCenter通过远程调用这些接口来管理多台物理 服务器。XenSever在标准Xen实现之上所实现的远程调用接口类似于其他虚拟化管理平台中所实现的Agent，因此XenServer是通过 Agent方式工作的。由于只考虑对Xen虚拟化技术的支持，XenServer的构架相对简单。</p>
<p><a href="http://www.qyjohn.net/wp-content/uploads/2011/05/xcp-architecture.png" target="_blank"><img src="" alt=""></a></p>
<p>XenServer 是一个虚拟化管理平台，不提供与Amazon EC2兼容的云管理接口。管理软件XenCenter是运行在Windows操作系统上的，对于需要随时随地访问管理功能的系统管理员来说有点不便。目前 有一些第三方提供的开放源代码的基于浏览器的XenServer管理工具，但是都还处于比较早期的阶段。</p>
<p>  <strong>Oracle  VM</strong></p>
<p>Oracle VM包括Oracle VM Server和Oracle VM Manager两个部分。Oracle VM Server在支持Xen的Oracle Linux上（Dom0）运行一个与Xen交互的Agent，该Agent为Oracle VM  Manager提供了远程调用接口。Oracle VM Manager通过一个Java应用程序来对多台Oracle VM Server上的虚拟资源进行管理和调度，同时提供基于浏览器的管理界面。由于只考虑对Xen虚拟化技术的支持，Oracle VM Server / Manager的构架相对简单。</p>
<p><a href="http://www.qyjohn.net/wp-content/uploads/2011/05/GW430.gif" target="_blank"><img src="" alt=""></a></p>
<p>Oracle VM是一个虚拟化管理平台，不提供与Amazon EC2兼容的云管理接口。</p>
<p>值 得注意的是，Oracle VM Manager还通过Web Service的方式提供了虚拟机软件生命周期管理的所有接口，使得用户可以自己使用不同的编程语言来调用这些接口来开发自己的虚拟化管理平台。不过由于 Oracle在开放源代码方面的负面形象，似乎没有看到有这方面的尝试。</p>
<p>  <strong>CloudStack</strong></p>
<p>与 OpenQRM类似，CloudStack采用了“框架 ＋ 插件”的系统构架，通过不同的插件来提供对不同虚拟化技术的支持。对于标准的Xen / KVM计算节点，CloudStack需要在计算节点上安装Agent与控制节点进行交互；对于XenServer / VMWare计算节点，CloudStack通过XenServer / VMWare所提供的XML-RPC远程调用接口与计算节点进行交互。</p>
<p><a href="http://www.qyjohn.net/wp-content/uploads/2011/05/Cloud.com-Architecture.jpg" target="_blank"><img src="" alt=""></a></p>
<p>CloudStack本身是一个虚拟化管理平台，但是它通过CloudBridge提供了与Amazon EC2相兼容的云管理接口，对外提供IaaS服务。</p>
<p>  <strong>ConVirt</strong></p>
<p>ConVirt 是一个虚拟化管理平台，使用无代理模式工作。当需要管理的目标节点提供SSH登录方式时，ConVirt通过SSH登陆到计算节点，在计算节点上直接运行 相对应的虚拟化管 理命令。当需要管理的目标节点提供HTTP／HTTPS／XML－RPC远程调用接口时，ConVirt插件通过目标节点所提供的远程调用接口实现对目标 平台的管理。</p>
<p><a href="http://www.qyjohn.net/wp-content/uploads/2011/05/799px-Architecture.png" target="_blank"><img src="" alt=""></a></p>
<p>ConVirt 是一个虚拟化管理平台，不提供与Amazon EC2兼容的云管理接口。但是ConVirt  3.0提供了与Amazon EC2 / Eucalyptus的用户接口，使得ConVirt用户能够在同一个Web  管理界面下同时管理Amazon EC2 / Eucalyptus提供的虚拟计算资源。</p>
<p>（2）云管理平台还是虚拟化管理平台？</p>
<p>在IaaS这个层面，云管理和虚拟化管理的概念非常接近，但是有一些细微的差别。</p>
<p>虚 拟化是指在同一台物理机器上提供多台虚拟机器（包括CPU、内存、存储、网络等计算资源）的能力。每一台虚拟机器都能够像普通的物理机器一样运行完整的操 作系统以及执行正常的应用程序。当需要管理的物理机器数量较小时，虚拟机生命周期管理（资源配置、启动、关闭等等）可以通过手工去操作。当需要管理的物理 机器数量较大时，就需要写一些脚本／程序来提高虚拟机生命周期管理的自动化程度。以管理和调度大量物理／虚拟计算资源为目的系统，属于虚拟化管理系统。这 样一个系统，通常用于管理企业内部计算资源。</p>
<p>云 计算是指通过网络访问物理／虚拟计算机并利用其计算资源的实践。通常来讲，云计算提供商以虚拟机的方式向用户提供计算资源。用户无须了解虚拟机背后实际的 物理资源状况，只需了解自己所能够使用的计算资源配额。因此，虚拟化技术是云计算的基础。任何一个云计算管理平台，都是构建在虚拟化管理平台的基础之上 的。如果某个虚拟化管理平台仅对某个集团内部提供服务，那么这个虚拟化管理平台也可以被称为“私有云”；如果某个虚拟化管理平台对公众提供服务，那么这个 虚拟化管理平台也可以被称为“公有云”。服务对象的不同，对虚拟化管理平台的构架和功能提出了不同的需求。</p>
<p>私 有云服务于集团内部的不同部门（或者应用），强调虚拟资源调度的灵活性。系统管理员需要为不同的部门（或者应用）定制不同的虚拟机，根据部门（或者应用） 对计算资源的需求对分配给某些虚拟机的计算资源进行调整。从这个意义上来讲，OpenQRM、XenServer、Oracle VM、CloudStack和ConVirt比较适合提供私有云服务。</p>
<p>公 有云服务于公众，强调虚拟资源的标准性。通过将计算资源切割成标准化的虚拟机配置（多个系列的产品，每个产品配置相同数量的CPU、内存、磁盘空间、网络 流量配额），公有云提供商可以通过标准的服务合同（Service  Level Agreement, SLA）以标准的价格出售计算资源。当用户对计算资源的需求出现改变的时候，用户只需要缩减或者是增加自己所使用的产品数量。由于Amazon  EC2是目前比较成功的公有云提供商，大部分云管理平台都在某种程度上模仿Amazon EC2的构架。从这个意义上来讲，Eucalyptus、OpenNebula和OpenStack提供了与Amazon EC2兼容或者是类似的接口，比较适合提供公有云服务。</p>
<p>公有云和私有云之间的界限，就像“内部／外部”和“部门／合作伙伴”的概念一样，并不十分明显。根据项目需求的不同，可能会有不同的解释。</p>
<p>功能篇：</p>
<p>（1）支持的虚拟化技术</p>
<p><strong>Xen</strong></p>
<p><strong>KVM</strong></p>
<p><strong>XenServer  / XCP</strong></p>
<p><strong>VMWare</strong></p>
<p><strong>LXC</strong></p>
<p><strong>openVZ</strong> <strong>Eucalyptus</strong></p>
<p>Y</p>
<p>Y</p>
<p>Y</p>
<p>  <strong>OpenStack</strong></p>
<p>Y</p>
<p>Y</p>
<p>Y</p>
<p>Y</p>
<p>Y</p>
<p>  <strong>OpenNebula</strong></p>
<p>Y</p>
<p>Y</p>
<p>Y</p>
<p>  <strong>OpenQRM</strong></p>
<p>Y</p>
<p>Y</p>
<p>Y</p>
<p>Y</p>
<p>Y</p>
<p>Y <strong>XenServer</strong></p>
<p>Y</p>
<p>  <strong>Oracle VM</strong></p>
<p>Y</p>
<p>  <strong>CloudStack</strong></p>
<p>Y</p>
<p>Y</p>
<p>Y</p>
<p>  <strong>ConVirt</strong></p>
<p>Y</p>
<p>Y</p>
<p>可以看出，Xen和KVM是目前获得最广泛的厂商虚拟化技术，紧随其后的是VMWare。需要注意的是，XenServer是对Xen的进一步封装，可以认为是一种新的虚拟化平台（用户在XenServer上不能直接执行Xend相关命令）。</p>
<p>（2）系统安装和配置</p>
<p>（3）前端  计算节点 备注</p>
<p><strong>前端</strong></p>
<p><strong>计算节点</strong></p>
<p><strong>备注</strong> <strong>Eucalyptus</strong></p>
<p>使用Ubuntu 10.04或者CentOS  5.5操作系统，通过apt-get  install或者yum install的方式直接安装二进制包，构建一个包含CLC、 Walrus、SC、CC的前端。根据官方网站提供的文档进行操作，是比较容易实现的。</p>
<p>使用Ubuntu 10.04或者CentOS 5.5操作系统，通过apt-get install或者yum  install的方式直接安装二进制包，构建一个提供NC服务的计算节点。根据官方网站提供的文档进行操作，是比较容易实现的。</p>
<p>Eucalyptus 包含了一个dhcpd，如果配置不好的话，会造成一定的麻烦。另外，计算节点（NC）与集群控制器（CC）必须在一个C类子网里（例如，掩码为  255.255.255.0）。如果NC和CC在一个超网里（例如，掩码为255.255.0.0），在注册服务的时候会出现一些问题。</p>
<p>  <strong>OpenStack</strong></p>
<p>在Ubuntu 10.04上利用官方网站提供的nova- install脚本进行安装，基本上没有遇到问题。</p>
<p>在Ubuntu 10.04上利用官方网站提供的nova- install脚本进行安装，基本上没有遇到问题。</p>
<p>对于一个简单的系统，安装配置比较简单。 <strong>OpenNebula</strong></p>
<p>使 用CentOS 5.5操作系统，配置好CentOS Karan源，启用kbs- CentOS- Testing条目。下载对应的rpm包，直接yum localinstall  -nogpgcheck  opennebula/*.rpm，就可以直接完成安装过程。按照官方文档创建/srv/cloud/one和/srv/cloud/images目录，通 过NFS共享/srv/cloud目录。创建cloud用户组和属于cloud用户组的oneadmin用户。</p>
<p>按照官方文档创建/srv/cloud/one和/srv/cloud/images目录，通过NFS共享/srv/cloud目录。创建cloud用户组和属于cloud用户组的oneadmin用户。</p>
<p>将前端服务器上oneadmin用户的ssh  key拷贝到计算节点上oneadmin用户的authorized_keys中。这样前端服务器才可以通过SSH登陆到计算节点上。</p>
<p>在CentOS  5.5 x86_64上进行安装的时候，如果按照官方网站提供的文档进行操作，先配置好必要的软件依赖关系再安装opennebula，就会出现xmlrpc-c包版本不对的错误。</p>
<p>网络上可以搜索到一些安装配置方面的文档和教程，但是对于熟悉Linux但是不熟悉OpenNebula的开发人员来说，很难按照这些文档完成安装和配置过程。</p>
<p>  <strong>OpenQRM</strong></p>
<p>在Ubuntu 10.04上通过SVN下载OpenQRM源代码，进入源代码目录后依次执行make / make  install / make start命令。按照官方文档的描述创建数据库，然后通过Web界面进行下一步的安装和配置。</p>
<p>计算节点配置好网桥和虚拟化支持之外不需要特别的安装和配置。在OpenQRM管理界面中启用相对应的插件即可通过插件对计算节点进行管理。</p>
<p>在Ubuntu  10.04上安装前端时，可能需要手工安装dhcp3- server。</p>
<p>启用插件管理虚拟资源的操作流程不够直观，并且缺乏详细的文档。</p>
<p>  <strong>XenServer</strong></p>
<p>前端为基于Windows操作系统的XenCenter。在Windows XP上可以安装，需要.NET  Framework Update 2的支持。安转过程非常简单，基本上不需要配置。</p>
<p>从Citrix的网站下载ISO，刻盘直接安装在裸机上即可。计算节点安装完毕后，在XenCenter中把新增计算资源添加到资源池即可。</p>
<p>每一台XenServer服务器都需要安装从Citrix获得License，并且每年更新一次。 <strong>Oracle VM</strong></p>
<p>在CentOS 5.5 x86_64上进行安装。将ISO文件mount起来后，执行runinstaller.sh即可。</p>
<p>从Oracle的网站下载ISO，刻盘直接安装在裸机上即可。计算节点安装完毕后，在Oracle VM Manager中把新增计算资源添加到资源池即可。</p>
<p>最好从Oracle的官方网站下载，不过速度很慢。通过迅雷等途径下载的文件，看起来似乎没有问题，但是ISO刻盘后在启动操作系统安装过程中会出现错误。</p>
<p>如果在Oracle  VM Server上安装Oracle  VM  Manager，建议分区的时候把/ 分得大一点，不然的话会由于磁盘空间不够而无法安装Oracle VM  Manager。</p>
<p>  <strong>CloudStack</strong></p>
<p>在CentOS 5.5和Ubuntu 10.4上，按照官方网站的安装文档顺序操作，基本没有问题。</p>
<p>计算节点上必须安装相应的Agent。</p>
<p>安装配置相对简单，但是在删除物理资源的时候存在较多的问题。 <strong>ConVirt</strong></p>
<p>在CentOS 5.5和Ubuntu 10.4上，按照官方网站的安装文档顺序操作，基本没有问题。</p>
<p>在Ubuntu 10.04上安装企业版，需要手工sudo apt- get install  libmysqlclient- dev。</p>
<p>在计算节点上的root用户必须允许管理节点上运行ConVirt服务的用户通过key auth方式登录。</p>
<p>安装配置相对简单。</p>
<p>不 同的虚拟化管理软件有不同的设计理念，采用不同的系统构架，类似的概念也采用不同的术语来表述，其学习曲线也各不相同。对于大部分用户来说，虚拟化管理软 件还是个新生事物。即使是粗略地尝试一下利用不同的虚拟化管理软件来安装、配置和测试一个最小规模的私有云系统，也需要花费不少的时间和精力。在这个过程 当中，遇见各种各样的问题都在所难免。不过，也只有亲身经验过这些形形色色的问题，才能够切身体会不同虚拟化管理软件的优点和缺点，并且在分析、总结、归 纳的基础上形成自己独特的观点。</p>
<p>用户界面</p>
<p><strong>概述</strong></p>
<p><strong>用户权限</strong></p>
<p><strong>资源池和虚拟机管理</strong> <strong>Eucalyptus</strong></p>
<p>Eucalyptus提供了一个基于浏览器的简单用户界面，可以完成用户注册，下载credentials，对提供的产品类型进行简单配置等。资源池和虚拟机生命周期管理需要通过euca2ools在命令行模式下完成。</p>
<p>euca2ools是一组基于命令行的工具，可以与Amazon  EC2/S3相兼容的Web  Service进行交互。该用具可以管理基于Amazon EC2、Eucalyptus和OpenStack，OpenNebula的云计算服务。</p>
<p>euca2tools的主要功能包括：</p>
<ul>
<li>查询可以使用的域</li>
<li>管理SSH Key</li>
<li>虚拟机生命周期管理</li>
<li>安全组管理</li>
<li>管理卷和快照</li>
<li>管理虚拟机映像</li>
<li>管理IP</li>
</ul>
<p>在Eucalyptus社区版中只有两种类型的用户：管理员，普通用户。在Eucalyptus企业版中进一步提供了用户组，属于某个用户组的用户可以管理属于该用户组的计算资源。</p>
<p>管理员可以通过注册或者是撤销注册某个计算节点，配置标准产品类型的计算资源（CPU、内存、存储）。普通用户只能够在标准配置的基础上创建、启动、关闭虚拟机，不能够定制化自己所需要的计算资源。</p>
<p>虚 拟机映像文件（EMI）的制作，以及虚拟机生命周期管理等等操作，需要通过euca2ools在命令行模式下完成。在FireFox浏览器中，可以利用 ElasticFox插件，在浏览器中启动、监控和关闭虚拟机。ElasticFox的界面不够美观，并且提供的功能非常有限。</p>
<p>Eucalyptus不提供console功能。用户可以通过SSH连接到自己所管理的虚拟机。</p>
<p>每一个公开发布的虚拟机映像（EMI），都是一个模板。用户创建虚拟机实例的时候，系统根据用户选择的EMI将相应的虚拟机映像拷贝到目标计算节点上运行。Eucalyptus根据某种算法自动决定用户的虚拟机将在哪个物理服务器上运行，用户对物理服务器的状况一无所知。</p>
<p>Eucalyptus 中的虚拟机实例只是原虚拟机映像（EMI）的一个副本，用户在运行的实例中对虚拟机所做的任何修改，不会被保存到原来的虚拟机映像中。如果用户将运行的虚  拟机实例关闭（例如：shutdown），用户对虚拟机所作的任何修改都会丢失。如果用户需要保存自己对虚拟机所做的修改，用户可以选择使用弹性块设备来 保存数据，或者将正在运行的虚拟机实例发布为新的EMI。（Amazon EC2自动地将停止运行的虚拟机实例保存为新的AMI，直到用户销毁该虚拟机实例为止。因此，用户可以shutdown自己的虚拟机实例，但是保存自己对 虚拟机所作的修改，直到用户选择销毁该虚拟机实例为止。）</p>
<p>  <strong>OpenStack</strong></p>
<p>OpenStack 不缺省地提供基于浏览器的用户界面。系统管理员需要手工创建用户。大部分的管理操作，需要在命令行下进行。 尽管OpenStack和Eucalyptus在构架上有很大的不同，但是所暴露给用户的界面是类似的（两者都模仿了Amazon EC2的用户接口规范）。因此，OpenStack同样可以使用Eucalyptus所提供的euca2ools进行管理。</p>
<p>OpenStack的openstack- dashboard项目和django- nova项目提供了一个基于浏览器的用户界面，没有被集成到OpenStack安装脚本中，需要单独安装。</p>
<p>OpenStack将用户分成如下几个类别：</p>
<p>admin - 云服务管理员，拥有所有管理权限。</p>
<p>itsec - IT安全管理员，具有隔离有问题的虚拟机实例的权限。</p>
<p>projectmanager - 项目管理员，可以增加属于该项目的新用户，管理虚拟机映像，管理虚拟机生命周期。</p>
<p>netadmin - 网络管理员，负责IP分配，管理防火墙。</p>
<p>developer - 开发人员，可以登录进入属于本项目的虚拟机，管理虚拟机生命周期</p>
<p>在模仿Amazon EC2的云平台（Eucalyptus,  OpenStack,  OpenNebula）中，OpenStack提供了颗粒度最细的用户权限管理模式。</p>
<p>与Eucalyptus类似，虚拟机映像文件（EMI）的制作，以及虚拟机生命周期管理等等操作，需要通过euca2ools在命令行模式下完成。同样，在FireFox浏览器中，可 以利用ElasticFox插件，在浏览器中启动、监控和关闭虚拟机。</p>
<p>OpenStack不提供虚拟机console功能。用户可以通过SSH连接到自己所管理的虚拟机。</p>
<p>正在开发中的openstack- dashboard，基于浏览器提供了比较完整的资源池管理功能和虚拟机生命周期管理功能。虽然界面还比较简单，但是已经处于可用的状态。</p>
<p>OpenStack的模板和虚拟机实例机制与Eucalyptus类似。与Eucalyptus类似，OpenStack根据某种算法自动决定用户的虚拟机将在哪个物理服务器上运行，用户对物理服务器的状况一无所知。</p>
<p>  <strong>OpenNebula</strong></p>
<p>OpenNebula不缺省地提供基于浏览器的用户界面。系统管理员需要手工创建用户。大部分的管理操作，需要在命令行下进行。</p>
<p>OpenNebula目前有两个基于浏览器的用户界面：SunStone和OneMC。这两个项目需要单独安装。</p>
<p>同样，OpenNebula提供了与Amazon EC2相兼容的Web Service接口。因此，可以通过FireFox所提供的ElasticFox插件和Eucalyptus提供的euca2ools工具集与OpenNebula云平台进行交互。</p>
<p>OpenNebula只有两种类型的用户：管理员，普通用户。</p>
<p>在早期版本中，OpenNebula管理员可以在后台通过命令行来管理资源池和虚拟机生命周期。 同样，在FireFox浏览器中，可 以利用ElasticFox插件，在浏览器中启动、监控和关闭虚拟机。</p>
<p>SunStone和OneMC这两个项目都提供了比较完整的资源池管理和虚拟机生命周期管理功能。两个项目的界面都比较简单，但是基本上处于可用的状态。SunStone没有提供虚拟机console功能，OneMC通过VNC协议提供了虚拟机console功能。</p>
<p>OpenNebula的模板和虚拟机实例机制与Eucalyptus类似。但是并不缺省地使用euca2ools作为工具。</p>
<p>与Eucalyptus类似，OpenNebula根据某种算法自动决定用户的虚拟机将在哪个物理服务器上运行，用户对物理服务器的状况一无所知。</p>
<p>  <strong>OpenQRM</strong></p>
<p>基于浏览器的用户界面，功能比较丰富。</p>
<p>OpenQRM的管理界面只有两种用户：管理用户，普通用户。普通用户只有查看权限，没有管理权限。</p>
<p>通过启用不同的插件，可以管理不同的计算资源。所有的资源池和虚拟机生命周期管理操作都可以通过浏览器界面完成。</p>
<p>OpenQRM的novnc插件可以提供基于VNC协议的虚拟机console功能。</p>
<p>  <strong>XenServer</strong></p>
<p>XenCenter是基于Windows的桌面应用，安装与操作都非常简单，界面美观，功能强大。</p>
<p>在参与评测的8 个软件中，XenCenter的用户界面是表现最出色的。基于Windows桌面的应用能够迅速地对用户的点击动作作出反应，从而提高用户体验的满意度。</p>
<p>系统管理员登录XenCenter之后，可以结合Active Directory在用户和用户组的层面分配管理权限。</p>
<p>授权用户可以通过图形界面方便地进行资源池和虚拟机生命周期管理。在图形界面上可以直观地监控物理服务器和虚拟机的计算资源使用情况（CPU、内存、存储、网络活动）。</p>
<p>提供基于VNC的虚拟机console。</p>
<p>可以基于模板的部署新的虚拟机。</p>
<p>  <strong>Oracle VM</strong></p>
<p>Oracle VM Manager提供了基于浏览器的管理界面。</p>
<p>Oracle VM Manager同时提供了role和group的概念。其中role定义了用户所具备的权限，属于同一个group的用户拥有该group所被授予的权限。</p>
<p>Oracle VM Manager提供了三种role：</p>
<p>user - 拥有指定资源池的虚拟机生命周期管理权限。</p>
<p>manager - 拥有除了用户管理之外的所有管理权限。</p>
<p>administrator - 拥有整个系统的管理权限。</p>
<p>授权用户可以通过图形界面方便地进行资源池和虚拟机生命周期管理。在图形界面上可以直观地监控物理服务器和虚拟机的计算资源使用情况（CPU、内存、存储、网络活动）。</p>
<p>提供基于VNC的虚拟机console。</p>
<p>可以基于模板的部署新的虚拟机。 <strong>CloudStack</strong></p>
<p>基于浏览器的用户界面，功能丰富，美观大方。</p>
<p>CloudStack根据用户的role将用户分成三个类型：</p>
<p>admin - 全局管理员。</p>
<p>domain－admin - 域管理员，可以对某个域下的物理和虚拟资源进行管理。</p>
<p>user - 个体用户，可以管理自己名下的虚拟机资源。</p>
<p>CloudStack 对物理资源的管理完整地模拟了一个物理机房的实际情况，按照“机房（Zones）－》机柜（Pods）－》集群（Cluster）－》服务器 （Server）”的结构对物理服务器进行组织，使得管理员能够在管理界面里面的计算资源和机房里面的计算资源建立起直观的一一对应关系。</p>
<p>授权用户可以通过图形界面方便地进行资源池和虚拟机生命周期管理。在图形界面上可以直观地监控物理服务器和虚拟机的计算资源使用情况（CPU、内存、存储、网络活动）。</p>
<p>提供基于VNC的虚拟机console。</p>
<p>可以基于模板的部署新的虚拟机。</p>
<p>  <strong>ConVirt</strong></p>
<p>基于浏览器的用户界面，功能丰富，美观大方。</p>
<p>社区版可以注册多个用户，并可将用户按照用户组进行分类，但是所有的用户拥有相同的全局管理权限。企业版则提供了更细致的用户权限管理机制。除此之外，企业版还提供了对LDAP的支持。</p>
<p>授权用户可以通过图形界面方便地进行资源池和虚拟机生命周期管理。在图形界面上可以直观地监控物理服务器和虚拟机的计算资源使用情况（CPU、内存、存储、网络活动）。提供基于VNC的虚拟机console。</p>
<p>可以基于模板的部署新的虚拟机。</p>
<p>ConVirt 的最大优点，在于其通过时程图的方式在不同的层次上直观地展示计算资源（包括物理资源和虚拟资源）的利用情况和健康状况。在整个数据中心和资源池的层 面，ConVirt实时显示资源池数量、物理服务器和虚拟机数量、虚拟机密度、存储资源使用状况、负载最高的N 台物理服务器和虚拟机。在物理服务器和虚拟机的层面，ConVirt实时显示CPU和内存使用情况，监控人员可以通过CPU和内存时程图及时地发现或者是 调查系统异常情况。</p>
<p>在 所有参与评测的虚拟化管理软件中，XenServer / XCP和ConVirt的图形用户界面是做的最好的。XenCenter的图形界面的优点在于提供了独一无二的用户体验，ConVirt的图形界面的优点 在于以图形的方式直观地展示了从机房到虚拟机的健康状况。CloudStack的图形界面非常大气，但是在功能上不如ConVirt那么实用。不过按照 CloudStack的目前的发展势头来看，下一个版本可能比较值得期待。</p>
<p>由于进行评测的时间较短，并且测试系统规模较小的原因，暂时无法对各个软件的稳定性、健壮性、扩展性等等关键问题作出评估。</p>
<p>商务篇：</p>
<p>目前市面上形形色色的虚拟化管理软件总数很多，这一系列文章所提及的几个软件仅仅其中的几个代表。作为一个机构、或者是一家企业，在向虚拟化过渡时都不可避免地要面临软件选型的问题。本文作为这一系列文章的最后一篇，从商务和功能两个方面提出自己的一点粗浅意见。</p>
<p>（1）商务评估</p>
<p>从 商务上进行软件选型，性价比通常是一个决定性的因素。在假定参与选型的软件全部满足技术要求的前提下，企业（机构）需要考虑的因素包括软件的授权协议是否 友好、许可证管理的难易程度、软件和服务的价格高低、运营团队在业界的声誉、开发者社区和用户社区的规模和活跃程度、商业与技术沟通的难易程度。</p>
<p>授 权协议/许可证管理 — 以全部开放源代码为10分，部分开放源代码（例如以企业版的形式提供某些高级功能，或者以服务的形式提供特别版本的安装包和补丁）扣1 分。商业版本需要在控制节点安装许可证不扣分，需要在所有计算节点安装许可证扣1 分，许可证需要每年更新者扣1 分。</p>
<p>价格指数 — 以全部功能免费使用为10分，以企业版的模式提供全部功能的软件，每台物理服务器每花费500美元扣1  分。</p>
<p>运营团队 — 以运营团队的规模、背景、影响力评分，存在的主观因素较多。</p>
<p>社区因素 — 以开发者和用户社区的规模和活跃程度评分，存在的主观因素较多。</p>
<p>沟通交流 — 以个人与运营团队、开发者社区、用户社区之间的沟通顺畅程度评分，存在的主观因素较多。</p>
<p>   授权协议</p>
<p><strong>授权协议</strong></p>
<p><strong>许可证管理</strong></p>
<p><strong>价格指数</strong></p>
<p><strong>运营团队</strong></p>
<p><strong>社区因素</strong></p>
<p><strong>沟通交流</strong></p>
<p><strong>总分</strong> <strong>Eucalyptus</strong></p>
<p>9</p>
<p>8</p>
<p>9</p>
<p>9</p>
<p>10</p>
<p>45 <strong>OpenStack</strong></p>
<p>10</p>
<p>10</p>
<p>8</p>
<p>8</p>
<p>7</p>
<p>43 <strong>OpenNebula</strong></p>
<p>9</p>
<p>9</p>
<p>7</p>
<p>8</p>
<p>9</p>
<p>42 <strong>OpenQRM</strong></p>
<p>9</p>
<p>8</p>
<p>6</p>
<p>7</p>
<p>8</p>
<p>37 <strong>XenServer</strong></p>
<p>7</p>
<p>8</p>
<p>9</p>
<p>10</p>
<p>9</p>
<p>43 <strong>Oracle VM</strong></p>
<p>9</p>
<p>7</p>
<p>7</p>
<p>6</p>
<p>7</p>
<p>36 <strong>CloudStack</strong></p>
<p>9</p>
<p>8</p>
<p>7</p>
<p>6</p>
<p>7</p>
<p>37 <strong>ConVirt</strong></p>
<p>9</p>
<p>8</p>
<p>8</p>
<p>9</p>
<p>10</p>
<p>44</p>
<p>（2）功能评估</p>
<p>从功能上进行虚拟化管理软件选型，需要考虑的因素包括该软件所支持的虚拟化技术、安装配置的难易程度、开发和使用文档的详尽程度、所提供的功能是否全面以及用户界面是否直观友好、二次开发的难易程度、是否提供物理资源和虚拟资源的监控报表等等。</p>
<p>虚拟化技术支持 — 仅支持一种虚拟化技术为6 分，每增加一种虚拟化技术加1 分，10分封顶。</p>
<p>安装配置 — 以按照官方文档进行安装配置的难易程度评分，存在的主观因素较多。</p>
<p>开发/使用文档 — 以官方所提供的开发与使用文档的详尽程度评分，文档详尽程度越高者得分越高。</p>
<p>功能与界面 — 综合评分，涵盖用户进行物理资源和虚拟资源管理、虚拟机生命周期管理、访问虚拟机资源和存储资源的难易程度，用户界面的美观易用程度，以及综合用户体验。</p>
<p>二次开发 — 基础得分6 分，提供与Amazon EC2相兼容的程序调用接口者加3 分，提供二次开发接口但是与Amazon  EC2不兼容者加2 分。</p>
<p>监控报表 — 基础得分6 分，依系统所提供监控与分析功能的详尽程度加分。</p>
<p><strong>虚拟化技术支持</strong></p>
<p><strong>安装配置</strong></p>
<p><strong>开发／使用文档</strong></p>
<p><strong>功能与界面</strong></p>
<p><strong>二次开发</strong></p>
<p><strong>监控报表</strong></p>
<p><strong>总分</strong> <strong>Eucalyptus</strong></p>
<p>8</p>
<p>8</p>
<p>9</p>
<p>4</p>
<p>9  (Amazon  WS)</p>
<p>6</p>
<p>44 <strong>OpenStack</strong></p>
<p>10</p>
<p>8</p>
<p>8</p>
<p>4</p>
<p>9  (Amazon  WS)</p>
<p>6</p>
<p>45 <strong>OpenNebula</strong></p>
<p>8</p>
<p>8</p>
<p>7</p>
<p>4</p>
<p>9  (Amazon  WS)</p>
<p>6</p>
<p>42 <strong>OpenQRM</strong></p>
<p>10</p>
<p>9</p>
<p>5</p>
<p>10</p>
<p>6 (OS)</p>
<p>7</p>
<p>47 <strong>XenServer</strong></p>
<p>6</p>
<p>10</p>
<p>10</p>
<p>10</p>
<p>8 (Plugin)</p>
<p>9</p>
<p>53 <strong>Oracle VM</strong></p>
<p>6</p>
<p>9</p>
<p>8</p>
<p>7</p>
<p>8 (WS)</p>
<p>7</p>
<p>45 <strong>CloudStack</strong></p>
<p>8</p>
<p>9</p>
<p>8</p>
<p>10</p>
<p>6 (OS)</p>
<p>8</p>
<p>49 <strong>ConVirt</strong></p>
<p>7</p>
<p>10</p>
<p>10</p>
<p>10</p>
<p>8 (API)</p>
<p>10</p>
<p>55</p>
<p>（3）综合评估</p>
<p>从 商务上考虑，Eucalyptus和ConVirt以微弱 的优势领先于其他选项。Eucalyptus是私有云管理平台的先行者。Ubuntu 10.04选择捆绑Eucalyptus作为UEC的基础构架，使得Ecualyptus比其他的私有云管理平台拥有更多的用户和更加活跃的社区。此 外，Ecualyptus在中国国内有销售和技术支持人员，在沟通上比选择其他软件要更加容易。ConVirt排名第二，根本原因在于其销售和技术支持团 队与（潜在的）客户保持积极而有效的沟通。Citrix XenServer仅仅与其他两个选项并列排名第三，输在其过于严苛的许可证管理政策。的确，要给100台以上的服务器单独安装许可证并且每年更新一次， 可不是一件有意思的事情。</p>
<p>从 功能上考虑，ConVirt与XenServer遥遥领先于其他选项。虽然ConVirt仅仅支持Xen和KVM两种虚拟化技术，但是其安装配置相对简 单，文档详尽、功能齐全、界面美观、是比较容易上手的虚拟化管理软件。更重要的是，ConVirt的监控报表功能直观地展示了从数据中心到虚拟机的 CPU、内存利用情况，使得用户对整个数据中心的健康状况一目了然。同样，XenServer虽然仅支持Xen一种虚拟化技术，但是在安装配置、操作文 档、用户界面等方面都不亚于ConVirt。如果用户对基于Windows的界面没有强烈的抵触情绪的话，XenServer是比较值得考虑的一个选型。</p>
<p>综 合如上考虑，对于希望利用虚拟化管理软件提高硬件资源利用率和虚拟化管理自动化程度的企业（机构）来说，建议使用ConVirt来管理企业（机构）的计算 资源。如果网管人员不希望深入了解Linux操作系统，并且所管理的物理服务器数量有限的话，XenServer也是一个不错的选择。ConVirt的浏 览器界面是开放源代码的，用户可以对其进行定制化，将自己所需要的其他功能添加到同一个用户界面中去。XenCenter则提供了一种插件机制，用户可以 通过插件的方式讲自己的功能集成到XenCenter中。</p>
<p>不 过，你的基础设施是否需要与Amazon EC2相兼容呢？也就是说，你的用户是否需要使用他们用于访问和操作Amazon EC2的脚本和工具来访问你的计算资源呢？如果是这样的话，你可能需要在Eucalyptus和OpenStack之间作一个选择（CloudStack 和OpenNebula同样提供了与Amazon EC2兼容的操作接口，但是CloudStack在商务方面得分不高，OpenNebula在功能方面得分不高）。Eucalyptus的历史比 OpenStack稍长，用户群比OpenStack要大，社区的活跃程度也比OpenStack要高。不过OpenStack的后台老板NASA比 Eucalyptus要财大气粗，Ubuntu 11.04也集成了OpenStack作为其UEC的基础构架之一，表明OpenStack已经得到了社区的重视和支持。总的来说，开放源代码的云构架， 还是一个不断发展之中的新生食物。笔者只能够建议用户亲自去安装使用每一个软件，最终基于自己的经验以及需求达到一个最适合自己的选择。</p>
<p>虚拟化管理软件比较 －－ 幻灯片</p>
<p>结合前段时间对不同虚拟化管理软件的评测工作，准备了一套讲座用的幻灯片。PDF版本的文件可以从这里下载。如果有人需要ODP版本的文件，直接跟我联系吧。</p>
<p><img src="" alt=""></p>
<p><img src="" alt=""></p>
<p><img src="" alt=""></p>
<p><img src="" alt=""></p>
<p><img src="" alt=""></p>
<p><img src="" alt=""></p>
<p><img src="" alt=""></p>
<p><img src="" alt=""></p>
<p><img src="" alt=""></p>
<p><img src="" alt=""></p>
<p><img src="" alt=""></p>
<p><img src="" alt=""></p>
<p><img src="" alt=""></p>
<p><img src="" alt=""></p>
<p><img src="" alt=""></p>
<p><img src="" alt=""></p>
<p><img src="" alt=""></p>
<p>1.4 对我司的作用</p>
<p>建议使用Convirt而不是OpenStack</p>
<p>当其与sunde等代表的云终端（支持由真实单机提供和分配多虚拟机的运行环境，最终用户通过sunde的非PC终端连结各自虚拟机）配合使用时。</p>
<p>可以组成OpenStack管理的真实计算机集群基础上按照业务类型（开发、测试、办公、演示、呼叫、培训）划分的虚拟母机，再分别在虚拟母机上运行sunde的管理端，以管理最终使用者的各个虚拟子机并分别组成各自虚拟子网和公司公关网络，最后由最终用户使用非PC终端接入各自使用的一个或者多个虚拟机进行合理工作。</p>
<p>非移动技术用环境：</p>
<p>比起开发、测试人员、销售使用的个人工作机相比，更紧缺的是各种服务器，譬如：</p>
<p>运维：线上完整模拟测试环境、线上环境备份、工作环境文件共享服务器，公司网站测试环境等。</p>
<p>开发：VS、PD、DB设计服务器、开发测试用完整模拟环境、架构测试实验室等。</p>
<p>测试：QC服务器、浏览器多版本环境测试服务器、独立项目测试用完整模拟环境等。</p>
<p>Call Center：简单应用而需多人处理的办公环境。</p>
<p>等用于管理多虚拟实例的环境管理节点（一台机器、一个环境）甚至线上备份。</p>
<p>但因其不能跨物理机调度、通信、资源整合、虚拟机效率低等原因，不可以用于：</p>
<p>分布式大数据量存储控制业务节点、分布式数据中心存储控制节点等需要整合物理机资源进行分布式并行处理的环境。</p>
<p>2 <img src="" alt=""> Hadoop</p>
<p>2.1 简介</p>
<p>一款分布式数据存储与业务系统架构平台，由Apache基金会开发。用户可以在不了解分布式底层细节的情况下，开发分布式程序。充分利用集群的威力高速运算和存储。</p>
<p>其可以在多台计算机（PC或者小型机）组成的网络集群上跨物理机器统一调配单机资源以适应多种可分布并行处理的业务服务。</p>
<p>2.2 原理</p>
<p>其架构由低到高分为 HDFS-&gt;MapReduce+BigTable（NoSQLDB）</p>
<p>HDFS算法中</p>
<p>结构分为星型的NameNode核心与DataNode外围，其中NameNode为其瓶颈，当NameNode再大时，只能再整体复制一个集群出去做分布或者将单一机器的NameNode再通过把Key分级做Hadoop化处理。</p>
<p>有效容积率：</p>
<p>假设容许损坏的机器为x台，机器总数为y台</p>
<p>那么y台机器有效数据量为y/(x+1),其有效数据容积率为(y/(x+1))/y=1/(x+1)</p>
<p>健壮率：</p>
<p>假设容许损坏的机器为x台，机器总数为y台</p>
<p>那么y台机器健壮率(最小需要几台机器才能稳定)为1-(y-x)/y = x/y</p>
<p>如果综合考虑 有效容积率与健壮率 那么 1/(x+1)=x/y 所以y = x（x+1）。</p>
<p>MapReduce算法：</p>
<p>典型分散综合分布式处理算法。</p>
<p>基础是处理可以分散处理再综合统计数据的业务类型。</p>
<p>2.3 对我司的作用</p>
<p>其分布并行Map-reduce算法可以用于很多非即时反馈的非事务性业务处理：且不依赖HDFS一种实现，只要是支持节点运算即可。</p>
<p>其HDFS系统可用于大量数据文件的存储。但是不论其节点数量多少，其有效容积率都是由可容忍的宕机数量决定的。可见HDFS算法中更侧重的是稳定而不是并行处理高效和负载，更针对建立索引等搜索类业务处理要求而对少写多读商务类业务处理针对性不强，从百度淘宝的实践看也都证明这一点。</p>
<p>所以其HDFS系统设计之上应根据实际情况加入中央NOSQL缓存扩展与单数据节点线性NOSQL缓存负载。再配合业务服务器自身各种优化措施才能成为公司分布式DB、FS处理负载设计框架。hadoop设计思路可以借鉴不能照搬。</p>

      
    </div>
    
    
      
    
    
        <footer id="post-meta">
        <span class="categories">Posted in<span class="breadcrumb fa fa-folder"><li><a href="/categories/hadoop/">hadoop</a></li></span></span> | <span class="tags">Tagged <a href="/tags/hadoop/" class="label label-primary">hadoop</a></span> | <span class="time">recent updated:<time title="2014-03-07 09:54:36"datetime="2014-03-07 09:54:36"> mar. 7 2014</time></span> | <span class="comment-link">
<a href="http://itsolife.com/2014/02/02/2014-02-02-hadoop--OpenStack_Hadoop/#comments" class="ds-thread-count comment-link" data-thread-key="2014-02-02-hadoop--OpenStack_Hadoop" data-count-type="comments">暂无评论</a></span>
        </footer>
    
    <div class="clearfix"></div>
  </div>
</div>
</article>



  <article>
<div id="post" class="post well">
  <div class="post-content">
    <header class="well-sm">
      <i class="fa icon fa-5x pull-left"></i>
      <h1 class="title"><a href="/2014/02/02/2014-02-02-hadoop--Hadoop知识分享文稿byquqi99-技术并艺术着/">Hadoop知识分享文稿 ( by quqi99 ) </a></h1>
      
        <span>Posted on<time datetime="2014-02-02T01:54:36.000Z"> <a href="/2014/02/02/2014-02-02-hadoop--Hadoop知识分享文稿byquqi99-技术并艺术着/">feb. 2 2014</a></time></span>      
    </header>
    
    
    <div class="entry">
      
        <h1 id="hadoop-by-quqi99-">Hadoop知识分享文稿 ( by quqi99 ) - 技术并艺术着</h1>
<p>您还未登录！|<a href="https://passport.csdn.net/account/login" target="_blank">登录</a>|<a href="https://passport.csdn.net/account/register" target="_blank">注册</a>|<a href="https://passport.csdn.net/help/faq" target="_blank">帮助</a></p>
<ul>
<li><a href="http://www.csdn.net/" target="_blank">首页</a></li>
<li><a href="http://news.csdn.net/" target="_blank">业界</a></li>
<li><a href="http://mobile.csdn.net/" target="_blank">移动</a></li>
<li><a href="http://cloud.csdn.net/" target="_blank">云计算</a></li>
<li><a href="http://sd.csdn.net/" target="_blank">研发</a></li>
<li><a href="http://bbs.csdn.net/" target="_blank">论坛</a></li>
<li><a href="http://blog.csdn.net/" target="_blank">博客</a></li>
<li><a href="http://download.csdn.net/" target="_blank">下载</a></li>
<li><h2 id="-"><a href="">更多</a></h2>
</li>
</ul>
<h1 id="-http-blog-csdn-net-quqi99-"><a href="http://blog.csdn.net/quqi99" target="_blank">技术并艺术着</a></h1>
<h2 id="-blog">张华的技术Blog</h2>
<ul>
<li><a href="http://blog.csdn.net/quqi99?viewmode=contents" target="_blank"><img src="" alt="">目录视图</a></li>
<li><a href="http://blog.csdn.net/quqi99?viewmode=list" target="_blank"><img src="" alt="">摘要视图</a></li>
<li><a href="http://blog.csdn.net/quqi99/rss/list" target="_blank"><img src="" alt="">订阅</a>
<a href="https://code.csdn.net/blog/12" target="_blank">公告：博客新增直接引用代码功能</a>        <a href="http://www.csdn.net/article/2013-08-06/2816471" target="_blank">专访李铁军：从医生到金山首席安全专家的转变</a>      <a href="http://blog.csdn.net/csdnproduct/article/details/9495139" target="_blank">CSDN博客频道自定义域名、标签搜索功能上线啦！</a>      <a href="http://blog.csdn.net/adali/article/details/9813651" target="_blank">独一无二的职位：开源社区经理</a></li>
</ul>
<h3 id="-hadoop-by-quqi99-"><a href="">[置顶] Hadoop知识分享文稿 ( by quqi99 )</a></h3>
<p>分类： <a href="http://blog.csdn.net/quqi99/article/category/328029" target="_blank">Seach Engine</a>  2011-03-31 15:19 1977人阅读 <a href="">评论</a>(0) <a href="&quot;收藏&quot;">收藏</a> <a href="&quot;举报&quot;">举报</a>
<a href="http://blog.csdn.net/tag/details.html?tag=hadoop" target="_blank">hadoop</a><a href="http://blog.csdn.net/tag/details.html?tag=%e4%bb%bb%e5%8a%a1" target="_blank">任务</a><a href="http://blog.csdn.net/tag/details.html?tag=mapreduce" target="_blank">mapreduce</a><a href="http://blog.csdn.net/tag/details.html?tag=%e4%bb%bb%e5%8a%a1%e8%b0%83%e5%ba%a6" target="_blank">任务调度</a><a href="http://blog.csdn.net/tag/details.html?tag=%e9%9b%86%e7%be%a4" target="_blank">集群</a><a href="http://blog.csdn.net/tag/details.html?tag=%e4%bd%9c%e4%b8%9a" target="_blank">作业</a></p>
<p>目录<a href="&quot;系统根据文章中H1到H6标签自动生成文章目录&quot;">(?)</a><a href="&quot;展开&quot;">[+]</a></p>
<ol>
<li><a href="">作者张华 写于2010-08-15   发表于2011-03-31 版权声明可以任意转载转载时请务必以超链接形式标明文章原始出处和作者信息及本版权声明</a></li>
<li><a href="">httpblogcsdnnetquqi99</a></li>
<li><p><a href="">hadoop 理论基础</a></p>
</li>
<li><p><a href="">hadoop 是什么</a></p>
</li>
<li><a href="">hadoop 项目</a></li>
<li><a href="">MapReduce 任务的运行流程</a></li>
<li><p><a href="">MapReduce 任务的数据流图</a></p>
</li>
<li><p><a href="">hadoop 入门实战</a></p>
</li>
<li><p><a href="">测试环境</a></p>
</li>
<li><a href="">测试程序</a></li>
<li><a href="">属性配置</a></li>
<li><a href="">免密码 SSH 设置</a></li>
<li><a href="">配置 hosts</a></li>
<li><a href="">格式化 HDFS 文件系统</a></li>
<li><a href="">启动守护进程</a></li>
<li><p><a href="">运行程序</a></p>
</li>
<li><p><a href="">hadoop 高级进阶</a></p>
</li>
<li><a href="">hadoop 应用案例</a></li>
<li><a href="">参考文献</a><pre><code>                       **Hadoop知识分享文稿 ( by quqi99 )**
</code></pre></li>
</ol>
<h2 id="-2010-08-15-2011-03-31"><a href=""></a>作者：张华 写于：2010-08-15   发表于：2011-03-31</h2>
<p>版权声明：可以任意转载，转载时请务必以超链接形式标明文章原始出处和作者信息及本版权声明</p>
<h2 id="-http-blog-csdn-net-quqi99-"><a href=""></a>( <a href="http://blog.csdn.net/quqi99">http://blog.csdn.net/quqi99</a> )</h2>
<p><strong>内容目录</strong></p>
<p>目 录</p>
<p>1 hadoop 理论基础 3</p>
<p>1.1 hadoop 是什么 3</p>
<p>1.2 hadoop 项目 3</p>
<p>1.3 Map/Reduce 任务的运行流程 4</p>
<p>1.4 Map/Reduce 任务的数据流图 5</p>
<p>2 hadoop 入门实战 7</p>
<p>2.1  测试环境 7</p>
<p>2.2  测试程序 7</p>
<p>2.3  属性配置 9</p>
<p>2.4  免密码SSH 设置 10</p>
<p>2.5  配置hosts 11</p>
<p>2.6  格式化HDFS 文件系统 11</p>
<p>2.7  启动守护进程 11</p>
<p>2.8  运行程序 11</p>
<p>3 hadoop 高级进阶 12</p>
<p>4 hadoop 应用案例 12</p>
<p>5  参考文献 12</p>
<h1 id="-1-hadoop-"><a href=""></a>1 hadoop  理论基础</h1>
<h2 id="-1-1-hadoop-"><a href=""></a>1.1 hadoop  是什么</h2>
<p>Hadoop  是 Doug Cutting  开发的，他是一个相当牛的哥们，他同时是大名鼎鼎的 Lucene  及 Nutch  的作者。</p>
<p>我是这样理解 hadoop  的，它就是用来对海量数据进行存储与分析的一个开源软件。它包括两块：</p>
<p>1  ） HDFS ( Hadoop Distrubuted File System )  ，可以对重要数据进行冗余存储，有点类似于冗余磁盘陈列。</p>
<p>2  ）对 Map/Reduce  编程模型的一个实现。当然，关系型数据库（ RDBMS  ）也能做类似的事情，但为什么不用 RDBMS  呢？我们知道，让计算移动于数据上比让数据移动到计算更有效率。这使得 Map/Reduce  适合数据被一次写入和多次读取的应用，而 RDBMS  更适合持续更新的数据集。</p>
<h2 id="-1-2-hadoop-"><a href=""></a>1.2 hadoop  项目</h2>
<p>如今，广义上的 Hadoop  已经发展成为一个分布式计算基础架构这把“大伞”下相关子项目的集合，其技术栈如下图所示：</p>
<p>图：</p>
<pre><code>                                     ![]()
</code></pre><p><img src="http://blog.csdn.net/root/Library/Caches/TemporaryItems/moz-screenshot-4.png" alt=""></p>
<pre><code>                                                图1 hadoop 的子项目
</code></pre><ul>
<li>Core ： 一系列分布式文件系统和通用I/O 的组件和接口( 序列化、Java RPC 和持久化数据结构) 。</li>
<li>Avro ： 用于数据的序列化，当然，JDK 中也有Seriable 接口，但hadoop 中有它自己的序列化方式，具说更有效率。</li>
<li>MapReduce ： 分布式数据处理模式和执行环境，运行于大型商用机集群。</li>
<li>HDFS ： 分布式文件系统，运行于大型商用机集群。</li>
<li>Pig ： HDFS 上的数据检索语言，类似于RDBMS 中的SQL 语言。</li>
<li>Hbase ： 一个分布式的、列存储数据库。HBase 使用HDFS 作为底层存储，同时支持MapReduce 的批量式计算和点查询( 随机读取) 。</li>
<li>ZooKeeper ： 一个分布式的、高可用性的协调服务。ZooKeeper 提供分布式锁之类的基本服务用于构建分布式应用。</li>
<li>Hive ： 分布式数据仓库。Hive 管理HDFS 中存储的数据，并提供基于SQL 的查询语言( 由运行时引擎翻译成MapReduce 作业) 用以查询数据。</li>
<li>Chukwa ： 分布式数据收集和分析系统。Chukwa 运行HDFS 中存储数据的收集器，它使用MapReduce 来生成报告。</li>
</ul>
<h2 id="-1-3-map-reduce-"><a href=""></a> 1.3 Map/Reduce  任务的运行流程</h2>
<pre><code>                 ![]()
</code></pre><p>JobClient  的  submitJob()  方法的作业提交过程如下：</p>
<p>1  ）向 Jobtraker  请求一个新作业 ID</p>
<p>2  ） 调用 JobTracker  的 getNewJobId()</p>
<p>3  ）  JobClient  进行作业划分，并将划分后的输入及作业的 JAR  文件、配置文件等复制到 HDFS  中去</p>
<p>4  ） 提交作业，会把此调用放入到一个内部的队列中，交由作业调度器进行调度。值得一提的是，针对  Map  任务与 Reduce  任务，任务调度器是优先选择 Map  任务的，另外，任务调度器在选择 Reduce  任务时并没有考虑数据的本地化。然而，针对一个 Map  任务，它考虑的是 Tasktracker  网络位置和选取一个距离其输入划分文件最近的 Tasktracker  ，它可能是数据本地化的，也可能是机架本地化的，还可能得到不同的机架上取数据。</p>
<p>5  ） 初始化包括创建一个代表该正在运行的作业的对象，它封装任务和记录信息，以便跟踪任务的状态和进度。</p>
<p>6  ） JobTracker  任务调度器首先从共享文件系统中获取 JobClient  已计算好的输入划分信息，然后为每个划分创建一个 Map  任务。创建 的 reduce  任务的数量是由 JobConf  的 Mapred.reduce.tasks  属性决定，它是用 setNumReduceTask()  方法来设置的。</p>
<p>7  ） TaskTracker  执行一个简单的循环，定期发送心跳（ Heartbeat  ）方法调用 Jobtracker  告诉是否还活着，同时，心跳还会报告任务运行的是否已经准备运行新的任务。</p>
<p>8  ） TaskTracker  已经被分配了任务，下一步是运行任务。首先它需要将它所需的全部文件从 HDFS  中复制到本地磁盘。</p>
<p>9  ）紧接着，它要启动一个新的 Java  虚拟机来运行每个任务，这使得用户所定义的 Map  和 Reduce  函数的任务缺陷都不会影响 TaskTracker  （比如导致它崩溃或者挂起）</p>
<p>10  ）运行 Map  任务或者 Reduce  任务，值得一提的是，这些任务使用标准输入与输出流，换句话说，你可以用任务语言（如 JAVA  ， C++  ， Shell  等）来实现 Map  和 Reduce  ，只要保证它们也使用标准输入与输出流，就可以将输出的键值对传回给 JAVA  进程了。</p>
<h2 id="-1-4-map-reduce-"><a href=""></a> 1.4 Map/Reduce  任务的数据流图</h2>
<p><img src="" alt=""></p>
<pre><code>    图3  Map/Reduce  中单一 Reduce  任务的数据流图
</code></pre><p><img src="" alt=""></p>
<pre><code>             图4  Map/Reduce  中多个 Reduce  任务的数据流图
</code></pre><p><img src="" alt=""></p>
<pre><code>            图5  MapReduce  中没有 Reduce  任务的数据流图
</code></pre><p><strong>任务粒度</strong>   ： 分片的个数，在将原始大数据切割成小数据集时，通常让小数据集小于或等于 HDFS  中的一个 Block  的大小（缺省是 64M)  ，这样能够保证一个小数据集位于一台计算机上，便于本地计算。 有 M   个 小数据集 待处理，就启动 M   个 Map   任务，注意这 M   个 Map   任务分布于 N   台计算机上并行运行，Reduce   任务的数量 R   则可由用户指定 。</p>
<p><strong>Map</strong>   ： 输入 <k1, v1>   输出 List(<k2,v2>)</p>
<p><strong>Reduce</strong>   ： 输入 <k2,List(v2)>   输出 <k3,v3></p>
<p><strong>分区（</strong>  <strong>Partition)</strong>  :   把 Map   任务输出的中间结果按 key   的范围划分成 R   份 ( R  是预先定义的 Reduce  任务的个数) ，划分时通常使用 hash  函数如: hash(key) mod R ，这样可以保证某一段范围内的 key ，一定是由一个 Reduce  任务来处理，可以简化 Reduce  的过程。</p>
<p><strong>Combine</strong>   :   在  partition   之前，还可以对中间结果先做  combine  ，即将中间结果中有相同  key  的  <key, value>   对合并成一对。 combine   的过程与  Reduce   的过程类似，很多情况下就可以直接使用  Reduce   函数，但  combine   是作为  Map   任务的一部分，在执行完  Map   函数后紧接着执行的。 Combine   能够减少中间结果中  <key, value>   对的数目，从而减少网络流量。</p>
<p>下面举个例子来着重说明 Combine  ， hadoop  允许用户声明一个 combiner  运行在 Map  的输出上，它的输出再作为 Reduce  的输入。例如，找出每一年的最调气温：</p>
<p>假如用户的输入的分片数是 2  ，那么：</p>
<p>1  ）第一个 Map  的输出如下：</p>
<p>（ 1950  ， 0  ）</p>
<p>（ 1950  ， 20  ）</p>
<p>（ 1950  ， 10  ）</p>
<p>2  ） 第二个 Map  的输出如下：</p>
<p>（ 1950  ， 25  ）</p>
<p>（ 1950  ， 15  ）</p>
<p>3  ） Reduce  的输入如下：</p>
<p>（ 1950  ，［ 0  ， 20  ， 10  ， 25  ， 15  ］）</p>
<p><strong>注意：如果有</strong>   <strong>combine</strong>    <strong>的话，此时</strong>   <strong>Reduce</strong>    <strong>的输入应该是：</strong></p>
<p><strong>max(0, 20, 10, 25, 15) = max(max(0,20,10), max(25,15)) = max(20,25)</strong></p>
<p><strong>combine</strong>    <strong>并不能取代</strong>   <strong>reduce,</strong>    <strong>例如，如果我们计算平均气温，便不能使用</strong>   <strong>combine</strong>    <strong>，因为：</strong></p>
<p><strong>mean(0,20,10,25,15) = 14</strong></p>
<p><strong>但是：</strong></p>
<p><strong>mean(mean(0,20,10), mean(25,15)) = mean(10,20) = 15</strong></p>
<p>4  ） Reduce  的输出如下：</p>
<p>（ 1950  ， 25  ）</p>
<h1 id="-2-hadoop-"><a href=""></a>2 hadoop  入门实战</h1>
<p>hadoop  有三种部署模式：</p>
<ul>
<li>单机模式：没有守护进程，一切都运行在单个 JVM  上，适合测试与调试。</li>
<li>伪集群模式：守护进程在本地运行，适合模拟集群。</li>
<li>集群模式：守护进程运行在集群的某台机器上。</li>
</ul>
<p>所以，在以上任一特定模式运行 hadoop  时，只需要做两件事情：</p>
<p>1  ） 设置适当属性</p>
<p>2  ）启动 hadoop  的守护进程（名称节点，二级名称节名，数据节点）</p>
<p>hadoop  默认的是单机模式，下面，我们将着重介绍在集群模式是如何部署？</p>
<h2 id="-2-1-"><a href=""></a>2.1   测试环境</h2>
<p>用两台机器做为测试环境 ,   通常，集群里的一台机器被指定为  NameNode  ，另一台不同的机器被指定为 JobTracker  ，这些机器是 <strong>masters;</strong>  余下的机器即作为 DataNode  <strong>也</strong> 作为 TaskTracker  ，这些机器是 <strong>slaves</strong>  <strong>。</strong></p>
<p>1  ）  master (JobTracker &amp; NameNode)  ：我的工作机  ( zhanghua  .quqi.com)</p>
<p>2  ）  slave (TaskTracker &amp; DataNode)  ：我的开发机 ( tadev03  .quqi.com)</p>
<p>3)   两机均已安装 ssh   与  rsync</p>
<h2 id="-2-2-"><a href=""></a>2.2   测试程序</h2>
<p>1  ）  /home/workspace/hadoopExample/input/file01:</p>
<p>Hello World Bye World</p>
<p>2) /home/workspace/hadoopExample/input/file02:</p>
<p>Hello  Hadoop    Goodbye  Hadoop</p>
<ol>
<li>WordCount.java</li>
</ol>
<p><strong>package</strong>    com.TripResearch.hadoop;</p>
<p><strong>import</strong>   java.io.IOException;</p>
<p><strong>import</strong>   java.util.Iterator;</p>
<p><strong>import</strong>   java.util.StringTokenizer;</p>
<p><strong>import</strong>   org.apache.hadoop.fs.Path;</p>
<p><strong>import</strong>   org.apache.hadoop.io.IntWritable;</p>
<p><strong>import</strong>   org.apache.hadoop.io.LongWritable;</p>
<p><strong>import</strong>   org.apache.hadoop.io.Text;</p>
<p><strong>import</strong>   org.apache.hadoop.mapred. FileInputFormat  ;</p>
<p><strong>import</strong>   org.apache.hadoop.mapred.FileOutputFormat;</p>
<p><strong>import</strong>   org.apache.hadoop.mapred.JobClient;</p>
<p><strong>import</strong>   org.apache.hadoop.mapred. JobConf  ;</p>
<p><strong>import</strong>   org.apache.hadoop.mapred. MapReduceBase  ;</p>
<p><strong>import</strong>   org.apache.hadoop.mapred. Mapper  ;</p>
<p><strong>import</strong>   org.apache.hadoop.mapred.OutputCollector;</p>
<p><strong>import</strong>   org.apache.hadoop.mapred. Reducer  ;</p>
<p><strong>import</strong>   org.apache.hadoop.mapred.Reporter;</p>
<p><strong>import</strong>   org.apache.hadoop.mapred. TextInputFormat  ;</p>
<p><strong>import</strong>   org.apache.hadoop.mapred. TextOutputFormat  ;</p>
<p>//<em>/</em></p>
<p>/<em>  <em>*@author</em></em>    huazhang</p>
<p>/*/</p>
<p>@SuppressWarnings ( &quot;deprecation&quot; )</p>
<p><strong>public</strong>    <strong>class</strong>   WordCount {</p>
<p><strong>public</strong>    <strong>static</strong>    <strong>class</strong>   MyMap  <strong>extends</strong>    MapReduceBase    <strong>implements</strong></p>
<p>Mapper <LongWritable, Text, Text, IntWritable> {</p>
<p><strong>private</strong>    <strong>final</strong>    <strong>static</strong>   IntWritable  <em>one</em>   =  <strong>new</strong>   IntWritable(1);</p>
<p><strong>private</strong>   Text  word  =  <strong>new</strong>   Text();</p>
<p><strong>public</strong>    <strong>void</strong>   map(LongWritable key, Text value,</p>
<p>OutputCollector<Text, IntWritable> output, Reporter reporter)</p>
<p><strong>throws</strong>   IOException {</p>
<p>String line = value.toString();</p>
<p>StringTokenizer tokenizer =  <strong>new</strong>   StringTokenizer(line);</p>
<p><strong>while</strong>   (tokenizer.hasMoreTokens()) {</p>
<p>word .set(tokenizer.nextToken());</p>
<p>output.collect( word ,  <em>one</em>  );</p>
<p>}</p>
<p>}</p>
<p>}</p>
<p><strong>public</strong>    <strong>static</strong>    <strong>class</strong>   MyReduce  <strong>extends</strong>    MapReduceBase    <strong>implements</strong></p>
<p>Reducer <Text, IntWritable, Text, IntWritable> {</p>
<p><strong>public</strong>    <strong>void</strong>   reduce(Text key, Iterator<IntWritable> values,</p>
<p>OutputCollector<Text, IntWritable> output, Reporter reporter)</p>
<p><strong>throws</strong>   IOException {</p>
<p><strong>int</strong>   sum = 0;</p>
<p><strong>while</strong>   (values.hasNext()) {</p>
<p>sum += values.next().get();</p>
<p>}</p>
<p>output.collect(key,  <strong>new</strong>   IntWritable(sum));</p>
<p>}</p>
<p>}</p>
<p><strong>public</strong>    <strong>static</strong>    <strong>void</strong>   main(String[] args)  <strong>throws</strong>   Exception {</p>
<p>JobConf   conf =  <strong>new</strong>   JobConf(WordCount. <strong>class</strong>  );</p>
<p>conf.setJobName( &quot;wordcount&quot; );</p>
<p>conf.setOutputKeyClass(Text. <strong>class</strong>  );</p>
<p>conf.setOutputValueClass(IntWritable. <strong>class</strong>  );</p>
<p>conf.setMapperClass(MyMap. <strong>class</strong>  );</p>
<p>conf.setCombinerClass(MyReduce. <strong>class</strong>  );</p>
<p>conf.setReducerClass(MyReduce. <strong>class</strong>  );</p>
<p>conf.setInputFormat( TextInputFormat  . <strong>class</strong>  );</p>
<p>conf.setOutputFormat( TextOutputFormat  . <strong>class</strong>  );</p>
<p>FileInputFormat  . <em>setInputPaths</em>  (conf,  <strong>new</strong>   Path(args[0]));</p>
<p>FileOutputFormat. <em>setOutputPath</em>  (conf,  <strong>new</strong>   Path(args[1]));</p>
<p>JobClient.<em>runJob</em> (conf);</p>
<p>}</p>
<p>}</p>
<p><img src="" alt=""></p>
<h2 id="-2-3-"><a href=""></a>2.3   属性配置</h2>
<p>按下图所示修改至少 3  个属性, 如下图所示：</p>
<p>   <img src="" alt=""></p>
<ol>
<li></li>
<li><p>conf/core-site.xml</p>
</li>
</ol>
<configuration>

<property>

<name>fs.default.name</name>

<value>hdfs://zhanghua  .quqi.com:9000</value>

</property>

</configuration>

<p>注意：此处如果是伪集群模式可配置为  hdfs://localhost:9000 ,    是本地模式则为：  localhost:9000   。另外，其他输入输入路径，是本地模式是本地文件系统的路径，是非地模式，用 hdfs  文件系统的路径格式。</p>
<ol>
<li>conf/hdfs-site.xml</li>
</ol>
<configuration>

<property>

<name>dfs.replication</name>

<value>1</value>

</property>

<p></configuration></p>
<ol>
<li>conf/mapred-site.xml</li>
</ol>
<configuration>

<property>

<name>mapred.job.tracker</name>

<value>zhanghua  .quqi.com:8021</value>

</property>

<p></configuration></p>
<ol>
<li>masters</li>
</ol>
<p>zhanghua  .quqi.com (   伪分布模式就配成  localhost)</p>
<ol>
<li>slaves</li>
</ol>
<p>tadev03  .quqi.com  (   伪分布模式就配成 localhost)</p>
<ol>
<li>将以上配置好的 hadoop  文件夹拷到所有机器的相同目录下：</li>
</ol>
<p>scp -r /home/soft/hadoop-0.20.2 <a href="mailto:root@tadev03.daodao.com">root@tadev03</a>   <a href="mailto:root@tadev03.daodao.com">.quqi.com</a>  :/home/soft/hadoop-0.20.2</p>
<p>注意：确保两台机器的  JAVA_HOME   的路径一致，如果不一致，就要改 。</p>
<p>hadoop  所有可配置的配置文件说明如下：</p>
<p>hadoop-env.sh   运行 hadoop  的脚本中使用的环境变量</p>
<p>core-site.xml hadoop  的核心配置，如 HDFS  和 MapReduce  中很普遍的 I/O  设置</p>
<p>hdfs-site.xml HDFS  后台程序设置的配置：名称节点，第二名称节点及数据节点</p>
<p>mapred-site.xml MapReduce  后台程序设置的配置： jobtracker  和 tasktracker</p>
<p>masters   记录运行第二名称节点 的机器（一行一个）的列表</p>
<p>slaves   记录运行数据节点的机器（一行一个）的列表</p>
<h2 id="-2-4-ssh-"><a href=""></a>2.4   免密码 SSH  设置</h2>
<p>免密码  ssh   设置， 保证至少从   master    可以不用口令登陆所有的   slaves    。</p>
<p>1  ）生成密钥对： ssh-keygen -t rsa -P &#39;&#39; -f /root/.ssh/id_rsa (  这样密钥就留在了客户端 )</p>
<p>2)   将公钥拷到要连接的服务器，</p>
<p>scp /root/.ssh/id_rsa.pub root@tadev03  .quqi.com:/tmp</p>
<p>ssh -l root tadev03  .quqi.com</p>
<p>more /tmp/id_rsa.pub &gt;&gt; /root/.ssh/authorized_keys</p>
<ol>
<li>ssh tadev03  .quqi.com   不需要输入密码即为成功。</li>
</ol>
<p>（注意：伪分布模式也要配置  ssh localhost   无密码登录，如果是  mac   ，请将  ssh   打开）</p>
<p>(  另外，在 mac  中请在 hadoop-config.sh  文件中配置  export JAVA_HOME=/System/Library/Frameworks/JavaVM.framework/Versions/1.6/Home)</p>
<p>三条控制线线：</p>
<p>SSH →   这样就可以直接从主节点远程启动从节点上的脚本，如  ssh tadev03  .quqi.com &#39;/var/aa.sh&#39;</p>
<p>NameNode (<a href="http://localhost:50030/" target="_blank"><a href="http://localhost:50070">http://localhost:50070</a></a> ) → DataNode</p>
<p>JobTracker ( <a href="http://localhost:50030/" target="_blank"><a href="http://localhost:50030">http://localhost:50030</a></a> )→ TaskTracker (<a href="http://localhost:50030/" target="_blank"><a href="http://localhost:50060">http://localhost:50060</a></a> )</p>
<h2 id="-2-5-hosts"><a href=""></a>2.5   配置 hosts</h2>
<p>必须配置 master   和 slaves   之间的双向 hosts.   修改 /etc/hosts   进行配置，略。</p>
<h2 id="-2-6-hdfs-"><a href=""></a>2.6   格式化 HDFS  文件系统</h2>
<p>和我们常见的 NTFS  ， FAT32  文件系统一样， NDFS  最开始也是需要格式化的。格式化过程用来创建存储目录以及名称节点的永久数据结构的初始版本来创建一个空的文件系统。命令如下：</p>
<p>hadoop namenode -format</p>
<p>已知问题：在重新格式化时，可能会报： SHUTDOWN_MSG: Shutting down NameNode</p>
<p>解决办法： rm -rf /tmp/hadoop-root/dfs/name</p>
<h2 id="-2-7-"><a href=""></a>2.7   启动守护进程</h2>
<p>1    ）启动   HDFS    守护进程：    start-dfs.sh</p>
<p>(      start-dfs.sh    脚本会参照 NameNode    上 ${HADOOP_CONF_DIR}/slaves    文件的内容，在所有列出的 slave    上启动 DataNode    守护进程。   )</p>
<p>已知问题：在已设置   JAVA_HOME    的情况下仍会报：   Error: JAVA_HOME is not set</p>
<p>解决办法：我是在  hadoop.sh  文件中加下面一句解决的：</p>
<p>JAVA_HOME=/System/Library/Frameworks/JavaVM.framework/Versions/1.6/Home</p>
<p>2  ）启动  Map/Reduce  守护进程：   start-mapred.sh</p>
<p>(      start-mapred.sh   脚本会参照 JobTracker   上 ${HADOOP_CONF_DIR}/slaves   文件的内容，在所有列出的 slave   上启动 TaskTracker   守护进程  )</p>
<p>3)   启动成功后，可以通过访问  <a href="http://localhost:50030" target="_blank">http://localhost:50030</a>   验证。</p>
<p>注意：也可直接使用  start-all.sh       与  stop-all.sh       脚本  ,       在主节点   master    上面启动   hadoop    ，主节点会启动  /    停止所有从节点的   hadoop    。会启动  5       个   java        进程  ,        同时会在   /tmp        目录下创建五个   pid        文件记录这些进程   ID        号。通过这五个文件，可以得知   namenode, datanode, secondary namenode, jobtracker, tasktracker        分别对应于哪一个   Java        进程。</p>
<p>已知问题：启动后，日志中报：  java.io.IOException: File /tmp/hadoop-root/mapred/system/jobtracker.info could only be replicated to 0 nodes, instead of 1</p>
<p>解决办法：原因是    从  tadev03     .quqi.com       机器上无法  ping zhanghua     .quqi.com</p>
<h2 id="-2-8-"><a href=""></a>2.8   运行程序</h2>
<p>先将测试数据及其他输入由本地文件系统拷到  HFDS  文件系统中去（注意：   jar   除外 ）</p>
<ol>
<li></li>
<li><p>hadoop fs -mkdir input</p>
</li>
<li>hadoop fs -ls .</li>
<li>hadoop fs -copyFromLocal /home/workspace/hadoopExample/input/file01 input/file01</li>
<li>hadoop fs -copyFromLocal /home/workspace/hadoopExample/input/file02 input/file02</li>
</ol>
<p>这时候就可以执行下列命令运行程序了，注意：后面的input , output  等目录都是HDFS  文件系统的路径。(  如果是本地模式，就用本地文件系统的绝对路径）</p>
<ol>
<li></li>
</ol>
<p>hadoop     jar   /home/workspace/hadoopExample/hadoopExample.jar com.TripResearch.hadoop.WordCount input/ output</p>
<p>已知问题：在集群模式下运行时任务会Pending</p>
<p>最后，运行下列命令查看结果：</p>
<p>/home/soft/hadoop-0.20.2/bin/hadoop fs -cat output/part-00000</p>
<p>也可访问下列地址查看状态：</p>
<p>NameNode – <a href="http://localhost:50070/" target="_blank"><a href="http://zhanghua">http://zhanghua</a></a>   <a href="http://localhost:50070/" target="_blank">.quqi.com</a> <a href="http://localhost:50070/" target="_blank">:50070/</a></p>
<p>JobTracker - <a href="http://localhost:50030/" target="_blank"><a href="http://zhanghua">http://zhanghua</a></a>   <a href="http://localhost:50030/" target="_blank">.quqi.com</a> <a href="http://localhost:50030/" target="_blank">:50030/</a></p>
<p>常用命令说明如下：</p>
<p>hadoop dfs –ls   查看 /usr/root  目录下的内容径；
hadoop dfs –rmr xxx xxx  就是删除目录；
hadoop dfsadmin -report   这个命令可以全局的查看 DataNode  的情况；
hadoop job -list   后面增加参数是对于当前运行的 Job  的操作，例如 list,kill  等；
hadoop balancer   均衡磁盘负载的命令。</p>
<h1 id="-3-hadoop-"><a href=""></a>3 hadoop  高级进阶</h1>
<h1 id="-4-hadoop-"><a href=""></a>4 hadoop  应用案例</h1>
<h1 id="-5-"><a href=""></a>5   参考文献</h1>
<ol>
<li><a href="http://hadoop.apache.org/common/docs/r0.18.2/cn/" target="_blank"><a href="http://hadoop.apache.org/common/docs/r0.18.2/cn/">http://hadoop.apache.org/common/docs/r0.18.2/cn/</a></a></li>
<li>hadoop 0.20.2  集群配置入门 <a href="http://dev.firnow.com/course/3_program/java/javajs/20100719/453042.html" target="_blank"><a href="http://dev.firnow.com/course/3_program/java/javajs/">http://dev.firnow.com/course/3_program/java/javajs/</a></a></li>
<li>Hadoop 分布式文件系统（HDFS ）初步实践 <a href="http://huatai.me/?p=352" target="_blank"><a href="http://huatai.me/?p=352">http://huatai.me/?p=352</a></a></li>
<li>Hadoop 分布式部署实验2_ 格式化分布式文件系统 <a href="http://hi.baidu.com/thinke365/blog/item/15602aa8f9074cf41e17a235.html" target="_blank"><a href="http://hi.baidu.com/thinke365/blog/item/15602aa8f9074cf41e17a235.html">http://hi.baidu.com/thinke365/blog/item/15602aa8f9074cf41e17a235.html</a></a></li>
<li>hadoop 安装出现问题（紧急），请前辈指教 <a href="http://forum.hadoop.tw/viewtopic.php?f=4&amp;t=90" target="_blank"><a href="http://forum.hadoop.tw/viewtopic.php?f=4&amp;t=90">http://forum.hadoop.tw/viewtopic.php?f=4&amp;t=90</a></a></li>
<li>用 Hadoop  进行分布式并行编程 <a href="http://www.ibm.com/developerworks/cn/opensource/os-cn-hadoop1/index.html" target="_blank"><a href="http://www.ibm.com/developerworks/cn/opensource/os-cn-hadoop1/index.html">http://www.ibm.com/developerworks/cn/opensource/os-cn-hadoop1/index.html</a></a></li>
<li>用 Hadoop  进行分布式数据处理 <a href="http://tech.ddvip.com/2010-06/1275983295155033.html" target="_blank"><a href="http://tech.ddvip.com/2010-06/1275983295155033.html">http://tech.ddvip.com/2010-06/1275983295155033.html</a></a></li>
</ol>
<p>分享到： <a href="&quot;分享到新浪微博&quot;"></a><a href="&quot;分享到腾讯微博&quot;"></a></p>
<ol>
<li>上一篇：<a href="http://blog.csdn.net/quqi99/article/details/6160846" target="_blank">Lucene Scoring 评分机制 （ by quqi99 )</a></li>
<li><p>下一篇：<a href="http://blog.csdn.net/quqi99/article/details/6292472" target="_blank">深入理解各JEE服务器Web层集群原理 ( by quqi99 )</a>
查看评论<a href=""></a></p>
<p>暂无评论
您还没有登录,请<a href="">[登录]</a>或<a href="http://passport.csdn.net/account/register?from=http%3A%2F%2Fblog.csdn.net%2Fquqi99%2Farticle%2Fdetails%2F6291788" target="_blank">[注册]</a></p>
</li>
</ol>
<p>/* 以上用户言论只代表其个人观点，不代表CSDN网站的观点或立场<a href=""></a><a href=""></a>
<a href="&quot;回到顶部&quot;"><img src="" alt="TOP"></a></p>
<p>个人资料</p>
<p><a href="http://my.csdn.net/quqi99" target="_blank"><img src="&quot;访问我的空间&quot;" alt=""></a>
<a href="http://my.csdn.net/quqi99" target="_blank">quqi99</a></p>
<p><a href="&quot;[加关注]&quot;"></a> <a href="&quot;[发私信]&quot;"></a>
<a href="http://medal.blog.csdn.net/allmedal.aspx" target="_blank"><img src="" alt=""></a></p>
<ul>
<li>访问：198660次</li>
<li>积分：3337分</li>
<li><p>排名：第1895名</p>
</li>
<li><p>原创：146篇</p>
</li>
<li>转载：23篇</li>
<li>译文：0篇</li>
<li>评论：123条</li>
</ul>
<p>文章搜索</p>
<p><a href=""></a></p>
<p>文章分类</p>
<ul>
<li><a href="http://blog.csdn.net/quqi99/article/category/875141" target="_blank">VM / Cloud</a>(7)</li>
<li><a href="http://blog.csdn.net/quqi99/article/category/557281" target="_blank">Middleware / Java AppServer</a>(13)</li>
<li><a href="http://blog.csdn.net/quqi99/article/category/328029" target="_blank">Seach Engine</a>(5)</li>
<li><a href="http://blog.csdn.net/quqi99/article/category/674417" target="_blank">Linux / Unix / Shell</a>(24)</li>
<li><a href="http://blog.csdn.net/quqi99/article/category/328188" target="_blank">J2SE / JEE</a>(40)</li>
<li><a href="http://blog.csdn.net/quqi99/article/category/347580" target="_blank">DB / NoSQL</a>(9)</li>
<li><a href="http://blog.csdn.net/quqi99/article/category/803236" target="_blank">Architecture</a>(0)</li>
<li><a href="http://blog.csdn.net/quqi99/article/category/351802" target="_blank">Android</a>(3)</li>
<li><a href="http://blog.csdn.net/quqi99/article/category/803239" target="_blank">Life</a>(4)</li>
<li><a href="http://blog.csdn.net/quqi99/article/category/689016" target="_blank">Other</a>(9)</li>
<li><a href="http://blog.csdn.net/quqi99/article/category/1112756" target="_blank">OpenStack</a>(37)</li>
<li><a href="http://blog.csdn.net/quqi99/article/category/1139084" target="_blank">Python</a>(1)</li>
<li><a href="http://blog.csdn.net/quqi99/article/category/1167554" target="_blank">C / C++</a>(2)</li>
<li><p><a href="http://blog.csdn.net/quqi99/article/category/1490633" target="_blank">Networking</a>(1)
文章存档</p>
</li>
<li><p><a href="http://blog.csdn.net/quqi99/article/month/2013/08" target="_blank">2013年08月</a>(4)</p>
</li>
<li><a href="http://blog.csdn.net/quqi99/article/month/2013/07" target="_blank">2013年07月</a>(13)</li>
<li><a href="http://blog.csdn.net/quqi99/article/month/2013/06" target="_blank">2013年06月</a>(6)</li>
<li><a href="http://blog.csdn.net/quqi99/article/month/2013/05" target="_blank">2013年05月</a>(2)</li>
<li><a href="http://blog.csdn.net/quqi99/article/month/2013/04" target="_blank">2013年04月</a>(2)</li>
<li><a href="http://blog.csdn.net/quqi99/article/month/2013/03" target="_blank">2013年03月</a>(3)</li>
<li><a href="http://blog.csdn.net/quqi99/article/month/2013/02" target="_blank">2013年02月</a>(1)</li>
<li><a href="http://blog.csdn.net/quqi99/article/month/2013/01" target="_blank">2013年01月</a>(9)</li>
<li><a href="http://blog.csdn.net/quqi99/article/month/2012/12" target="_blank">2012年12月</a>(3)</li>
<li><a href="http://blog.csdn.net/quqi99/article/month/2012/11" target="_blank">2012年11月</a>(2)</li>
<li><a href="http://blog.csdn.net/quqi99/article/month/2012/08" target="_blank">2012年08月</a>(1)</li>
<li><a href="http://blog.csdn.net/quqi99/article/month/2012/07" target="_blank">2012年07月</a>(1)</li>
<li><a href="http://blog.csdn.net/quqi99/article/month/2012/06" target="_blank">2012年06月</a>(7)</li>
<li><a href="http://blog.csdn.net/quqi99/article/month/2012/05" target="_blank">2012年05月</a>(2)</li>
<li><a href="http://blog.csdn.net/quqi99/article/month/2012/04" target="_blank">2012年04月</a>(6)</li>
<li><a href="http://blog.csdn.net/quqi99/article/month/2012/03" target="_blank">2012年03月</a>(2)</li>
<li><a href="http://blog.csdn.net/quqi99/article/month/2012/02" target="_blank">2012年02月</a>(1)</li>
<li><a href="http://blog.csdn.net/quqi99/article/month/2011/12" target="_blank">2011年12月</a>(1)</li>
<li><a href="http://blog.csdn.net/quqi99/article/month/2011/09" target="_blank">2011年09月</a>(5)</li>
<li><a href="http://blog.csdn.net/quqi99/article/month/2011/08" target="_blank">2011年08月</a>(2)</li>
<li><a href="http://blog.csdn.net/quqi99/article/month/2011/06" target="_blank">2011年06月</a>(2)</li>
<li><a href="http://blog.csdn.net/quqi99/article/month/2011/04" target="_blank">2011年04月</a>(5)</li>
<li><a href="http://blog.csdn.net/quqi99/article/month/2011/03" target="_blank">2011年03月</a>(2)</li>
<li><a href="http://blog.csdn.net/quqi99/article/month/2011/01" target="_blank">2011年01月</a>(4)</li>
<li><a href="http://blog.csdn.net/quqi99/article/month/2010/12" target="_blank">2010年12月</a>(1)</li>
<li><a href="http://blog.csdn.net/quqi99/article/month/2010/07" target="_blank">2010年07月</a>(1)</li>
<li><a href="http://blog.csdn.net/quqi99/article/month/2010/05" target="_blank">2010年05月</a>(1)</li>
<li><a href="http://blog.csdn.net/quqi99/article/month/2010/04" target="_blank">2010年04月</a>(2)</li>
<li><a href="http://blog.csdn.net/quqi99/article/month/2010/03" target="_blank">2010年03月</a>(2)</li>
<li><a href="http://blog.csdn.net/quqi99/article/month/2010/02" target="_blank">2010年02月</a>(4)</li>
<li><a href="http://blog.csdn.net/quqi99/article/month/2010/01" target="_blank">2010年01月</a>(6)</li>
<li><a href="http://blog.csdn.net/quqi99/article/month/2009/12" target="_blank">2009年12月</a>(6)</li>
<li><a href="http://blog.csdn.net/quqi99/article/month/2009/11" target="_blank">2009年11月</a>(5)</li>
<li><a href="http://blog.csdn.net/quqi99/article/month/2009/10" target="_blank">2009年10月</a>(6)</li>
<li><a href="http://blog.csdn.net/quqi99/article/month/2009/09" target="_blank">2009年09月</a>(1)</li>
<li><a href="http://blog.csdn.net/quqi99/article/month/2009/08" target="_blank">2009年08月</a>(1)</li>
<li><a href="http://blog.csdn.net/quqi99/article/month/2009/06" target="_blank">2009年06月</a>(4)</li>
<li><a href="http://blog.csdn.net/quqi99/article/month/2009/03" target="_blank">2009年03月</a>(1)</li>
<li><a href="http://blog.csdn.net/quqi99/article/month/2008/11" target="_blank">2008年11月</a>(1)</li>
<li><a href="http://blog.csdn.net/quqi99/article/month/2008/10" target="_blank">2008年10月</a>(2)</li>
<li><a href="http://blog.csdn.net/quqi99/article/month/2008/08" target="_blank">2008年08月</a>(1)</li>
<li><a href="http://blog.csdn.net/quqi99/article/month/2008/06" target="_blank">2008年06月</a>(3)</li>
<li><a href="http://blog.csdn.net/quqi99/article/month/2008/04" target="_blank">2008年04月</a>(1)</li>
<li><a href="http://blog.csdn.net/quqi99/article/month/2008/03" target="_blank">2008年03月</a>(1)</li>
<li><a href="http://blog.csdn.net/quqi99/article/month/2008/02" target="_blank">2008年02月</a>(1)</li>
<li><a href="http://blog.csdn.net/quqi99/article/month/2008/01" target="_blank">2008年01月</a>(3)</li>
<li><a href="http://blog.csdn.net/quqi99/article/month/2007/12" target="_blank">2007年12月</a>(4)</li>
<li><a href="http://blog.csdn.net/quqi99/article/month/2007/11" target="_blank">2007年11月</a>(1)</li>
<li><a href="http://blog.csdn.net/quqi99/article/month/2007/10" target="_blank">2007年10月</a>(4)</li>
<li><a href="http://blog.csdn.net/quqi99/article/month/2007/08" target="_blank">2007年08月</a>(5)</li>
<li><a href="http://blog.csdn.net/quqi99/article/month/2007/07" target="_blank">2007年07月</a>(6)</li>
<li><a href="http://blog.csdn.net/quqi99/article/month/2007/06" target="_blank">2007年06月</a>(1)</li>
<li><a href="http://blog.csdn.net/quqi99/article/month/2007/05" target="_blank">2007年05月</a>(8)</li>
</ul>
<p>展开</p>
<p>阅读排行</p>
<ul>
<li><a href="http://blog.csdn.net/quqi99/article/details/1916553" title="Android学习笔记 ( by quqi99 )" target="_blank">Android学习笔记 ( by quqi99 )</a>(22132)</li>
<li><a href="http://blog.csdn.net/quqi99/article/details/1680647" title="java.lang.NoClassDefFoundError: org/apache/commons/io/output/DeferredFileOutputStream" target="_blank">java.lang.NoClassDefFoundError: org/apache/commons/io/output/DeferredFileOutputStream</a>(17851)</li>
<li><a href="http://blog.csdn.net/quqi99/article/details/7433285" title="建立openstack quantum开发环境" target="_blank">建立openstack quantum开发环境</a>(6747)</li>
<li><a href="http://blog.csdn.net/quqi99/article/details/1755642" title="在linux中安装字库( by quqi99 )" target="_blank">在linux中安装字库( by quqi99 )</a>(5151)</li>
<li><a href="http://blog.csdn.net/quqi99/article/details/1930304" title="Android手机客户端与Servlet交换数据(by quqi99)" target="_blank">Android手机客户端与Servlet交换数据(by quqi99)</a>(5140)</li>
<li><a href="http://blog.csdn.net/quqi99/article/details/5298017" title="ReentrantLock与synchronized的区别 ( by quqi99 )" target="_blank">ReentrantLock与synchronized的区别 ( by quqi99 )</a>(4864)</li>
<li><a href="http://blog.csdn.net/quqi99/article/details/5604732" title="个人户口档案转移笔记（适用北京集体户口）" target="_blank">个人户口档案转移笔记（适用北京集体户口）</a>(4802)</li>
<li><a href="http://blog.csdn.net/quqi99/article/details/1624225" title="Magnolia学习笔记（一个基于JSR170的内容管理系统） ( by quqi99 )" target="_blank">Magnolia学习笔记（一个基于JSR170的内容管理系统） ( by quqi99 )</a>(4342)</li>
<li><a href="http://blog.csdn.net/quqi99/article/details/1624218" title="JSpider学习笔记 ( by quqi99 )" target="_blank">JSpider学习笔记 ( by quqi99 )</a>(4149)</li>
<li><p><a href="http://blog.csdn.net/quqi99/article/details/3099945" title="Plone学习笔记 ( by quqi99 )" target="_blank">Plone学习笔记 ( by quqi99 )</a>(4057)
评论排行</p>
</li>
<li><p><a href="http://blog.csdn.net/quqi99/article/details/1916553" title="Android学习笔记 ( by quqi99 )" target="_blank">Android学习笔记 ( by quqi99 )</a>(21)</p>
</li>
<li><a href="http://blog.csdn.net/quqi99/article/details/1680647" title="java.lang.NoClassDefFoundError: org/apache/commons/io/output/DeferredFileOutputStream" target="_blank">java.lang.NoClassDefFoundError: org/apache/commons/io/output/DeferredFileOutputStream</a>(18)</li>
<li><a href="http://blog.csdn.net/quqi99/article/details/1930304" title="Android手机客户端与Servlet交换数据(by quqi99)" target="_blank">Android手机客户端与Servlet交换数据(by quqi99)</a>(12)</li>
<li><a href="http://blog.csdn.net/quqi99/article/details/5604732" title="个人户口档案转移笔记（适用北京集体户口）" target="_blank">个人户口档案转移笔记（适用北京集体户口）</a>(8)</li>
<li><a href="http://blog.csdn.net/quqi99/article/details/1624225" title="Magnolia学习笔记（一个基于JSR170的内容管理系统） ( by quqi99 )" target="_blank">Magnolia学习笔记（一个基于JSR170的内容管理系统） ( by quqi99 )</a>(7)</li>
<li><a href="http://blog.csdn.net/quqi99/article/details/2591768" title="使用itext生成word格式的报表(by quqi99)" target="_blank">使用itext生成word格式的报表(by quqi99)</a>(5)</li>
<li><a href="http://blog.csdn.net/quqi99/article/details/1755642" title="在linux中安装字库( by quqi99 )" target="_blank">在linux中安装字库( by quqi99 )</a>(4)</li>
<li><a href="http://blog.csdn.net/quqi99/article/details/6305061" title="Android分享文稿 ( by quqi99 )" target="_blank">Android分享文稿 ( by quqi99 )</a>(4)</li>
<li><a href="http://blog.csdn.net/quqi99/article/details/9156497" title="OpenDaylight学习 ( by quqi99 )" target="_blank">OpenDaylight学习 ( by quqi99 )</a>(3)</li>
<li><a href="http://blog.csdn.net/quqi99/article/details/2590703" title="使用jacob生成word(by quqi99)" target="_blank">使用jacob生成word(by quqi99)</a>(3)</li>
</ul>
<p>推荐文章
最新评论</p>
<ul>
<li><a href="http://blog.csdn.net/quqi99/article/details/9156497#comments" target="_blank">OpenDaylight学习 ( by quqi99 )</a></li>
</ul>
<p><a href="http://blog.csdn.net/quqi99" target="_blank">quqi99</a>: hi whoeversucks, 谢谢你的实时信息，非常有用，我已经更新到博客里了。另外，问个问题，...</p>
<ul>
<li><a href="http://blog.csdn.net/quqi99/article/details/9156497#comments" target="_blank">OpenDaylight学习 ( by quqi99 )</a></li>
</ul>
<p><a href="http://blog.csdn.net/whoeversucks" target="_blank">whoeversucks</a>: 注意，OpenDayLight Controller和OSCP实际上2个独立的SDN控制器项目（分别...</p>
<ul>
<li><a href="http://blog.csdn.net/quqi99/article/details/9102745#comments" target="_blank">给Linux虚机扩充硬盘空间 ( by quqi99 )</a></li>
</ul>
<p><a href="http://blog.csdn.net/quqi99" target="_blank">quqi99</a>: hi dalinhuang, 谢谢你的回复，你给的这个方法是只适合LVM场景的啊，我没有使用LVM。</p>
<ul>
<li><a href="http://blog.csdn.net/quqi99/article/details/9102745#comments" target="_blank">给Linux虚机扩充硬盘空间 ( by quqi99 )</a></li>
</ul>
<p><a href="http://blog.csdn.net/dalinhuang" target="_blank">dalinhuang</a>: 给根（/）扩充的步骤：（以你的virtualbox并使用LVM为例）1. 新增一块虚拟硬盘，给虚机。...</p>
<ul>
<li><a href="http://blog.csdn.net/quqi99/article/details/9156497#comments" target="_blank">OpenDaylight学习 ( by quqi99 )</a></li>
</ul>
<p><a href="http://blog.csdn.net/piaochenping" target="_blank">piaochenping</a>: 你好，为什么我安装时老是出现这个错误呢？ Failed to execute goal org.co...</p>
<ul>
<li><a href="http://blog.csdn.net/quqi99/article/details/7659905#comments" target="_blank">OpenStack CI测试之devstack-gate ( by quqi99 )</a></li>
</ul>
<p><a href="http://blog.csdn.net/quqi99" target="_blank">quqi99</a>: @sunyilong2012: 这种错误应该是差模块吧，可以单独安装一下试试, sudo pip i...</p>
<ul>
<li><a href="http://blog.csdn.net/quqi99/article/details/7411091#comments" target="_blank">Fedora 16上源码建立pydev + eclipse的OpenStack开发环境笔记草稿 ( by quqi99 )</a></li>
</ul>
<p><a href="http://blog.csdn.net/quqi99" target="_blank">quqi99</a>: openstack因为用到了一些linux特有的东西，如iptables，所以目前只能跑在linux...</p>
<ul>
<li><a href="http://blog.csdn.net/quqi99/article/details/7411091#comments" target="_blank">Fedora 16上源码建立pydev + eclipse的OpenStack开发环境笔记草稿 ( by quqi99 )</a></li>
</ul>
<p><a href="http://blog.csdn.net/javaerss" target="_blank">javaerss</a>: 大神...看哭了，为此特地跑去下载fedora 16来做实验。之前用ubuntu下用eclipse ...</p>
<ul>
<li><a href="http://blog.csdn.net/quqi99/article/details/6576375#comments" target="_blank">玩转play framework ( by quqi99 )</a></li>
</ul>
<p><a href="http://blog.csdn.net/nanfu08" target="_blank">nanfu08</a>: 你能看得清，如果只是自己看的话我没话说，这样的文字叫人怎么读？？</p>
<ul>
<li><a href="http://blog.csdn.net/quqi99/article/details/7659905#comments" target="_blank">OpenStack CI测试之devstack-gate ( by quqi99 )</a></li>
</ul>
<p><a href="http://blog.csdn.net/sunyilong2012" target="_blank">dragonsun</a>: 您好，我在做这个测试的时候遇到了无法导入statsd的问题，请问您有解决的方法吗？+ /home/j...</p>
<p><a href="http://www.csdn.net/company/about.html" target="_blank">公司简介</a>|<a href="http://www.csdn.net/company/recruit.html" target="_blank">招贤纳士</a>|<a href="http://www.csdn.net/company/marketing.html" target="_blank">广告服务</a>|<a href="http://www.csdn.net/company/account.html" target="_blank">银行汇款帐号</a>|<a href="http://www.csdn.net/company/contact.html" target="_blank">联系方式</a>|<a href="http://www.csdn.net/company/statement.html" target="_blank">版权声明</a>|<a href="http://www.csdn.net/company/layer.html" target="_blank">法律顾问</a>|<a href="mailto:webmaster@csdn.net">问题报告</a><a href="http://wpa.qq.com/msgrd?v=3&amp;uin=2355263776&amp;site=qq&amp;menu=yes" target="_blank">QQ客服</a> <a href="http://e.weibo.com/csdnsupport/profile" target="_blank">微博客服</a> <a href="http://bbs.csdn.net/forums/Service" target="_blank">论坛反馈</a> <a href="mailto:webmaster@csdn.net">联系邮箱：webmaster@csdn.net</a> 服务热线：400-600-2320京 ICP 证 070598 号北京创新乐知信息技术有限公司 版权所有世纪乐知(北京)网络技术有限公司 提供技术支持江苏乐知网络技术有限公司 提供商务支持Copyright © 1999-2012, CSDN.NET, All Rights Reserved <a href="http://www.hd315.gov.cn/beian/view.asp?bianhao=010202001032100010" target="_blank"><img src="" alt="GongshangLogo"></a>
<img src="http://counter.csdn.net/pv.aspx?id=24" alt=""></p>

      
    </div>
    
    
      
    
    
        <footer id="post-meta">
        <span class="categories">Posted in<span class="breadcrumb fa fa-folder"><li><a href="/categories/hadoop/">hadoop</a></li></span></span> | <span class="tags">Tagged <a href="/tags/hadoop/" class="label label-primary">hadoop</a></span> | <span class="time">recent updated:<time title="2014-03-07 09:54:36"datetime="2014-03-07 09:54:36"> mar. 7 2014</time></span> | <span class="comment-link">
<a href="http://itsolife.com/2014/02/02/2014-02-02-hadoop--Hadoop知识分享文稿byquqi99-技术并艺术着/#comments" class="ds-thread-count comment-link" data-thread-key="2014-02-02-hadoop--Hadoop知识分享文稿byquqi99-技术并艺术着" data-count-type="comments">暂无评论</a></span>
        </footer>
    
    <div class="clearfix"></div>
  </div>
</div>
</article>





<ul id="pagination" class="pagination pagination-lg">
  <li><a class="extend prev" href="/page/105/">&laquo;</a></li><li><a class="page-number" href="/">1</a></li><li><a class="page-number" href="/page/2/">2</a></li><li><span class="space">&hellip;</span></li><li><a class="page-number" href="/page/103/">103</a></li><li><a class="page-number" href="/page/104/">104</a></li><li><a class="page-number" href="/page/105/">105</a></li><li class="active"><li><span class="page-number current">106</span></li><li><a class="page-number" href="/page/107/">107</a></li><li><a class="page-number" href="/page/108/">108</a></li><li><a class="page-number" href="/page/109/">109</a></li><li><span class="space">&hellip;</span></li></li><li><a class="page-number" href="/page/161/">161</a></li><li><a class="page-number" href="/page/162/">162</a></li><li><a class="extend next" href="/page/107/">&raquo;</a></li>
  <div class="clearfix"></div>
</ul></div><!--wapper-->
       </div><!-- ID main-col END -->
       <aside id="sidebar" class="alignright col-sx-6 col-sm-4 col-md-3 col-lg-3">
<div id="widget_search" class="widget panel panel-primary">
    <form action="//google.com/search" method="get" accept-charset="utf-8">
  <div class="input-group">
    <input class="form-control" id="searchbox" type="search" name="q" results="0" placeholder="search">
    <span class="input-group-btn">
      <button class="btn btn-default" type="submit">Go!</button>
    </span>
    <input type="hidden" name="q" value="site:itsolife.com">
  </div>
</form>
</div>

<div id="widget_category" class="widget panel panel-primary">
  <div class="panel-heading">category</div>  <div data-src='category' class='ajax_widgets'>正在加载...</div>
</div>

<div id="widget_recent_posts" class="widget panel panel-primary">
  <div class="panel-heading">recent posts</div>  <div data-src='recent_posts' class='ajax_widgets'>正在加载...</div></div>

<div id="widget_tagcloud" class="widget panel panel-primary">
  <div class="panel-heading">tagcloud</div>  <div data-src='tagcloud' class='ajax_widgets'>正在加载...</div></div>

<div id="widget_latest_update_posts" class="widget panel panel-primary">
  <div class="panel-heading">最近更新</div>  <div data-src='latest_update_posts' class='ajax_widgets'>正在加载...</div></div>

<div id="widget_recent_comments" class="widget panel panel-primary">
  <div class="panel-heading">recent comments</div>  

<div class="list-group-item ds-recent-comments" data-num-items="6" data-show-avatars="1" data-show-time="1" data-show-title="1" data-show-admin="1" data-excerpt-length="50"></div>



</div>

</aside>
       <div class="clearfix"></div>
     </div><!-- row END -->
  </div>
  <footer id="footer" class="container">
  <div class="panel panel-info">
  <section id='footer_widget'></section>  <div class="panel-footer">
  <div id="site-info">
    <span class='author'>
  
  &copy; 2014 RobinChia
  
    &nbsp;&nbsp;</span>
  
  <span id='analytics-51la'></span><span id='analytics-google'>
<script type="text/javascript">
  var _gaq = _gaq || [];
  _gaq.push(['_setAccount', 'UA-48559895-1']);
  _gaq.push(['_trackPageview']);
  _js2load.push({src:('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js'});
</script></span><span id='analytics-cnzz'>
<script type="text/javascript">var cnzz_protocol = (("https:" == document.location.protocol) ? " https://" : " http://");document.write(unescape("%3Cspan id='cnzz_stat_icon_5774006'%3E%3C/span%3E%3Cscript src='" + cnzz_protocol + "s17.cnzz.com/stat.php%3Fid%3D5774006%26show%3Dpic' type='text/javascript'%3E%3C/script%3E"));</script>
</span><span id='analytics-baidu'>
<script>
var _hmt = _hmt || [];
_js2load.push({src:"//hm.baidu.com/hm.js?1442f50724afc42380b51f097c43082c"});
</script>
</span>  </div>
  <div id="copyright">Site powered by <a href='http://zespia.tw/hexo/'><strong>hexo</strong></a>  update time: <em>2014-04-07 19:05:47</em></span></div>
</div>
<div class="clearfix"></div>


  </div>
  </footer>
  
        <script src="http://cdn.bootcss.com/jquery/1.10.2/jquery.min.js"></script>        
        <script src="http://cdn.staticfile.org/twitter-bootstrap/3.1.0/js/bootstrap.min.js"></script>        
                
        <script src="http://cdn.bootcss.com/prettify/r298/prettify.min.js"></script>    <script type="text/javascript">
   var lang=["bsh", "c", "cc", "cpp", "cs", "csh", "cyc", "cv", "htm", "html",
    "java", "js", "m", "mxml", "perl", "pl", "pm", "py", "rb", "sh",
    "xhtml", "xml", "xsl"];
   var pretty_base='';
   $('script').each(function(){
	var c = $(this).attr('src');
	if (!c)
	    return;
	if (c.match(/(\/)?prettify(\.min)?\.js/i))
	{
	    var index = c.lastIndexOf('/');
	    if (index != -1)
		pretty_base = c.substr(0,index + 1);
	    return false;
	}
   })
   $('pre code').each(function(){
	var c = $(this).attr('class')
	if (!c)
	    return;
	c = c.match(/\s?(lang\-\w+)/i);
	if (c && lang.indexOf(c[1]) == -1)
	{
	    lang.push(c[1]);
	    $.getScript(pretty_base + c[1] + '.min.js');
	}
   })

    $(window).load(function(){
       $("pre").addClass("prettyprint");
       prettyPrint();
    })
</script>    
            <script type="text/javascript">
var duoshuoQuery = {short_name:"robinchia"};
_js2load.push({src:'http://static.duoshuo.com/embed.js',charset:'UTF-8'});
</script>
    
            <!--wumii_relatedItems-->
    
        <script src="http://cdn.bootcss.com/fancybox/2.1.5/jquery.fancybox.min.js"></script>    <script type="text/javascript">
(function($){
  $('.entry').each(function(i){
    $(this).find('img').each(function(){
      var alt = this.alt;

      if (alt){
        $(this).before('<span class="caption">' + alt + '</span>');
      }

      $(this).wrap('<a href="' + this.src + '" title="' + alt + '" class="fancybox" rel="fancybox' + i + '" />');
    });
  });

  $('.fancybox').fancybox();
})(jQuery);
</script>    
        <script src="http://cdn.bootcss.com/mathjax/2.3/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>    <script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      processEscapes: true
    }
  });
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>
    
<script type="text/javascript">
$('.ajax_widgets').each(function(){var src=$(this).attr('data-src');if(src)$(this).load('/widgets/'+src+'.html');});
$.each(_js2load,function(index,obj){loadjs(obj.src,obj.charset)});
</script>

<div id="scroll2top">
<img src="/scroll2top/arrow.png"/>
</div>
<script src="/scroll2top/scroll2top.min.js"></script>
<div id="winterland">
  <canvas></canvas>
</div>
<script src="/js/winterland.min.js"></script>

  </body>
</html>
