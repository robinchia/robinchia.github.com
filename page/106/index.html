
<!DOCTYPE HTML>
<html>
<head>
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<title>The 106 Page | It so life</title>
<meta name="author" content="RobinChia">

<meta name="description" content="It so life">


<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">


<meta property="og:site_name" content="It so life"/>

<!--[if IE]><style>.testIE.IE{display:inline;}</style><![endif]-->
<!--[if lte IE 7]><link rel="stylesheet" href="/css/ie7.css" type="text/css"><![endif]-->
<!--[if (lt IE 9)&(gt IE 7)]><style>.testIE.IE8{display:inline;}</style><![endif]-->
<!--[if gt IE 8]><style>.testIE.IE9{display:inline;}</style><![endif]-->

<link href="/favicon.png" rel="icon">
<link rel="alternate" href="/atom.xml" title="It so life Feed" type="application/atom+xml">

<link rel="stylesheet" href="http://cdn.staticfile.org/twitter-bootstrap/3.1.0/css/bootstrap.min.css" type="text/css">
<link rel="stylesheet" href="http://cdn.bootcss.com/font-awesome/4.0.3/css/font-awesome.min.css" type="text/css">
<link rel="stylesheet" href="http://cdn.bootcss.com/prettify/r298/prettify.min.css" type="text/css">
<link rel="stylesheet" href="http://cdn.bootcss.com/fancybox/2.1.5/jquery.fancybox.min.css" type="text/css">
<link rel="stylesheet" href="/css/style.css" type="text/css">
<!--[if lt IE 9]>
   <style>article,aside,dialog,footer,header,section,footer,nav,figure,menu{display:block}</style>
   <script src="http://cdn.staticfile.org/html5shiv/r29/html5.min.js"></script>
   <script src="http://cdn.staticfile.org/respond.js/1.4.2/respond.min.js"></script>
<link href="http://cdn.staticfile.org/respond.js/1.4.2/respond-proxy.html" id="respond-proxy" rel="respond-proxy" />
<link href="/js/respond.proxy.gif" id="respond-redirect" rel="respond-redirect" />
<script src="/js/respond.proxy.js"></script>
   <script src="http://cdn.bootcss.com/selectivizr/1.0.2/selectivizr-min.js"></script>
<![endif]-->
<script type="text/javascript">
function loadjs(c,d){var a=document.createElement("script");a.async=!0;a.type="text/javascript";a.src=c;a.charset=d||"gbk";(document.getElementsByTagName("HEAD")[0]||document.getElementsByTagName("BODY")[0]).appendChild(a)};
var _protocol = (("https:" == document.location.protocol) ? " https://" : " http://");
var _js2load = [];
</script>

</head>
<body>
      <header id="header" class="container"><nav id="main-nav" class="navbar navbar-default navbar-fixed-top " role="navigation">
  <div class="container">
    <div class="navbar-header">
    <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".navbar-collapse">
      <span class="sr-only">Toggle navigation</span>
      <span class="icon-bar"></span>
      <span class="icon-bar"></span>
      <span class="icon-bar"></span>
    </button>
    <a class="navbar-brand" href="/">It so life</a>
    </div>
    <div  class="collapse navbar-collapse">
      <ul  class="nav navbar-nav">
  
        <li><a href="/" title="Home">Home</a></li>      
        <li><a href="/about/" title="About">About</a></li>      
        <li><a href="/archives/" title="Archives">Archives</a></li>      
      <li class='dropdown'>
        <a class='dropdown-toggle' data-toggle='dropdown' href='#'>website<b class='caret'></b></a>
        <ul class='dropdown-menu pure-menu-selected'>
    
          <li><a href="//groups.google.com/forum/#!forum/pongba" title="TopLanguage">TopLanguage</a></li>    
          <li><a href="//itpub.net/" title="ITPub">ITPub</a></li>    
          <li><a href="//blog.jobbole.com/" title="Bole">Bole</a></li>    
          <li><a href="//nosql-database.org/" title="nosql">nosql</a></li>    
          <li><a href="//gitimmersion.googol.im/" title="Git">Git</a></li>    
        </ul>
      </li>
    
      </ul>
      <ul class="nav navbar-nav navbar-right">
      
        <li><a href="/atom.xml">RSS</a></li>
      
      
        <li><a href="https://twitter.com/robinchia">twitter</a></li>
      
      
      
      
        <li><a href="https://github.com/robinchia">github</a></li>
      
      </ul>
    </div>
  </div>
</nav>
<div class="clearfix"></div>
</header>
  <div id='content' class="container">
     <div class="page-header-wrapper">
      <!--[if lt IE 9]><div class="alert alert-warning alert-dismissable"><button type="button" class="close" data-dismiss="alert" aria-hidden="true">&times;</button><strong>提示:</strong>您的浏览器版本太低了,建议升级到 <strong><a href="http://windows.microsoft.com/zh-cn/internet-explorer/download-ie" title="IE9">IE9</a></strong> 以上,本站使用<a href="https://www.google.com/intl/zh-CN/chrome/">Chrome浏览器</a>可以获得最好的显示效果.</div><![endif]-->
      <div class="page-header"><h1 align="center"><big>It so life</big> </h1>
        <h5 align="center"><big>love as life</big></h5>
      </div>
     </div>
     <div class="row">
       <div id="main-col" class="alignleft col-sx-12 col-sm-8 col-md-9 col-lg-9">
      <section id='header_widget'></section>
          <div id="wrapper"><article>
<div id="post" class="post well">
  <div class="post-content">
    <header class="well-sm">
      <i class="fa icon fa-5x pull-left"></i>
      <h1 class="title"><a href="/2014/02/02/2014-02-02-linux--linux之awk用法/">linux之awk用法</a></h1>
      
        <span>Posted on<time datetime="2014-02-02T01:54:36.000Z"> <a href="/2014/02/02/2014-02-02-linux--linux之awk用法/">feb. 2 2014</a></time></span>      
    </header>
    
    
    <div class="entry">
      
        <h1 id="linux-awk-">linux之awk用法</h1>
<p>awk是一个非常棒的数字处理工具。相比于sed常常作用于一整行的处理，awk则比较倾向于将一行分为数个“字段”来处理。运行效率高，而且代码简单，对格式化的文本处理能力超强。先来一个例子：
文件a，统计文件a的第一列中是浮点数的行的浮点数的平均值。用awk来实现只需要一句话就可以搞定
$cat a
1.021 33
1/#.ll   44
2.53 6
ss    7
awk &#39;BEGIN{total = 0;len = 0} {if($1~/^[0-9]+.[0-9]/<em>/){total += $1; len++}} END{print total/len}&#39; a
（分析：$1~/^[0-9]+.[0-9]/</em>/表示$1与“/ /”里面的正则表达式进行匹配，若匹配，则total加上$1，且len自增，即数目加1.“^[0-9]+.[0-9]/<em>”是个正则表达式，“^[0-9]”表示以数字开头，“.”是转义的意思，表示“.”为小数点的意思。“[0-9]/</em>”表示0个或多个数字）</p>
<p>awk的一般语法格式为：
awk [-参数 变量] &#39;BEGIN{初始化}条件类型1{动作1}条件类型2{动作2}。。。。END{后处理}&#39;
其中：BEGIN和END中的语句分别在开始读取文件（in_file）之前和读取完文件之后发挥作用，可以理解为初始化和扫尾。
<strong>（1）参数说明：</strong>
 -F re：允许awk更改其字段分隔符
      -v var=$v 把v值赋值给var，如果有多个变量要赋值，那么就写多个-v，每个变量赋值对应一个-v
e.g. 要打印文件a的第num行到num+num1行之间的行，
awk -v num=$num -v num1=$num1 &#39;NR==num,NR==num+num1{print}&#39; a
-f progfile：允许awk调用并执行progfile程序文件，当然progfile必须是一个符合awk语法的程序文件。</p>
<p><strong>（2）awk内置变量：</strong>
<strong>ARGC</strong>    命令行参数的个数
<strong>ARGV  </strong> 命令行参数数组
<strong>ARGIND</strong> 当前被处理文件的ARGV标志符
e.g 有两个文件a 和b
awk &#39;{if(ARGIND==1){print &quot;处理a文件&quot;} if(ARGIND==2){print &quot;处理b文件&quot;}}&#39; a b
文件处理的顺序是先扫描完a文件，再扫描b文件</p>
<p><strong>NR 　　</strong>已经读出的记录数
<strong>FNR</strong>   　当前文件的记录数
上面的例子也可以写成这样：
awk &#39;NR==FNR{print &quot;处理文件a&quot;} NR &gt; FNR{print &quot;处理文件b&quot;}&#39; a b
输入文件a和b，由于先扫描a，所以扫描a的时候必然有NR==FNR，然后扫描b的时候，FNR从1开始计数，而NR则接着a的行数继续计数，所以NR &gt; FNR</p>
<p>e.g 要显示文件的第10行至第15行
awk &#39;NR==10,NR==15{print}&#39; a</p>
<p><strong>FS 　　</strong>输入字段分隔符（缺省为:space:），相当于-F选项
awk -F &#39;:&#39; &#39;{print}&#39; a    和   awk &#39;BEGIN{FS=&quot;:&quot;}{print}&#39; a 是一样的</p>
<p><strong>OFS</strong>输出字段分隔符（缺省为:space:）
awk -F &#39;:&#39; &#39;BEGIN{OFS=&quot;;&quot;}{print $1,$2,$3}&#39; b
如果cat b为
1:2:3
4:5:6
那么把OFS设置成&quot;;&quot;后就会输出
1;2;3
4;5;6
（小注释：awk把分割后的第1、2、3个字段用$1,$2,$3...表示，$0表示整个记录（一般就是一整行））</p>
<p><strong>NF</strong>：当前记录中的字段个数
awk -F &#39;:&#39; &#39;{print NF}&#39; b的输出为
3
3
表明b的每一行用分隔符&quot;:&quot;分割后都3个字段
可以用NF来控制输出符合要求的字段数的行，这样可以处理掉一些异常的行
awk -F &#39;:&#39; &#39;{if (NF == 3)print}&#39; b</p>
<p><strong>RS</strong>：输入记录分隔符，缺省为&quot;\n&quot;
缺省情况下，awk把一行看作一个记录；如果设置了RS，那么awk按照RS来分割记录
例如，如果文件c，cat c为
hello world; I want to go swimming tomorrow;hiahia
运行 awk &#39;BEGIN{ RS = &quot;;&quot; } {print}&#39; c 的结果为
hello world
I want to go swimming tomorrow
hiahia
合理的使用RS和FS可以使得awk处理更多模式的文档，例如可以一次处理多行，例如文档d cat d的输出为
1 2
3 4 5
6 7
8 9 10
11 12</p>
<p>hello
每个记录使用空行分割，每个字段使用换行符分割，这样的awk也很好写
awk &#39;BEGIN{ FS = &quot;\n&quot;; RS = &quot;&quot;} {print NF}&#39; d 输出
2
3
1</p>
<p><strong>ORS</strong>：输出记录分隔符，缺省为换行符，控制每个print语句后的输出符号
awk &#39;BEGIN{ FS = &quot;\n&quot;; RS = &quot;&quot;; ORS = &quot;;&quot;} {print NF}&#39; d 输出
2;3;1
<strong>（3）awk读取shell中的变量</strong>
可以使用-v选项实现功能
     $b=1
     $cat f
     apple
$awk -v var=$b &#39;{print var, $var}&#39; f
1 apple
至于有没有办法把awk中的变量传给shell呢，这个问题我是这样理解的。shell调用awk实际上是fork一个子进程出来，而子进程是无法向父进程传递变量的，除非用重定向（包括管道）
a=$(awk &#39;{print $b, &#39;$b&#39;}&#39; f)
$echo $a
apple 1</p>
<p><strong>**（4）</strong>输出重定向**</p>
<p>awk的输出重定向类似于shell的重定向。重定向的目标文件名必须用双引号引用起来。
$awk &#39;$4 &gt;=70 {print $1,$2 &gt; &quot;destfile&quot; }&#39; filename
$awk &#39;$4 &gt;=70 {print $1,$2 &gt;&gt; &quot;destfile&quot; }&#39; filename</p>
<p><strong>（5）awk中调用shell命令：</strong></p>
<p>1)使用<strong>管道</strong>
awk中的管道概念和shell的管道类似，都是使用&quot;|&quot;符号。如果在awk程序中打开了管道，必须先关闭该管道才能打开另一个管道。也就是说一次只能打开一个管道。shell命令必须被双引号引用起来。“如果打算再次在awk程序中使用某个文件或管道进行读写，则可能要先关闭程序，因为其中的管道会保持打开状态直至脚本运行结束。注意，管道一旦被打开，就会保持打开状态直至awk退出。因此END块中的语句也会收到管道的影响。（可以在END的第一行关闭管道）”
awk中使用管道有两种语法，分别是：
awk output | shell input
shell output | awk input</p>
<p>对于awk output | shell input来说，shell接收awk的输出，并进行处理。需要注意的是，awk的output是先缓存在pipe中，等输出完毕后再调用shell命令 处理，shell命令只处理一次，而且处理的时机是“awk程序结束时，或者管道关闭时（需要显式的关闭管道）”
$awk &#39;/west/{count++} {printf &quot;%s %s\t\t%-15s\n&quot;, $3,$4,$1 | &quot;sort +1&quot;} END{close &quot;sort +1&quot;; printf &quot;The number of sales pers in the western&quot;; printf &quot;region is &quot; count &quot;.&quot; }&#39; datafile （解释：/west/{count++}表示与“wes”t进行匹配，若匹配，则count自增）
printf函数用于将输出格式化并发送给管道。所有输出集齐后，被一同发送给sort命令。必须用与打开时完全相同的命令来关闭管道(sort +1)，否则END块中的语句将与前面的输出一起被排序。此处的sort命令只执行一次。</p>
<p>在shell output | awk input中awk的input只能是getline函数。shell执行的结果缓存于pipe中，再传送给awk处理，如果有多行数据，awk的getline命令可能调用多次。
来源： <a href="[http://www.cnblogs.com/dong008259/archive/2011/12/06/2277287.html](http://www.cnblogs.com/dong008259/archive/2011/12/06/2277287.html)">[http://www.cnblogs.com/dong008259/archive/2011/12/06/2277287.html](http://www.cnblogs.com/dong008259/archive/2011/12/06/2277287.html)</a> </p>

      
    </div>
    
    
      
    
    
        <footer id="post-meta">
        <span class="categories">Posted in<span class="breadcrumb fa fa-folder"><li><a href="/categories/linux/">linux</a></li></span></span> | <span class="tags">Tagged <a href="/tags/linux/" class="label label-primary">linux</a></span> | <span class="time">recent updated:<time title="2014-03-07 09:54:36"datetime="2014-03-07 09:54:36"> mar. 7 2014</time></span> | <span class="comment-link">
<a href="http://itsolife.com/2014/02/02/2014-02-02-linux--linux之awk用法/#comments" class="ds-thread-count comment-link" data-thread-key="2014-02-02-linux--linux之awk用法" data-count-type="comments">暂无评论</a></span>
        </footer>
    
    <div class="clearfix"></div>
  </div>
</div>
</article>



  <article>
<div id="post" class="post well">
  <div class="post-content">
    <header class="well-sm">
      <i class="fa icon fa-5x pull-left"></i>
      <h1 class="title"><a href="/2014/02/02/2014-02-02-hadoop--Hadoop知识分享文稿byquqi99-技术并艺术着/">Hadoop知识分享文稿 ( by quqi99 ) </a></h1>
      
        <span>Posted on<time datetime="2014-02-02T01:54:36.000Z"> <a href="/2014/02/02/2014-02-02-hadoop--Hadoop知识分享文稿byquqi99-技术并艺术着/">feb. 2 2014</a></time></span>      
    </header>
    
    
    <div class="entry">
      
        <h1 id="hadoop-by-quqi99-">Hadoop知识分享文稿 ( by quqi99 ) - 技术并艺术着</h1>
<p>您还未登录！|<a href="https://passport.csdn.net/account/login" target="_blank">登录</a>|<a href="https://passport.csdn.net/account/register" target="_blank">注册</a>|<a href="https://passport.csdn.net/help/faq" target="_blank">帮助</a></p>
<ul>
<li><a href="http://www.csdn.net/" target="_blank">首页</a></li>
<li><a href="http://news.csdn.net/" target="_blank">业界</a></li>
<li><a href="http://mobile.csdn.net/" target="_blank">移动</a></li>
<li><a href="http://cloud.csdn.net/" target="_blank">云计算</a></li>
<li><a href="http://sd.csdn.net/" target="_blank">研发</a></li>
<li><a href="http://bbs.csdn.net/" target="_blank">论坛</a></li>
<li><a href="http://blog.csdn.net/" target="_blank">博客</a></li>
<li><a href="http://download.csdn.net/" target="_blank">下载</a></li>
<li><h2 id="-"><a href="">更多</a></h2>
</li>
</ul>
<h1 id="-http-blog-csdn-net-quqi99-"><a href="http://blog.csdn.net/quqi99" target="_blank">技术并艺术着</a></h1>
<h2 id="-blog">张华的技术Blog</h2>
<ul>
<li><a href="http://blog.csdn.net/quqi99?viewmode=contents" target="_blank"><img src="" alt="">目录视图</a></li>
<li><a href="http://blog.csdn.net/quqi99?viewmode=list" target="_blank"><img src="" alt="">摘要视图</a></li>
<li><a href="http://blog.csdn.net/quqi99/rss/list" target="_blank"><img src="" alt="">订阅</a>
<a href="https://code.csdn.net/blog/12" target="_blank">公告：博客新增直接引用代码功能</a>        <a href="http://www.csdn.net/article/2013-08-06/2816471" target="_blank">专访李铁军：从医生到金山首席安全专家的转变</a>      <a href="http://blog.csdn.net/csdnproduct/article/details/9495139" target="_blank">CSDN博客频道自定义域名、标签搜索功能上线啦！</a>      <a href="http://blog.csdn.net/adali/article/details/9813651" target="_blank">独一无二的职位：开源社区经理</a></li>
</ul>
<h3 id="-hadoop-by-quqi99-"><a href="">[置顶] Hadoop知识分享文稿 ( by quqi99 )</a></h3>
<p>分类： <a href="http://blog.csdn.net/quqi99/article/category/328029" target="_blank">Seach Engine</a>  2011-03-31 15:19 1977人阅读 <a href="">评论</a>(0) <a href="&quot;收藏&quot;">收藏</a> <a href="&quot;举报&quot;">举报</a>
<a href="http://blog.csdn.net/tag/details.html?tag=hadoop" target="_blank">hadoop</a><a href="http://blog.csdn.net/tag/details.html?tag=%e4%bb%bb%e5%8a%a1" target="_blank">任务</a><a href="http://blog.csdn.net/tag/details.html?tag=mapreduce" target="_blank">mapreduce</a><a href="http://blog.csdn.net/tag/details.html?tag=%e4%bb%bb%e5%8a%a1%e8%b0%83%e5%ba%a6" target="_blank">任务调度</a><a href="http://blog.csdn.net/tag/details.html?tag=%e9%9b%86%e7%be%a4" target="_blank">集群</a><a href="http://blog.csdn.net/tag/details.html?tag=%e4%bd%9c%e4%b8%9a" target="_blank">作业</a></p>
<p>目录<a href="&quot;系统根据文章中H1到H6标签自动生成文章目录&quot;">(?)</a><a href="&quot;展开&quot;">[+]</a></p>
<ol>
<li><a href="">作者张华 写于2010-08-15   发表于2011-03-31 版权声明可以任意转载转载时请务必以超链接形式标明文章原始出处和作者信息及本版权声明</a></li>
<li><a href="">httpblogcsdnnetquqi99</a></li>
<li><p><a href="">hadoop 理论基础</a></p>
</li>
<li><p><a href="">hadoop 是什么</a></p>
</li>
<li><a href="">hadoop 项目</a></li>
<li><a href="">MapReduce 任务的运行流程</a></li>
<li><p><a href="">MapReduce 任务的数据流图</a></p>
</li>
<li><p><a href="">hadoop 入门实战</a></p>
</li>
<li><p><a href="">测试环境</a></p>
</li>
<li><a href="">测试程序</a></li>
<li><a href="">属性配置</a></li>
<li><a href="">免密码 SSH 设置</a></li>
<li><a href="">配置 hosts</a></li>
<li><a href="">格式化 HDFS 文件系统</a></li>
<li><a href="">启动守护进程</a></li>
<li><p><a href="">运行程序</a></p>
</li>
<li><p><a href="">hadoop 高级进阶</a></p>
</li>
<li><a href="">hadoop 应用案例</a></li>
<li><a href="">参考文献</a><pre><code>                       **Hadoop知识分享文稿 ( by quqi99 )**
</code></pre></li>
</ol>
<h2 id="-2010-08-15-2011-03-31"><a href=""></a>作者：张华 写于：2010-08-15   发表于：2011-03-31</h2>
<p>版权声明：可以任意转载，转载时请务必以超链接形式标明文章原始出处和作者信息及本版权声明</p>
<h2 id="-http-blog-csdn-net-quqi99-"><a href=""></a>( <a href="http://blog.csdn.net/quqi99">http://blog.csdn.net/quqi99</a> )</h2>
<p><strong>内容目录</strong></p>
<p>目 录</p>
<p>1 hadoop 理论基础 3</p>
<p>1.1 hadoop 是什么 3</p>
<p>1.2 hadoop 项目 3</p>
<p>1.3 Map/Reduce 任务的运行流程 4</p>
<p>1.4 Map/Reduce 任务的数据流图 5</p>
<p>2 hadoop 入门实战 7</p>
<p>2.1  测试环境 7</p>
<p>2.2  测试程序 7</p>
<p>2.3  属性配置 9</p>
<p>2.4  免密码SSH 设置 10</p>
<p>2.5  配置hosts 11</p>
<p>2.6  格式化HDFS 文件系统 11</p>
<p>2.7  启动守护进程 11</p>
<p>2.8  运行程序 11</p>
<p>3 hadoop 高级进阶 12</p>
<p>4 hadoop 应用案例 12</p>
<p>5  参考文献 12</p>
<h1 id="-1-hadoop-"><a href=""></a>1 hadoop  理论基础</h1>
<h2 id="-1-1-hadoop-"><a href=""></a>1.1 hadoop  是什么</h2>
<p>Hadoop  是 Doug Cutting  开发的，他是一个相当牛的哥们，他同时是大名鼎鼎的 Lucene  及 Nutch  的作者。</p>
<p>我是这样理解 hadoop  的，它就是用来对海量数据进行存储与分析的一个开源软件。它包括两块：</p>
<p>1  ） HDFS ( Hadoop Distrubuted File System )  ，可以对重要数据进行冗余存储，有点类似于冗余磁盘陈列。</p>
<p>2  ）对 Map/Reduce  编程模型的一个实现。当然，关系型数据库（ RDBMS  ）也能做类似的事情，但为什么不用 RDBMS  呢？我们知道，让计算移动于数据上比让数据移动到计算更有效率。这使得 Map/Reduce  适合数据被一次写入和多次读取的应用，而 RDBMS  更适合持续更新的数据集。</p>
<h2 id="-1-2-hadoop-"><a href=""></a>1.2 hadoop  项目</h2>
<p>如今，广义上的 Hadoop  已经发展成为一个分布式计算基础架构这把“大伞”下相关子项目的集合，其技术栈如下图所示：</p>
<p>图：</p>
<pre><code>                                     ![]()
</code></pre><p><img src="http://blog.csdn.net/root/Library/Caches/TemporaryItems/moz-screenshot-4.png" alt=""></p>
<pre><code>                                                图1 hadoop 的子项目
</code></pre><ul>
<li>Core ： 一系列分布式文件系统和通用I/O 的组件和接口( 序列化、Java RPC 和持久化数据结构) 。</li>
<li>Avro ： 用于数据的序列化，当然，JDK 中也有Seriable 接口，但hadoop 中有它自己的序列化方式，具说更有效率。</li>
<li>MapReduce ： 分布式数据处理模式和执行环境，运行于大型商用机集群。</li>
<li>HDFS ： 分布式文件系统，运行于大型商用机集群。</li>
<li>Pig ： HDFS 上的数据检索语言，类似于RDBMS 中的SQL 语言。</li>
<li>Hbase ： 一个分布式的、列存储数据库。HBase 使用HDFS 作为底层存储，同时支持MapReduce 的批量式计算和点查询( 随机读取) 。</li>
<li>ZooKeeper ： 一个分布式的、高可用性的协调服务。ZooKeeper 提供分布式锁之类的基本服务用于构建分布式应用。</li>
<li>Hive ： 分布式数据仓库。Hive 管理HDFS 中存储的数据，并提供基于SQL 的查询语言( 由运行时引擎翻译成MapReduce 作业) 用以查询数据。</li>
<li>Chukwa ： 分布式数据收集和分析系统。Chukwa 运行HDFS 中存储数据的收集器，它使用MapReduce 来生成报告。</li>
</ul>
<h2 id="-1-3-map-reduce-"><a href=""></a> 1.3 Map/Reduce  任务的运行流程</h2>
<pre><code>                 ![]()
</code></pre><p>JobClient  的  submitJob()  方法的作业提交过程如下：</p>
<p>1  ）向 Jobtraker  请求一个新作业 ID</p>
<p>2  ） 调用 JobTracker  的 getNewJobId()</p>
<p>3  ）  JobClient  进行作业划分，并将划分后的输入及作业的 JAR  文件、配置文件等复制到 HDFS  中去</p>
<p>4  ） 提交作业，会把此调用放入到一个内部的队列中，交由作业调度器进行调度。值得一提的是，针对  Map  任务与 Reduce  任务，任务调度器是优先选择 Map  任务的，另外，任务调度器在选择 Reduce  任务时并没有考虑数据的本地化。然而，针对一个 Map  任务，它考虑的是 Tasktracker  网络位置和选取一个距离其输入划分文件最近的 Tasktracker  ，它可能是数据本地化的，也可能是机架本地化的，还可能得到不同的机架上取数据。</p>
<p>5  ） 初始化包括创建一个代表该正在运行的作业的对象，它封装任务和记录信息，以便跟踪任务的状态和进度。</p>
<p>6  ） JobTracker  任务调度器首先从共享文件系统中获取 JobClient  已计算好的输入划分信息，然后为每个划分创建一个 Map  任务。创建 的 reduce  任务的数量是由 JobConf  的 Mapred.reduce.tasks  属性决定，它是用 setNumReduceTask()  方法来设置的。</p>
<p>7  ） TaskTracker  执行一个简单的循环，定期发送心跳（ Heartbeat  ）方法调用 Jobtracker  告诉是否还活着，同时，心跳还会报告任务运行的是否已经准备运行新的任务。</p>
<p>8  ） TaskTracker  已经被分配了任务，下一步是运行任务。首先它需要将它所需的全部文件从 HDFS  中复制到本地磁盘。</p>
<p>9  ）紧接着，它要启动一个新的 Java  虚拟机来运行每个任务，这使得用户所定义的 Map  和 Reduce  函数的任务缺陷都不会影响 TaskTracker  （比如导致它崩溃或者挂起）</p>
<p>10  ）运行 Map  任务或者 Reduce  任务，值得一提的是，这些任务使用标准输入与输出流，换句话说，你可以用任务语言（如 JAVA  ， C++  ， Shell  等）来实现 Map  和 Reduce  ，只要保证它们也使用标准输入与输出流，就可以将输出的键值对传回给 JAVA  进程了。</p>
<h2 id="-1-4-map-reduce-"><a href=""></a> 1.4 Map/Reduce  任务的数据流图</h2>
<p><img src="" alt=""></p>
<pre><code>    图3  Map/Reduce  中单一 Reduce  任务的数据流图
</code></pre><p><img src="" alt=""></p>
<pre><code>             图4  Map/Reduce  中多个 Reduce  任务的数据流图
</code></pre><p><img src="" alt=""></p>
<pre><code>            图5  MapReduce  中没有 Reduce  任务的数据流图
</code></pre><p><strong>任务粒度</strong>   ： 分片的个数，在将原始大数据切割成小数据集时，通常让小数据集小于或等于 HDFS  中的一个 Block  的大小（缺省是 64M)  ，这样能够保证一个小数据集位于一台计算机上，便于本地计算。 有 M   个 小数据集 待处理，就启动 M   个 Map   任务，注意这 M   个 Map   任务分布于 N   台计算机上并行运行，Reduce   任务的数量 R   则可由用户指定 。</p>
<p><strong>Map</strong>   ： 输入 <k1, v1>   输出 List(<k2,v2>)</p>
<p><strong>Reduce</strong>   ： 输入 <k2,List(v2)>   输出 <k3,v3></p>
<p><strong>分区（</strong>  <strong>Partition)</strong>  :   把 Map   任务输出的中间结果按 key   的范围划分成 R   份 ( R  是预先定义的 Reduce  任务的个数) ，划分时通常使用 hash  函数如: hash(key) mod R ，这样可以保证某一段范围内的 key ，一定是由一个 Reduce  任务来处理，可以简化 Reduce  的过程。</p>
<p><strong>Combine</strong>   :   在  partition   之前，还可以对中间结果先做  combine  ，即将中间结果中有相同  key  的  <key, value>   对合并成一对。 combine   的过程与  Reduce   的过程类似，很多情况下就可以直接使用  Reduce   函数，但  combine   是作为  Map   任务的一部分，在执行完  Map   函数后紧接着执行的。 Combine   能够减少中间结果中  <key, value>   对的数目，从而减少网络流量。</p>
<p>下面举个例子来着重说明 Combine  ， hadoop  允许用户声明一个 combiner  运行在 Map  的输出上，它的输出再作为 Reduce  的输入。例如，找出每一年的最调气温：</p>
<p>假如用户的输入的分片数是 2  ，那么：</p>
<p>1  ）第一个 Map  的输出如下：</p>
<p>（ 1950  ， 0  ）</p>
<p>（ 1950  ， 20  ）</p>
<p>（ 1950  ， 10  ）</p>
<p>2  ） 第二个 Map  的输出如下：</p>
<p>（ 1950  ， 25  ）</p>
<p>（ 1950  ， 15  ）</p>
<p>3  ） Reduce  的输入如下：</p>
<p>（ 1950  ，［ 0  ， 20  ， 10  ， 25  ， 15  ］）</p>
<p><strong>注意：如果有</strong>   <strong>combine</strong>    <strong>的话，此时</strong>   <strong>Reduce</strong>    <strong>的输入应该是：</strong></p>
<p><strong>max(0, 20, 10, 25, 15) = max(max(0,20,10), max(25,15)) = max(20,25)</strong></p>
<p><strong>combine</strong>    <strong>并不能取代</strong>   <strong>reduce,</strong>    <strong>例如，如果我们计算平均气温，便不能使用</strong>   <strong>combine</strong>    <strong>，因为：</strong></p>
<p><strong>mean(0,20,10,25,15) = 14</strong></p>
<p><strong>但是：</strong></p>
<p><strong>mean(mean(0,20,10), mean(25,15)) = mean(10,20) = 15</strong></p>
<p>4  ） Reduce  的输出如下：</p>
<p>（ 1950  ， 25  ）</p>
<h1 id="-2-hadoop-"><a href=""></a>2 hadoop  入门实战</h1>
<p>hadoop  有三种部署模式：</p>
<ul>
<li>单机模式：没有守护进程，一切都运行在单个 JVM  上，适合测试与调试。</li>
<li>伪集群模式：守护进程在本地运行，适合模拟集群。</li>
<li>集群模式：守护进程运行在集群的某台机器上。</li>
</ul>
<p>所以，在以上任一特定模式运行 hadoop  时，只需要做两件事情：</p>
<p>1  ） 设置适当属性</p>
<p>2  ）启动 hadoop  的守护进程（名称节点，二级名称节名，数据节点）</p>
<p>hadoop  默认的是单机模式，下面，我们将着重介绍在集群模式是如何部署？</p>
<h2 id="-2-1-"><a href=""></a>2.1   测试环境</h2>
<p>用两台机器做为测试环境 ,   通常，集群里的一台机器被指定为  NameNode  ，另一台不同的机器被指定为 JobTracker  ，这些机器是 <strong>masters;</strong>  余下的机器即作为 DataNode  <strong>也</strong> 作为 TaskTracker  ，这些机器是 <strong>slaves</strong>  <strong>。</strong></p>
<p>1  ）  master (JobTracker &amp; NameNode)  ：我的工作机  ( zhanghua  .quqi.com)</p>
<p>2  ）  slave (TaskTracker &amp; DataNode)  ：我的开发机 ( tadev03  .quqi.com)</p>
<p>3)   两机均已安装 ssh   与  rsync</p>
<h2 id="-2-2-"><a href=""></a>2.2   测试程序</h2>
<p>1  ）  /home/workspace/hadoopExample/input/file01:</p>
<p>Hello World Bye World</p>
<p>2) /home/workspace/hadoopExample/input/file02:</p>
<p>Hello  Hadoop    Goodbye  Hadoop</p>
<ol>
<li>WordCount.java</li>
</ol>
<p><strong>package</strong>    com.TripResearch.hadoop;</p>
<p><strong>import</strong>   java.io.IOException;</p>
<p><strong>import</strong>   java.util.Iterator;</p>
<p><strong>import</strong>   java.util.StringTokenizer;</p>
<p><strong>import</strong>   org.apache.hadoop.fs.Path;</p>
<p><strong>import</strong>   org.apache.hadoop.io.IntWritable;</p>
<p><strong>import</strong>   org.apache.hadoop.io.LongWritable;</p>
<p><strong>import</strong>   org.apache.hadoop.io.Text;</p>
<p><strong>import</strong>   org.apache.hadoop.mapred. FileInputFormat  ;</p>
<p><strong>import</strong>   org.apache.hadoop.mapred.FileOutputFormat;</p>
<p><strong>import</strong>   org.apache.hadoop.mapred.JobClient;</p>
<p><strong>import</strong>   org.apache.hadoop.mapred. JobConf  ;</p>
<p><strong>import</strong>   org.apache.hadoop.mapred. MapReduceBase  ;</p>
<p><strong>import</strong>   org.apache.hadoop.mapred. Mapper  ;</p>
<p><strong>import</strong>   org.apache.hadoop.mapred.OutputCollector;</p>
<p><strong>import</strong>   org.apache.hadoop.mapred. Reducer  ;</p>
<p><strong>import</strong>   org.apache.hadoop.mapred.Reporter;</p>
<p><strong>import</strong>   org.apache.hadoop.mapred. TextInputFormat  ;</p>
<p><strong>import</strong>   org.apache.hadoop.mapred. TextOutputFormat  ;</p>
<p>//<em>/</em></p>
<p>/<em>  <em>*@author</em></em>    huazhang</p>
<p>/*/</p>
<p>@SuppressWarnings ( &quot;deprecation&quot; )</p>
<p><strong>public</strong>    <strong>class</strong>   WordCount {</p>
<p><strong>public</strong>    <strong>static</strong>    <strong>class</strong>   MyMap  <strong>extends</strong>    MapReduceBase    <strong>implements</strong></p>
<p>Mapper <LongWritable, Text, Text, IntWritable> {</p>
<p><strong>private</strong>    <strong>final</strong>    <strong>static</strong>   IntWritable  <em>one</em>   =  <strong>new</strong>   IntWritable(1);</p>
<p><strong>private</strong>   Text  word  =  <strong>new</strong>   Text();</p>
<p><strong>public</strong>    <strong>void</strong>   map(LongWritable key, Text value,</p>
<p>OutputCollector<Text, IntWritable> output, Reporter reporter)</p>
<p><strong>throws</strong>   IOException {</p>
<p>String line = value.toString();</p>
<p>StringTokenizer tokenizer =  <strong>new</strong>   StringTokenizer(line);</p>
<p><strong>while</strong>   (tokenizer.hasMoreTokens()) {</p>
<p>word .set(tokenizer.nextToken());</p>
<p>output.collect( word ,  <em>one</em>  );</p>
<p>}</p>
<p>}</p>
<p>}</p>
<p><strong>public</strong>    <strong>static</strong>    <strong>class</strong>   MyReduce  <strong>extends</strong>    MapReduceBase    <strong>implements</strong></p>
<p>Reducer <Text, IntWritable, Text, IntWritable> {</p>
<p><strong>public</strong>    <strong>void</strong>   reduce(Text key, Iterator<IntWritable> values,</p>
<p>OutputCollector<Text, IntWritable> output, Reporter reporter)</p>
<p><strong>throws</strong>   IOException {</p>
<p><strong>int</strong>   sum = 0;</p>
<p><strong>while</strong>   (values.hasNext()) {</p>
<p>sum += values.next().get();</p>
<p>}</p>
<p>output.collect(key,  <strong>new</strong>   IntWritable(sum));</p>
<p>}</p>
<p>}</p>
<p><strong>public</strong>    <strong>static</strong>    <strong>void</strong>   main(String[] args)  <strong>throws</strong>   Exception {</p>
<p>JobConf   conf =  <strong>new</strong>   JobConf(WordCount. <strong>class</strong>  );</p>
<p>conf.setJobName( &quot;wordcount&quot; );</p>
<p>conf.setOutputKeyClass(Text. <strong>class</strong>  );</p>
<p>conf.setOutputValueClass(IntWritable. <strong>class</strong>  );</p>
<p>conf.setMapperClass(MyMap. <strong>class</strong>  );</p>
<p>conf.setCombinerClass(MyReduce. <strong>class</strong>  );</p>
<p>conf.setReducerClass(MyReduce. <strong>class</strong>  );</p>
<p>conf.setInputFormat( TextInputFormat  . <strong>class</strong>  );</p>
<p>conf.setOutputFormat( TextOutputFormat  . <strong>class</strong>  );</p>
<p>FileInputFormat  . <em>setInputPaths</em>  (conf,  <strong>new</strong>   Path(args[0]));</p>
<p>FileOutputFormat. <em>setOutputPath</em>  (conf,  <strong>new</strong>   Path(args[1]));</p>
<p>JobClient.<em>runJob</em> (conf);</p>
<p>}</p>
<p>}</p>
<p><img src="" alt=""></p>
<h2 id="-2-3-"><a href=""></a>2.3   属性配置</h2>
<p>按下图所示修改至少 3  个属性, 如下图所示：</p>
<p>   <img src="" alt=""></p>
<ol>
<li></li>
<li><p>conf/core-site.xml</p>
</li>
</ol>
<configuration>

<property>

<name>fs.default.name</name>

<value>hdfs://zhanghua  .quqi.com:9000</value>

</property>

</configuration>

<p>注意：此处如果是伪集群模式可配置为  hdfs://localhost:9000 ,    是本地模式则为：  localhost:9000   。另外，其他输入输入路径，是本地模式是本地文件系统的路径，是非地模式，用 hdfs  文件系统的路径格式。</p>
<ol>
<li>conf/hdfs-site.xml</li>
</ol>
<configuration>

<property>

<name>dfs.replication</name>

<value>1</value>

</property>

<p></configuration></p>
<ol>
<li>conf/mapred-site.xml</li>
</ol>
<configuration>

<property>

<name>mapred.job.tracker</name>

<value>zhanghua  .quqi.com:8021</value>

</property>

<p></configuration></p>
<ol>
<li>masters</li>
</ol>
<p>zhanghua  .quqi.com (   伪分布模式就配成  localhost)</p>
<ol>
<li>slaves</li>
</ol>
<p>tadev03  .quqi.com  (   伪分布模式就配成 localhost)</p>
<ol>
<li>将以上配置好的 hadoop  文件夹拷到所有机器的相同目录下：</li>
</ol>
<p>scp -r /home/soft/hadoop-0.20.2 <a href="mailto:root@tadev03.daodao.com">root@tadev03</a>   <a href="mailto:root@tadev03.daodao.com">.quqi.com</a>  :/home/soft/hadoop-0.20.2</p>
<p>注意：确保两台机器的  JAVA_HOME   的路径一致，如果不一致，就要改 。</p>
<p>hadoop  所有可配置的配置文件说明如下：</p>
<p>hadoop-env.sh   运行 hadoop  的脚本中使用的环境变量</p>
<p>core-site.xml hadoop  的核心配置，如 HDFS  和 MapReduce  中很普遍的 I/O  设置</p>
<p>hdfs-site.xml HDFS  后台程序设置的配置：名称节点，第二名称节点及数据节点</p>
<p>mapred-site.xml MapReduce  后台程序设置的配置： jobtracker  和 tasktracker</p>
<p>masters   记录运行第二名称节点 的机器（一行一个）的列表</p>
<p>slaves   记录运行数据节点的机器（一行一个）的列表</p>
<h2 id="-2-4-ssh-"><a href=""></a>2.4   免密码 SSH  设置</h2>
<p>免密码  ssh   设置， 保证至少从   master    可以不用口令登陆所有的   slaves    。</p>
<p>1  ）生成密钥对： ssh-keygen -t rsa -P &#39;&#39; -f /root/.ssh/id_rsa (  这样密钥就留在了客户端 )</p>
<p>2)   将公钥拷到要连接的服务器，</p>
<p>scp /root/.ssh/id_rsa.pub root@tadev03  .quqi.com:/tmp</p>
<p>ssh -l root tadev03  .quqi.com</p>
<p>more /tmp/id_rsa.pub &gt;&gt; /root/.ssh/authorized_keys</p>
<ol>
<li>ssh tadev03  .quqi.com   不需要输入密码即为成功。</li>
</ol>
<p>（注意：伪分布模式也要配置  ssh localhost   无密码登录，如果是  mac   ，请将  ssh   打开）</p>
<p>(  另外，在 mac  中请在 hadoop-config.sh  文件中配置  export JAVA_HOME=/System/Library/Frameworks/JavaVM.framework/Versions/1.6/Home)</p>
<p>三条控制线线：</p>
<p>SSH →   这样就可以直接从主节点远程启动从节点上的脚本，如  ssh tadev03  .quqi.com &#39;/var/aa.sh&#39;</p>
<p>NameNode (<a href="http://localhost:50030/" target="_blank"><a href="http://localhost:50070">http://localhost:50070</a></a> ) → DataNode</p>
<p>JobTracker ( <a href="http://localhost:50030/" target="_blank"><a href="http://localhost:50030">http://localhost:50030</a></a> )→ TaskTracker (<a href="http://localhost:50030/" target="_blank"><a href="http://localhost:50060">http://localhost:50060</a></a> )</p>
<h2 id="-2-5-hosts"><a href=""></a>2.5   配置 hosts</h2>
<p>必须配置 master   和 slaves   之间的双向 hosts.   修改 /etc/hosts   进行配置，略。</p>
<h2 id="-2-6-hdfs-"><a href=""></a>2.6   格式化 HDFS  文件系统</h2>
<p>和我们常见的 NTFS  ， FAT32  文件系统一样， NDFS  最开始也是需要格式化的。格式化过程用来创建存储目录以及名称节点的永久数据结构的初始版本来创建一个空的文件系统。命令如下：</p>
<p>hadoop namenode -format</p>
<p>已知问题：在重新格式化时，可能会报： SHUTDOWN_MSG: Shutting down NameNode</p>
<p>解决办法： rm -rf /tmp/hadoop-root/dfs/name</p>
<h2 id="-2-7-"><a href=""></a>2.7   启动守护进程</h2>
<p>1    ）启动   HDFS    守护进程：    start-dfs.sh</p>
<p>(      start-dfs.sh    脚本会参照 NameNode    上 ${HADOOP_CONF_DIR}/slaves    文件的内容，在所有列出的 slave    上启动 DataNode    守护进程。   )</p>
<p>已知问题：在已设置   JAVA_HOME    的情况下仍会报：   Error: JAVA_HOME is not set</p>
<p>解决办法：我是在  hadoop.sh  文件中加下面一句解决的：</p>
<p>JAVA_HOME=/System/Library/Frameworks/JavaVM.framework/Versions/1.6/Home</p>
<p>2  ）启动  Map/Reduce  守护进程：   start-mapred.sh</p>
<p>(      start-mapred.sh   脚本会参照 JobTracker   上 ${HADOOP_CONF_DIR}/slaves   文件的内容，在所有列出的 slave   上启动 TaskTracker   守护进程  )</p>
<p>3)   启动成功后，可以通过访问  <a href="http://localhost:50030" target="_blank">http://localhost:50030</a>   验证。</p>
<p>注意：也可直接使用  start-all.sh       与  stop-all.sh       脚本  ,       在主节点   master    上面启动   hadoop    ，主节点会启动  /    停止所有从节点的   hadoop    。会启动  5       个   java        进程  ,        同时会在   /tmp        目录下创建五个   pid        文件记录这些进程   ID        号。通过这五个文件，可以得知   namenode, datanode, secondary namenode, jobtracker, tasktracker        分别对应于哪一个   Java        进程。</p>
<p>已知问题：启动后，日志中报：  java.io.IOException: File /tmp/hadoop-root/mapred/system/jobtracker.info could only be replicated to 0 nodes, instead of 1</p>
<p>解决办法：原因是    从  tadev03     .quqi.com       机器上无法  ping zhanghua     .quqi.com</p>
<h2 id="-2-8-"><a href=""></a>2.8   运行程序</h2>
<p>先将测试数据及其他输入由本地文件系统拷到  HFDS  文件系统中去（注意：   jar   除外 ）</p>
<ol>
<li></li>
<li><p>hadoop fs -mkdir input</p>
</li>
<li>hadoop fs -ls .</li>
<li>hadoop fs -copyFromLocal /home/workspace/hadoopExample/input/file01 input/file01</li>
<li>hadoop fs -copyFromLocal /home/workspace/hadoopExample/input/file02 input/file02</li>
</ol>
<p>这时候就可以执行下列命令运行程序了，注意：后面的input , output  等目录都是HDFS  文件系统的路径。(  如果是本地模式，就用本地文件系统的绝对路径）</p>
<ol>
<li></li>
</ol>
<p>hadoop     jar   /home/workspace/hadoopExample/hadoopExample.jar com.TripResearch.hadoop.WordCount input/ output</p>
<p>已知问题：在集群模式下运行时任务会Pending</p>
<p>最后，运行下列命令查看结果：</p>
<p>/home/soft/hadoop-0.20.2/bin/hadoop fs -cat output/part-00000</p>
<p>也可访问下列地址查看状态：</p>
<p>NameNode – <a href="http://localhost:50070/" target="_blank"><a href="http://zhanghua">http://zhanghua</a></a>   <a href="http://localhost:50070/" target="_blank">.quqi.com</a> <a href="http://localhost:50070/" target="_blank">:50070/</a></p>
<p>JobTracker - <a href="http://localhost:50030/" target="_blank"><a href="http://zhanghua">http://zhanghua</a></a>   <a href="http://localhost:50030/" target="_blank">.quqi.com</a> <a href="http://localhost:50030/" target="_blank">:50030/</a></p>
<p>常用命令说明如下：</p>
<p>hadoop dfs –ls   查看 /usr/root  目录下的内容径；
hadoop dfs –rmr xxx xxx  就是删除目录；
hadoop dfsadmin -report   这个命令可以全局的查看 DataNode  的情况；
hadoop job -list   后面增加参数是对于当前运行的 Job  的操作，例如 list,kill  等；
hadoop balancer   均衡磁盘负载的命令。</p>
<h1 id="-3-hadoop-"><a href=""></a>3 hadoop  高级进阶</h1>
<h1 id="-4-hadoop-"><a href=""></a>4 hadoop  应用案例</h1>
<h1 id="-5-"><a href=""></a>5   参考文献</h1>
<ol>
<li><a href="http://hadoop.apache.org/common/docs/r0.18.2/cn/" target="_blank"><a href="http://hadoop.apache.org/common/docs/r0.18.2/cn/">http://hadoop.apache.org/common/docs/r0.18.2/cn/</a></a></li>
<li>hadoop 0.20.2  集群配置入门 <a href="http://dev.firnow.com/course/3_program/java/javajs/20100719/453042.html" target="_blank"><a href="http://dev.firnow.com/course/3_program/java/javajs/">http://dev.firnow.com/course/3_program/java/javajs/</a></a></li>
<li>Hadoop 分布式文件系统（HDFS ）初步实践 <a href="http://huatai.me/?p=352" target="_blank"><a href="http://huatai.me/?p=352">http://huatai.me/?p=352</a></a></li>
<li>Hadoop 分布式部署实验2_ 格式化分布式文件系统 <a href="http://hi.baidu.com/thinke365/blog/item/15602aa8f9074cf41e17a235.html" target="_blank"><a href="http://hi.baidu.com/thinke365/blog/item/15602aa8f9074cf41e17a235.html">http://hi.baidu.com/thinke365/blog/item/15602aa8f9074cf41e17a235.html</a></a></li>
<li>hadoop 安装出现问题（紧急），请前辈指教 <a href="http://forum.hadoop.tw/viewtopic.php?f=4&amp;t=90" target="_blank"><a href="http://forum.hadoop.tw/viewtopic.php?f=4&amp;t=90">http://forum.hadoop.tw/viewtopic.php?f=4&amp;t=90</a></a></li>
<li>用 Hadoop  进行分布式并行编程 <a href="http://www.ibm.com/developerworks/cn/opensource/os-cn-hadoop1/index.html" target="_blank"><a href="http://www.ibm.com/developerworks/cn/opensource/os-cn-hadoop1/index.html">http://www.ibm.com/developerworks/cn/opensource/os-cn-hadoop1/index.html</a></a></li>
<li>用 Hadoop  进行分布式数据处理 <a href="http://tech.ddvip.com/2010-06/1275983295155033.html" target="_blank"><a href="http://tech.ddvip.com/2010-06/1275983295155033.html">http://tech.ddvip.com/2010-06/1275983295155033.html</a></a></li>
</ol>
<p>分享到： <a href="&quot;分享到新浪微博&quot;"></a><a href="&quot;分享到腾讯微博&quot;"></a></p>
<ol>
<li>上一篇：<a href="http://blog.csdn.net/quqi99/article/details/6160846" target="_blank">Lucene Scoring 评分机制 （ by quqi99 )</a></li>
<li><p>下一篇：<a href="http://blog.csdn.net/quqi99/article/details/6292472" target="_blank">深入理解各JEE服务器Web层集群原理 ( by quqi99 )</a>
查看评论<a href=""></a></p>
<p>暂无评论
您还没有登录,请<a href="">[登录]</a>或<a href="http://passport.csdn.net/account/register?from=http%3A%2F%2Fblog.csdn.net%2Fquqi99%2Farticle%2Fdetails%2F6291788" target="_blank">[注册]</a></p>
</li>
</ol>
<p>/* 以上用户言论只代表其个人观点，不代表CSDN网站的观点或立场<a href=""></a><a href=""></a>
<a href="&quot;回到顶部&quot;"><img src="" alt="TOP"></a></p>
<p>个人资料</p>
<p><a href="http://my.csdn.net/quqi99" target="_blank"><img src="&quot;访问我的空间&quot;" alt=""></a>
<a href="http://my.csdn.net/quqi99" target="_blank">quqi99</a></p>
<p><a href="&quot;[加关注]&quot;"></a> <a href="&quot;[发私信]&quot;"></a>
<a href="http://medal.blog.csdn.net/allmedal.aspx" target="_blank"><img src="" alt=""></a></p>
<ul>
<li>访问：198660次</li>
<li>积分：3337分</li>
<li><p>排名：第1895名</p>
</li>
<li><p>原创：146篇</p>
</li>
<li>转载：23篇</li>
<li>译文：0篇</li>
<li>评论：123条</li>
</ul>
<p>文章搜索</p>
<p><a href=""></a></p>
<p>文章分类</p>
<ul>
<li><a href="http://blog.csdn.net/quqi99/article/category/875141" target="_blank">VM / Cloud</a>(7)</li>
<li><a href="http://blog.csdn.net/quqi99/article/category/557281" target="_blank">Middleware / Java AppServer</a>(13)</li>
<li><a href="http://blog.csdn.net/quqi99/article/category/328029" target="_blank">Seach Engine</a>(5)</li>
<li><a href="http://blog.csdn.net/quqi99/article/category/674417" target="_blank">Linux / Unix / Shell</a>(24)</li>
<li><a href="http://blog.csdn.net/quqi99/article/category/328188" target="_blank">J2SE / JEE</a>(40)</li>
<li><a href="http://blog.csdn.net/quqi99/article/category/347580" target="_blank">DB / NoSQL</a>(9)</li>
<li><a href="http://blog.csdn.net/quqi99/article/category/803236" target="_blank">Architecture</a>(0)</li>
<li><a href="http://blog.csdn.net/quqi99/article/category/351802" target="_blank">Android</a>(3)</li>
<li><a href="http://blog.csdn.net/quqi99/article/category/803239" target="_blank">Life</a>(4)</li>
<li><a href="http://blog.csdn.net/quqi99/article/category/689016" target="_blank">Other</a>(9)</li>
<li><a href="http://blog.csdn.net/quqi99/article/category/1112756" target="_blank">OpenStack</a>(37)</li>
<li><a href="http://blog.csdn.net/quqi99/article/category/1139084" target="_blank">Python</a>(1)</li>
<li><a href="http://blog.csdn.net/quqi99/article/category/1167554" target="_blank">C / C++</a>(2)</li>
<li><p><a href="http://blog.csdn.net/quqi99/article/category/1490633" target="_blank">Networking</a>(1)
文章存档</p>
</li>
<li><p><a href="http://blog.csdn.net/quqi99/article/month/2013/08" target="_blank">2013年08月</a>(4)</p>
</li>
<li><a href="http://blog.csdn.net/quqi99/article/month/2013/07" target="_blank">2013年07月</a>(13)</li>
<li><a href="http://blog.csdn.net/quqi99/article/month/2013/06" target="_blank">2013年06月</a>(6)</li>
<li><a href="http://blog.csdn.net/quqi99/article/month/2013/05" target="_blank">2013年05月</a>(2)</li>
<li><a href="http://blog.csdn.net/quqi99/article/month/2013/04" target="_blank">2013年04月</a>(2)</li>
<li><a href="http://blog.csdn.net/quqi99/article/month/2013/03" target="_blank">2013年03月</a>(3)</li>
<li><a href="http://blog.csdn.net/quqi99/article/month/2013/02" target="_blank">2013年02月</a>(1)</li>
<li><a href="http://blog.csdn.net/quqi99/article/month/2013/01" target="_blank">2013年01月</a>(9)</li>
<li><a href="http://blog.csdn.net/quqi99/article/month/2012/12" target="_blank">2012年12月</a>(3)</li>
<li><a href="http://blog.csdn.net/quqi99/article/month/2012/11" target="_blank">2012年11月</a>(2)</li>
<li><a href="http://blog.csdn.net/quqi99/article/month/2012/08" target="_blank">2012年08月</a>(1)</li>
<li><a href="http://blog.csdn.net/quqi99/article/month/2012/07" target="_blank">2012年07月</a>(1)</li>
<li><a href="http://blog.csdn.net/quqi99/article/month/2012/06" target="_blank">2012年06月</a>(7)</li>
<li><a href="http://blog.csdn.net/quqi99/article/month/2012/05" target="_blank">2012年05月</a>(2)</li>
<li><a href="http://blog.csdn.net/quqi99/article/month/2012/04" target="_blank">2012年04月</a>(6)</li>
<li><a href="http://blog.csdn.net/quqi99/article/month/2012/03" target="_blank">2012年03月</a>(2)</li>
<li><a href="http://blog.csdn.net/quqi99/article/month/2012/02" target="_blank">2012年02月</a>(1)</li>
<li><a href="http://blog.csdn.net/quqi99/article/month/2011/12" target="_blank">2011年12月</a>(1)</li>
<li><a href="http://blog.csdn.net/quqi99/article/month/2011/09" target="_blank">2011年09月</a>(5)</li>
<li><a href="http://blog.csdn.net/quqi99/article/month/2011/08" target="_blank">2011年08月</a>(2)</li>
<li><a href="http://blog.csdn.net/quqi99/article/month/2011/06" target="_blank">2011年06月</a>(2)</li>
<li><a href="http://blog.csdn.net/quqi99/article/month/2011/04" target="_blank">2011年04月</a>(5)</li>
<li><a href="http://blog.csdn.net/quqi99/article/month/2011/03" target="_blank">2011年03月</a>(2)</li>
<li><a href="http://blog.csdn.net/quqi99/article/month/2011/01" target="_blank">2011年01月</a>(4)</li>
<li><a href="http://blog.csdn.net/quqi99/article/month/2010/12" target="_blank">2010年12月</a>(1)</li>
<li><a href="http://blog.csdn.net/quqi99/article/month/2010/07" target="_blank">2010年07月</a>(1)</li>
<li><a href="http://blog.csdn.net/quqi99/article/month/2010/05" target="_blank">2010年05月</a>(1)</li>
<li><a href="http://blog.csdn.net/quqi99/article/month/2010/04" target="_blank">2010年04月</a>(2)</li>
<li><a href="http://blog.csdn.net/quqi99/article/month/2010/03" target="_blank">2010年03月</a>(2)</li>
<li><a href="http://blog.csdn.net/quqi99/article/month/2010/02" target="_blank">2010年02月</a>(4)</li>
<li><a href="http://blog.csdn.net/quqi99/article/month/2010/01" target="_blank">2010年01月</a>(6)</li>
<li><a href="http://blog.csdn.net/quqi99/article/month/2009/12" target="_blank">2009年12月</a>(6)</li>
<li><a href="http://blog.csdn.net/quqi99/article/month/2009/11" target="_blank">2009年11月</a>(5)</li>
<li><a href="http://blog.csdn.net/quqi99/article/month/2009/10" target="_blank">2009年10月</a>(6)</li>
<li><a href="http://blog.csdn.net/quqi99/article/month/2009/09" target="_blank">2009年09月</a>(1)</li>
<li><a href="http://blog.csdn.net/quqi99/article/month/2009/08" target="_blank">2009年08月</a>(1)</li>
<li><a href="http://blog.csdn.net/quqi99/article/month/2009/06" target="_blank">2009年06月</a>(4)</li>
<li><a href="http://blog.csdn.net/quqi99/article/month/2009/03" target="_blank">2009年03月</a>(1)</li>
<li><a href="http://blog.csdn.net/quqi99/article/month/2008/11" target="_blank">2008年11月</a>(1)</li>
<li><a href="http://blog.csdn.net/quqi99/article/month/2008/10" target="_blank">2008年10月</a>(2)</li>
<li><a href="http://blog.csdn.net/quqi99/article/month/2008/08" target="_blank">2008年08月</a>(1)</li>
<li><a href="http://blog.csdn.net/quqi99/article/month/2008/06" target="_blank">2008年06月</a>(3)</li>
<li><a href="http://blog.csdn.net/quqi99/article/month/2008/04" target="_blank">2008年04月</a>(1)</li>
<li><a href="http://blog.csdn.net/quqi99/article/month/2008/03" target="_blank">2008年03月</a>(1)</li>
<li><a href="http://blog.csdn.net/quqi99/article/month/2008/02" target="_blank">2008年02月</a>(1)</li>
<li><a href="http://blog.csdn.net/quqi99/article/month/2008/01" target="_blank">2008年01月</a>(3)</li>
<li><a href="http://blog.csdn.net/quqi99/article/month/2007/12" target="_blank">2007年12月</a>(4)</li>
<li><a href="http://blog.csdn.net/quqi99/article/month/2007/11" target="_blank">2007年11月</a>(1)</li>
<li><a href="http://blog.csdn.net/quqi99/article/month/2007/10" target="_blank">2007年10月</a>(4)</li>
<li><a href="http://blog.csdn.net/quqi99/article/month/2007/08" target="_blank">2007年08月</a>(5)</li>
<li><a href="http://blog.csdn.net/quqi99/article/month/2007/07" target="_blank">2007年07月</a>(6)</li>
<li><a href="http://blog.csdn.net/quqi99/article/month/2007/06" target="_blank">2007年06月</a>(1)</li>
<li><a href="http://blog.csdn.net/quqi99/article/month/2007/05" target="_blank">2007年05月</a>(8)</li>
</ul>
<p>展开</p>
<p>阅读排行</p>
<ul>
<li><a href="http://blog.csdn.net/quqi99/article/details/1916553" title="Android学习笔记 ( by quqi99 )" target="_blank">Android学习笔记 ( by quqi99 )</a>(22132)</li>
<li><a href="http://blog.csdn.net/quqi99/article/details/1680647" title="java.lang.NoClassDefFoundError: org/apache/commons/io/output/DeferredFileOutputStream" target="_blank">java.lang.NoClassDefFoundError: org/apache/commons/io/output/DeferredFileOutputStream</a>(17851)</li>
<li><a href="http://blog.csdn.net/quqi99/article/details/7433285" title="建立openstack quantum开发环境" target="_blank">建立openstack quantum开发环境</a>(6747)</li>
<li><a href="http://blog.csdn.net/quqi99/article/details/1755642" title="在linux中安装字库( by quqi99 )" target="_blank">在linux中安装字库( by quqi99 )</a>(5151)</li>
<li><a href="http://blog.csdn.net/quqi99/article/details/1930304" title="Android手机客户端与Servlet交换数据(by quqi99)" target="_blank">Android手机客户端与Servlet交换数据(by quqi99)</a>(5140)</li>
<li><a href="http://blog.csdn.net/quqi99/article/details/5298017" title="ReentrantLock与synchronized的区别 ( by quqi99 )" target="_blank">ReentrantLock与synchronized的区别 ( by quqi99 )</a>(4864)</li>
<li><a href="http://blog.csdn.net/quqi99/article/details/5604732" title="个人户口档案转移笔记（适用北京集体户口）" target="_blank">个人户口档案转移笔记（适用北京集体户口）</a>(4802)</li>
<li><a href="http://blog.csdn.net/quqi99/article/details/1624225" title="Magnolia学习笔记（一个基于JSR170的内容管理系统） ( by quqi99 )" target="_blank">Magnolia学习笔记（一个基于JSR170的内容管理系统） ( by quqi99 )</a>(4342)</li>
<li><a href="http://blog.csdn.net/quqi99/article/details/1624218" title="JSpider学习笔记 ( by quqi99 )" target="_blank">JSpider学习笔记 ( by quqi99 )</a>(4149)</li>
<li><p><a href="http://blog.csdn.net/quqi99/article/details/3099945" title="Plone学习笔记 ( by quqi99 )" target="_blank">Plone学习笔记 ( by quqi99 )</a>(4057)
评论排行</p>
</li>
<li><p><a href="http://blog.csdn.net/quqi99/article/details/1916553" title="Android学习笔记 ( by quqi99 )" target="_blank">Android学习笔记 ( by quqi99 )</a>(21)</p>
</li>
<li><a href="http://blog.csdn.net/quqi99/article/details/1680647" title="java.lang.NoClassDefFoundError: org/apache/commons/io/output/DeferredFileOutputStream" target="_blank">java.lang.NoClassDefFoundError: org/apache/commons/io/output/DeferredFileOutputStream</a>(18)</li>
<li><a href="http://blog.csdn.net/quqi99/article/details/1930304" title="Android手机客户端与Servlet交换数据(by quqi99)" target="_blank">Android手机客户端与Servlet交换数据(by quqi99)</a>(12)</li>
<li><a href="http://blog.csdn.net/quqi99/article/details/5604732" title="个人户口档案转移笔记（适用北京集体户口）" target="_blank">个人户口档案转移笔记（适用北京集体户口）</a>(8)</li>
<li><a href="http://blog.csdn.net/quqi99/article/details/1624225" title="Magnolia学习笔记（一个基于JSR170的内容管理系统） ( by quqi99 )" target="_blank">Magnolia学习笔记（一个基于JSR170的内容管理系统） ( by quqi99 )</a>(7)</li>
<li><a href="http://blog.csdn.net/quqi99/article/details/2591768" title="使用itext生成word格式的报表(by quqi99)" target="_blank">使用itext生成word格式的报表(by quqi99)</a>(5)</li>
<li><a href="http://blog.csdn.net/quqi99/article/details/1755642" title="在linux中安装字库( by quqi99 )" target="_blank">在linux中安装字库( by quqi99 )</a>(4)</li>
<li><a href="http://blog.csdn.net/quqi99/article/details/6305061" title="Android分享文稿 ( by quqi99 )" target="_blank">Android分享文稿 ( by quqi99 )</a>(4)</li>
<li><a href="http://blog.csdn.net/quqi99/article/details/9156497" title="OpenDaylight学习 ( by quqi99 )" target="_blank">OpenDaylight学习 ( by quqi99 )</a>(3)</li>
<li><a href="http://blog.csdn.net/quqi99/article/details/2590703" title="使用jacob生成word(by quqi99)" target="_blank">使用jacob生成word(by quqi99)</a>(3)</li>
</ul>
<p>推荐文章
最新评论</p>
<ul>
<li><a href="http://blog.csdn.net/quqi99/article/details/9156497#comments" target="_blank">OpenDaylight学习 ( by quqi99 )</a></li>
</ul>
<p><a href="http://blog.csdn.net/quqi99" target="_blank">quqi99</a>: hi whoeversucks, 谢谢你的实时信息，非常有用，我已经更新到博客里了。另外，问个问题，...</p>
<ul>
<li><a href="http://blog.csdn.net/quqi99/article/details/9156497#comments" target="_blank">OpenDaylight学习 ( by quqi99 )</a></li>
</ul>
<p><a href="http://blog.csdn.net/whoeversucks" target="_blank">whoeversucks</a>: 注意，OpenDayLight Controller和OSCP实际上2个独立的SDN控制器项目（分别...</p>
<ul>
<li><a href="http://blog.csdn.net/quqi99/article/details/9102745#comments" target="_blank">给Linux虚机扩充硬盘空间 ( by quqi99 )</a></li>
</ul>
<p><a href="http://blog.csdn.net/quqi99" target="_blank">quqi99</a>: hi dalinhuang, 谢谢你的回复，你给的这个方法是只适合LVM场景的啊，我没有使用LVM。</p>
<ul>
<li><a href="http://blog.csdn.net/quqi99/article/details/9102745#comments" target="_blank">给Linux虚机扩充硬盘空间 ( by quqi99 )</a></li>
</ul>
<p><a href="http://blog.csdn.net/dalinhuang" target="_blank">dalinhuang</a>: 给根（/）扩充的步骤：（以你的virtualbox并使用LVM为例）1. 新增一块虚拟硬盘，给虚机。...</p>
<ul>
<li><a href="http://blog.csdn.net/quqi99/article/details/9156497#comments" target="_blank">OpenDaylight学习 ( by quqi99 )</a></li>
</ul>
<p><a href="http://blog.csdn.net/piaochenping" target="_blank">piaochenping</a>: 你好，为什么我安装时老是出现这个错误呢？ Failed to execute goal org.co...</p>
<ul>
<li><a href="http://blog.csdn.net/quqi99/article/details/7659905#comments" target="_blank">OpenStack CI测试之devstack-gate ( by quqi99 )</a></li>
</ul>
<p><a href="http://blog.csdn.net/quqi99" target="_blank">quqi99</a>: @sunyilong2012: 这种错误应该是差模块吧，可以单独安装一下试试, sudo pip i...</p>
<ul>
<li><a href="http://blog.csdn.net/quqi99/article/details/7411091#comments" target="_blank">Fedora 16上源码建立pydev + eclipse的OpenStack开发环境笔记草稿 ( by quqi99 )</a></li>
</ul>
<p><a href="http://blog.csdn.net/quqi99" target="_blank">quqi99</a>: openstack因为用到了一些linux特有的东西，如iptables，所以目前只能跑在linux...</p>
<ul>
<li><a href="http://blog.csdn.net/quqi99/article/details/7411091#comments" target="_blank">Fedora 16上源码建立pydev + eclipse的OpenStack开发环境笔记草稿 ( by quqi99 )</a></li>
</ul>
<p><a href="http://blog.csdn.net/javaerss" target="_blank">javaerss</a>: 大神...看哭了，为此特地跑去下载fedora 16来做实验。之前用ubuntu下用eclipse ...</p>
<ul>
<li><a href="http://blog.csdn.net/quqi99/article/details/6576375#comments" target="_blank">玩转play framework ( by quqi99 )</a></li>
</ul>
<p><a href="http://blog.csdn.net/nanfu08" target="_blank">nanfu08</a>: 你能看得清，如果只是自己看的话我没话说，这样的文字叫人怎么读？？</p>
<ul>
<li><a href="http://blog.csdn.net/quqi99/article/details/7659905#comments" target="_blank">OpenStack CI测试之devstack-gate ( by quqi99 )</a></li>
</ul>
<p><a href="http://blog.csdn.net/sunyilong2012" target="_blank">dragonsun</a>: 您好，我在做这个测试的时候遇到了无法导入statsd的问题，请问您有解决的方法吗？+ /home/j...</p>
<p><a href="http://www.csdn.net/company/about.html" target="_blank">公司简介</a>|<a href="http://www.csdn.net/company/recruit.html" target="_blank">招贤纳士</a>|<a href="http://www.csdn.net/company/marketing.html" target="_blank">广告服务</a>|<a href="http://www.csdn.net/company/account.html" target="_blank">银行汇款帐号</a>|<a href="http://www.csdn.net/company/contact.html" target="_blank">联系方式</a>|<a href="http://www.csdn.net/company/statement.html" target="_blank">版权声明</a>|<a href="http://www.csdn.net/company/layer.html" target="_blank">法律顾问</a>|<a href="mailto:webmaster@csdn.net">问题报告</a><a href="http://wpa.qq.com/msgrd?v=3&amp;uin=2355263776&amp;site=qq&amp;menu=yes" target="_blank">QQ客服</a> <a href="http://e.weibo.com/csdnsupport/profile" target="_blank">微博客服</a> <a href="http://bbs.csdn.net/forums/Service" target="_blank">论坛反馈</a> <a href="mailto:webmaster@csdn.net">联系邮箱：webmaster@csdn.net</a> 服务热线：400-600-2320京 ICP 证 070598 号北京创新乐知信息技术有限公司 版权所有世纪乐知(北京)网络技术有限公司 提供技术支持江苏乐知网络技术有限公司 提供商务支持Copyright © 1999-2012, CSDN.NET, All Rights Reserved <a href="http://www.hd315.gov.cn/beian/view.asp?bianhao=010202001032100010" target="_blank"><img src="" alt="GongshangLogo"></a>
<img src="http://counter.csdn.net/pv.aspx?id=24" alt=""></p>

      
    </div>
    
    
      
    
    
        <footer id="post-meta">
        <span class="categories">Posted in<span class="breadcrumb fa fa-folder"><li><a href="/categories/hadoop/">hadoop</a></li></span></span> | <span class="tags">Tagged <a href="/tags/hadoop/" class="label label-primary">hadoop</a></span> | <span class="time">recent updated:<time title="2014-03-07 09:54:36"datetime="2014-03-07 09:54:36"> mar. 7 2014</time></span> | <span class="comment-link">
<a href="http://itsolife.com/2014/02/02/2014-02-02-hadoop--Hadoop知识分享文稿byquqi99-技术并艺术着/#comments" class="ds-thread-count comment-link" data-thread-key="2014-02-02-hadoop--Hadoop知识分享文稿byquqi99-技术并艺术着" data-count-type="comments">暂无评论</a></span>
        </footer>
    
    <div class="clearfix"></div>
  </div>
</div>
</article>



  <article>
<div id="post" class="post well">
  <div class="post-content">
    <header class="well-sm">
      <i class="fa icon fa-5x pull-left"></i>
      <h1 class="title"><a href="/2014/02/02/2014-02-02-linux--Vim命令合集/">Vim命令合集</a></h1>
      
        <span>Posted on<time datetime="2014-02-02T01:54:36.000Z"> <a href="/2014/02/02/2014-02-02-linux--Vim命令合集/">feb. 2 2014</a></time></span>      
    </header>
    
    
    <div class="entry">
      
        <h1 id="vim-">Vim命令合集</h1>
<p>命令历史</p>
<p>以:和/开头的命令都有历史纪录，可以首先键入:或/然后按上下箭头来选择某个历史命令。</p>
<h1 id="-vim">启动vim</h1>
<p>在命令行窗口中输入以下命令即可</p>
<p>vim 直接启动vim</p>
<p>vim filename 打开vim并创建名为filename的文件</p>
<h1 id="-">文件命令</h1>
<p>打开单个文件</p>
<p>vim file</p>
<p>同时打开多个文件</p>
<p>vim file1 file2 file3 ...</p>
<p>在vim窗口中打开一个新文件</p>
<p>:open file</p>
<p>在新窗口中打开文件</p>
<p>:split file</p>
<p>切换到下一个文件</p>
<p>:bn</p>
<p>切换到上一个文件</p>
<p>:bp</p>
<p>查看当前打开的文件列表，当前正在编辑的文件会用[]括起来。</p>
<p>:args</p>
<p>打开远程文件，比如ftp或者share folder</p>
<p>:e ftp://192.168.10.76/abc.txt</p>
<p>:e \qadrive\test\1.txt</p>
<h1 id="vim-">vim的模式</h1>
<p>正常模式（按Esc或Ctrl+[进入） 左下角显示文件名或为空
插入模式（按i键进入） 左下角显示--INSERT--
可视模式（不知道如何进入） 左下角显示--VISUAL--</p>
<h1 id="-">导航命令</h1>
<p>% 括号匹配</p>
<h1 id="-">插入命令</h1>
<p>i 在当前位置生前插入</p>
<p>I 在当前行首插入</p>
<p>a 在当前位置后插入</p>
<p>A 在当前行尾插入</p>
<p>o 在当前行之后插入一行</p>
<p>O 在当前行之前插入一行</p>
<h1 id="-">查找命令</h1>
<p>/text　　查找text，按n健查找下一个，按N健查找前一个。</p>
<p>?text　　查找text，反向查找，按n健查找下一个，按N健查找前一个。</p>
<p>vim中有一些特殊字符在查找时需要转义　　./*[]^%/?~$</p>
<p>:set ignorecase　　忽略大小写的查找</p>
<p>:set noignorecase　　不忽略大小写的查找</p>
<p>查找很长的词，如果一个词很长，键入麻烦，可以将光标移动到该词上，按/*或/#键即可以该单词进行搜索，相当于/搜索。而/#命令相当于?搜索。</p>
<p>:set hlsearch　　高亮搜索结果，所有结果都高亮显示，而不是只显示一个匹配。</p>
<p>:set nohlsearch　　关闭高亮搜索显示</p>
<p>:nohlsearch　　关闭当前的高亮显示，如果再次搜索或者按下n或N键，则会再次高亮。</p>
<p>:set incsearch　　逐步搜索模式，对当前键入的字符进行搜索而不必等待键入完成。</p>
<p>:set wrapscan　　重新搜索，在搜索到文件头或尾时，返回继续搜索，默认开启。</p>
<h1 id="-">替换命令</h1>
<p>ra 将当前字符替换为a，当期字符即光标所在字符。</p>
<p>s/old/new/ 用old替换new，替换当前行的第一个匹配</p>
<p>s/old/new/g 用old替换new，替换当前行的所有匹配</p>
<p>%s/old/new/ 用old替换new，替换所有行的第一个匹配</p>
<p>%s/old/new/g 用old替换new，替换整个文件的所有匹配</p>
<p>:10,20 s/^/    /g 在第10行知第20行每行前面加四个空格，用于缩进。</p>
<p>ddp 交换光标所在行和其下紧邻的一行。</p>
<h1 id="-">移动命令</h1>
<p>h 左移一个字符
l 右移一个字符，这个命令很少用，一般用w代替。
k 上移一个字符
j 下移一个字符
以上四个命令可以配合数字使用，比如20j就是向下移动20行，5h就是向左移动5个字符，在Vim中，很多命令都可以配合数字使用，比如删除10个字符10x，在当前位置后插入3个！，3a！<Esc>，这里的Esc是必须的，否则命令不生效。</p>
<p>w 向前移动一个单词（光标停在单词首部），如果已到行尾，则转至下一行行首。此命令快，可以代替l命令。</p>
<p>b 向后移动一个单词 2b 向后移动2个单词</p>
<p>e，同w，只不过是光标停在单词尾部</p>
<p>ge，同b，光标停在单词尾部。</p>
<p>^ 移动到本行第一个非空白字符上。</p>
<p>0（数字0）移动到本行第一个字符上，</p>
<p><HOME> 移动到本行第一个字符。同0健。</p>
<p>$ 移动到行尾 3$ 移动到下面3行的行尾</p>
<p>gg 移动到文件头。 = [[</p>
<p>G（shift + g） 移动到文件尾。 = ]]</p>
<p>f（find）命令也可以用于移动，fx将找到光标后第一个为x的字符，3fd将找到第三个为d的字符。</p>
<p>F 同f，反向查找。</p>
<p>跳到指定行，冒号+行号，回车，比如跳到240行就是 :240回车。另一个方法是行号+G，比如230G跳到230行。</p>
<p>Ctrl + e 向下滚动一行</p>
<p>Ctrl + y 向上滚动一行</p>
<p>Ctrl + d 向下滚动半屏</p>
<p>Ctrl + u 向上滚动半屏</p>
<p>Ctrl + f 向下滚动一屏</p>
<p>Ctrl + b 向上滚动一屏</p>
<h1 id="-">撤销和重做</h1>
<p>u 撤销（Undo）
U 撤销对整行的操作
Ctrl + r 重做（Redo），即撤销的撤销。</p>
<h1 id="-">删除命令</h1>
<p>x 删除当前字符</p>
<p>3x 删除当前光标开始向后三个字符</p>
<p>X 删除当前字符的前一个字符。X=dh</p>
<p>dl 删除当前字符， dl=x</p>
<p>dh 删除前一个字符</p>
<p>dd 删除当前行</p>
<p>dj 删除上一行</p>
<p>dk 删除下一行</p>
<p>10d 删除当前行开始的10行。</p>
<p>D 删除当前字符至行尾。D=d$</p>
<p>d$ 删除当前字符之后的所有字符（本行）</p>
<p>kdgg 删除当前行之前所有行（不包括当前行）</p>
<p>jdG（jd shift + g）   删除当前行之后所有行（不包括当前行）</p>
<p>:1,10d 删除1-10行</p>
<p>:11,$d 删除11行及以后所有的行</p>
<p>:1,$d 删除所有行</p>
<p>J(shift + j)　　删除两行之间的空行，实际上是合并两行。</p>
<h1 id="-">拷贝和粘贴</h1>
<p>yy 拷贝当前行</p>
<p>nyy 拷贝当前后开始的n行，比如2yy拷贝当前行及其下一行。</p>
<p>p  在当前光标后粘贴,如果之前使用了yy命令来复制一行，那么就在当前行的下一行粘贴。</p>
<p>shift+p 在当前行前粘贴</p>
<p>:1,10 co 20 将1-10行插入到第20行之后。</p>
<p>:1,$ co $ 将整个文件复制一份并添加到文件尾部。</p>
<p>正常模式下按v（逐字）或V（逐行）进入可视模式，然后用jklh命令移动即可选择某些行或字符，再按y即可复制</p>
<p>ddp交换当前行和其下一行</p>
<p>xp交换当前字符和其后一个字符</p>
<h1 id="-">剪切命令</h1>
<p>正常模式下按v（逐字）或V（逐行）进入可视模式，然后用jklh命令移动即可选择某些行或字符，再按d即可剪切</p>
<p>ndd 剪切当前行之后的n行。利用p命令可以对剪切的内容进行粘贴</p>
<p>:1,10d 将1-10行剪切。利用p命令可将剪切后的内容进行粘贴。</p>
<p>:1, 10 m 20 将第1-10行移动到第20行之后。</p>
<h1 id="-">退出命令</h1>
<p>:wq 保存并退出</p>
<p>ZZ 保存并退出</p>
<p>:q! 强制退出并忽略所有更改</p>
<p>:e! 放弃所有修改，并打开原来文件。</p>
<h1 id="-">窗口命令</h1>
<p>:split或new 打开一个新窗口，光标停在顶层的窗口上</p>
<p>:split file或:new file 用新窗口打开文件</p>
<p>split打开的窗口都是横向的，使用vsplit可以纵向打开窗口。</p>
<p>Ctrl+ww 移动到下一个窗口</p>
<p>Ctrl+wj 移动到下方的窗口</p>
<p>Ctrl+wk 移动到上方的窗口</p>
<p>关闭窗口</p>
<p>:close 最后一个窗口不能使用此命令，可以防止意外退出vim。</p>
<p>:q 如果是最后一个被关闭的窗口，那么将退出vim。</p>
<p>ZZ 保存并退出。</p>
<p>关闭所有窗口，只保留当前窗口</p>
<p>:only</p>
<p>录制宏</p>
<p>按q键加任意字母开始录制，再按q键结束录制（这意味着vim中的宏不可嵌套），使用的时候@加宏名，比如qa。。。q录制名为a的宏，@a使用这个宏。</p>
<h1 id="-shell-">执行shell命令</h1>
<p>:!command</p>
<p>:!ls 列出当前目录下文件</p>
<p>:!perl -c script.pl 检查perl脚本语法，可以不用退出vim，非常方便。</p>
<p>:!perl script.pl 执行perl脚本，可以不用退出vim，非常方便。</p>
<p>:suspend或Ctrl - Z 挂起vim，回到shell，按fg可以返回vim。</p>
<h1 id="-">注释命令</h1>
<p>perl程序中/#开始的行为注释，所以要注释某些行，只需在行首加入/#</p>
<p>3,5 s/^//#/g 注释第3-5行</p>
<p>3,5 s/^/#//g 解除3-5行的注释</p>
<p>1,$ s/^//#/g 注释整个文档。</p>
<p>:%s/^//#/g 注释整个文档，此法更快。</p>
<h1 id="-">帮助命令</h1>
<p>:help or F1 显示整个帮助
:help xxx 显示xxx的帮助，比如 :help i, :help CTRL-[（即Ctrl+[的帮助）。
:help &#39;number&#39; Vim选项的帮助用单引号括起
:help <Esc> 特殊键的帮助用&lt;&gt;扩起
:help -t Vim启动参数的帮助用-
：help i<em><Esc> 插入模式下Esc的帮助，某个模式下的帮助用模式</em>主题的模式
帮助文件中位于||之间的内容是超链接，可以用Ctrl+]进入链接，Ctrl+o（Ctrl + t）返回</p>
<h1 id="-">其他非编辑命令</h1>
<p>. 重复前一次命令</p>
<p>:set ruler?　　查看是否设置了ruler，在.vimrc中，使用set命令设制的选项都可以通过这个命令查看</p>
<p>:scriptnames　　查看vim脚本文件的位置，比如.vimrc文件，语法文件及plugin等。</p>
<p>:set list 显示非打印字符，如tab，空格，行尾等。如果tab无法显示，请确定用set lcs=tab:&gt;-命令设置了.vimrc文件，并确保你的文件中的确有tab，如果开启了expendtab，那么tab将被扩展为空格。</p>
<p>Vim教程
在Unix系统上
$ vimtutor
在Windows系统上
:help tutor
:syntax 列出已经定义的语法项
:syntax clear 清除已定义的语法规则
:syntax case match 大小写敏感，int和Int将视为不同的语法元素
:syntax case ignore 大小写无关，int和Int将视为相同的语法元素，并使用同样的配色方案</p>

      
    </div>
    
    
      
    
    
        <footer id="post-meta">
        <span class="categories">Posted in<span class="breadcrumb fa fa-folder"><li><a href="/categories/linux/">linux</a></li></span></span> | <span class="tags">Tagged <a href="/tags/linux/" class="label label-primary">linux</a></span> | <span class="time">recent updated:<time title="2014-03-07 09:54:36"datetime="2014-03-07 09:54:36"> mar. 7 2014</time></span> | <span class="comment-link">
<a href="http://itsolife.com/2014/02/02/2014-02-02-linux--Vim命令合集/#comments" class="ds-thread-count comment-link" data-thread-key="2014-02-02-linux--Vim命令合集" data-count-type="comments">暂无评论</a></span>
        </footer>
    
    <div class="clearfix"></div>
  </div>
</div>
</article>



  <article>
<div id="post" class="post well">
  <div class="post-content">
    <header class="well-sm">
      <i class="fa icon fa-5x pull-left"></i>
      <h1 class="title"><a href="/2014/02/02/2014-02-02-linux--Linux大文件传输/">Linux大文件传输</a></h1>
      
        <span>Posted on<time datetime="2014-02-02T01:54:36.000Z"> <a href="/2014/02/02/2014-02-02-linux--Linux大文件传输/">feb. 2 2014</a></time></span>      
    </header>
    
    
    <div class="entry">
      
        <h2 id="linux-">Linux大文件传输</h2>
<p>作者: <a href="http://www.yankay.com/author/admin/" title="查看 颜开 的所有文章" target="_blank">颜开</a> 日期: 2012 年 2 月 7 日<br>我们经常需要在机器之间传输文件。比如备份，复制数据等等。这个是很常见，也是很简单的。用scp或者rsync就能很好的完成任务。但是如果文件很大，需要占用一些传输时间的时候，怎样又快又好地完成任务就很重要了。在我的测试用例中，一个最佳的方案比最差的方案，性能提高了10倍。</p>
<h3 id="-"><strong>复制文件</strong></h3>
<p>如果我们是复制一个<strong>未压缩</strong>的文件。这里走如下步骤：</p>
<ol>
<li>压缩数据</li>
<li>发送到另外一台机器上</li>
<li>数据解压缩</li>
<li>校验正确性
这样做会很有效率，数据压缩后可以更有效的利用带宽</li>
</ol>
<h3 id="-zip-scp-"><strong>使用ZIP+SCP</strong></h3>
<p>我们可以通过ZIP+SCP的组合实现这个功能。</p>
<p>gzip -c /home/yankay/data | ssh yankay01 &quot;gunzip -c - &gt; /home/yankay/data&quot;</p>
<p>这条命令是将/home/yankay/data经过GZIP压缩，通过ssh传输到yankay01的机器上。
data文件的大小是1.1GB,经过Zip压缩后是183MB，执行上面的命令需要45.6s。平均吞吐量为24.7MB/s</p>
<p>我们会发现Scp也有压缩功能，所以上面的语句可以写成
scp -C -c blowfish /home/yankay/data yankay01:/home/yankay/data</p>
<p>这样运行效果是相同的，不通之处在于我使用了blowfish算法作为Scp的密匙算法，使用这个算法可以比默认的情况快很多。单单对与scp,使用了blowfish 吞吐量是62MB/s,不使用只有46MB/s。</p>
<p>可是我执行上面一条命令的时候，发现还是需要45s。平均吞吐量还为24MB/s。没有丝毫的提升，可见瓶颈不在网络上。
那瓶颈在哪里呢？</p>
<h3 id="-"><strong>性能分析</strong></h3>
<p>我们先定义几个变量</p>
<ul>
<li>压缩工具的压缩比是 CompressRadio</li>
<li>压缩工具的压缩吞吐是CompressSpeed MB/s</li>
<li>网络传输的吞吐是 NetSpeed MB/s</li>
</ul>
<p>由于使用了管道，管道的性能取决于管道中最慢的部分的性能，所以整体的性能是：</p>
<p>speed=min(NetSpeed/CompressRadio,CompressSpeed)</p>
<p>当压缩吞吐较网络传输慢的时候，压缩是瓶颈；但网络较慢的时候，网络传输/吞吐 是瓶颈。</p>
<p>根据现有的测试数据(纯文本)，可以得到表格：
压缩比 吞吐量 千兆网卡(100MB/s)吞吐量 千兆网卡吞吐量,基于ssh(62MB/s) 百兆网卡(10MB/s)吞吐量 ZLIB 35.80% 9.6 9.6 9.6 9.6 LZO 54.40% 101.7 101.7 101.7 18.38235294 LIBLZF 54.60% 134.3 134.3 113.5531136 18.31501832 QUICKLZ 54.90% 183.4 182.1493625 112.9326047 18.21493625 FASTLZ 56.20% 134.4 134.4 110.3202847 17.79359431 SNAPPY 59.80% 189 167.2240803 103.6789298 16.72240803 NONE 100% 300 100 62 10</p>
<p>可以看出来。在千兆网卡下，使用QuickLZ作为压缩算法，可以达到最高的性能。如果使用SSH作为数据传输通道，则远远没有达到网卡可以达到的最佳性能。在百兆网卡的情况下，各个算法相近。对比下来QuickLZ是有优势的。</p>
<p>对于不同的数据和不同的机器，可以得出不同的最佳压缩算法。但有一点是肯定的，尽量把瓶颈压在网络上。对于较慢的网络环境，高压缩比的算法会比较有优势；相反对于较快的网络环境，低压缩比的算法会更好。</p>
<h3 id="-"><strong>结论</strong></h3>
<p>根据上面的分析结果，我们不能是用SSH作为网络传输通道，可以使用NC这个基本网络工具，提高性能。同时使用qpress作为压缩算法。
scp /usr/bin/qpress yankay01:/usr/bin/qpress ssh yankay01 &quot;nc -l 12345 | qpress -dio &gt; /home/yankay/data&quot; &amp; qpress -o /home/yankay/data |nc yankay01 12345</p>
<p>第一行是将gpress安装到远程机器上，第二行在远程机器上使用nc监听一个端口，第三行压缩并传送数据。</p>
<p>执行上面的命令需要2.8s。平均吞吐量为402MB/s,比使用Gzip+Scp快了16倍！！</p>
<p>根据上文的公式，和自己的数据，可以绘出上面的表格，就可以选择出最适合的压缩算法和传输方式。达到满意的效果。如果是一个长期运行的脚本的话，这么做是值得的。
Share the post &quot;Linux大文件传输&quot;</p>

      
    </div>
    
    
      
    
    
        <footer id="post-meta">
        <span class="categories">Posted in<span class="breadcrumb fa fa-folder"><li><a href="/categories/linux/">linux</a></li></span></span> | <span class="tags">Tagged <a href="/tags/linux/" class="label label-primary">linux</a></span> | <span class="time">recent updated:<time title="2014-03-30 13:46:15"datetime="2014-03-30 13:46:15"> mar. 30 2014</time></span> | <span class="comment-link">
<a href="http://itsolife.com/2014/02/02/2014-02-02-linux--Linux大文件传输/#comments" class="ds-thread-count comment-link" data-thread-key="2014-02-02-linux--Linux大文件传输" data-count-type="comments">暂无评论</a></span>
        </footer>
    
    <div class="clearfix"></div>
  </div>
</div>
</article>



  <article>
<div id="post" class="post well">
  <div class="post-content">
    <header class="well-sm">
      <i class="fa icon fa-5x pull-left"></i>
      <h1 class="title"><a href="/2014/02/02/2014-02-02-hadoop--深入理解Hadoop集群和网络/">深入理解Hadoop集群和网络</a></h1>
      
        <span>Posted on<time datetime="2014-02-02T01:54:36.000Z"> <a href="/2014/02/02/2014-02-02-hadoop--深入理解Hadoop集群和网络/">feb. 2 2014</a></time></span>      
    </header>
    
    
    <div class="entry">
      
        <h1 id="-hadoop-">深入理解Hadoop集群和网络</h1>
<p>原文出处： <a href="http://bradhedlund.com/2011/09/10/understanding-hadoop-clusters-and-the-network/" target="_blank">bradhedlund</a>   译文出处： <a href="http://blog.csdn.net/kickxxx/article/details/8230328" target="_blank">kickxxx</a></p>
<p>本文侧重于Hadoop集群的体系结构和方法，以及它与网络和服务器基础设施的关系。文章的素材主要来自于研究工作以及同现实生活中运行Hadoop集群客户的讨论。如果你也在你的数据中心运行产品级的Hadoop集群，那么我希望你能写下有价值的评论。</p>
<p><a href="http://cdn2.jobbole.com/2013/07/srljhjqhwl01.png" title="srljhjqhwl01" target="_blank"><img src="&quot;srljhjqhwl01&quot;" alt=""></a></p>
<p>Hadoop集群部署时有三个角色：Client machines、 Master nodes和Slave nodes。</p>
<p>Master nodes负责Hadoop的两个关键功能：数据存储（HDFS）；以及运行在这个数据之上的并行计算，又称为Map-Reduce。Name node负责调度数据存储，而Job Tracker则负责并行数据处理的调度（使用Map-Reduce技术）。Slave nodes由大量的机器组成，完成数据存储以及运行计算这样的脏活。每个slave node都运行Data node和Task Tracker daemon，这些slave daemon和master nodes的相应daemon进行通信。Task tracker daemon由Job Tracker管理，Data node Daemon由Name node管理。</p>
<p>Client机器包含了Hadoop集群的所有设置，但是它既不是Master也不是Slave。Client的角色是向集群保存数据，提交Map-Reduce jobs（描述如何处理数据），获取查看MR jobs的计算结果。在小型集群中（40节点）你可能会发现一个物理机器扮演多个角色，比如既是Job Tracker又是Name node，在中等或者大规模集群中，一般都是用独立的服务器负责单独的角色。</p>
<p>在真正的产品集群中，不存在虚拟服务器和虚拟机平台，因为他们仅会导致不必要的性能损耗。Hadoop最好运行在linux机器上，直接工作于底层硬件之上。换句话说，Hadoop可以工作在虚拟机之上，对于学习Hadoop是一个不错的廉价方法，我本身就有一个6-node的Hadoop cluster运行Windows 7 laptop的VMware Workstation之上</p>
<p><a href="http://cdn2.jobbole.com/2013/07/srljhjqhwl02.png" title="srljhjqhwl02" target="_blank"><img src="&quot;srljhjqhwl02&quot;" alt=""></a></p>
<p>上图是Hadoop集群的典型架构。机架服务器（不是刀锋服务器）分布在多个机架中，连接到一个1GB(2GB)带宽的机架交换机，机架交换机连接到上一层的交换机，这样所有的节点通过最上层的交换机连接到一起。大部分的服务器都是Slave nodes配置有大的磁盘存储 中等的CPU和DRAM，少部分是Master nodes配置较少的存储空间 但是有强大的CPU和大DRAM</p>
<p>本文中，我们不讨论网络设计的各种细节，而是把精力放在其他方面。首先我们看下应用的工作流程。</p>
<p><a href="http://cdn2.jobbole.com/2013/07/srljhjqhwl03.png" title="srljhjqhwl03" target="_blank"><img src="&quot;srljhjqhwl03&quot;" alt=""></a></p>
<p>为什么会出现Hadoop？ 它又解决了什么问题？ 简单的说，是由于商业和政府有大量的数据需要快速分析和处理，如果我们把这些巨大的数据切分为小的数据块分散到多台计算机中，并且用这些计算机并行处理分配给他们的小块数据，可以快速的得到结果，这就是Hadoop能做的事情。</p>
<p>在我们的简单例子中，我们有一个大文件保存着所有发给客户服务部分的邮件，想要快速统计单词”Refund”被输入了多少次，这个结果有助于评估对退换货部门的需求，以指定相应对策。这是一个简单的单词计数练习。Client上传数据到集群(File.txt)，提交一个job来描述如何分析数据，集群存储结果到一个新文件(Result.txt)，然后Client读取结果文件。</p>
<p><a href="http://cdn2.jobbole.com/2013/07/srljhjqhwl04.png" title="srljhjqhwl04" target="_blank"><img src="&quot;srljhjqhwl04&quot;" alt=""></a></p>
<p>没有加载数据，Hadoop集群就没有任何用途。所以我们首先从加载大文件File.txt到集群中以便处理。目标是能够快速并行处理许多数据，为了达到这个目的需要尽可能多的机器能够同时操纵文件数据。Client把数据文件切分成许多小blocks，然后把他们分散到集群的不同机器上。块数越多，用来处理这些数据的机器就越多。同时这些机器一定会失效，所以要确保每个数据块保存到多个机器上以防止数据丢失。所以每个数据块在存入集群时会进行复制。Hadoop的标准设定是集群中的每个block有三份copy，这个配置可以由hdfs-site.xml的dfs.replication 参数设置</p>
<p>Client把文件File.txt分成3块，对于每一块，Client和Name node协商获取可以保存它以及备份块的Data nodes列表。Client然后直接写block数据到Data node，Data node负责写入数据并且复制copy到其他的Data nodes。重复操作，直到所有的块写完。Name node本身不参与数据保存，Name node仅仅提供文件数据位置以及数据可用位置（文件系统元数据）</p>
<p><a href="http://cdn2.jobbole.com/2013/07/srljhjqhwl05.png" title="srljhjqhwl05" target="_blank"><img src="&quot;srljhjqhwl05&quot;" alt=""></a></p>
<p>Hadoop引入了Rack Awareness的概念。作为Hadoop系统管理员，你能够手动定义集群中每一个slave Data node的rack号。为什么要把自己置于这种麻烦呢？有两个关键的原因：数据丢失和网络性能。记住每一个数据块都会被复制到多台Data node上，以防止某台机器失效导致数据丢失。有没有可能一份数据的所有备份恰好都在同一个机架上，而这个机架又出现了故障，比如交换机失效或者电源失效。为了防止这种情况，Name node需要知道Data nodes在网络拓扑中的位置，并且使用这个信息决定数据应该复制到集群的什么位置。</p>
<p>我们还假定同一机架的两台机器间相比不同机架的两台机器间 有更高的带宽和更低的网络延迟，在大部分情况下是正确的。机架交换机的上行带宽通常小于下行带宽。此外，机架内延迟通常低于机架间延迟。如果上面的假定成立，那么采用Rack Awareness既可以通过优化位置保护数据，同时提升网络性能，岂不是太cool了。是的，的确是这样，非常cool，对不对。</p>
<p>别急，Not cool的是Rack Awareness是需要手工定义的，并且要持续的更新它，并且保证这个信息精确。如果机架交换机可以自动提供它下面的Data node列表给Name node，那就更cool了。或者如果Data nodes可以自动告知Name node他们属于哪个交换机，一样很cool.</p>
<p>此外，让人感兴趣的是<a href="http://bradhedlund.com/2011/04/21/data-center-scale-openflow-sdn/" target="_blank">OpenFlow</a> 网络，Name node可以请求OpenFlow控制器关于节点位置的拓扑情况。</p>
<p><a href="http://cdn2.jobbole.com/2013/07/srljhjqhwl06.png" title="srljhjqhwl06" target="_blank"><img src="&quot;srljhjqhwl06&quot;" alt=""></a></p>
<p>Client准备写文件File.txt到集群时，把文件划分为块，块A为第一个块。Client向Name node申请写文件File.txt，从Name node获得许可，并且接收到一个Data nodes列表（3个表项），每一个Data nodes用来写入块A的一个copy。Name node使用Rack Awareness来决定这个Data nodes列表。规则是：对于每一个数据块，两个 copy存放在一个机架上，另外一个copy存放在另外一个机架上。</p>
<p>在Client写Block A之前，它要知道准备接受”Block A“ copy的Data nodes是否以及做好了准备。首先，Client选择list中的第一个节点Data Node1，打开一个TCP50010链接然后请求：“hey，准备接受一个block，这是一个Data nodes列表（2个表项），Data node5和Data node6”，请确保他们两个也准备好了“；于是Data Node1打开一个到Data node5的TCP500100连接然后说：”Hey，准备接受一个block，这是一个Data nodes列表（1个表项），Data node6”，请确保他准备好了“；Data Node5同样会问Data Node6：“Hey, 准备好接收一个block吗“</p>
<p>”准备就绪“的响应通过已经创建好的TCP pipeline传回来，直到Data Node1发送一个”Ready”给Client。现在Client可以开始写入数据了。</p>
<p><a href="http://cdn2.jobbole.com/2013/07/srljhjqhwl07.png" title="srljhjqhwl07" target="_blank"><img src="&quot;srljhjqhwl07&quot;" alt=""></a></p>
<p>写入数据的过程中，在涉及写操作的Data nodes之间创建一个复制pipeline。也就是说一个数据节点接收数据的同时，同时会把这份数据通过pipeline push到下一个Node中。</p>
<p>从上图可以看到，Rack Awareness起到了改善集群性能的做用。Data node5和Data node6在同一个机架上，因此pipeline的最后一步复制是发生在机架内，这就受益于机架内带宽以及低延迟。在Data node1, Data node5, Data node6完成block A之前，block B的操作不会开始。</p>
<p><a href="http://cdn2.jobbole.com/2013/07/srljhjqhwl08.png" title="srljhjqhwl08" target="_blank"><img src="&quot;srljhjqhwl08&quot;" alt=""></a></p>
<p>当三个Nodes成功的接收到Block A后，挥发送”Block received”报告给Name node，同时发送”Success”到pipeline中，然后关闭TCP事务。Client在接收到Success信息后，通知Name node数据块已经成功的写入。Name node更新File.txt中Block A的metadata信息（包含Name locations信息）</p>
<p>Client现在可以开始Block B的传输了</p>
<p><a href="http://cdn2.jobbole.com/2013/07/srljhjqhwl09.png" title="srljhjqhwl09" target="_blank"><img src="&quot;srljhjqhwl09&quot;" alt=""></a></p>
<p>随着File.txt的块被写入，越来越多的Data nodes涉及到pipeline中，散落到机架内的热点，以及跨机架的复制</p>
<p>Hadoop占用了很多的网络带宽和存储空间。Hadoop专为处理大文件而生，比如TB级尺寸的文件。每一个文件在网络和存储中都被复制了三次。如果你有一个1TB的文件，那么将消耗3TB的网络带宽，同时要消耗3TB的磁盘空间存贮这个文件。</p>
<p><a href="http://cdn2.jobbole.com/2013/07/srljhjqhwl10.png" title="srljhjqhwl10" target="_blank"><img src="&quot;srljhjqhwl10&quot;" alt=""></a></p>
<p>随着每块的复制pipeline的完成，文件被成功的写入集群。文件散落在集群内的机器上，每个机器保存文件的一小部分数据。组成文件的块数目越多，数据散落的机器就越多，将来更多的CPU和磁盘驱动器就能参与到并行处理中来，提供更强大更快的处理能力。这也是建造巨大集群的原动力。当机器的数变多，集群则变得wide，网络也相应的需要扩展。</p>
<p>扩展集群的另外一种方法是deep扩展。就是维持机器数据不便，而是增加机器的CPU处理能力和磁盘驱动器的数目。在这种情况下，需要提高网络的I/O吞吐量以适应增大的机器处理能力，因此如何让Hadoop集群运行10GB nodes称为一个重要的考虑。</p>
<p><a href="http://cdn2.jobbole.com/2013/07/srljhjqhwl11.png" title="srljhjqhwl11" target="_blank"><img src="&quot;srljhjqhwl11&quot;" alt=""></a></p>
<p>Name node保存集群内所有文件的metadata，监督Data nodes的健康以及协调数据的存取。Name node是HDFS的控制中心。它本身并不保存任何cluster data。Name node知道一个文件由哪些块组成以及这些块存放在集群内的什么地方。Name node告诉Client需要和哪些Data node交互，管理集群的存储容量，掌握Data node的健康状况，确保每一个数据块都符合系统备份策略。</p>
<p>Data node每3秒钟发送一个heartbeats给Name node ，二者使用TCP9000端口的TCP握手来实现heartbeats。每十个heartbeats会有一个block report，Data node告知它所保存的数据块。block report使得Namenode能够重建它的metadata以确保每个数据block有足够的copy，并且分布在不同的机架上。</p>
<p>Name node是Hadoop Distributed File System(HDFS)的关键部件。没有Name node，clients无法从HDFS读写数据，也无法执行Map Reduce jobs。因此Name node 最好配置为一台高冗余的企业级服务器：双电源，热插拔风扇，冗余NIC连接等。</p>
<p><a href="http://cdn2.jobbole.com/2013/07/srljhjqhwl12.png" title="srljhjqhwl12" target="_blank"><img src="&quot;srljhjqhwl12&quot;" alt=""></a></p>
<p>如果Name node收不到某Data node的heartbeats，那么Name node假定这个Data node死机并且Data node上的所有数据也丢失了。通过这台dead Data node的block report，Name node知道哪些block copies需要复制到其他Data nodes。Name node参考Rack Awareness数据来选择接收新copy的Data node，并遵守以下复制规则：一个机架保存两份copies，另一个机架保存第三份copy。</p>
<p>考虑由于机架交换机或者机架电源失败导致的整个机架Data node都失效的情况。Name node将指导集群内的剩余Data nodes开始复制失效机架上的所有数据。如果失效机架上服务器的存储容量为12TB，那么这将导致数百TB的数据在网络中传输。</p>
<p><a href="http://cdn2.jobbole.com/2013/07/srljhjqhwl13.png" title="srljhjqhwl13" target="_blank"><img src="&quot;srljhjqhwl13&quot;" alt=""></a></p>
<p>Secondary Name node是Hadoop的一种服务器角色。一个很普遍的误解是它提供了对Name node的高可用性备份，实际上不是。</p>
<p>Secondary Name node偶尔会连接到Name node(缺省为每小时)，同步Name node in-memory metadata以及保存metadata的文件。Secondary Name node合并这些信息到一个组新的文件中，保存到本地的同时把这些文件发送回Name Node。</p>
<p>当Name node宕机，保存在Secondary Name node中的文件可以用来恢复Name node。在一个繁忙的集群中，系统管理员可以配置同步时间为更小的时间间隔，比如每分钟。</p>
<p><a href="http://cdn2.jobbole.com/2013/07/srljhjqhwl14.png" title="srljhjqhwl14" target="_blank"><img src="&quot;srljhjqhwl14&quot;" alt=""></a></p>
<p>当一个Client想要从HDFS获取一个文件时，比如job的输出结果。Client首先从Name node查询文件block的位置。Name node返回一个包含所有block位置的链表，每个block位置包含全部copies所在的Data node</p>
<p>Client选取一个数据块的Data node位置，通过TCP50010端口从这个Data node读取一块，在读取完当前块之前，Client不会处理下一块。</p>
<p><a href="http://cdn2.jobbole.com/2013/07/srljhjqhwl15.png" title="srljhjqhwl15" target="_blank"><img src="&quot;srljhjqhwl15&quot;" alt=""></a></p>
<p>在某些情况下Data node daemon本身需要从HDFS读取数据块。比如Data Node被请求处理自身不存在的数据，因而它必须从网络上的其他Data node获得数据然后才能开始处理。</p>
<p>另外一种情况是Name node的Rack Awareness信息提供的网络优化行为。当Data node向Name node查询数据块位置信息，Name node优先查看请求者所在的机架内的Data nodes包含这个数据块。如果包含，那么Name node把这个Data node提供给请求的Data node。这样可以保证数据仅在in-rack内流动，可以加快数据的处理速度以及job的完成速度</p>
<p><a href="http://cdn2.jobbole.com/2013/07/srljhjqhwl16.png" title="srljhjqhwl16" target="_blank"><img src="&quot;srljhjqhwl16&quot;" alt=""></a></p>
<p>现在File.txt分散到集群的机器中这样就可以提供更快更有效的并行处理速度。Hadoop并行处理框架称为Map Reduce，名称来自于并行处理的两个重要步骤：Map和Reduce</p>
<p>第一步是Map 过程，这个步骤同时请求所有包含数据的Data node运行计算。在我们的例子中则是请求这些Data node统计存储在他们上的File.txt数据块包含多少此Refund</p>
<p>要达到这个目的，Client首先提交Map Reduce job给Job tracker，发送请求：“How many times does Refund occur in file.txt”。Job tracker向Name node查询哪些Data nodes包含文件File.txt的数据块。然后Job Tracker在这些Data nodes上运行Java代码 在Data node的本地数据上执行Map计算。Task Tracker启动一个Map task监测这些tasks的执行。Task Tracker通过heartbeats向Job Tracker汇报task的状态。</p>
<p>当每一个Map task都完成后，计算结果保存在这些节点的临时存储区内，我们称之为”intermediate data”。下一步是把这些中间数据通过网络发送给运行Reduce的节点以便完成最后的计算。</p>
<p><a href="http://cdn2.jobbole.com/2013/07/srljhjqhwl17.png" title="srljhjqhwl17" target="_blank"><img src="&quot;srljhjqhwl17&quot;" alt=""></a></p>
<p>Job tracker总是尝试选择包含待处理数据的Data node做Map task，但是有时不会这样。比如，所有包含这块数据的Data node已经有太多的tasks正在运行，不再接收其他的task.</p>
<p>这种情况下，Job Tracker将询问Name node，Name node会根据Rack Awareness建议一个in-rack Data node。Job tracker把task分配给这个in-rack Data node。这个Data node会在Name node的指导下从包含待处理数据的in-rack Data node获取数据。</p>
<p><a href="http://cdn2.jobbole.com/2013/07/srljhjqhwl18.png" title="srljhjqhwl18" target="_blank"><img src="&quot;srljhjqhwl18&quot;" alt=""></a></p>
<p>Map Reduce 框架的第二部分叫做Reduce。Map task已经完成了计算，计算结果保存在intermediate data。现在我们需要把所有的中间数据汇集到一起作进一步处理得到最终结果。</p>
<p>Job Tracker可以在集群内的任意一个node上执行Reduce，它指导Reduce task从所有完成Map trasks的Data node获取中间数据。这些Map tasks可能同时响应Reducer，这就导致了很多nodes几乎同时向单一节点发起TCP连接。我们称之为incast或者fan-in(微突发流)。如果这种微突发流比较多，那么就要求网络交换机有良好的内部流量管理能力，以及相应的buffers。这种间歇性的buffers使用可能会影响其他的网络行为。这需要另开一篇详细讨论。</p>
<p>Reducer task已经收集了所有intermediate data，现在可以做最后计算了。在这个例子中，我们只需简单的把数字相加就得到了最终结果，写入result.txt</p>
<p>我们的这个例子并没有导致很多的intermediate data在网络间传输。然而其他的jobs可能会产生大量的intermediate data：比如，TB级数据的排序，输出的结果是原始数据集的重新排序，结果尺寸和原始文件大小一致。Map Reduce过程会产生多大的网络流量王权依赖于给定的Job类型。</p>
<p>如果你对网络管理很感兴趣，那么你将了解更多Map Reduce和你运行集群的Jobs类型，以及这些Jobs类型如何影响到网络。如果你是一个Hadoop网络的狂热爱好者，那么你可能会建议写更好的Map Reduce jobs代码来优化网络性能，更快的完成Job</p>
<p><a href="http://cdn2.jobbole.com/2013/07/srljhjqhwl19.png" title="srljhjqhwl19" target="_blank"><img src="&quot;srljhjqhwl19&quot;" alt=""></a></p>
<p>Hadoop通过在现有数据的基础上提供某种商业价值，从而在你的组织内获得成功。当人们意识到它的价值，那么你可能获得更多的资金购买更多的机架和服务器，来扩展现有的Hadoop集群。</p>
<p>当增加一个装满服务器的新机架到Hadoop集群中时，你可能会面临集群不平衡的局面。在上图的例子中，Rack1和Rack2是已经存在的机器，保存着文件File.txt并且正在运行Map Reduce jogs。当我们增加两个新的机架到集群中时，File.txt 数据并不会神奇的自动散布到新的机架中。</p>
<p>新的Data node服务器由于没有数据只能空闲着，直到Client开始保存新的数据到集群中。此外当Rack1和Rack2上的服务器都满负荷的工作，那么Job Tracker可能没有别的选择，只能把作用在File.txt上的Map task分配到这些没有数据的新服务器上，新服务器需要通过网络跨机架获取数据。这就导致更多的网络流量，更慢的处理速度。</p>
<p><a href="http://cdn2.jobbole.com/2013/07/srljhjqhwl20.png" title="srljhjqhwl20" target="_blank"><img src="&quot;srljhjqhwl20&quot;" alt=""></a></p>
<p>为了处理这种情况，Hadoop包含一个时髦的工具叫做 balancer</p>
<p>Balancer查看节点可用存储的差异性，在达到特定的阀值后尝试执行balance。有很多空闲空间的新节点将被检测到，然后balancer开始从空闲空间很少的Data node拷贝数据到这个新节点。Balancer通过控制台的命令行启动，通过控制台取消或者关闭balancer</p>
<p>Balancer可用的网络流量是非常低的，缺省设置为1MB/s。可以通过hdfs-site.xml的df.balance.bandwidthPerSec参数来修改。</p>
<p>Balancer是你的集群的好管家。在你增加服务器时一定会用到它，定期（每周）运行一次也是一个好主意。Balancer使用缺省带宽可能会导致很长时间才能完成工作，比如几天或者几周。</p>
<p>本文是基于<a href="http://www.cloudera.com/hadoop-training/" target="_blank">Training from Cloudera</a> 的学习 以及对我的Hadoop实验环境的观测。这里讨论的内容都是基于<a href="https://ccp.cloudera.com/display/SUPPORT/Downloads" target="_blank">latest stable release of Cloudera’s CDH3 distribution of Hadoop</a> 。本文并没有讨论Hadoop的新技术，比如：<a href="http://hadoop.apache.org/common/docs/r0.21.0/hod_scheduler.html#Introduction" target="_blank">Hadoop on Demand(HOD)</a>和<a href="http://www.hortonworks.com/an-introduction-to-hdfs-federation/" target="_blank">HDFS Federation</a> ，但是这些的确值得花时间去研究。
<img src="&quot;1 vote, average: 5.00 out of 5&quot;" alt="1 vote, average: 5.00 out of 5"><img src="&quot;1 vote, average: 5.00 out of 5&quot;" alt="1 vote, average: 5.00 out of 5"><img src="&quot;1 vote, average: 5.00 out of 5&quot;" alt="1 vote, average: 5.00 out of 5"><img src="&quot;1 vote, average: 5.00 out of 5&quot;" alt="1 vote, average: 5.00 out of 5"><img src="&quot;1 vote, average: 5.00 out of 5&quot;" alt="1 vote, average: 5.00 out of 5"> (<strong>*1</strong> 个评分，平均: <strong>5.00*</strong>)</p>
<p><img src="&quot;Loading ...&quot;" alt="Loading ..."> Loading ...</p>

      
    </div>
    
    
      
    
    
        <footer id="post-meta">
        <span class="categories">Posted in<span class="breadcrumb fa fa-folder"><li><a href="/categories/hadoop/">hadoop</a></li></span></span> | <span class="tags">Tagged <a href="/tags/hadoop/" class="label label-primary">hadoop</a></span> | <span class="time">recent updated:<time title="2014-03-07 09:54:36"datetime="2014-03-07 09:54:36"> mar. 7 2014</time></span> | <span class="comment-link">
<a href="http://itsolife.com/2014/02/02/2014-02-02-hadoop--深入理解Hadoop集群和网络/#comments" class="ds-thread-count comment-link" data-thread-key="2014-02-02-hadoop--深入理解Hadoop集群和网络" data-count-type="comments">暂无评论</a></span>
        </footer>
    
    <div class="clearfix"></div>
  </div>
</div>
</article>





<ul id="pagination" class="pagination pagination-lg">
  <li><a class="extend prev" href="/page/105/">&laquo;</a></li><li><a class="page-number" href="/">1</a></li><li><a class="page-number" href="/page/2/">2</a></li><li><span class="space">&hellip;</span></li><li><a class="page-number" href="/page/103/">103</a></li><li><a class="page-number" href="/page/104/">104</a></li><li><a class="page-number" href="/page/105/">105</a></li><li class="active"><li><span class="page-number current">106</span></li><li><a class="page-number" href="/page/107/">107</a></li><li><a class="page-number" href="/page/108/">108</a></li><li><a class="page-number" href="/page/109/">109</a></li><li><span class="space">&hellip;</span></li></li><li><a class="page-number" href="/page/161/">161</a></li><li><a class="page-number" href="/page/162/">162</a></li><li><a class="extend next" href="/page/107/">&raquo;</a></li>
  <div class="clearfix"></div>
</ul></div><!--wapper-->
       </div><!-- ID main-col END -->
       <aside id="sidebar" class="alignright col-sx-6 col-sm-4 col-md-3 col-lg-3">
<div id="widget_search" class="widget panel panel-primary">
    <form action="//google.com/search" method="get" accept-charset="utf-8">
  <div class="input-group">
    <input class="form-control" id="searchbox" type="search" name="q" results="0" placeholder="search">
    <span class="input-group-btn">
      <button class="btn btn-default" type="submit">Go!</button>
    </span>
    <input type="hidden" name="q" value="site:itsolife.com">
  </div>
</form>
</div>

<div id="widget_category" class="widget panel panel-primary">
  <div class="panel-heading">category</div>  <div data-src='category' class='ajax_widgets'>正在加载...</div>
</div>

<div id="widget_recent_posts" class="widget panel panel-primary">
  <div class="panel-heading">recent posts</div>  <div data-src='recent_posts' class='ajax_widgets'>正在加载...</div></div>

<div id="widget_tagcloud" class="widget panel panel-primary">
  <div class="panel-heading">tagcloud</div>  <div data-src='tagcloud' class='ajax_widgets'>正在加载...</div></div>

<div id="widget_latest_update_posts" class="widget panel panel-primary">
  <div class="panel-heading">最近更新</div>  <div data-src='latest_update_posts' class='ajax_widgets'>正在加载...</div></div>

<div id="widget_recent_comments" class="widget panel panel-primary">
  <div class="panel-heading">recent comments</div>  

<div class="list-group-item ds-recent-comments" data-num-items="6" data-show-avatars="1" data-show-time="1" data-show-title="1" data-show-admin="1" data-excerpt-length="50"></div>



</div>

</aside>
       <div class="clearfix"></div>
     </div><!-- row END -->
  </div>
  <footer id="footer" class="container">
  <div class="panel panel-info">
  <section id='footer_widget'></section>  <div class="panel-footer">
  <div id="site-info">
    <span class='author'>
  
  &copy; 2014 RobinChia
  
    &nbsp;&nbsp;</span>
  
  <span id='analytics-51la'></span><span id='analytics-google'>
<script type="text/javascript">
  var _gaq = _gaq || [];
  _gaq.push(['_setAccount', 'UA-48559895-1']);
  _gaq.push(['_trackPageview']);
  _js2load.push({src:('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js'});
</script></span><span id='analytics-cnzz'>
<script type="text/javascript">var cnzz_protocol = (("https:" == document.location.protocol) ? " https://" : " http://");document.write(unescape("%3Cspan id='cnzz_stat_icon_5774006'%3E%3C/span%3E%3Cscript src='" + cnzz_protocol + "s17.cnzz.com/stat.php%3Fid%3D5774006%26show%3Dpic' type='text/javascript'%3E%3C/script%3E"));</script>
</span><span id='analytics-baidu'>
<script>
var _hmt = _hmt || [];
_js2load.push({src:"//hm.baidu.com/hm.js?1442f50724afc42380b51f097c43082c"});
</script>
</span>  </div>
  <div id="copyright">Site powered by <a href='http://zespia.tw/hexo/'><strong>hexo</strong></a>  update time: <em>2014-04-07 17:30:36</em></span></div>
</div>
<div class="clearfix"></div>


  </div>
  </footer>
  
        <script src="http://cdn.bootcss.com/jquery/1.10.2/jquery.min.js"></script>        
        <script src="http://cdn.staticfile.org/twitter-bootstrap/3.1.0/js/bootstrap.min.js"></script>        
                
        <script src="http://cdn.bootcss.com/prettify/r298/prettify.min.js"></script>    <script type="text/javascript">
   var lang=["bsh", "c", "cc", "cpp", "cs", "csh", "cyc", "cv", "htm", "html",
    "java", "js", "m", "mxml", "perl", "pl", "pm", "py", "rb", "sh",
    "xhtml", "xml", "xsl"];
   var pretty_base='';
   $('script').each(function(){
	var c = $(this).attr('src');
	if (!c)
	    return;
	if (c.match(/(\/)?prettify(\.min)?\.js/i))
	{
	    var index = c.lastIndexOf('/');
	    if (index != -1)
		pretty_base = c.substr(0,index + 1);
	    return false;
	}
   })
   $('pre code').each(function(){
	var c = $(this).attr('class')
	if (!c)
	    return;
	c = c.match(/\s?(lang\-\w+)/i);
	if (c && lang.indexOf(c[1]) == -1)
	{
	    lang.push(c[1]);
	    $.getScript(pretty_base + c[1] + '.min.js');
	}
   })

    $(window).load(function(){
       $("pre").addClass("prettyprint");
       prettyPrint();
    })
</script>    
            <script type="text/javascript">
var duoshuoQuery = {short_name:"robinchia"};
_js2load.push({src:'http://static.duoshuo.com/embed.js',charset:'UTF-8'});
</script>
    
            <!--wumii_relatedItems-->
    
        <script src="http://cdn.bootcss.com/fancybox/2.1.5/jquery.fancybox.min.js"></script>    <script type="text/javascript">
(function($){
  $('.entry').each(function(i){
    $(this).find('img').each(function(){
      var alt = this.alt;

      if (alt){
        $(this).before('<span class="caption">' + alt + '</span>');
      }

      $(this).wrap('<a href="' + this.src + '" title="' + alt + '" class="fancybox" rel="fancybox' + i + '" />');
    });
  });

  $('.fancybox').fancybox();
})(jQuery);
</script>    
        <script src="http://cdn.bootcss.com/mathjax/2.3/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>    <script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      processEscapes: true
    }
  });
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>
    
<script type="text/javascript">
$('.ajax_widgets').each(function(){var src=$(this).attr('data-src');if(src)$(this).load('/widgets/'+src+'.html');});
$.each(_js2load,function(index,obj){loadjs(obj.src,obj.charset)});
</script>

<div id="scroll2top">
<img src="/scroll2top/arrow.png"/>
</div>
<script src="/scroll2top/scroll2top.min.js"></script>
<div id="winterland">
  <canvas></canvas>
</div>
<script src="/js/winterland.min.js"></script>

  </body>
</html>
