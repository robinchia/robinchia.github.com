
<!DOCTYPE HTML>
<html>
<head>
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<title>The 107 Page | It so life</title>
<meta name="author" content="RobinChia">

<meta name="description" content="It so life">


<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">


<meta property="og:site_name" content="It so life"/>

<!--[if IE]><style>.testIE.IE{display:inline;}</style><![endif]-->
<!--[if lte IE 7]><link rel="stylesheet" href="/css/ie7.css" type="text/css"><![endif]-->
<!--[if (lt IE 9)&(gt IE 7)]><style>.testIE.IE8{display:inline;}</style><![endif]-->
<!--[if gt IE 8]><style>.testIE.IE9{display:inline;}</style><![endif]-->

<link href="/favicon.png" rel="icon">
<link rel="alternate" href="/atom.xml" title="It so life Feed" type="application/atom+xml">

<link rel="stylesheet" href="http://cdn.staticfile.org/twitter-bootstrap/3.1.0/css/bootstrap.min.css" type="text/css">
<link rel="stylesheet" href="http://cdn.bootcss.com/font-awesome/4.0.3/css/font-awesome.min.css" type="text/css">
<link rel="stylesheet" href="http://cdn.bootcss.com/prettify/r298/prettify.min.css" type="text/css">
<link rel="stylesheet" href="http://cdn.bootcss.com/fancybox/2.1.5/jquery.fancybox.min.css" type="text/css">
<link rel="stylesheet" href="/css/style.css" type="text/css">
<!--[if lt IE 9]>
   <style>article,aside,dialog,footer,header,section,footer,nav,figure,menu{display:block}</style>
   <script src="http://cdn.staticfile.org/html5shiv/r29/html5.min.js"></script>
   <script src="http://cdn.staticfile.org/respond.js/1.4.2/respond.min.js"></script>
<link href="http://cdn.staticfile.org/respond.js/1.4.2/respond-proxy.html" id="respond-proxy" rel="respond-proxy" />
<link href="/js/respond.proxy.gif" id="respond-redirect" rel="respond-redirect" />
<script src="/js/respond.proxy.js"></script>
   <script src="http://cdn.bootcss.com/selectivizr/1.0.2/selectivizr-min.js"></script>
<![endif]-->
<script type="text/javascript">
function loadjs(c,d){var a=document.createElement("script");a.async=!0;a.type="text/javascript";a.src=c;a.charset=d||"gbk";(document.getElementsByTagName("HEAD")[0]||document.getElementsByTagName("BODY")[0]).appendChild(a)};
var _protocol = (("https:" == document.location.protocol) ? " https://" : " http://");
var _js2load = [];
</script>

</head>
<body>
      <header id="header" class="container"><nav id="main-nav" class="navbar navbar-default navbar-fixed-top " role="navigation">
  <div class="container">
    <div class="navbar-header">
    <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".navbar-collapse">
      <span class="sr-only">Toggle navigation</span>
      <span class="icon-bar"></span>
      <span class="icon-bar"></span>
      <span class="icon-bar"></span>
    </button>
    <a class="navbar-brand" href="/">It so life</a>
    </div>
    <div  class="collapse navbar-collapse">
      <ul  class="nav navbar-nav">
  
        <li><a href="/" title="Home">Home</a></li>      
        <li><a href="/about/" title="About">About</a></li>      
        <li><a href="/archives/" title="Archives">Archives</a></li>      
      <li class='dropdown'>
        <a class='dropdown-toggle' data-toggle='dropdown' href='#'>website<b class='caret'></b></a>
        <ul class='dropdown-menu pure-menu-selected'>
    
          <li><a href="//groups.google.com/forum/#!forum/pongba" title="TopLanguage">TopLanguage</a></li>    
          <li><a href="//itpub.net/" title="ITPub">ITPub</a></li>    
          <li><a href="//blog.jobbole.com/" title="Bole">Bole</a></li>    
          <li><a href="//nosql-database.org/" title="nosql">nosql</a></li>    
          <li><a href="//gitimmersion.googol.im/" title="Git">Git</a></li>    
        </ul>
      </li>
    
      </ul>
      <ul class="nav navbar-nav navbar-right">
      
        <li><a href="/atom.xml">RSS</a></li>
      
      
        <li><a href="https://twitter.com/robinchia">twitter</a></li>
      
      
      
      
        <li><a href="https://github.com/robinchia">github</a></li>
      
      </ul>
    </div>
  </div>
</nav>
<div class="clearfix"></div>
</header>
  <div id='content' class="container">
     <div class="page-header-wrapper">
      <!--[if lt IE 9]><div class="alert alert-warning alert-dismissable"><button type="button" class="close" data-dismiss="alert" aria-hidden="true">&times;</button><strong>提示:</strong>您的浏览器版本太低了,建议升级到 <strong><a href="http://windows.microsoft.com/zh-cn/internet-explorer/download-ie" title="IE9">IE9</a></strong> 以上,本站使用<a href="https://www.google.com/intl/zh-CN/chrome/">Chrome浏览器</a>可以获得最好的显示效果.</div><![endif]-->
      <div class="page-header"><h1 align="center"><big>It so life</big> </h1>
        <h5 align="center"><big>love as life</big></h5>
      </div>
     </div>
     <div class="row">
       <div id="main-col" class="alignleft col-sx-12 col-sm-8 col-md-9 col-lg-9">
      <section id='header_widget'></section>
          <div id="wrapper"><article>
<div id="post" class="post well">
  <div class="post-content">
    <header class="well-sm">
      <i class="fa icon fa-5x pull-left"></i>
      <h1 class="title"><a href="/2014/02/02/2014-02-02-脚本-VB--VBS打造自己的重启删除工具/">VBS打造自己的重启删除工具</a></h1>
      
        <span>Posted on<time datetime="2014-02-02T01:54:37.000Z"> <a href="/2014/02/02/2014-02-02-脚本-VB--VBS打造自己的重启删除工具/">feb. 2 2014</a></time></span>      
    </header>
    
    
    <div class="entry">
      
        <h1 id="vbs-">VBS打造自己的重启删除工具</h1>
<p>安装文件(REG),Install.reg,双击导入即可完成安装
1</p>
<p>Windows Registry Editor Version 5.00</p>
<p>2</p>
<p>3</p>
<p>[HKEY_CLASSES_ROOT\/*\shell\重启删除\</p>
<p>command</p>
<p>]</p>
<p>4</p>
<p>@=</p>
<p>&quot;WScript.exe //nologo c:\windows\system32\rerase.vbs \&quot;%1\&quot;&quot;</p>
<p>rerase.vbs，需拷贝至c:\windows\system32\rerase.vbs ，当然也可是其它目录，但是要修改Insatll.reg
01</p>
<p>On Error Resume Next</p>
<p>02</p>
<p>TargetFile = Wscript.Arguments(0)
03</p>
<p>04</p>
<p>Set ObjRUN = Wscript.createObject(</p>
<p>&quot;Wscript.shell&quot;</p>
<p>)
05</p>
<p>Set ObjFSO = CreateObject(</p>
<p>&quot;Scripting.FileSystemObject&quot;</p>
<p>)</p>
<p>06</p>
<p>07</p>
<p>If ObjFSO.FileExists(TargetFile) Then</p>
<p>08</p>
<p>09</p>
<p>Set GetFile = ObjFSO.GetFile(TargetFile)</p>
<p>10</p>
<p>Set InfFile = ObjFSO.CreateTextFile(objFSO.GetParentFolderName(GetFile) &amp; </p>
<p>&quot;\INFFILE.INF&quot;</p>
<p>)
11</p>
<p>12</p>
<p>InfFile.WriteLine(</p>
<p>&quot;[Version]&quot;</p>
<p>)
13</p>
<p>InfFile.WriteLine(</p>
<p>&quot;Signature = &quot;</p>
<p>&quot;$Chicago$&quot;</p>
<p>&quot;&quot;</p>
<p>)</p>
<p>14</p>
<p>InfFile.WriteLine(</p>
<p>&quot;[DestinationDirs]&quot;</p>
<p>)
15</p>
<p>InfFile.WriteLine(</p>
<p>&quot;DefaultDestDir = 01&quot;</p>
<p>)</p>
<p>16</p>
<p>InfFile.WriteLine(</p>
<p>&quot;[DefaultInstall]&quot;</p>
<p>)
17</p>
<p>InfFile.WriteLine(</p>
<p>&quot;DelFiles = DELETELIST&quot;</p>
<p>)</p>
<p>18</p>
<p>InfFile.WriteLine(</p>
<p>&quot;[DELETELIST]&quot;</p>
<p>)
19</p>
<p>InfFile.WriteLine(</p>
<p>&quot;INFFILE.INF&quot;</p>
<p>)</p>
<p>20</p>
<p>InfFile.WriteLine(ObjFSO.GetFileName(GetFile) &amp; </p>
<p>&quot;,,,1&quot;</p>
<p>)
21</p>
<p>InfFile.Close</p>
<p>22</p>
<p>23</p>
<p>ObjRUN.Run </p>
<p>&quot;RUNDLL32.EXE SETUPAPI.DLL,InstallHinfSection DefaultInstall 128 &quot;</p>
<p>&amp; objFSO.GetParentFolderName(GetFile) &amp; </p>
<p>&quot;.\INFFILE.INF&quot;</p>
<p>, 0, TRUE</p>
<p>24</p>
<p>25</p>
<p>WScript.Sleep 200</p>
<p>26</p>
<p>27</p>
<p>If ObjFSO.FileExists(TargetFile) Then</p>
<p>28</p>
<p>Return = Msgbox(</p>
<p>&quot;必须重启才能完成删除操作，你想现在重启吗?&quot;</p>
<p>, vbYesNo + vbInformation + vbDefaultButton2, </p>
<p>&quot;提示&quot;</p>
<p>)
29</p>
<p>IF Return = vbYes Then</p>
<p>30</p>
<p>ObjRUN.Run </p>
<p>&quot;shutdown -r -t 0&quot;
31</p>
<p>End If</p>
<p>32</p>
<p>End If
33</p>
<p>34</p>
<p>Else
35</p>
<p>MsgBox </p>
<p>&quot;文件未找到!&quot;</p>
<p>, 16, </p>
<p>&quot;错误&quot;</p>
<p>36</p>
<p>End If</p>
<p>来源： <a href="[http://my.oschina.net/veterans/blog/27839](http://my.oschina.net/veterans/blog/27839)">[http://my.oschina.net/veterans/blog/27839](http://my.oschina.net/veterans/blog/27839)</a></p>

      
    </div>
    
    
      
    
    
        <footer id="post-meta">
        <span class="categories">Posted in<span class="breadcrumb fa fa-folder"><li><a href="/categories/脚本/">脚本</a></li></span><span class="breadcrumb"><li><a href="/categories/脚本/">脚本</a></li><li><a href="/categories/脚本/VB/">VB</a></li></span></span> | <span class="tags">Tagged <a href="/tags/VB/" class="label label-primary">VB</a><a href="/tags/脚本/" class="label label-success">脚本</a></span> | <span class="time">recent updated:<time title="2014-03-07 09:54:37"datetime="2014-03-07 09:54:37"> mar. 7 2014</time></span> | <span class="comment-link">
<a href="http://itsolife.com/2014/02/02/2014-02-02-脚本-VB--VBS打造自己的重启删除工具/#comments" class="ds-thread-count comment-link" data-thread-key="2014-02-02-脚本-VB--VBS打造自己的重启删除工具" data-count-type="comments">暂无评论</a></span>
        </footer>
    
    <div class="clearfix"></div>
  </div>
</div>
</article>



  <article>
<div id="post" class="post well">
  <div class="post-content">
    <header class="well-sm">
      <i class="fa icon fa-5x pull-left"></i>
      <h1 class="title"><a href="/2014/02/02/2014-02-02-hadoop--基于Hadoop220的高可用性集群搭建步骤（64位）/">基于Hadoop 2.2.0的高可用性集群搭建步骤（64位）</a></h1>
      
        <span>Posted on<time datetime="2014-02-02T01:54:36.000Z"> <a href="/2014/02/02/2014-02-02-hadoop--基于Hadoop220的高可用性集群搭建步骤（64位）/">feb. 2 2014</a></time></span>      
    </header>
    
    
    <div class="entry">
      
        <h1 id="-hadoop-2-2-0-64-">基于Hadoop 2.2.0的高可用性集群搭建步骤（64位）</h1>
<p> 内容概要: CentSO_64bit集群搭建， hadoop2.2(64位)编译，安装，配置以及测试步骤</p>
<p>新版亮点:基于yarn计算框架和高可用性DFS的第一个稳定版本。</p>
<p>注1：官网只提供32位release版本, 若机器为64位，需要手动编译。</p>
<p>注2：目前网上传的2.2版本的安装步骤几乎都有问题，没有一个版本是完全正确的。若不懂新框架内部机制，不要照抄网传的版本。</p>
<p><strong>0. 编译前的准备</strong></p>
<p><strong>虚拟机vmware准备，64bit CentOS准备</strong></p>
<p><strong>节点ip</strong></p>
<p>cluster1   172.16.102. 201</p>
<p>cluster2  172.16.102. 202</p>
<p>cluster3  172.16.102. 203</p>
<p>cluster4  172.16.102. 204</p>
<p><strong>各节点职能划分</strong>     </p>
<p>cluster1        resourcemanager, nodemanager,</p>
<p>proxyserver,historyserver, datanode, namenode,</p>
<p>cluster2        datanode, nodemanager</p>
<p>cluster3 datanode, nodemanager</p>
<p>cluster4 datanode, nodemanager</p>
<p><strong>说明</strong></p>
<p>以下关于修改hostname, 设置静态ip, ssh免登陆，关闭防火墙等步骤，放在文章末尾，这些内容并不是本文讨论的重点。</p>
<p><strong>1. hadoop2.2**</strong>编译**
1说明：标准的</p>
<p>bash</p>
<p>提示符，root用户为</p>
<p>&#39;/#&#39;</p>
<p>，普通用户为</p>
<p>&#39;%&#39;</p>
<p>，由于博客编辑器的缘故，</p>
<p>&#39;/#&#39;</p>
<p>提示符会被默认为comment, 因此在这篇博文中不再区分root和普通user的提示符， 默认全部为</p>
<p>&#39;$&#39;</p>
<p>因为我们安装的CentOS是64bit的，而官方release的hadoop2.2.0版本没有对应的64bit安装包，故需要自行编译。</p>
<p>首先需要去oracle下载64位jdk:
1</p>
<p>2
$</p>
<p>su</p>
<p>root</p>
<p>$wget http:</p>
<p>//download</p>
<p>.oracle.com</p>
<p>/otn-pub/java/jdk/7u45-b18/jdk-7u45-linux-x64</p>
<p>.</p>
<p>tar</p>
<p>.gz</p>
<p>注: prompt（提示符）为%默认为当前用户， /#则为root,注意以下各步骤中的prompt类型。</p>
<p>下面为hadoop编译步骤（注：中间部分的文本框里内容提要只是一些补充说明，不要执行框里的命令）</p>
<p>1.1 BOTPROTO改为”dhcp”
1</p>
<p>2
$</p>
<p>su</p>
<p>root</p>
<p>$</p>
<p>sed</p>
<p>–i  s</p>
<p>/static/dhcp/g</p>
<p>/etc/sysconfig/network-scripts/ifcfg-eth0</p>
<p>/#servicenetwork restart</p>
<p>1.2 下载hadoop2.2.0 源码</p>
<p>1</p>
<p>2
3$</p>
<p>su</p>
<p>grid</p>
<p>$</p>
<p>cd</p>
<p>~
wget  http:</p>
<p>//apache</p>
<p>.dataguru.cn</p>
<p>/hadoop/common/stable/hadoop-2</p>
<p>.2.0-src.</p>
<p>tar</p>
<p>.gz</p>
<p>1.3 安装maven</p>
<p>1</p>
<p>2
3</p>
<p>4
5$</p>
<p>su</p>
<p>root</p>
<p>$</p>
<p>cd</p>
<p>/opt
wget http:</p>
<p>//apache</p>
<p>.fayea.com</p>
<p>/apache-mirror/maven/maven-3/3</p>
<p>.1.1</p>
<p>/binaries/apache-maven-3</p>
<p>.1.1-bin.</p>
<p>tar</p>
<p>.gz</p>
<p>$</p>
<p>tar</p>
<p>zxvf apache-maven-3.1.1-bin.</p>
<p>tar</p>
<p>.gz
$</p>
<p>cd</p>
<p>apache-maven-3.1.1</p>
<p><em>**</em>修改系统环境变量有两种方式，修改/etc/profile， 或者在/etc/profile.d/下添加定制的shell文件，</p>
<p>鉴于profile文件的重要性，尽量不要在profile文件里添加内容，官方建议采用第二种，以保证profile文件的绝对安全。</p>
<p>下面采用第二种方式：</p>
<p>首先，创建一个简单shell脚脚本并添加相关内容进去：
1</p>
<p>2
$</p>
<p>cd</p>
<p>/etc/profile</p>
<p>.d/</p>
<p>$</p>
<p>touch</p>
<p>maven.sh     其次，maven.sh里添加内容如下：1</p>
<p>2
3/#environmentvariable settings for mavn2</p>
<p>export</p>
<p>MAVEN_HOME=</p>
<p>&#39;/opt/apache-maven-3.1.1&#39;</p>
<p>3
export</p>
<p>PATH=$MAVEN_HOME</p>
<p>/bin</p>
<p>:$PATH</p>
<p>最后，source一下</p>
<p>1$</p>
<p>source</p>
<p>/etc/profile</p>
<p><br><br>$</p>
<p>source</p>
<p>/etc/profile1</p>
<p>2
3&lt;em</p>
<p>id</p>
<p>=</p>
<p>&quot;__mceDel&quot;</p>
<blockquote>
<p>$mvn -version</p>
</blockquote>
<p>Apache Maven 3.1.1
&lt;</p>
<p>/em</p>
<p>&gt;</p>
<p>1.4 安装protobuf</p>
<p>注意apache官方网站上的提示“NOTE: You will need protoc 2.5.0 installed.”
1</p>
<p>2
3</p>
<p>4
5</p>
<p>6
$</p>
<p>su</p>
<p>root</p>
<p>$</p>
<p>cd</p>
<p>/opt
wget https:</p>
<p>//protobuf</p>
<p>.googlecode.com</p>
<p>/files/protobuf-2</p>
<p>.5.0.</p>
<p>tar</p>
<p>.bz2</p>
<p>$</p>
<p>tar</p>
<p>xvf protobuf-2.5.0.</p>
<p>tar</p>
<p>.bz2 (注意压缩文件后缀， maven安装包是—</p>
<p>gzip</p>
<p>文件，解压时需加–z)
$</p>
<p>cd</p>
<p>protobuf-2.5.0</p>
<p>.</p>
<p>/configure</p>
<p>安装protobuf时提示报错”configure: error: C++ preprocessor &quot;/lib/cpp&quot; failssanity check”</p>
<p>安装gcc
1$yum</p>
<p>install</p>
<p>gcc</p>
<p>1.5 编译hadoop</p>
<p>首先从官网下载hadoop2.2.0source code:
1</p>
<p>2
3$</p>
<p>su</p>
<p>grid;</p>
<p>$</p>
<p>cd</p>
<p>~grid/
wget http:</p>
<p>//apache</p>
<p>.dataguru.cn</p>
<p>/hadoop/common/stable/hadoop-2</p>
<p>.2.0-src.</p>
<p>tar</p>
<p>.gz</p>
<p>好了，痛苦的编译过程来了。</p>
<p>解压之：
1</p>
<p>2
$</p>
<p>tar</p>
<p>zxvf hadoop-2.2.0-src.</p>
<p>tar</p>
<p>.gz</p>
<p>$</p>
<p>cd</p>
<p>hadoop-2.2.0-src</p>
<p>1.6 给maven指定国内镜像源</p>
<p>1.6.1. 切换root权限, 修改/opt/apache-maven-3.1.1/conf/settings.xml
1</p>
<p>2
$</p>
<p>su</p>
<p>root</p>
<p>$vim</p>
<p>/opt/apache-maven-3</p>
<p>.1.1</p>
<p>/conf/settings</p>
<p>.xml</p>
<p>修改1. 在<mirrors>…</mirrors>里添加国内源（注意，保留原本就有的<mirrors>...</mirrors>）：</p>
<p><a href="&quot;复制代码&quot;"><img src="http://common.cnblogs.com/images/copycode.gif" alt="复制代码"></a>
1  <mirrors> 2     <mirror>3         <id>nexus-osc</id>4         <mirrorOf>/*</mirrorOf>5     <name>Nexusosc</name>6     <url><a href="http://maven.oschina.net/content/groups/public/" target="_blank">http://maven.oschina.net/content/groups/public/</a></url>7     </mirror>8 </mirrors></p>
<p><a href="&quot;复制代码&quot;"><img src="http://common.cnblogs.com/images/copycode.gif" alt="复制代码"></a></p>
<p>修改2. 在<profiles>标签中增加以下内容（保留原来的<profiles>…</profiles>， jdk版本根据用户的情况填写）</p>
<p><a href="&quot;复制代码&quot;"><img src="http://common.cnblogs.com/images/copycode.gif" alt="复制代码"></a>
 1  <profile> 2       <id>jdk-1.4</id> 3       <activation> 4         <jdk>1.4</jdk> 5       </activation> 6       <repositories> 7         <repository> 8           <id>nexus</id> 9           <name>local private nexus</name>10           <url><a href="http://maven.oschina.net/content/groups/public/">http://maven.oschina.net/content/groups/public/</a></url>11           <releases>12             <enabled>true</enabled>13           </releases>14           <snapshots>15             <enabled>false</enabled>16           </snapshots>17         </repository>18       </repositories>19       <pluginRepositories>20         <pluginRepository>21           <id>nexus</id>22           <name>local private nexus</name>23           <url><a href="http://maven.oschina.net/content/groups/public/" target="_blank">http://maven.oschina.net/content/groups/public/</a></url>24           <releases>25             <enabled>true</enabled>26           </releases>27           <snapshots>28             <enabled>false</enabled>29           </snapshots>30         </pluginRepository>31       </pluginRepositories>32     </profile>33 </profiles></p>
<p><a href="&quot;复制代码&quot;"><img src="http://common.cnblogs.com/images/copycode.gif" alt="复制代码"></a></p>
<p>   1.6.2 将刚才修改的配置文件拷到当前用户home目录下</p>
<p><a href="&quot;复制代码&quot;"><img src="http://common.cnblogs.com/images/copycode.gif" alt="复制代码"></a>
$ su grid $ sudo cp /opt/apache-maven-3.1.1/conf/settings.xml ~/.m2//#若提示该用户不在sudoers里，执行以下步骤： $ su root  /#在sudoers里第99行添加当前用户（下面行号不要加）： $ cat /etc/sudoers98  root     ALL=(ALL)       ALL99  grid     ALL=(ALL)       ALL</p>
<p><a href="&quot;复制代码&quot;"><img src="http://common.cnblogs.com/images/copycode.gif" alt="复制代码"></a></p>
<p>1.7 现在执行官方的clean步骤： $mvn clean install –DskipTests</p>
<p>漫长的等待后发现安装一切正常。</p>
<p>1.8 安装3个依赖包
1</p>
<p>2
3Cmake</p>
<p>ncurses-devel
openssl-devel</p>
<p>执行以下步骤：</p>
<p>1</p>
<p>2
3</p>
<p>4
$</p>
<p>su</p>
<p>root</p>
<p>$yum</p>
<p>install</p>
<p>ncurses-devel
$yum</p>
<p>install</p>
<p>openssl-devel</p>
<p>$yum</p>
<p>install</p>
<p>cmake89</p>
<p>以上安装完成后，切回用户grid：</p>
<p>1</p>
<p>2
$</p>
<p>su</p>
<p>grid</p>
<p>$</p>
<p>cd</p>
<p>~</p>
<p>/hadoop-2</p>
<p>.2.0-src</p>
<p><em>**</em>1.9 所有依赖已安装完毕，开始编译</p>
<p>1$mvn package-Pdist,native -DskipTests -Dtar</p>
<p>漫长的等待后，编译成功，查看结果：</p>
<p><img src="http://m1.img.libdd.com/farm4/d/2013/1114/11/160569527B88184A5CFD1B4C6D260A60_B500_900_500_214.png" alt=""></p>
<p>一切正常。至此，hadoop2.2.0编译完成。</p>
<p><em>**</em>1.10 验证</p>
<p>下面验证编译结果是否符合预期, 注意我们当前是在目录~/hadoop-2.2.0-src下，
1</p>
<p>2
3$</p>
<p>cd</p>
<p>hadoop-dist/</p>
<p>$</p>
<p>ls
pom.xml  target</p>
<p>以上为maven编译的配置文件</p>
<p>1</p>
<p>2
3</p>
<p>4
5</p>
<p>6
7$</p>
<p>cd</p>
<p>target</p>
<p>$</p>
<p>ls</p>
<p>-sF
total 276M</p>
<p>4.0K antrun/                    92M hadoop-2.2.0.</p>
<p>tar</p>
<p>.gz            4.0K maven-archiver/
4.0K dist-layout-stitching.sh  4.0K hadoop-dist-2.2.0.jar          4.0K</p>
<p>test</p>
<p>-</p>
<p>dir</p>
<p>/</p>
<p>4.0K dist-</p>
<p>tar</p>
<p>-stitching.sh     184M hadoop-dist-2.2.0-javadoc.jar
4.0K hadoop-2.2.0/             4.0K javadoc-bundle-options/</p>
<p>以上为maven编译后自动生成的目录文件，进入hadoop-2.2.0:</p>
<p>1</p>
<p>2
3$</p>
<p>cd</p>
<p>hadoop-2.2.023</p>
<p>$</p>
<p>ls
bin  etc  include  lib libexec  sbin  share</p>
<p>这才是和官方release2.2.0版本（官方只有32bit版本）的相同的目录结构。</p>
<p>1.10.1 下面主要验证两项：</p>
<p><em>**</em>a.验证版本号
1</p>
<p>2
3</p>
<p>4
5</p>
<p>6
7$bin</p>
<p>/hadoop</p>
<p>version</p>
<p>Hadoop 2.2.0
Subversion Unknown -r Unknown</p>
<p>Compiled by grid on 2013-11-06T13:51Z
Compiled with protoc 2.5.0</p>
<p>From</p>
<p>source</p>
<p>with checksum 79e53ce7994d1628b240f09af91e1af4
This</p>
<p>command</p>
<p>was run using</p>
<p>/home/grid/hadoop-2</p>
<p>.2.0-src</p>
<p>/hadoop-dist/target/hadoop-2</p>
<p>.2.0</p>
<p>/share/hadoop/common/hadoop-common-2</p>
<p>.2.0.jar</p>
<p>可以看到hadoop版本号，编译工具（protoc2.5.0版本号与官方要求一致）以及编译日期.</p>
<p><em>**</em>b.验证hadoop lib的位数
1</p>
<p>2
3</p>
<p>4
5</p>
<p>6
$</p>
<p>file</p>
<p>lib</p>
<p>//native/</p>
<p>/*</p>
<p>lib</p>
<p>//native/libhadoop</p>
<p>.a:        current ar archive
lib</p>
<p>//native/libhadooppipes</p>
<p>.a:   current ar archive</p>
<p>lib</p>
<p>//native/libhadoop</p>
<p>.so:       symbolic link to `libhadoop.so.1.0.0&#39;
lib</p>
<p>//native/libhadoop</p>
<p>.so.1.0.0:<strong>ELF 64-bitLSB&lt;</p>
<p>/strong</p>
<blockquote>
<p>shared object, x86-64, version 1 (SYSV), dynamically linked, notstripped1011 lib</p>
</blockquote>
<p>//native/libhadooputils</p>
<p>.a:   current ar archive1213 lib</p>
<p>//native/libhdfs</p>
<p>.a:          current ar archive1415 lib</p>
<p>//native/libhdfs</p>
<p>.so:         symbolic link to `libhdfs.so.0.0.0&#39;</p>
<p>lib</p>
<p>//native/libhdfs</p>
<p>.so.0.0.0:   <strong>ELF 64-bit LSB shared object&lt;</p>
<p>/strong</p>
<blockquote>
<p>, x86-64,version 1 (SYSV), dynamically linked, not stripped</p>
</blockquote>
<p>看到黑色的“ELF-64bit LSB”证明64bit hadoop2.2.0初步编译成功，查看我们之前的hadoop0.20.3版本，会发现lib//native/libhadoop.so.1.0.0是32bit，这是不正确的！。^_^</p>
<p><strong>2. hadoop2.2**</strong>配置**</p>
<p><strong>**</strong>2.1 home设置**</p>
<p>为了和MRv1区别， 2.2版本的home目录直接命名为yarn:
1</p>
<p>2
3</p>
<p>4
$</p>
<p>su</p>
<p>hadoop</p>
<p>$</p>
<p>cd</p>
<p>~
$</p>
<p>mkdir</p>
<p>–p yarn</p>
<p>/yarn_data67</p>
<p>$</p>
<p>cp</p>
<p>–a ~hadoop</p>
<p>/hadoop-2</p>
<p>.2.0-src</p>
<p>/hadoop-dist/target/hadoop-2</p>
<p>.2.0  ~hadoop</p>
<p>/yarn</p>
<p><strong>2.2 环境变量设置</strong></p>
<p>~/.bashrc里添加新环境变量：</p>
<p>1</p>
<p>2
3</p>
<p>4
5</p>
<p>6
7</p>
<p>8
9</p>
<p>10
11</p>
<p>12
/# javaenv</p>
<p>export</p>
<p>JAVA_HOME=</p>
<p>&quot;/usr/java/jdk1.7.0_45&quot;</p>
<p>exportPATH=</p>
<p>&quot;$JAVA_HOME/bin:$PATH&quot;
/# hadoopvariable settings</p>
<p>export</p>
<p>HADOOP_HOME=</p>
<p>&quot;$HOME/yarn/hadoop-2.2.0&quot;
export</p>
<p>HADOOP_PREFIX=</p>
<p>&quot;$HADOOP_HOME/&quot;</p>
<p>export</p>
<p>YARN_HOME=$HADOOP_HOME
export</p>
<p>HADOOP_MAPRED_HOME=</p>
<p>&quot;$HADOOP_HOME&quot;</p>
<p>export</p>
<p>HADOOP_COMMON_HOME=</p>
<p>&quot;$HADOOP_HOME&quot;
export</p>
<p>HADOOP_HDFS_HOME=</p>
<p>&quot;$HADOOP_HOME&quot;</p>
<p>export</p>
<p>HADOOP_CONF_DIR=</p>
<p>&quot;$HADOOP_HOME/etc/hadoop/&quot;
export</p>
<p>YARN_CONF_DIR=$HADOOP_CONF_DIR</p>
<p>export</p>
<p>PATH=</p>
<p>&quot;$HADOOP_HOME/bin:$HADOOP_HOME/sbin:$PATH&quot;</p>
<p>以上操作注意2点：</p>
<ol>
<li>jdk一定要保证是64bit的</li>
</ol>
<p>2.HADOOP_PREFIX极其重要，主要是为了兼容MRv1，优先级最高（比 如寻找conf目录，即使我们配置了HADOOP_CONF_DIR,启动脚本依然会优先从$HADOOP_PREFIX/conf/里查找），一定要保 证此变量正确配置(也可不设置，则默认使用HADOOP_HOME/etc/hadoop/下的配置文件)</p>
<p><strong>**</strong>2.3 改官方启动脚本的bug**</p>
<p><em>**</em>说明：此版本虽然是release稳定版，但是依然有非常弱智的bug存在。</p>
<p>正常情况下，启动守护进程$YARN_HOME/sbin/hadoop-daemons.sh中可以指定node.</p>
<p>我们看下该启动脚本的说明：
1</p>
<p>2
$sbin</p>
<p>/hadoop-daemons</p>
<p>.sh</p>
<p>Usage:hadoop-daemons.sh [--config confdir] [--hosts hostlistfile] [start|stop]</p>
<p>command</p>
<p>args...</p>
<p>可以看到--hosts可以指定需要启动的存放节点名的文件名：</p>
<p>但这是无效的，此脚本调用的$YARN_HOME/libexec/hadoop-config.sh脚本有bug.</p>
<p>先建一个文件datanodefile 添加datanode节点，放入conf/文件夹下，然后执行一下启动脚本：
1</p>
<p>2
$sbin</p>
<p>/hadoop-daemons</p>
<p>.sh --hosts datanodefile start datanode</p>
<p>at:</p>
<p>/home/grid/yarn/hadoop-2</p>
<p>.2.0</p>
<p>/etc/hadoop//126571</p>
<p>:No such</p>
<p>file</p>
<p>or directory</p>
<p>分析脚本，定位到嵌套脚本$YARN_HOME/libexec/hadoop-config.sh第96行：</p>
<p>196        </p>
<p>export</p>
<p>HADOOP_SLAVES=</p>
<p>&quot;${HADOOP_CONF_DIR}/$$1&quot;</p>
<p>看到红色部分是不对的，应该修改为：</p>
<p>196        </p>
<p>export</p>
<p>HADOOP_SLAVES=</p>
<p>&quot;${HADOOP_CONF_DIR}/$1&quot;</p>
<p>备注1：此版本11月初发布至今，网上教程不论中文还是英文，均未提及此错误。</p>
<p>备注2：$YARN_HOME/libexec/hadoop-config.sh中分析hostfile的逻辑非常脑残：
1</p>
<p>2
3</p>
<p>4
593    </p>
<p>if</p>
<p>[</p>
<p>&quot;--hosts&quot;</p>
<p>=</p>
<p>&quot;$1&quot;</p>
<p>]</p>
<p>94    </p>
<p>then
95        </p>
<p>shift</p>
<p>96        </p>
<p>export</p>
<p>HADOOP_SLAVES=</p>
<p>&quot;${HADOOP_CONF_DIR}/%$1&quot;
97        </p>
<p>shift</p>
<p>因此，用户只能将自己的hostfile放在${HADOOP_CONF_DIR}/ 下面，放在其它地方是无效的。</p>
<p>备注3：按照$YARN_HOME/libexec/hadoop-config.sh脚本的逻辑，还有一种方式指定host
1$hadoop-daemons.sh –hostnames cluster1 start datanode</p>
<p>注意，因为bash脚本区分输入参数的分割符为\t或\s，所以限制了此种方式只能指定一个单独的节点</p>
<p>总结以上分析和修改步骤：
1</p>
<p>2
3</p>
<p>4
5$</p>
<p>cd</p>
<p>$YARN_HOME</p>
<p>/libexec/</p>
<p>$vim hadoop-config.sh
/#修改第96行代码为：</p>
<p>export</p>
<p>HADOOP_SLAVES=</p>
<p>&quot;${HADOOP_CONF_DIR}/$1&quot;
/#保存退出vim</p>
<p><strong>**</strong>2.4 配置文件设置**</p>
<p><a href="&quot;复制代码&quot;"><img src="http://common.cnblogs.com/images/copycode.gif" alt="复制代码"></a>
 1 <!--$YARN_HOME/etc/hadoop/core-site.xml--> 2                                                                                                                                                                                                                                  3 &lt;?xml version=&quot;1.0&quot;?&gt; 4 &lt;?xml-stylesheet type=&quot;text/xsl&quot; href=&quot;configuration.xsl&quot;?&gt; 5                                                                                                                                                                                                                                  6 <configuration> 7 <property> 8 <name>fs.defaultFS</name> 9   <value>hdfs://cluster1:9000</value>10 </property>11                                                                                                                                                                                                                                 12 <property>13 <name>hadoop.tmp.dir</name>14   <value>/home/grid/hadoop-2.2.0-src/hadoop-dist/target/hadoop-2.2.0//yarn_data/tmp/hadoop-grid</value>15 </property>16 </configuration>17   备注1：注意fs.defaultFS为新的变量，代替旧的：fs.default.name18 19  备注2：tmp文件夹放在我们刚才新建的$HOME/yarn/yarn_data/下面。20 21     <!--$YARN_HOME/etc/hadoop/hdfs-site.xml-->22 <configuration>23     <property>24     <name>dfs.replication</name>25     <value>3</value>26     </property>27 </configuration></p>
<p><a href="&quot;复制代码&quot;"><img src="http://common.cnblogs.com/images/copycode.gif" alt="复制代码"></a></p>
<p><em>**</em>备注1. 新：dfs.namenode.name.dir，旧：dfs.name.dir，新：dfs.datanode.name.dir，旧：dfs.data.dir</p>
<p><em>**</em>备注2. dfs.replication确定 data block的副本数目，hadoop基于rackawareness(机架感知)默认复制3份分block,（同一个rack下两个，另一个rack下一 份，按照最短距离确定具体所需block, 一般很少采用跨机架数据块，除非某个机架down了）</p>
<p><a href="&quot;复制代码&quot;"><img src="http://common.cnblogs.com/images/copycode.gif" alt="复制代码"></a>
 1     <!--$YARN_HOME/etc/hadoop/yarn-site.xml--> 2                                                                                                                                                                                                                             3 <configuration> 4    <property> 5      <name>yarn.nodemanager.aux-services</name> 6      <value>mapreduce_shuffle</value> 7   </property> 8                                                                                                                                                                                                                             9   <property>10      <name>yarn.resourcemanager.address</name>11      <value>cluster1:8032</value>12   </property>13                                                                                                                                                                                                                            14   <property>15       <name>yarn.resourcemanager.resource-tracker.address</name>16       <value>cluster1:8031</value>17   </property>18                                                                                                                                                                                                                            19   <property>20       <name>yarn.resourcemanager.admin.address</name>21       <value>cluster1:8033</value>22   </property>23                                                                                                                                                                                                                            24   <property>25       <name>yarn.resourcemanager.scheduler.address</name>26       <value>cluster1:8030</value>27   </property>28                                                                                                                                                                                                                            29   <property>30       <name>yarn.nodemanager.loacl-dirs</name>31       <value>/home/grid/hadoop-2.2.0-src/hadoop-dist/target//hadoop-2.2.0/yarn_data/mapred/nodemanager</value>32       <final>true</final>33   </property>34                                                                                                                                                                                                                            35   <property>36       <name>yarn.web-proxy.address</name>37       <value>cluster1:8888</value>38   </property>39                                                                                                                                                                                                                            40   <property>41      <name>yarn.nodemanager.aux-services.mapreduce.shuffle.class</name>42      <value>org.apache.hadoop.mapred.ShuffleHandler</value>43   </property>44 </configuration></p>
<p><a href="&quot;复制代码&quot;"><img src="http://common.cnblogs.com/images/copycode.gif" alt="复制代码"></a></p>
<p><strong>在此特意提醒：</strong></p>
<p>(1) 目前网上盛传的各种hadoop2.2的基于分布式集群的安装教程大部分都是错的,hadoop2.2配置里最重要的也莫过于yarn-site.xml里的变量了吧？</p>
<p>(2)不同于单机安装，基于集群的搭建必须设置</p>
<p>yarn-site.xml可选变量yarn.web-proxy.address
和</p>
<p>yarn.nodemanager.loacl-dirs
外的其它所有变量！后续会专门讨论yarn-site.xml里各个端口配置的含义。</p>
<p><a href="&quot;复制代码&quot;"><img src="http://common.cnblogs.com/images/copycode.gif" alt="复制代码"></a>
1 <!--$YARN_HOME/etc/hadoop/mapred-site.xml-->2                                                                                                                                                                                                            3 <configuration>4     <property>5         <name>mapreduce.framework.name</name>6         <value>yarn</value>7     </property>8 </configuration></p>
<p><a href="&quot;复制代码&quot;"><img src="http://common.cnblogs.com/images/copycode.gif" alt="复制代码"></a></p>
<p>备注1：新的计算框架取消了实体上的jobtracker, 故不需要再指定mapreduce.jobtracker.addres，而是要指定一种框架，这里选择yarn. 备注2：hadoop2.2.还支持第三方的计算框架，但没怎么关注过。
          配置好以后将$HADOOP_HOME下的所有文件，包括hadoop目录分别copy到其它3个节点上。</p>
<p><strong>2.5**</strong>各节点功能规划**</p>
<p>确保在每主机的/etc/hosts里添加了所有node的域名解析表（i.e.cluster1   198.0.0.1）；iptables已关闭；/etc/sysconfig/network-script/ifcfg-eth0里 BOTPROTO=static；/etc/sysconfig/network文件里已设置了各台主机的hostname, 静态ip地址，且已经重启过每台机器；jdk和hadoop都为64bit；ssh免登陆已配置；完成以上几项后，就可以启动hadoop2.2.0了。</p>
<p>注意到从头到尾都没有提到Master, Slave,也没有提到namenode,datanode,是因为，新的计算框架，新的hdfs中不存在物理上的Master节点，所有的节点都是等价的。</p>
<p>各节点职能划分在篇首已有说明， 在此重提一下：
1</p>
<p>2
3</p>
<p>4
cluster1    resourcemanager, nodemanager, proxyserver,historyserver, datanode, namenode,</p>
<p>cluster2    datanode, nodemanager
cluster3    datanode, nodemanager</p>
<p>cluster4    datanode, nodemanager</p>
<p><strong>2.6 hdfs 格式化</strong></p>
<p>1$bin</p>
<p>/hdfs</p>
<p>namenode –</p>
<p>format</p>
<p>(注意：hadoop 2.2.0的格式化步骤和旧版本不一样，旧的为 $YARN_HOME/bin/hadoop namenode –format)</p>
<p><strong>2.7 hadoop整体启动</strong></p>
<p><em>**</em>启动方式（1）分别登录各自主机开启</p>
<p>在cluster1节点上，分别启动resourcemanager,nodemanager, proxyserver, historyserver, datanode, namenode,</p>
<p>在cluster2, cluster3, cluster4 节点主机上，分别启动datanode,nodemanager</p>
<p><strong>备注</strong>：如果resourcemanager是 独立的，则除了resourcemanager,其余每一个节点都需要一个nodemanager，我们可以在$YARN_HOME/etc /hadoop/下新建一个nodehosts, 在里面添加所有的除了resourcemanager外的所有node，因为此处我们配置的resourcemanager和namenode是同一台主 机，所以此处也需要添加nodemanager</p>
<p>执行步骤如下：
1</p>
<p>2
3</p>
<p>4
5</p>
<p>6
7</p>
<p>8
9</p>
<p>10
11</p>
<p>12
13$</p>
<p>hostname</p>
<p>/#查看host名字</p>
<p>cluster1
$sbin</p>
<p>/hadoop-daemon</p>
<p>.sh  --script hdfs start namenode</p>
<p>/# 启动namenode</p>
<p>$sbin</p>
<p>/hadoop-daemon</p>
<p>.sh --script hdfs start datanode</p>
<p>/# 启动datanode
$sbin</p>
<p>/yarndaemon</p>
<p>.shstart nodemanager</p>
<p>/#启动nodemanager</p>
<p>$sbin</p>
<p>/yarn-daemon</p>
<p>.sh   start resourcemanager</p>
<p>/# 启动resourcemanager
$sbin</p>
<p>/yarn-daemon</p>
<p>.shstart proxyserver</p>
<p>/# 启动web App proxy, 作用类似jobtracker,若yarn-site.xml里没有设置yarn.web-proxy.address的host和端口，或者设置了和 resourcemanager相同的host和端口，则hadoop默认proxyserver和resourcemanager共享 host:port</p>
<p>$sbin</p>
<p>/mr-jobhistory-daemon</p>
<p>.sh   start historyserver</p>
<p>/#你懂得
$</p>
<p>ssh</p>
<p>cluster2 </p>
<p>/#登录cluster2</p>
<p>$</p>
<p>hostname</p>
<p>/#查看host名字cluster2
$sbin</p>
<p>/yarndaemon</p>
<p>.shstart nodemanager</p>
<p>/# 启动nodemanager</p>
<p>$sbin</p>
<p>/hadoop-daemon</p>
<p>.sh  --script hdfs start datanode</p>
<p>/# 启动datanode
$</p>
<p>ssh</p>
<p>cluster3 </p>
<p>/#登录cluster3.../# cluster2, cluster3, cluster4启动方式和cluster2一样。</p>
<p>启动方式（2）使用hadoop自带的批处理脚本开启</p>
<p>Step1.确认已登录cluster1:
1$</p>
<p>hostname</p>
<p>cluster1</p>
<p>在$YARN_HOME/etc/hadoop/下新建namenodehosts,添加所有namenode节点</p>
<p>1</p>
<p>2</p>
<p>$</p>
<p>cat</p>
<p>$YARN_HOME</p>
<p>/etc/hadoop/namenodehosts</p>
<p>cluster1</p>
<p>在$YARN_HOME/etc/hadoop/下新建datanodehosts,添加所有datanode节点</p>
<p>1</p>
<p>2
3</p>
<p>4
$</p>
<p>cat</p>
<p>$YARN_HOME</p>
<p>/etc/hadoop/datanodehosts</p>
<p>cluster2
cluster3</p>
<p>cluster4</p>
<p>在$YARN_HOME/etc/hadoop/下新建nodehosts,添加所有datanode和namenode节点</p>
<p>1</p>
<p>2
3</p>
<p>4
5$</p>
<p>cat</p>
<p>$YARN_HOME</p>
<p>/etc/hadoop/datanodehosts</p>
<p>cluster1
cluster2</p>
<p>cluster3
cluster4</p>
<p><strong>备注</strong>：以上的hostfile名字是随便起的，可以是任意的file1,file2,file3, 但是必须放在$YARN_HOME/etc/hadoop/下面！</p>
<p>Step2.执行脚本
1</p>
<p>2
3</p>
<p>4
5</p>
<p>6
$sbin</p>
<p>/hadoop-daemons</p>
<p>.sh--hosts namenodehosts --script  hdfsstart  namenode</p>
<p>$sbin</p>
<p>/hadoop-daemons</p>
<p>.sh--hosts datanodehosts --script  hdfsstart  datanode
$sbin</p>
<p>/yarn-daemons</p>
<p>.sh--hostnames cluster1 start resourcemanager</p>
<p>$sbin</p>
<p>/yarn-daemons</p>
<p>.sh--hosts allnodehosts start nodemanager
$sbin</p>
<p>/yarn-daemons</p>
<p>.sh--hostnames cluster1 start proxyserver</p>
<p>$sbin</p>
<p>/mr-jobhistory-daemon</p>
<p>.sh   start  historyserver</p>
<p><strong>在这里不得不纠正一个其他教程上关于对hadoop2.2做相关配置和运行时的错误!</strong></p>
<p>我们在core-site.xml指定了default filesystem为cluster1, 并指定了其节点ip（或节点名）和端口号， 这意味着若启动hadoop时不额外添加配置（启动hadoop时命令行里加--conf指定用户配置文件的存放位置)，则默认的namenode就一 个，那就是cluster1, 如果随意指定namenode必然会出现错误！</p>
<p>如果你要还要再启动一个新的namenode(以cluster3为例)，则必须如下操作：</p>
<p>a. 新建一个core-site.xml，添加fs.defaultFS的value为hdfs://cluster3:9000</p>
<p>b. command：
1$sbin</p>
<p>/hadoop-daemon</p>
<p>.sh --hostnames cluster3 --conf you_conf_dir --script hdfs --start namenode</p>
<p>我们之前启动的namenode为cluster1, 假如要查看放在此文件系统根目录下的文件input_file，则它的完整路径为 hdfs://cluster1:9000/input_file</p>
<p>Step3.查看启动情况</p>
<p>在cluster1上：
1</p>
<p>2
3</p>
<p>4
5</p>
<p>6
$jps</p>
<p>22411 NodeManager
23356 Jps</p>
<p>22292 ResourceManager
22189 DataNode</p>
<p>22507 WebAppProxyServer</p>
<p>在cluster2上</p>
<p>1</p>
<p>2
3</p>
<p>4
$jps</p>
<p>8147 DataNode
8355 Jps</p>
<p>8234 NodeManagercluster3，cluster4上和cluster2一样。</p>
<p><em>**</em>Step4.查看各节点状态以及yarncluster运行状态</p>
<p>（1）查看各节点状态</p>
<p>FireFox进入： <a href="http://cluster1:50070(cluster1为namenode所在节点" target="_blank">http://cluster1:50070(cluster1为namenode所在节点</a>)</p>
<p>在主页面（第一张图）上点击Live Node，查看（第二张图）上各Live Node状态：</p>
<p><img src="http://m1.img.libdd.com/farm4/d/2013/1114/14/5D0856F11A09F93C0BD97246AE6DEE43_B500_900_500_218.png" alt=""></p>
<p>（2）查看resourcemanager上cluster运行状态</p>
<p>Firefox进入：<a href="http://cluster2:8088(node1为resourcemanager所在节点" target="_blank">http://cluster2:8088(node1为resourcemanager所在节点</a>)</p>
<p><img src="http://m3.img.libdd.com/farm4/d/2013/1114/14/5B46215F81D52CF3C3DB80D2DEFF8770_B500_900_500_195.png" alt=""></p>
<p><strong> </strong>Step5. Cluster上MapReduce测试</p>
<p>现提供3个test cases</p>
<p><strong>Test Case 1 testimated_value_of_pi</strong></p>
<p>command:
$sbin/yarnjar $YARN_HOME/share/hadoop//mapreduce/hadoop-mapreduce-examples-2.2.0.jar \pi 101000000</p>
<p>console输出：</p>
<p><a href="&quot;复制代码&quot;"><img src="http://common.cnblogs.com/images/copycode.gif" alt="复制代码"></a>
Number of Maps  =10                        Samples per Map = 1000000                         Wrote input for Map /#0                          Wrote input for Map /#1                          Wrote input for Map /#2                         Wrote input for Map /#3                        Wrote input for Map /#4                       Wrote input for Map /#5                         Wrote input for Map /#6                        Wrote input for Map /#7                        Wrote input for Map /#8                        Wrote input for Map /#9                        Starting Job                        13/11/06 23:20:07 INFO Configuration.deprecation: mapred.map.tasksis deprecated. Instead, use mapreduce.job.maps                        13/11/06 23:20:07 INFO Configuration.deprecation:mapred.output.key.class is deprecated. Instead, usemapreduce.job.output.key.class                         13/11/06 23:20:07 INFO Configuration.deprecation:mapred.working.dir is deprecated. Instead, use mapreduce.job.working.dir                         13/11/06 23:20:11 INFO mapreduce.JobSubmitter: Submittingtokens for job: job_1383806445149_0001                         13/11/06 23:20:15 INFO impl.YarnClientImpl: Submittedapplication application_1383806445149_0001 to ResourceManager at /0.0.0.0:8032                       13/11/06 23:20:16 INFO mapreduce.Job: The url to trackthe job: <a href="http://Node1:8088/proxy/application_1383806445149_0001/" target="_blank">http://Node1:8088/proxy/application_1383806445149_0001/</a>                        13/11/06 23:20:16 INFO mapreduce.Job: Running job:job_1383806445149_0001                         13/11/06 23:21:09 INFO mapreduce.Job: Jobjob_1383806445149_0001 running in uber mode : false                        13/11/06 23:21:10 INFO mapreduce.Job:  map 0% reduce 0%                          13/11/06 23:24:28 INFO mapreduce.Job:  map 20% reduce 0%                         13/11/06 23:24:30 INFO mapreduce.Job:  map 30% reduce 0%                          13/11/06 23:26:56 INFO mapreduce.Job:  map 57% reduce 0%                          13/11/06 23:26:58 INFO mapreduce.Job:  map 60% reduce 0%                          13/11/06 23:28:33 INFO mapreduce.Job:  map 70% reduce 20%                         13/11/06 23:28:35 INFO mapreduce.Job:  map 80% reduce 20%                         13/11/06 23:28:39 INFO mapreduce.Job:  map 80% reduce 27%                         13/11/06 23:30:06 INFO mapreduce.Job:  map 90% reduce 27%                         13/11/06 23:30:09 INFO mapreduce.Job:  map 100% reduce 27%                         13/11/06 23:30:12 INFO mapreduce.Job:  map 100% reduce 33%                         13/11/06 23:30:25 INFO mapreduce.Job:  map 100% reduce 100%                          13/11/06 23:30:54 INFO mapreduce.Job: Jobjob_1383806445149_0001 completed successfully                          13/11/06 23:31:10 INFO mapreduce.Job: Counters: 43                                    File SystemCounters                                             FILE:Number of bytes read=226                                             FILE:Number of bytes written=879166                                              FILE:Number of read operations=0                                             FILE:Number of large read operations=0                                            FILE:Number of write operations=0                                             HDFS:Number of bytes read=2590                                               HDFS:Number of bytes written=215                                               HDFS:Number of read operations=43                                               HDFS:Number of large read operations=0                                                 HDFS:Number of write operations=3                                    JobCounters                                              Launchedmap tasks=10                                                Launchedreduce tasks=1                                               Data-localmap tasks=10                                                Totaltime spent by all maps in occupied slots (ms)=1349359                                             Totaltime spent by all reduces in occupied slots (ms)=190811                                    Map-ReduceFramework                                             Mapinput records=10                                              Mapoutput records=20                                                Mapoutput bytes=180                                                 Mapoutput materialized bytes=280                                                Inputsplit bytes=1410                                                 Combineinput records=0                                                Combineoutput records=0                                                  Reduceinput groups=2                                                 Reduceshuffle bytes=280                                              Reduceinput records=20                                                Reduceoutput records=0                                                 SpilledRecords=40                                                 ShuffledMaps =10                                                  FailedShuffles=0                                                MergedMap outputs=10                                                 GCtime elapsed (ms)=45355                                                 CPUtime spent (ms)=29860                                                 Physicalmemory (bytes) snapshot=1481818112                                                 Virtualmemory (bytes) snapshot=9214468096                                                 Totalcommitted heap usage (bytes)=1223008256                                        ShuffleErrors                                              BAD_ID=0                                              CONNECTION=0                                             IO_ERROR=0                                              WRONG_LENGTH=0                                               WRONG_MAP=0                                            WRONG_REDUCE=0                                    File InputFormat Counters                                              BytesRead=1180                                    File OutputFormat Counters                                             BytesWritten=97                           13/11/06 23:31:15 INFO mapred.ClientServiceDelegate:Application state is completed. FinalApplicationStatus=SUCCEEDED. Redirectingto job history server                           Job Finished in 719.041 seconds                           Estimated value of Pi is 3.14158440000000000000</p>
<p><a href="&quot;复制代码&quot;"><img src="http://common.cnblogs.com/images/copycode.gif" alt="复制代码"></a></p>
<p><strong>说明</strong>：可以看到最后输出值为该job使用了 10个maps, job id为job<em>1383806445149_000, 最后计算得Pi的值为13.14158440000000000000， job Id分配原则为job</em>年月日时分<em>job序列号，序列号从0开始，上限值为1000， task id分配原则为job</em>年月日时分<em>job序列号_task序列号_m, job</em>年月日时分_job序列号_task序列号_r, m代表map taskslot , r代表reduce task slot, task 序列号从0开始，上限值为1000.</p>
<p><strong>Test Case 2 random_writting</strong>
/#command line $sbin/yarnjar $YARN_HOME/share/hadoop//mapreduce/hadoop-mapreduce-examples-2.2.0.jar \randomwriter/user/grid/test/test_randomwriter/out</p>
<p><a href="&quot;复制代码&quot;"><img src="http://common.cnblogs.com/images/copycode.gif" alt="复制代码"></a></p>
<p>/#Console输出摘录：                                                                                                                                                    Running 10 maps.Job started: Wed Nov 0623:42:17 PST 201313/11/0623:42:17 INFO client.RMProxy: Connecting toResourceManager at /0.0.0.0:803213/11/0623:42:19 INFO mapreduce.JobSubmitter: number ofsplits:1013/11/0623:42:20 INFO mapreduce.JobSubmitter: Submittingtokens for job: job_1383806445149_000213/11/0623:42:21 INFO impl.YarnClientImpl: Submittedapplication application_1383806445149_0002 to ResourceManager at /0.0.0.0:803213/11/0623:42:21 INFO mapreduce.Job: The url to trackthe job: <a href="http://Master:8088/proxy/application_1383806445149_0002/13/11/0623:42:21" target="_blank">http://Master:8088/proxy/application_1383806445149_0002/13/11/0623:42:21</a> INFO mapreduce.Job: Running job:job_1383806445149_000213/11/0623:42:40 INFO mapreduce.Job: Jobjob_1383806445149_0002 running in uber mode : false13/11/0623:42:40 INFO mapreduce.Job:  map 0% reduce 0%                  13/11/0623:55:02 INFO mapreduce.Job:  map 10% reduce 0%                    13/11/0623:55:14 INFO mapreduce.Job:  map 20% reduce 0%                  13/11/0623:55:42 INFO mapreduce.Job:  map 30% reduce 0%                    13/11/0700:06:55 INFO mapreduce.Job:  map 40% reduce 0%                    13/11/0700:07:10 INFO mapreduce.Job:  map 50% reduce 0%                   13/11/0700:07:36 INFO mapreduce.Job:  map 60% reduce 0%                    13/11/0700:13:47 INFO mapreduce.Job:  map 70% reduce 0%                     13/11/0700:13:54 INFO mapreduce.Job:  map 80% reduce 0%                     13/11/0700:13:58 INFO mapreduce.Job:  map 90% reduce 0%                    13/11/0700:16:29 INFO mapreduce.Job:  map 100% reduce 0%                    13/11/0700:16:37 INFO mapreduce.Job: Jobjob_1383806445149_0002 completed successfully        File OutputFormat Counters                  BytesWritten=10772852496Job ended: Thu Nov 0700:16:40 PST 2013The job took 2062 seconds.</p>
<p><a href="&quot;复制代码&quot;"><img src="http://common.cnblogs.com/images/copycode.gif" alt="复制代码"></a></p>
<p><strong>说明</strong>：电脑存储空间足够的话，可以从hdfs里down下来看看。</p>
<p>现只能看一看输出文件存放的具体形式：</p>
<p><a href="&quot;复制代码&quot;"><img src="http://common.cnblogs.com/images/copycode.gif" alt="复制代码"></a>
./bin/hadoopfs -ls /user/grid/test/test_randomwriter/out/Found 11items-rw-r--r--   2 grid supergroup          02013-11-0700:16/user/grid/test/test_randomwriter/out/_SUCCESS-rw-r--r--   2 grid supergroup 10772782142013-11-0623:54 /user/grid/test/test_randomwriter/out/part-m-00000                                                                                                                                                   -rw-r--r--   2 grid supergroup 10772827512013-11-0623:55 /user/grid/test/test_randomwriter/out/part-m-00001                                                                                                                                                   -rw-r--r--   2 grid supergroup 10772802982013-11-0623:55 /user/grid/test/test_randomwriter/out/part-m-00002                                                                                                                                                   -rw-r--r--   2 grid supergroup 10773031522013-11-0700:07 /user/grid/test/test_randomwriter/out/part-m-00003                                                                                                                                                   -rw-r--r--   2 grid supergroup 10772842402013-11-0700:06 /user/grid/test/test_randomwriter/out/part-m-00004                                                                                                                                                   -rw-r--r--   2 grid supergroup 10772866042013-11-0700:07 /user/grid/test/test_randomwriter/out/part-m-00005                                                                                                                                                   -rw-r--r--   2 grid supergroup 10772843362013-11-0700:13 /user/grid/test/test_randomwriter/out/part-m-00006                                                                                                                                                   -rw-r--r--   2 grid supergroup 10772848292013-11-0700:13 /user/grid/test/test_randomwriter/out/part-m-00007                                                                                                                                                   -rw-r--r--   2 grid supergroup 10772897062013-11-0700:13 /user/grid/test/test_randomwriter/out/part-m-00008                                                                                                                                                   -rw-r--r--   2 grid supergroup 10772783662013-11-0700:16 /user/grid/test/test_randomwriter/out/part-m-00009</p>
<p><a href="&quot;复制代码&quot;"><img src="http://common.cnblogs.com/images/copycode.gif" alt="复制代码"></a></p>
<p><strong>Test Case3 word_count</strong></p>
<p>（1）Locaol上创建文件：
$mkdirinput%echo ‘hello,world’ &gt;&gt; input/file1.in$echo ‘hello, ruby’ &gt;&gt; input/file2.in</p>
<p>（2）上传到hdfs上：
./bin/hadoop fs -mkdir -p /user/grid/test/test_wordcount/./bin/hadoop fs –put input/user/grid/test/test_wordcount/in</p>
<p>（3）用yarn新计算框架运行mapreduce：</p>
<p><a href="&quot;复制代码&quot;"><img src="http://common.cnblogs.com/images/copycode.gif" alt="复制代码"></a>
/#command line $bin/yarn jar$YARN_HOME/share/hadoop//mapreduce/hadoop-mapreduce-examples-2.2.0.jarwordcount  /user/grid/test/test_wordcount/in/user/grid/test/test_wordcount/out                                                                                                                                             /#ConSole输出摘录：3/11/0700:35:03 INFO client.RMProxy:Connecting to ResourceManager at /0.0.0.0:803213/11/0700:35:05 INFO input.FileInputFormat:Total input paths to process : 213/11/0700:35:05 INFO mapreduce.JobSubmitter:number of splits:213/11/0700:35:06 INFO mapreduce.JobSubmitter:Submitting tokens for job: job_1383806445149_000313/11/0700:35:08 INFO impl.YarnClientImpl:Submitted application application_1383806445149_0003 to ResourceManager at /0.0.0.0:803213/11/0700:35:08 INFO mapreduce.Job: The urlto track the job: <a href="http://Master:8088/proxy/application_1383806445149_0003/13/11/0700:35:08" target="_blank">http://Master:8088/proxy/application_1383806445149_0003/13/11/0700:35:08</a> INFO mapreduce.Job: Runningjob: job_1383806445149_000313/11/0700:35:25 INFO mapreduce.Job: Jobjob_1383806445149_0003 running in uber mode : false13/11/0700:35:25 INFO mapreduce.Job:  map 0% reduce 0%                                                                                                                                              13/11/0700:37:50 INFO mapreduce.Job:  map 33% reduce 0%                                                                                                                                              13/11/0700:37:54 INFO mapreduce.Job:  map 67% reduce 0%                                                                                                                                              13/11/0700:37:55 INFO mapreduce.Job:  map 83% reduce 0%                                                                                                                                              13/11/0700:37:58 INFO mapreduce.Job:  map 100% reduce 0%                                                                                                                                              13/11/0700:38:51 INFO mapreduce.Job:  map 100% reduce 100%                                                                                                                                              13/11/0700:38:54 INFO mapreduce.Job: Jobjob_1383806445149_0003 completed successfully13/11/0700:38:56 INFO mapreduce.Job:Counters: 43</p>
<p><a href="&quot;复制代码&quot;"><img src="http://common.cnblogs.com/images/copycode.gif" alt="复制代码"></a></p>
<p><strong>说明</strong>：查看word count的计算结果：
1</p>
<p>2
3</p>
<p>4
$bin</p>
<p>/hadoop</p>
<p>fs -</p>
<p>cat</p>
<p>/user/grid/test//test_wordcount/out/</p>
<p>/*</p>
<p>hadoop 1
hello  1</p>
<p>ruby</p>
<p><strong>补充</strong>：因为新的YARN为了保持与MRv1框 架的旧版本兼容性，很多老的API还是可以用，但是会有INFO。此处通过修改$YARN_HOME/etc/hadoop /log4j.properties可以turn offconfiguration deprecation warnings.</p>
<p>建议去掉第138行的注释（可选），确保错误级别为WARN（默认为INFO级别，详见第20行：hadoop.root.logger=INFO,console）：
1138 log4j.logger.org.apache.hadoop.conf.Configuration.deprecation=WARN</p>
<p>附文：</p>
<p><strong>集群搭建、配置步骤（基于CentOS_64bit）</strong></p>
<p><strong>0. 说明</strong></p>
<p>大体规划如下：</p>
<p>虚拟机： VMware-workstation-full-8.0.3-703057（VMware10中文版不完整，此版本内含vmware tools，为设定共享文件夹所必须）</p>
<p>电脑1，VMWare,内装2个虚拟系统，(cluster1, cluster2)</p>
<p>电脑2,，VMware内装2个虚拟系统，(cluster3, cluster4)</p>
<p>虚拟主机： CentOS x86 64bit</p>
<p>局域网IP设置：<em>**</em>
1</p>
<p>2
3</p>
<p>4</p>
<p>cluster1  172.16.102. 201</p>
<p>cluster2   172.16.102. 202
cluster3   172.16.102. 203</p>
<p>cluster4   172.16.102. 204</p>
<p>网关</p>
<p>1172.16.102.254</p>
<p><strong>1. Linux集群安装</strong></p>
<p>(1) 准备</p>
<p>Vmware: VMware-workstation-full-8.0.3-703057(此安装包自带VMWare Tools)</p>
<p>Linux:CentOS.iso</p>
<p>(2) VMWare配置</p>
<p>VMWare以及所有安装的虚拟机均为桥接</p>
<p>Step1. 配置VMWare 联网方式： Editor-&gt;Virtual Network Editor-&gt;选择Bridged， 点击确定</p>
<p>Step2. 安装虚拟机</p>
<p>Step3.配置各虚拟机接网方式：右键已安装虚拟机-&gt;点击NetworkAdapter, 选择桥接，确定</p>
<p>Step4. 为所有安装好的虚拟系统设置一个共享目录(类似FTP，但是设置共享目录更方便) ：右键已安装虚拟机-&gt;点击Virtual Machine Settings对话框上部Options， 选择Shared Folder， 在本地新建SharedFolder并添加进来,确定。</p>
<p>(3) linux下网卡以及IP配置</p>
<p>以下配置在三个虚拟系统里均相同, 以cluster1为例：</p>
<p>配置前需切换为root</p>
<p>Step1. 修改主机名， 设置为开启网络</p>
<p>配置/etc/sysconfig/network：
1</p>
<p>2
3[root@localhost ~]</p>
<p>/# cat /etc/sysconfig/network</p>
<p>NETWORKING=</p>
<p>yes
HOSTNAME=cluster1</p>
<p>Step2.修改当前机器的IP， 默认网关， IP分配方式， 设置网络接口为系统启动时有效：</p>
<p>a.查看配置前的ip:
1</p>
<p>2
3</p>
<p>4
[root@localhost ~]</p>
<p>/# ifconfig</p>
<p>eth0      Link encap:Ethernet  HWaddr 00:0C:29:E1:FB:95 
inet addr:172.16.102.3  Bcast:172.16.102.255  Mask:255.255.255.0</p>
<p>inet6 addr: fe80::20c:29ff:fee1:fb95</p>
<p>/64</p>
<p>Scope:Lin</p>
<p>b.配置/etc/sysconfig/network-scripts/ifcfg-eth0</p>
<p>注意以下几项：</p>
<p>BOOTPROTO=&quot;static&quot; (IP分配方式, 设为static则主机ip即为IPADDR里所设，若为”dhcp”, 此时只有局域网有效，则vmware会启动自带虚拟”dhcp”, 动态分配ip， 此时可以连接互联网 )</p>
<p><a href="&quot;复制代码&quot;"><img src="http://common.cnblogs.com/images/copycode.gif" alt="复制代码"></a>
IPV6INIT=&quot;no&quot;  /#你懂得 IPADDR=&quot;172.16.102.201&quot; /#ip地址 GETEWAY=&quot;172.16.102.254&quot; /#默认网关地址 ONBOOT=&quot;yes&quot; /#系统启动时网络接口有效 [root@localhost ~]/# cat /etc/sysconfig/network-scripts/ifcfg-eth0  DEVICE=&quot;eth0&quot;BOOTPROTO=&quot;static&quot;HWADDR=&quot;00:0C:29:E1:FB:95&quot;BOOTPROTO=&quot;dhcp&quot;IPV6INIT=&quot;no&quot;IPADDR=&quot;172.16.102.201&quot;GETEWAY=&quot;172.16.102.254&quot;ONBOOT=&quot;yes&quot;NM_CONTROLLED=&quot;yes&quot;TYPE=&quot;Ethernet&quot;UUID=&quot;79612b26-326b-472c-94af-9ab151fc2831</p>
<p><a href="&quot;复制代码&quot;"><img src="http://common.cnblogs.com/images/copycode.gif" alt="复制代码"></a></p>
<p>c.使当前设置生效：
$service network restart $ifdown eth0 /#关闭默认网卡 $ifup eth/#重新启用默认网卡 $service network restart; ifdown eth0; ifup eth0</p>
<p>d.查看新设置的ip:
$ifconfigeth0      Link encap:Ethernet  HWaddr 00:0C:29:E1:FB:95  inet addr:192.168.1.200  Bcast:192.168.1.255  Mask:255.255.255.0inet6 addr: fe80::20c:29ff:fee1:fb95/64 Scope:Link UP BROADCAST RUNNING MULTICAST  MTU:1500  Metric:1</p>
<p>Step3. 修改hosts文件，此文件会被ＤＮＳ解析，类似linux里的alias, 设置后以后，hostname就是ip地址， 两者可以互换。</p>
<p>配置/etc/hosts, 添加三行如下， （注意，此3行在3个虚拟主机里都相同，切必须全部都要加上）</p>
<p><a href="&quot;复制代码&quot;"><img src="http://common.cnblogs.com/images/copycode.gif" alt="复制代码"></a>
[root@localhost ~]/# cat /etc/hosts127.0.0.1   localhost localhost.localdomain localhost4 localhost4.localdomain4 ::1         localhost localhost.localdomain localhost6 localhost6.localdomain6 cluster1   172.16.102. 201cluster2   172.16.102. 202cluster3   172.16.102. 203cluster4   172.16.102. 204</p>
<p><a href="&quot;复制代码&quot;"><img src="http://common.cnblogs.com/images/copycode.gif" alt="复制代码"></a></p>
<p>Step4. 按照以上3个步骤分别设置两外两个虚拟主机，</p>
<p>配置完以后必须按照之前的三个命令分别重启 network, ifeth0:
$service network restart $ifdown eth0 $ifup eth0</p>
<p>重新初始化以后查看各自主机的ip配置是否正确。</p>
<p>在任意一台主机上执行： ping cluster1; ping cluster2; ping cluster3; ping cluster4</p>
<p>若配置正确，一定可以ping通，否则，请检查各个主机的/etc/hosts文件是否已经添加新的映射！至此，linux集群已成功配置。</p>
<p><strong>2. 设置 ssh免登陆</strong></p>
<p>a. 新建一个用户</p>
<p>在三台主机上分别以root权限新建一个用户，此处默认为grid:</p>
<p>cluster1,cluster2, cluster3, cluster4上：
$useradd –m grid $passwd grid 1qaz!QAZ</p>
<p>注意一定要保证4台主机上有相同的用户名， 以实现同一个用户在4台主机上登录。</p>
<p>b. 在cluster1上生成RSA密钥</p>
<p>切换回user: grid
$su grid $cd ~</p>
<p>生成密钥对：</p>
<p>1</p>
<p>2
3$</p>
<p>ssh</p>
<p>-keygen –t rsa</p>
<p>/#一路回车到最后（ 此处生成无需密码的秘钥-公钥对）。
/#上一个步骤 ssh-keygen –t rsa会在grid的home目录下生成一个.ssh文件夹</p>
<p>之后：</p>
<p>$cd ~/.ssh/$cp id_rsa.pub authorized_keys</p>
<p>c. 在另外3个主机上的grid用户home目录下也声称相应的密钥对, 并在.ssh目录下生成一个</p>
<p>authorized_keys
文件</p>
<p>d. 想办法将4台主机grid用户下刚生成的</p>
<p>authorized_keys
里的内容整合成一个完整的</p>
<p>authorized_keys.</p>
<p>比如将4个authorized_keys里的内容全部复制到cluster1上的authorized_keys里， 然后：
$chmod 600 authorized_keys $scp .ssh/authorized_keys  cluster2:/home/grid/.ssh/$scp .ssh/authorized_keys  cluster3:/home/grid/.ssh/$scp .ssh/authorized_keys  cluster4:/home/grid/.ssh/</p>
<p>若要求输入密码，则输入之前新建用户grid时设置的密码， 一路回车到最后.</p>
<p>e. 下面尝试ssh无秘钥登录：</p>
<p>在cluster1主机上：ssh cluster2， 依次尝试登陆cluster2, cluster3, cluster4</p>
<p>若均可可以免密码登录，则直接跳到下一步骤，否则，请看下面的解决方案：</p>
<p>可能出现的问题有3，</p>
<p>第一种可能，.ssh文件夹非自动生成，而是你手动新建的，若如此，会出现.ssh的安全上下文问题， 解决方案， 在三个主机上以grid用户，删除刚才生成的.ssh文件夹，重新进入home目录，务必用 /# ssh-keygen –t rsa 生成秘钥， 此过程ssh程序会自动在home目录下成成一个.ssh文件夹</p>
<p>第二种可能, authorized_keys权限不一致。 在各自.ssh目录下：ls–alauthorizedkeys,查看此文件的权限，一定为600(−rw−−−−−−−),即只对当前用户grid开放可读可写权限，若不是，则修改authorizedkeys文件权限chmod 600 authorized_keys</p>
<p>若经过以上两步还不行，则执行以下命令，重新启动ssh-agent， 且加入当前用户秘钥id_rsa
$ssh-add ~grid/.ssh/id_rsa</p>
<p>经过以上三步，一定可以实现grid从cluster1到其它3个节点的免秘钥登录。</p>
<p>因为hadoop2.2新架构的缘故，我们还应该设置为每一个节点到其它任意节点免登陆。</p>
<p>具体步骤:</p>
<p>1.在将cluster2, cluster3, cluster4上各自的.ssh/id_rsa.pub 分别复制到cluster1的.ssh/下： scp id_rsa.pub cluster1:/home/grid/.ssh/tmp2, scp id_rsa.pub cluster1:/home/grid/.ssh/tmp3 ...</p>
<p>2.将cluster1节点/home/grid/.ssh/下的tmp2, tmp3, tmp4分别appand到authorized_keys里， 并请将各自节点上的id_rsa.pub也append到各自节点的authorized_keys里，以实现本地登陆（类似： ssh localhost)</p>
<p>至此，ssh免秘钥登陆设置完成。</p>
<p><strong>3. 安装jdk</strong></p>
<p>Step1. 记得之前在安装cluster1时在VMWare里设置的共享目录吗？</p>
<p>在Windows下将Hadoop安装包，jdk安装包Copy到共享目录下，然后在linux下从/mnt/hgfs/Data-shared/下cp到/home/grid/下，直接执行jdk安装包，不需要解压。</p>
<p>注意：若在安装过程中提示”Extracting… install.sfx 5414: /lib/ld-linux.50.2 No such file or directory, 则执行以下命令:</p>
<p>a.我们之前为了测试自定义的ip是否有效，已将各台主机的ip分配方式设为了BOOTPROTO=”static”, 这种方式是无法连入外部网络的，所以此时为了安装缺省的包，切换到root, 修改/etc/sysconfig/network-script/ifcfg-eth0，将BOOTPROTO=”static” 修改为BOOTPROTO=”dhcp”,</p>
<p>b.重启网络服务和网卡: 在root权限下:
$service network restart; ifdown eth0; ifup eth0 $yum install glibc.i686</p>
<p>d.切换回grid,重新安装jdk</p>
<p>$cd /usr/java/  /#注意必须进入java文件夹，因为java安装包默认安装在当前目录下 $./jdk-6u25-linux-i586.bin /#安装jdk</p>
<p>安装完以后，记得将eth0文件修改回来：</p>
<p>$sed -i s/dhcp/static/g /etc/sysconfig/network-scripts/ifcfg-eth0 /#此处用sed直接替换，若不放心也可以用编辑器修改 /#service network restart; ifdown eth0;ifup eth0</p>
<p>至此，jdk已在linux下安装完毕。</p>
<p>最后，将java安装好的路径append到$PATH变量里（处于个人习惯，新环境变量一律添加到所需用户的.bashrc文件里， 即只对当前用户有效）
$su grid $vim ~/.bashrc （修改.bashrc文件，添加如下两行：） $tail -2 ~/.bashrc export JAVA_HOME=&quot;/usr/java/jdk1.6.0_25/&quot;export PATH=&quot;${PATH}:${JAVA_HOME}/bin&quot;</p>
<p>测试一下java是否可以正常启动：</p>
<p>$source ~/.bashrc $which java $/usr/java/jdk1.6.0_25/bin/java</p>
<p>至此，jdk安装完毕。</p>
<p>用同样的方式在另外3台虚拟主机上安装jdk, 提示：先复制到home下
$scp -r /mnt/hgfs/Data-shared/Archive/jdk-6u25-linux-i586.bin  cluster2:/home/grid/$scp -r /mnt/hgfs/Data-shared/Archive/jdk-6u25-linux-i586.bin  cluster3:/home/grid/$scp -r /mnt/hgfs/Data-shared/Archive/jdk-6u25-linux-i586.bin  cluster4:/home/grid/</p>
<p>再切root, copy到/usr/java下， cd /usr/java, 然后再安装.</p>

      
    </div>
    
    
      
    
    
        <footer id="post-meta">
        <span class="categories">Posted in<span class="breadcrumb fa fa-folder"><li><a href="/categories/hadoop/">hadoop</a></li></span></span> | <span class="tags">Tagged <a href="/tags/hadoop/" class="label label-primary">hadoop</a></span> | <span class="time">recent updated:<time title="2014-03-07 09:54:36"datetime="2014-03-07 09:54:36"> mar. 7 2014</time></span> | <span class="comment-link">
<a href="http://itsolife.com/2014/02/02/2014-02-02-hadoop--基于Hadoop220的高可用性集群搭建步骤（64位）/#comments" class="ds-thread-count comment-link" data-thread-key="2014-02-02-hadoop--基于Hadoop220的高可用性集群搭建步骤（64位）" data-count-type="comments">暂无评论</a></span>
        </footer>
    
    <div class="clearfix"></div>
  </div>
</div>
</article>



  <article>
<div id="post" class="post well">
  <div class="post-content">
    <header class="well-sm">
      <i class="fa icon fa-5x pull-left"></i>
      <h1 class="title"><a href="/2014/02/02/2014-02-02-hadoop--mapred_tutorial/">mapred_tutorial</a></h1>
      
        <span>Posted on<time datetime="2014-02-02T01:54:36.000Z"> <a href="/2014/02/02/2014-02-02-hadoop--mapred_tutorial/">feb. 2 2014</a></time></span>      
    </header>
    
    
    <div class="entry">
      
        <h1 id="mapred_tutorial">mapred_tutorial</h1>
<p>Map/Reduce Tutorial
Table of contents
1 2 3 4 5
Purpose...............................................................................................................................2 Pre-requisites......................................................................................................................2 Overview............................................................................................................................2 Inputs and Outputs............................................................................................................. 3 Example: WordCount v1.0................................................................................................ 3
5.1 5.2 5.3
Source Code...................................................................................................................3 Usage............................................................................................................................. 6 Walk-through.................................................................................................................7 Payload.......................................................................................................................... 9 Job Configuration........................................................................................................ 13 Task Execution &amp; Environment.................................................................................. 14 Job Submission and Monitoring..................................................................................21 Job Input...................................................................................................................... 22 Job Output................................................................................................................... 23 Other Useful Features..................................................................................................25 Source Code.................................................................................................................31 Sample Runs................................................................................................................37 Highlights.................................................................................................................... 39
6
Map/Reduce - User Interfaces............................................................................................9
6.1 6.2 6.3 6.4 6.5 6.6 6.7
7
Example: WordCount v2.0.............................................................................................. 30
7.1 7.2 7.3
Copyright © 2008 The Apache Software Foundation. All rights reserved.
Map/Reduce Tutorial</p>
<ol>
<li>Purpose
This document comprehensively describes all user-facing facets of the Hadoop Map/Reduce framework and serves as a tutorial.</li>
<li>Pre-requisites
Ensure that Hadoop is installed, configured and is running. More details: • Hadoop Quick Start for first-time users. • Hadoop Cluster Setup for large, distributed clusters.</li>
<li>Overview
Hadoop Map/Reduce is a software framework for easily writing applications which process vast amounts of data (multi-terabyte data-sets) in-parallel on large clusters (thousands of nodes) of commodity hardware in a reliable, fault-tolerant manner. A Map/Reduce job usually splits the input data-set into independent chunks which are processed by the map tasks in a completely parallel manner. The framework sorts the outputs of the maps, which are then input to the reduce tasks. Typically both the input and the output of the job are stored in a file-system. The framework takes care of scheduling tasks, monitoring them and re-executes the failed tasks. Typically the compute nodes and the storage nodes are the same, that is, the Map/Reduce framework and the Hadoop Distributed File System (see HDFS Architecture ) are running on the same set of nodes. This configuration allows the framework to effectively schedule tasks on the nodes where data is already present, resulting in very high aggregate bandwidth across the cluster. The Map/Reduce framework consists of a single master JobTracker and one slave TaskTracker per cluster-node. The master is responsible for scheduling the jobs&#39; component tasks on the slaves, monitoring them and re-executing the failed tasks. The slaves execute the tasks as directed by the master. Minimally, applications specify the input/output locations and supply map and reduce functions via implementations of appropriate interfaces and/or abstract-classes. These, and other job parameters, comprise the job configuration. The Hadoop job client then submits the job (jar/executable etc.) and configuration to the JobTracker which then assumes the responsibility of distributing the software/configuration to the slaves, scheduling tasks and monitoring them, providing status and diagnostic information to the job-client.
Page 2
Copyright © 2008 The Apache Software Foundation. All rights reserved.
Map/Reduce Tutorial
Although the Hadoop framework is implemented in JavaTM, Map/Reduce applications need not be written in Java. • Hadoop Streaming is a utility which allows users to create and run jobs with any executables (e.g. shell utilities) as the mapper and/or the reducer. • Hadoop Pipes is a SWIG- compatible C++ API to implement Map/Reduce applications (non JNITM based).</li>
<li>Inputs and Outputs
The Map/Reduce framework operates exclusively on <key, value> pairs, that is, the framework views the input to the job as a set of <key, value> pairs and produces a set of <key, value> pairs as the output of the job, conceivably of different types. The key and value classes have to be serializable by the framework and hence need to implement the Writable interface. Additionally, the key classes have to implement the WritableComparable interface to facilitate sorting by the framework. Input and Output types of a Map/Reduce job: (input) <k1, v1> -&gt; map -&gt; <k2, v2> -&gt; combine -&gt; <k2, v2> -&gt; reduce -&gt; <k3, v3> (output)</li>
<li>Example: WordCount v1.0
Before we jump into the details, lets walk through an example Map/Reduce application to get a flavour for how they work. WordCount is a simple application that counts the number of occurences of each word in a given input set. This works with a local-standalone, pseudo-distributed or fully-distributed Hadoop installation(see Hadoop Quick Start).
5.1. Source Code
WordCount.java 1. 2. 3. 4. import java.io.IOException; import java.util./*; package org.myorg;
Page 3
Copyright © 2008 The Apache Software Foundation. All rights reserved.
Map/Reduce Tutorial</li>
<li><ol>
<li><ol>
<li><ol>
<li><ol>
<li><ol>
<li><ol>
<li><ol>
<li><ol>
<li><ol>
<li>public static class Map extends MapReduceBase implements Mapper<LongWritable, Text, Text, IntWritable> { private final static IntWritable one = new IntWritable(1); private Text word = new Text(); public class WordCount { import org.apache.hadoop.fs.Path; import org.apache.hadoop.conf./<em>; import org.apache.hadoop.io./</em>; import org.apache.hadoop.mapred./<em>; import org.apache.hadoop.util./</em>;</li>
</ol>
</li>
</ol>
</li>
</ol>
</li>
</ol>
</li>
</ol>
</li>
</ol>
</li>
</ol>
</li>
</ol>
</li>
</ol>
</li>
<li><ol>
<li><ol>
<li>18.
public void map(LongWritable key, Text value, OutputCollector<Text, IntWritable> output, Reporter reporter) throws IOException { String line = value.toString(); StringTokenizer tokenizer = new StringTokenizer(line); while (tokenizer.hasMoreTokens()) { word.set(tokenizer.nextToken());</li>
</ol>
</li>
</ol>
</li>
<li><ol>
<li><ol>
<li><ol>
<li><ol>
<li><ol>
<li>25.
output.collect(word, one); } }
Page 4
Copyright © 2008 The Apache Software Foundation. All rights reserved.
Map/Reduce Tutorial</li>
</ol>
</li>
</ol>
</li>
</ol>
</li>
</ol>
</li>
</ol>
</li>
<li><ol>
<li>28.
}
public static class Reduce extends MapReduceBase implements Reducer<Text, IntWritable, Text, IntWritable> { public void reduce(Text key, Iterator<IntWritable> values, OutputCollector<Text, IntWritable> output, Reporter reporter) throws IOException { int sum = 0; while (values.hasNext()) { sum += values.next().get(); } output.collect(key, new IntWritable(sum)); } }
29.</li>
</ol>
</li>
<li><ol>
<li><ol>
<li><ol>
<li><ol>
<li><ol>
<li><ol>
<li><ol>
<li><ol>
<li><ol>
<li><ol>
<li><ol>
<li>42.
public static void main(String[] args) throws Exception { JobConf conf = new JobConf(WordCount.class); conf.setJobName(&quot;wordcount&quot;);
conf.setOutputKeyClass(Text.class); 43. conf.setOutputValueClass(IntWritable.class); 44. 45. conf.setMapperClass(Map.class);
Page 5
Copyright © 2008 The Apache Software Foundation. All rights reserved.
Map/Reduce Tutorial</li>
</ol>
</li>
</ol>
</li>
</ol>
</li>
</ol>
</li>
</ol>
</li>
</ol>
</li>
</ol>
</li>
</ol>
</li>
</ol>
</li>
</ol>
</li>
</ol>
</li>
<li>conf.setCombinerClass(Reduce.class); 47. conf.setReducerClass(Reduce.class); 48. 49. conf.setInputFormat(TextInputFormat.class); 50. conf.setOutputFormat(TextOutputFormat.class); 51. 52. FileInputFormat.setInputPaths(conf, new Path(args[0])); 53. FileOutputFormat.setOutputPath(conf, new Path(args[1])); 54. 55. 57. 58. 59. } JobClient.runJob(conf); }
5.2. Usage
Assuming HADOOP_HOME is the root of the installation and HADOOP_VERSION is the Hadoop version installed, compile WordCount.java and create a jar: $ mkdir wordcount_classes $ javac -classpath ${HADOOP_HOME}/hadoop-${HADOOP_VERSION}-core.jar -d wordcount_classes WordCount.java $ jar -cvf /usr/joe/wordcount.jar -C wordcount_classes/ . Assuming that: • /usr/joe/wordcount/input - input directory in HDFS
Page 6
Copyright © 2008 The Apache Software Foundation. All rights reserved.
Map/Reduce Tutorial
•
/usr/joe/wordcount/output - output directory in HDFS
Sample text-files as input: $ bin/hadoop dfs -ls /usr/joe/wordcount/input/ /usr/joe/wordcount/input/file01 /usr/joe/wordcount/input/file02 $ bin/hadoop dfs -cat /usr/joe/wordcount/input/file01 Hello World Bye World $ bin/hadoop dfs -cat /usr/joe/wordcount/input/file02 Hello Hadoop Goodbye Hadoop Run the application: $ bin/hadoop jar /usr/joe/wordcount.jar org.myorg.WordCount /usr/joe/wordcount/input /usr/joe/wordcount/output Output: $ bin/hadoop dfs -cat /usr/joe/wordcount/output/part-00000 Bye 1 Goodbye 1 Hadoop 2 Hello 2 World 2 Applications can specify a comma separated list of paths which would be present in the current working directory of the task using the option -files. The -libjars option allows applications to add jars to the classpaths of the maps and reduces. The -archives allows them to pass archives as arguments that are unzipped/unjarred and a link with name of the jar/zip are created in the current working directory of tasks. More details about the command line options are available at Hadoop Command Guide. Running wordcount example with -libjars and -files: hadoop jar hadoop-examples.jar wordcount -files cachefile.txt -libjars mylib.jar input output
5.3. Walk-through
The WordCount application is quite straight-forward. The Mapper implementation (lines 14-26), via the map method (lines 18-25), processes one line at a time, as provided by the specified TextInputFormat (line 49). It then splits the line into tokens separated by whitespaces, via the StringTokenizer, and emits a
Page 7
Copyright © 2008 The Apache Software Foundation. All rights reserved.
Map/Reduce Tutorial
key-value pair of &lt; <word>, 1&gt;. For the given sample input the first map emits: &lt; Hello, 1&gt; &lt; World, 1&gt; &lt; Bye, 1&gt; &lt; World, 1&gt; The second map emits: &lt; Hello, 1&gt; &lt; Hadoop, 1&gt; &lt; Goodbye, 1&gt; &lt; Hadoop, 1&gt; We&#39;ll learn more about the number of maps spawned for a given job, and how to control them in a fine-grained manner, a bit later in the tutorial. WordCount also specifies a combiner (line 46). Hence, the output of each map is passed through the local combiner (which is same as the Reducer as per the job configuration) for local aggregation, after being sorted on the keys. The output of the first map: &lt; Bye, 1&gt; &lt; Hello, 1&gt; &lt; World, 2&gt; The output of the second map: &lt; Goodbye, 1&gt; &lt; Hadoop, 2&gt; &lt; Hello, 1&gt; The Reducer implementation (lines 28-36), via the reduce method (lines 29-35) just sums up the values, which are the occurence counts for each key (i.e. words in this example). Thus the output of the job is: &lt; Bye, 1&gt; &lt; Goodbye, 1&gt; &lt; Hadoop, 2&gt; &lt; Hello, 2&gt; &lt; World, 2&gt; The run method specifies various facets of the job, such as the input/output paths (passed via the command line), key/value types, input/output formats etc., in the JobConf. It then calls the JobClient.runJob (line 55) to submit the and monitor its progress.
Page 8
Copyright © 2008 The Apache Software Foundation. All rights reserved.
Map/Reduce Tutorial
We&#39;ll learn more about JobConf, JobClient, Tool and other interfaces and classes a bit later in the tutorial.</li>
<li>Map/Reduce - User Interfaces
This section provides a reasonable amount of detail on every user-facing aspect of the Map/Reduce framwork. This should help users implement, configure and tune their jobs in a fine-grained manner. However, please note that the javadoc for each class/interface remains the most comprehensive documentation available; this is only meant to be a tutorial. Let us first take the Mapper and Reducer interfaces. Applications typically implement them to provide the map and reduce methods. We will then discuss other core interfaces including JobConf, JobClient, Partitioner, OutputCollector, Reporter, InputFormat, OutputFormat, OutputCommitter and others. Finally, we will wrap up by discussing some useful features of the framework such as the DistributedCache, IsolationRunner etc.
6.1. Payload
Applications typically implement the Mapper and Reducer interfaces to provide the map and reduce methods. These form the core of the job. 6.1.1. Mapper Mapper maps input key/value pairs to a set of intermediate key/value pairs. Maps are the individual tasks that transform input records into intermediate records. The transformed intermediate records do not need to be of the same type as the input records. A given input pair may map to zero or many output pairs. The Hadoop Map/Reduce framework spawns one map task for each InputSplit generated by the InputFormat for the job. Overall, Mapper implementations are passed the JobConf for the job via the JobConfigurable.configure(JobConf) method and override it to initialize themselves. The framework then calls map(WritableComparable, Writable, OutputCollector, Reporter) for each key/value pair in the InputSplit for that task. Applications can then override the Closeable.close() method to perform any required cleanup. Output pairs do not need to be of the same types as input pairs. A given input pair may map
Page 9
Copyright © 2008 The Apache Software Foundation. All rights reserved.
Map/Reduce Tutorial
to zero or many output pairs. Output pairs are collected with calls to OutputCollector.collect(WritableComparable,Writable). Applications can use the Reporter to report progress, set application-level status messages and update Counters, or just indicate that they are alive. All intermediate values associated with a given output key are subsequently grouped by the framework, and passed to the Reducer(s) to determine the final output. Users can control the grouping by specifying a Comparator via JobConf.setOutputKeyComparatorClass(Class). The Mapper outputs are sorted and then partitioned per Reducer. The total number of partitions is the same as the number of reduce tasks for the job. Users can control which keys (and hence records) go to which Reducer by implementing a custom Partitioner. Users can optionally specify a combiner, via JobConf.setCombinerClass(Class), to perform local aggregation of the intermediate outputs, which helps to cut down the amount of data transferred from the Mapper to the Reducer. The intermediate, sorted outputs are always stored in a simple (key-len, key, value-len, value) format. Applications can control if, and how, the intermediate outputs are to be compressed and the CompressionCodec to be used via the JobConf.
6.1.1.1. How Many Maps?
The number of maps is usually driven by the total size of the inputs, that is, the total number of blocks of the input files. The right level of parallelism for maps seems to be around 10-100 maps per-node, although it has been set up to 300 maps for very cpu-light map tasks. Task setup takes awhile, so it is best if the maps take at least a minute to execute. Thus, if you expect 10TB of input data and have a blocksize of 128MB, you&#39;ll end up with 82,000 maps, unless setNumMapTasks(int) (which only provides a hint to the framework) is used to set it even higher. 6.1.2. Reducer Reducer reduces a set of intermediate values which share a key to a smaller set of values. The number of reduces for the job is set by the user via JobConf.setNumReduceTasks(int). Overall, Reducer implementations are passed the JobConf for the job via the JobConfigurable.configure(JobConf) method and can override it to initialize themselves. The
Page 10
Copyright © 2008 The Apache Software Foundation. All rights reserved.
Map/Reduce Tutorial
framework then calls reduce(WritableComparable, Iterator, OutputCollector, Reporter) method for each <key, (list of values)> pair in the grouped inputs. Applications can then override the Closeable.close() method to perform any required cleanup. Reducer has 3 primary phases: shuffle, sort and reduce.
6.1.2.1. Shuffle
Input to the Reducer is the sorted output of the mappers. In this phase the framework fetches the relevant partition of the output of all the mappers, via HTTP.
6.1.2.2. Sort
The framework groups Reducer inputs by keys (since different mappers may have output the same key) in this stage. The shuffle and sort phases occur simultaneously; while map-outputs are being fetched they are merged.
Secondary Sort
If equivalence rules for grouping the intermediate keys are required to be different from those for grouping keys before reduction, then one may specify a Comparator via JobConf.setOutputValueGroupingComparator(Class). Since JobConf.setOutputKeyComparatorClass(Class) can be used to control how intermediate keys are grouped, these can be used in conjunction to simulate secondary sort on values.
6.1.2.3. Reduce
In this phase the reduce(WritableComparable, Iterator, OutputCollector, Reporter) method is called for each <key, (list of values)> pair in the grouped inputs. The output of the reduce task is typically written to the FileSystem via OutputCollector.collect(WritableComparable, Writable). Applications can use the Reporter to report progress, set application-level status messages and update Counters, or just indicate that they are alive. The output of the Reducer is not sorted.
6.1.2.4. How Many Reduces?
The right number of reduces seems to be 0.95 or 1.75 multiplied by (<no. of nodes> /<em> mapred.tasktracker.reduce.tasks.maximum).
Page 11
Copyright © 2008 The Apache Software Foundation. All rights reserved.
Map/Reduce Tutorial
With 0.95 all of the reduces can launch immediately and start transfering map outputs as the maps finish. With 1.75 the faster nodes will finish their first round of reduces and launch a second wave of reduces doing a much better job of load balancing. Increasing the number of reduces increases the framework overhead, but increases load balancing and lowers the cost of failures. The scaling factors above are slightly less than whole numbers to reserve a few reduce slots in the framework for speculative-tasks and failed tasks.
6.1.2.5. Reducer NONE
It is legal to set the number of reduce-tasks to zero if no reduction is desired. In this case the outputs of the map-tasks go directly to the FileSystem, into the output path set by setOutputPath(Path). The framework does not sort the map-outputs before writing them out to the FileSystem. 6.1.3. Partitioner Partitioner partitions the key space. Partitioner controls the partitioning of the keys of the intermediate map-outputs. The key (or a subset of the key) is used to derive the partition, typically by a hash function. The total number of partitions is the same as the number of reduce tasks for the job. Hence this controls which of the m reduce tasks the intermediate key (and hence the record) is sent to for reduction. HashPartitioner is the default Partitioner. 6.1.4. Reporter Reporter is a facility for Map/Reduce applications to report progress, set application-level status messages and update Counters. Mapper and Reducer implementations can use the Reporter to report progress or just indicate that they are alive. In scenarios where the application takes a significant amount of time to process individual key/value pairs, this is crucial since the framework might assume that the task has timed-out and kill that task. Another way to avoid this is to set the configuration parameter mapred.task.timeout to a high-enough value (or even set it to zero for no time-outs). Applications can also update Counters using the Reporter.
Page 12
Copyright © 2008 The Apache Software Foundation. All rights reserved.
Map/Reduce Tutorial
6.1.5. OutputCollector OutputCollector is a generalization of the facility provided by the Map/Reduce framework to collect data output by the Mapper or the Reducer (either the intermediate outputs or the output of the job). Hadoop Map/Reduce comes bundled with a library of generally useful mappers, reducers, and partitioners.
6.2. Job Configuration
JobConf represents a Map/Reduce job configuration. JobConf is the primary interface for a user to describe a Map/Reduce job to the Hadoop framework for execution. The framework tries to faithfully execute the job as described by JobConf, however: • f Some configuration parameters may have been marked as final by administrators and hence cannot be altered. • While some job parameters are straight-forward to set (e.g. setNumReduceTasks(int)), other parameters interact subtly with the rest of the framework and/or job configuration and are more complex to set (e.g. setNumMapTasks(int)). JobConf is typically used to specify the Mapper, combiner (if any), Partitioner, Reducer, InputFormat, OutputFormat and OutputCommitter implementations. JobConf also indicates the set of input files (setInputPaths(JobConf, Path...) /addInputPath(JobConf, Path)) and (setInputPaths(JobConf, String) /addInputPaths(JobConf, String)) and where the output files should be written (setOutputPath(Path)). Optionally, JobConf is used to specify other advanced facets of the job such as the Comparator to be used, files to be put in the DistributedCache, whether intermediate and/or job outputs are to be compressed (and how), debugging via user-provided scripts (setMapDebugScript(String)/setReduceDebugScript(String)) , whether job tasks can be executed in a speculative manner (setMapSpeculativeExecution(boolean))/(setReduceSpeculativeExecution(boolean)) , maximum number of attempts per task (setMaxMapAttempts(int)/setMaxReduceAttempts(int)) , percentage of tasks failure which can be tolerated by the job (setMaxMapTaskFailuresPercent(int)/setMaxReduceTaskFailuresPercent(int)) etc. Of course, users can use set(String, String)/get(String, String) to set/get arbitrary parameters needed by applications. However, use the DistributedCache for large amounts of
Page 13
Copyright © 2008 The Apache Software Foundation. All rights reserved.
Map/Reduce Tutorial
(read-only) data.
6.3. Task Execution &amp; Environment
The TaskTracker executes the Mapper/ Reducer task as a child process in a separate jvm. The child-task inherits the environment of the parent TaskTracker. The user can specify additional options to the child-jvm via the mapred.child.java.opts configuration parameter in the JobConf such as non-standard paths for the run-time linker to search shared libraries via -Djava.library.path=&lt;&gt; etc. If the mapred.child.java.opts contains the symbol @taskid@ it is interpolated with value of taskid of the map/reduce task. Here is an example with multiple arguments and substitutions, showing jvm GC logging, and start of a passwordless JVM JMX agent so that it can connect with jconsole and the likes to watch child memory, threads and get thread dumps. It also sets the maximum heap-size of the child jvm to 512MB and adds an additional path to the java.library.path of the child-jvm. <property> <name>mapred.child.java.opts</name> <value> -Xmx512M -Djava.library.path=/home/mycompany/lib -verbose:gc -Xloggc:/tmp/@taskid@.gc -Dcom.sun.management.jmxremote.authenticate=false -Dcom.sun.management.jmxremote.ssl=false </value> </property> 6.3.1. Memory management Users/admins can also specify the maximum virtual memory of the launched child-task, and any sub-process it launches recursively, using mapred.child.ulimit. Note that the value set here is a per process limit. The value for mapred.child.ulimit should be specified in kilo bytes (KB). And also the value must be greater than or equal to the -Xmx passed to JavaVM, else the VM might not start. Note: mapred.child.java.opts are used only for configuring the launched child tasks from task tracker. Configuring the memory options for daemons is documented in cluster_setup.html The memory available to some parts of the framework is also configurable. In map and
Page 14
Copyright © 2008 The Apache Software Foundation. All rights reserved.
Map/Reduce Tutorial
reduce tasks, performance may be influenced by adjusting parameters influencing the concurrency of operations and the frequency with which data will hit disk. Monitoring the filesystem counters for a job- particularly relative to byte counts from the map and into the reduce- is invaluable to the tuning of these parameters. 6.3.2. Map Parameters A record emitted from a map will be serialized into a buffer and metadata will be stored into accounting buffers. As described in the following options, when either the serialization buffer or the metadata exceed a threshold, the contents of the buffers will be sorted and written to disk in the background while the map continues to output records. If either buffer fills completely while the spill is in progress, the map thread will block. When the map is finished, any remaining records are written to disk and all on-disk segments are merged into a single file. Minimizing the number of spills to disk can decrease map time, but a larger buffer also decreases the memory available to the mapper.
Name io.sort.mb int Type Description The cumulative size of the serialization and accounting buffers storing records emitted from the map, in megabytes. The ratio of serialization to accounting space can be adjusted. Each serialized record requires 16 bytes of accounting information in addition to its serialized size to effect the sort. This percentage of space allocated from io.sort.mb affects the probability of a spill to disk being caused by either exhaustion of the serialization buffer or the accounting space. Clearly, for a map outputting small records, a higher value than the default will likely decrease the number of spills to disk. This is the threshold for the accounting and serialization buffers. When this percentage of either buffer has filled, their
io.sort.record.percent
float
io.sort.spill.percent
float
Page 15
Copyright © 2008 The Apache Software Foundation. All rights reserved.
Map/Reduce Tutorial
contents will be spilled to disk in the background. Let io.sort.record.percent be r, io.sort.mb be x, and this value be q. The maximum number of records collected before the collection thread will spill is r /</em> x /<em> q /</em> 2^16. Note that a higher value may decrease the number of- or even eliminate- merges, but will also increase the probability of the map task getting blocked. The lowest average map times are usually obtained by accurately estimating the size of the map output and preventing multiple spills.
Other notes • If either spill threshold is exceeded while a spill is in progress, collection will continue until the spill is finished. For example, if io.sort.buffer.spill.percent is set to 0.33, and the remainder of the buffer is filled while the spill runs, the next spill will include all the collected records, or 0.66 of the buffer, and will not generate additional spills. In other words, the thresholds are defining triggers, not blocking. • A record larger than the serialization buffer will first trigger a spill, then be spilled to a separate file. It is undefined whether or not this record will first pass through the combiner. 6.3.3. Shuffle/Reduce Parameters As described previously, each reduce fetches the output assigned to it by the Partitioner via HTTP into memory and periodically merges these outputs to disk. If intermediate compression of map outputs is turned on, each output is decompressed into memory. The following options affect the frequency of these merges to disk prior to the reduce and the memory allocated to map output during the reduce.
Name io.sort.factor int Type Description Specifies the number of segments on disk to be merged at the same time. It limits the number of open files and compression codecs during the merge. If the number of files
Page 16
Copyright © 2008 The Apache Software Foundation. All rights reserved.
Map/Reduce Tutorial
exceeds this limit, the merge will proceed in several passes. Though this limit also applies to the map, most jobs should be configured so that hitting this limit is unlikely there. mapred.inmem.merge.threshold int The number of sorted map outputs fetched into memory before being merged to disk. Like the spill thresholds in the preceding note, this is not defining a unit of partition, but a trigger. In practice, this is usually set very high (1000) or disabled (0), since merging in-memory segments is often less expensive than merging from disk (see notes following this table). This threshold influences only the frequency of in-memory merges during the shuffle. The memory threshold for fetched map outputs before an in-memory merge is started, expressed as a percentage of memory allocated to storing map outputs in memory. Since map outputs that can&#39;t fit in memory can be stalled, setting this high may decrease parallelism between the fetch and merge. Conversely, values as high as 1.0 have been effective for reduces whose input can fit entirely in memory. This parameter influences only the frequency of in-memory merges during the shuffle. The percentage of memoryrelative to the maximum heapsize as typically specified in mapred.child.java.optsthat can be allocated to storing
mapred.job.shuffle.merge.percentfloat
mapred.job.shuffle.input.buffer.percent float
Page 17
Copyright © 2008 The Apache Software Foundation. All rights reserved.
Map/Reduce Tutorial
map outputs during the shuffle. Though some memory should be set aside for the framework, in general it is advantageous to set this high enough to store large and numerous map outputs. mapred.job.reduce.input.buffer.percent float The percentage of memory relative to the maximum heapsize in which map outputs may be retained during the reduce. When the reduce begins, map outputs will be merged to disk until those that remain are under the resource limit this defines. By default, all map outputs are merged to disk before the reduce begins to maximize the memory available to the reduce. For less memory-intensive reduces, this should be increased to avoid trips to disk.
Other notes • If a map output is larger than 25 percent of the memory allocated to copying map outputs, it will be written directly to disk without first staging through memory. • When running with a combiner, the reasoning about high merge thresholds and large buffers may not hold. For merges started before all map outputs have been fetched, the combiner is run while spilling to disk. In some cases, one can obtain better reduce times by spending resources combining map outputs- making disk spills small and parallelizing spilling and fetching- rather than aggressively increasing buffer sizes. • When merging in-memory map outputs to disk to begin the reduce, if an intermediate merge is necessary because there are segments to spill and at least io.sort.factor segments already on disk, the in-memory map outputs will be part of the intermediate merge. 6.3.4. Directory Structure The task tracker has local directory, ${mapred.local.dir}/taskTracker/ to create localized cache and localized job. It can define multiple local directories (spanning multiple disks) and then each filename is assigned to a semi-random local directory. When the job starts, task tracker creates a localized job directory relative to the local directory specified in
Page 18
Copyright © 2008 The Apache Software Foundation. All rights reserved.
Map/Reduce Tutorial
the configuration. Thus the task tracker directory structure looks the following: • ${mapred.local.dir}/taskTracker/archive/ : The distributed cache. This directory holds the localized distributed cache. Thus localized distributed cache is shared among all the tasks and jobs • ${mapred.local.dir}/taskTracker/jobcache/$jobid/ : The localized job directory • ${mapred.local.dir}/taskTracker/jobcache/$jobid/work/ : The job-specific shared directory. The tasks can use this space as scratch space and share files among them. This directory is exposed to the users through the configuration property job.local.dir. The directory can accessed through api JobConf.getJobLocalDir(). It is available as System property also. So, users (streaming etc.) can call System.getProperty(&quot;job.local.dir&quot;) to access the directory. • ${mapred.local.dir}/taskTracker/jobcache/$jobid/jars/ : The jars directory, which has the job jar file and expanded jar. The job.jar is the application&#39;s jar file that is automatically distributed to each machine. It is expanded in jars directory before the tasks for the job start. The job.jar location is accessible to the application through the api JobConf.getJar() . To access the unjarred directory, JobConf.getJar().getParent() can be called. • ${mapred.local.dir}/taskTracker/jobcache/$jobid/job.xml : The job.xml file, the generic job configuration, localized for the job. • ${mapred.local.dir}/taskTracker/jobcache/$jobid/$taskid : The task directory for each task attempt. Each task directory again has the following structure : • ${mapred.local.dir}/taskTracker/jobcache/$jobid/$taskid/job.xml : A job.xml file, task localized job configuration, Task localization means that properties have been set that are specific to this particular task within the job. The properties localized for each task are described below. • ${mapred.local.dir}/taskTracker/jobcache/$jobid/$taskid/output : A directory for intermediate output files. This contains the temporary map reduce data generated by the framework such as map output files etc. • ${mapred.local.dir}/taskTracker/jobcache/$jobid/$taskid/work : The curernt working directory of the task. With jvm reuse enabled for tasks, this directory will be the directory on which the jvm has started • ${mapred.local.dir}/taskTracker/jobcache/$jobid/$taskid/work/tmp : The temporary directory for the task. (User can specify the property mapred.child.tmp to set the value of temporary directory for map and reduce tasks. This defaults to ./tmp. If the value is not an absolute path, it is prepended with task&#39;s working directory. Otherwise, it is directly assigned. The directory will be created if it doesn&#39;t exist. Then, the child java tasks are executed
Page 19
Copyright © 2008 The Apache Software Foundation. All rights reserved.
Map/Reduce Tutorial
with option -Djava.io.tmpdir=&#39;the absolute path of the tmp dir&#39;. Anp pipes and streaming are set with environment variable, TMPDIR=&#39;the absolute path of the tmp dir&#39;). This directory is created, if mapred.child.tmp has the value ./tmp
6.3.5. Task JVM Reuse Jobs can enable task JVMs to be reused by specifying the job configuration mapred.job.reuse.jvm.num.tasks. If the value is 1 (the default), then JVMs are not reused (i.e. 1 task per JVM). If it is -1, there is no limit to the number of tasks a JVM can run (of the same job). One can also specify some value greater than 1 using the api JobConf.setNumTasksToExecutePerJvm(int) The following properties are localized in the job configuration for each task&#39;s execution:
Name mapred.job.id mapred.jar job.local.dir mapred.tip.id mapred.task.id mapred.task.is.map mapred.task.partition map.input.file map.input.start map.input.length mapred.work.output.dir String String String String String boolean int String long long String Type The job id job.jar location in job directory The job specific shared scratch space The task id The task attempt id Is this a map task The id of the task within the job The filename that the map is reading from The offset of the start of the map input split The number of bytes in the map input split The task&#39;s temporary output directory Description
The standard output (stdout) and error (stderr) streams of the task are read by the TaskTracker and logged to ${HADOOP<em>LOG_DIR}/userlogs
Page 20
Copyright © 2008 The Apache Software Foundation. All rights reserved.
Map/Reduce Tutorial
The DistributedCache can also be used to distribute both jars and native libraries for use in the map and/or reduce tasks. The child-jvm always has its current working directory added to the java.library.path and LD_LIBRARY_PATH. And hence the cached libraries can be loaded via System.loadLibrary or System.load. More details on how to load shared libraries through distributed cache are documented at native_libraries.html
6.4. Job Submission and Monitoring
JobClient is the primary interface by which user-job interacts with the JobTracker. JobClient provides facilities to submit jobs, track their progress, access component-tasks&#39; reports and logs, get the Map/Reduce cluster&#39;s status information and so on. The job submission process involves: 1. Checking the input and output specifications of the job. 2. Computing the InputSplit values for the job. 3. Setting up the requisite accounting information for the DistributedCache of the job, if necessary. 4. Copying the job&#39;s jar and configuration to the Map/Reduce system directory on the FileSystem. 5. Submitting the job to the JobTracker and optionally monitoring it&#39;s status. Job history files are also logged to user specified directory hadoop.job.history.user.location which defaults to job output directory. The files are stored in &quot;_logs/history/&quot; in the specified directory. Hence, by default they will be in mapred.output.dir/_logs/history. User can stop logging by giving the value none for hadoop.job.history.user.location User can view the history logs summary in specified directory using the following command $ bin/hadoop job -history output-dir This command will print job details, failed and killed tip details. More details about the job such as successful tasks and task attempts made for each task can be viewed using the following command $ bin/hadoop job -history all output-dir User can use OutputLogFilter to filter log files from the output directory listing. Normally the user creates the application, describes various facets of the job via JobConf, and then uses the JobClient to submit the job and monitor its progress. 6.4.1. Job Control Users may need to chain Map/Reduce jobs to accomplish complex tasks which cannot be
Page 21
Copyright © 2008 The Apache Software Foundation. All rights reserved.
Map/Reduce Tutorial
done via a single Map/Reduce job. This is fairly easy since the output of the job typically goes to distributed file-system, and the output, in turn, can be used as the input for the next job. However, this also means that the onus on ensuring jobs are complete (success/failure) lies squarely on the clients. In such cases, the various job-control options are: • runJob(JobConf) : Submits the job and returns only after the job has completed. • submitJob(JobConf) : Only submits the job, then poll the returned handle to the RunningJob to query status and make scheduling decisions. • JobConf.setJobEndNotificationURI(String) : Sets up a notification upon job-completion, thus avoiding polling.
6.5. Job Input
InputFormat describes the input-specification for a Map/Reduce job. The Map/Reduce framework relies on the InputFormat of the job to: 1. Validate the input-specification of the job. 2. Split-up the input file(s) into logical InputSplit instances, each of which is then assigned to an individual Mapper. 3. Provide the RecordReader implementation used to glean input records from the logical InputSplit for processing by the Mapper. The default behavior of file-based InputFormat implementations, typically sub-classes of FileInputFormat, is to split the input into logical InputSplit instances based on the total size, in bytes, of the input files. However, the FileSystem blocksize of the input files is treated as an upper bound for input splits. A lower bound on the split size can be set via mapred.min.split.size. Clearly, logical splits based on input-size is insufficient for many applications since record boundaries must be respected. In such cases, the application should implement a RecordReader, who is responsible for respecting record-boundaries and presents a record-oriented view of the logical InputSplit to the individual task. TextInputFormat is the default InputFormat. If TextInputFormat is the InputFormat for a given job, the framework detects input-files with the .gz extensions and automatically decompresses them using the appropriate CompressionCodec. However, it must be noted that compressed files with the above extensions cannot be split and each compressed file is processed in its entirety by a single mapper.
Page 22
Copyright © 2008 The Apache Software Foundation. All rights reserved.
Map/Reduce Tutorial
6.5.1. InputSplit InputSplit represents the data to be processed by an individual Mapper. Typically InputSplit presents a byte-oriented view of the input, and it is the responsibility of RecordReader to process and present a record-oriented view. FileSplit is the default InputSplit. It sets map.input.file to the path of the input file for the logical split. 6.5.2. RecordReader RecordReader reads <key, value> pairs from an InputSplit. Typically the RecordReader converts the byte-oriented view of the input, provided by the InputSplit, and presents a record-oriented to the Mapper implementations for processing. RecordReader thus assumes the responsibility of processing record boundaries and presents the tasks with keys and values.
6.6. Job Output
OutputFormat describes the output-specification for a Map/Reduce job. The Map/Reduce framework relies on the OutputFormat of the job to: 1. Validate the output-specification of the job; for example, check that the output directory doesn&#39;t already exist. 2. Provide the RecordWriter implementation used to write the output files of the job. Output files are stored in a FileSystem. TextOutputFormat is the default OutputFormat. 6.6.1. OutputCommitter OutputCommitter describes the commit of task output for a Map/Reduce job. The Map/Reduce framework relies on the OutputCommitter of the job to: 1. Setup the job during initialization. For example, create the temporary output directory for the job during the initialization of the job. Job setup is done by a separate task when the job is in PREP state and after initializing tasks. Once the setup task completes, the job will be moved to RUNNING state. 2. Cleanup the job after the job completion. For example, remove the temporary output directory after the job completion. Job cleanup is done by a separate task at the end of the
Page 23
Copyright © 2008 The Apache Software Foundation. All rights reserved.
Map/Reduce Tutorial
job. Job is declared SUCCEDED/FAILED/KILLED after the cleanup task completes. 3. Setup the task temporary output. Task setup is done as part of the same task, during task initialization. 4. Check whether a task needs a commit. This is to avoid the commit procedure if a task does not need commit. 5. Commit of the task output. Once task is done, the task will commit it&#39;s output if required. 6. Discard the task commit. If the task has been failed/killed, the output will be cleaned-up. If task could not cleanup (in exception block), a separate task will be launched with same attempt-id to do the cleanup. FileOutputCommitter is the default OutputCommitter. Job setup/cleanup tasks occupy map or reduce slots, whichever is free on the TaskTracker. And JobCleanup task, TaskCleanup tasks and JobSetup task have the highest priority, and in that order. 6.6.2. Task Side-Effect Files In some applications, component tasks need to create and/or write to side-files, which differ from the actual job-output files. In such cases there could be issues with two instances of the same Mapper or Reducer running simultaneously (for example, speculative tasks) trying to open and/or write to the same file (path) on the FileSystem. Hence the application-writer will have to pick unique names per task-attempt (using the attemptid, say attempt_200709221812_0001_m_000000_0), not just per task. To avoid these issues the Map/Reduce framework, when the OutputCommitter is FileOutputCommitter, maintains a special ${mapred.output.dir}/_temporary/</em>${taskid} sub-directory accessible via ${mapred.work.output.dir} for each task-attempt on the FileSystem where the output of the task-attempt is stored. On successful completion of the task-attempt, the files in the ${mapred.output.dir}/<em>temporary/</em>${taskid} (only) are promoted to ${mapred.output.dir}. Of course, the framework discards the sub-directory of unsuccessful task-attempts. This process is completely transparent to the application. The application-writer can take advantage of this feature by creating any side-files required in ${mapred.work.output.dir} during execution of a task via FileOutputFormat.getWorkOutputPath(), and the framework will promote them similarly for succesful task-attempts, thus eliminating the need to pick unique paths per task-attempt. Note: The value of ${mapred.work.output.dir} during execution of a particular task-attempt is actually ${mapred.output.dir}/<em>temporary/</em>{$taskid}, and this value is set by the Map/Reduce framework. So, just create any side-files in the path returned by FileOutputFormat.getWorkOutputPath() from map/reduce task to take advantage
Page 24
Copyright © 2008 The Apache Software Foundation. All rights reserved.
Map/Reduce Tutorial
of this feature. The entire discussion holds true for maps of jobs with reducer=NONE (i.e. 0 reduces) since output of the map, in that case, goes directly to HDFS. 6.6.3. RecordWriter RecordWriter writes the output <key, value> pairs to an output file. RecordWriter implementations write the job outputs to the FileSystem.
6.7. Other Useful Features
6.7.1. Submitting Jobs to Queues Users submit jobs to Queues. Queues, as collection of jobs, allow the system to provide specific functionality. For example, queues use ACLs to control which users who can submit jobs to them. Queues are expected to be primarily used by Hadoop Schedulers. Hadoop comes configured with a single mandatory queue, called &#39;default&#39;. Queue names are defined in the mapred.queue.names property of the Hadoop site configuration. Some job schedulers, such as the Capacity Scheduler, support multiple queues. A job defines the queue it needs to be submitted to through the mapred.job.queue.name property, or through the setQueueName(String) API. Setting the queue name is optional. If a job is submitted without an associated queue name, it is submitted to the &#39;default&#39; queue. 6.7.2. Counters Counters represent global counters, defined either by the Map/Reduce framework or applications. Each Counter can be of any Enum type. Counters of a particular Enum are bunched into groups of type Counters.Group. Applications can define arbitrary Counters (of type Enum) and update them via Reporter.incrCounter(Enum, long) or Reporter.incrCounter(String, String, long) in the map and/or reduce methods. These counters are then globally aggregated by the framework. 6.7.3. DistributedCache DistributedCache distributes application-specific, large, read-only files efficiently. DistributedCache is a facility provided by the Map/Reduce framework to cache files
Page 25
Copyright © 2008 The Apache Software Foundation. All rights reserved.
Map/Reduce Tutorial
(text, archives, jars and so on) needed by applications. Applications specify the files to be cached via urls (hdfs://) in the JobConf. The DistributedCache assumes that the files specified via hdfs:// urls are already present on the FileSystem. The framework will copy the necessary files to the slave node before any tasks for the job are executed on that node. Its efficiency stems from the fact that the files are only copied once per job and the ability to cache archives which are un-archived on the slaves. DistributedCache tracks the modification timestamps of the cached files. Clearly the cache files should not be modified by the application or externally while the job is executing. DistributedCache can be used to distribute simple, read-only data/text files and more complex types such as archives and jars. Archives (zip, tar, tgz and tar.gz files) are un-archived at the slave nodes. Files have execution permissions set. The files/archives can be distributed by setting the property mapred.cache.{files|archives}. If more than one file/archive has to be distributed, they can be added as comma separated paths. The properties can also be set by APIs DistributedCache.addCacheFile(URI,conf)/ DistributedCache.addCacheArchive(URI,conf) and DistributedCache.setCacheFiles(URIs,conf)/ DistributedCache.setCacheArchives(URIs,conf) where URI is of the form hdfs://host:port/absolute-path/#link-name. In Streaming, the files can be distributed through command line option -cacheFile/-cacheArchive. Optionally users can also direct the DistributedCache to symlink the cached file(s) into the current working directory of the task via the DistributedCache.createSymlink(Configuration) api. Or by setting the configuration property mapred.create.symlink as yes. The DistributedCache will use the fragment of the URI as the name of the symlink. For example, the URI hdfs://namenode:port/lib.so.1/#lib.so will have the symlink name as lib.so in task&#39;s cwd for the file lib.so.1 in distributed cache. The DistributedCache can also be used as a rudimentary software distribution mechanism for use in the map and/or reduce tasks. It can be used to distribute both jars and native libraries. The DistributedCache.addArchiveToClassPath(Path, Configuration) or DistributedCache.addFileToClassPath(Path, Configuration) api can be used to cache files/jars and also add them to the classpath of child-jvm. The same can be done by setting the configuration properties mapred.job.classpath.{files|archives}. Similarly the cached files that are symlinked into the working directory of the task can be used to distribute native libraries and load them.
Page 26
Copyright © 2008 The Apache Software Foundation. All rights reserved.
Map/Reduce Tutorial
6.7.4. Tool The Tool interface supports the handling of generic Hadoop command-line options. Tool is the standard for any Map/Reduce tool or application. The application should delegate the handling of standard command-line options to GenericOptionsParser via ToolRunner.run(Tool, String[]) and only handle its custom arguments. The generic Hadoop command-line options are: -conf <configuration file> -D <property=value> -fs <local|namenode:port> -jt <local|jobtracker:port> 6.7.5. IsolationRunner IsolationRunner is a utility to help debug Map/Reduce programs. To use the IsolationRunner, first set keep.failed.tasks.files to true (also see keep.tasks.files.pattern). Next, go to the node on which the failed task ran and go to the TaskTracker&#39;s local directory and run the IsolationRunner: $ cd <local path>/taskTracker/${taskid}/work $ bin/hadoop org.apache.hadoop.mapred.IsolationRunner ../job.xml IsolationRunner will run the failed task in a single jvm, which can be in the debugger, over precisely the same input. 6.7.6. Profiling Profiling is a utility to get a representative (2 or 3) sample of built-in java profiler for a sample of maps and reduces. User can specify whether the system should collect profiler information for some of the tasks in the job by setting the configuration property mapred.task.profile. The value can be set using the api JobConf.setProfileEnabled(boolean). If the value is set true, the task profiling is enabled. The profiler information is stored in the user log directory. By default, profiling is not enabled for the job. Once user configures that profiling is needed, she/he can use the configuration property mapred.task.profile.{maps|reduces} to set the ranges of map/reduce tasks to
Page 27
Copyright © 2008 The Apache Software Foundation. All rights reserved.
Map/Reduce Tutorial
profile. The value can be set using the api JobConf.setProfileTaskRange(boolean,String). By default, the specified range is 0-2.
User can also specify the profiler configuration arguments by setting the configuration property mapred.task.profile.params. The value can be specified using the api JobConf.setProfileParams(String). If the string contains a %s, it will be replaced with the name of the profiling output file when the task runs. These parameters are passed to the task child JVM on the command line. The default value for the profiling parameters is -agentlib:hprof=cpu=samples,heap=sites,force=n,thread=y,verbose=n,file=%s 6.7.7. Debugging The Map/Reduce framework provides a facility to run user-provided scripts for debugging. When a map/reduce task fails, a user can run a debug script, to process task logs for example. The script is given access to the task&#39;s stdout and stderr outputs, syslog and jobconf. The output from the debug script&#39;s stdout and stderr is displayed on the console diagnostics and also as part of the job UI. In the following sections we discuss how to submit a debug script with a job. The script file needs to be distributed and submitted to the framework.
6.7.7.1. How to distribute the script file:
The user needs to use DistributedCache to distribute and symlink the script file.
6.7.7.2. How to submit the script:
A quick way to submit the debug script is to set values for the properties mapred.map.task.debug.script and mapred.reduce.task.debug.script, for debugging map and reduce tasks respectively. These properties can also be set by using APIs JobConf.setMapDebugScript(String) and JobConf.setReduceDebugScript(String) . In streaming mode, a debug script can be submitted with the command-line options -mapdebug and -reducedebug, for debugging map and reduce tasks respectively. The arguments to the script are the task&#39;s stdout, stderr, syslog and jobconf files. The debug command, run on the node where the map/reduce task failed, is: $script $stdout $stderr $syslog $jobconf Pipes programs have the c++ program name as a fifth argument for the command. Thus for the pipes programs the command is $script $stdout $stderr $syslog $jobconf $program
Page 28
Copyright © 2008 The Apache Software Foundation. All rights reserved.
Map/Reduce Tutorial
6.7.7.3. Default Behavior:
For pipes, a default script is run to process core dumps under gdb, prints stack trace and gives info about running threads. 6.7.8. JobControl JobControl is a utility which encapsulates a set of Map/Reduce jobs and their dependencies. 6.7.9. Data Compression Hadoop Map/Reduce provides facilities for the application-writer to specify compression for both intermediate map-outputs and the job-outputs i.e. output of the reduces. It also comes bundled with CompressionCodec implementation for the zlib compression algorithm. The gzip file format is also supported. Hadoop also provides native implementations of the above compression codecs for reasons of both performance (zlib) and non-availability of Java libraries. More details on their usage and availability are available here.
6.7.9.1. Intermediate Outputs
Applications can control compression of intermediate map-outputs via the JobConf.setCompressMapOutput(boolean) api and the CompressionCodec to be used via the JobConf.setMapOutputCompressorClass(Class) api.
6.7.9.2. Job Outputs
Applications can control compression of job-outputs via the FileOutputFormat.setCompressOutput(JobConf, boolean) api and the CompressionCodec to be used can be specified via the FileOutputFormat.setOutputCompressorClass(JobConf, Class) api. If the job outputs are to be stored in the SequenceFileOutputFormat, the required SequenceFile.CompressionType (i.e. RECORD / BLOCK - defaults to RECORD) can be specified via the SequenceFileOutputFormat.setOutputCompressionType(JobConf, SequenceFile.CompressionType) api. 6.7.10. Skipping Bad Records Hadoop provides an option where a certain set of bad input records can be skipped when processing map inputs. Applications can control this feature through the SkipBadRecords
Page 29
Copyright © 2008 The Apache Software Foundation. All rights reserved.
Map/Reduce Tutorial
class. This feature can be used when map tasks crash deterministically on certain input. This usually happens due to bugs in the map function. Usually, the user would have to fix these bugs. This is, however, not possible sometimes. The bug may be in third party libraries, for example, for which the source code is not available. In such cases, the task never completes successfully even after multiple attempts, and the job fails. With this feature, only a small portion of data surrounding the bad records is lost, which may be acceptable for some applications (those performing statistical analysis on very large data, for example). By default this feature is disabled. For enabling it, refer to SkipBadRecords.setMapperMaxSkipRecords(Configuration, long) and SkipBadRecords.setReducerMaxSkipGroups(Configuration, long). With this feature enabled, the framework gets into &#39;skipping mode&#39; after a certain number of map failures. For more details, see SkipBadRecords.setAttemptsToStartSkipping(Configuration, int). In &#39;skipping mode&#39;, map tasks maintain the range of records being processed. To do this, the framework relies on the processed record counter. See SkipBadRecords.COUNTER_MAP_PROCESSED_RECORDS and SkipBadRecords.COUNTER_REDUCE_PROCESSED_GROUPS. This counter enables the framework to know how many records have been processed successfully, and hence, what record range caused a task to crash. On further attempts, this range of records is skipped. The number of records skipped depends on how frequently the processed record counter is incremented by the application. It is recommended that this counter be incremented after every record is processed. This may not be possible in some applications that typically batch their processing. In such cases, the framework may skip additional records surrounding the bad record. Users can control the number of skipped records through SkipBadRecords.setMapperMaxSkipRecords(Configuration, long) and SkipBadRecords.setReducerMaxSkipGroups(Configuration, long). The framework tries to narrow the range of skipped records using a binary search-like approach. The skipped range is divided into two halves and only one half gets executed. On subsequent failures, the framework figures out which half contains bad records. A task will be re-executed till the acceptable skipped value is met or all task attempts are exhausted. To increase the number of task attempts, use JobConf.setMaxMapAttempts(int) and JobConf.setMaxReduceAttempts(int). Skipped records are written to HDFS in the sequence file format, for later analysis. The location can be changed through SkipBadRecords.setSkipOutputPath(JobConf, Path).</li>
<li>Example: WordCount v2.0
Page 30
Copyright © 2008 The Apache Software Foundation. All rights reserved.
Map/Reduce Tutorial
Here is a more complete WordCount which uses many of the features provided by the Map/Reduce framework we discussed so far. This needs the HDFS to be up and running, especially for the DistributedCache-related features. Hence it only works with a pseudo-distributed or fully-distributed Hadoop installation.
7.1. Source Code
WordCount.java 1. 2. 3. 4. 5. 6. 7. 8. 9. 10. 11. 12. 13. 14. 15. public static class Map extends MapReduceBase implements Mapper<LongWritable, Text, Text, IntWritable> { public class WordCount extends Configured implements Tool { import org.apache.hadoop.fs.Path; import org.apache.hadoop.filecache.DistributedCache; import org.apache.hadoop.conf./<em>; import org.apache.hadoop.io./</em>; import org.apache.hadoop.mapred./<em>; import org.apache.hadoop.util./</em>; import java.io./<em>; import java.util./</em>; package org.myorg;</li>
<li><ol>
<li>static enum Counters { INPUT_WORDS }
Page 31
Copyright © 2008 The Apache Software Foundation. All rights reserved.
Map/Reduce Tutorial</li>
</ol>
</li>
<li><ol>
<li><ol>
<li><ol>
<li><ol>
<li><ol>
<li>private boolean caseSensitive = true; private Set<String> patternsToSkip = new HashSet<String>(); private final static IntWritable one = new IntWritable(1); private Text word = new Text();</li>
</ol>
</li>
</ol>
</li>
</ol>
</li>
</ol>
</li>
</ol>
</li>
<li><ol>
<li><ol>
<li><ol>
<li><ol>
<li><ol>
<li>public void configure(JobConf job) { caseSensitive = job.getBoolean(&quot;wordcount.case.sensitive&quot;, true); inputFile = job.get(&quot;map.input.file&quot;); private long numRecords = 0; private String inputFile;</li>
</ol>
</li>
</ol>
</li>
</ol>
</li>
</ol>
</li>
</ol>
</li>
<li><ol>
<li>32.
if (job.getBoolean(&quot;wordcount.skip.patterns&quot;, false)) { Path[] patternsFiles = new Path[0]; try { patternsFiles = DistributedCache.getLocalCacheFiles(job); } catch (IOException ioe) { System.err.println(&quot;Caught</li>
</ol>
</li>
<li><ol>
<li><ol>
<li><ol>
<li>37.
Page 32
Copyright © 2008 The Apache Software Foundation. All rights reserved.
Map/Reduce Tutorial
exception while getting cached files: &quot; + StringUtils.stringifyException(ioe)); 38. 39. 40. 41. 42. 43. 44. 45. 46. 47. private void parseSkipFile(Path patternsFile) { try { BufferedReader fis = new BufferedReader(new FileReader(patternsFile.toString())); String pattern = null; while ((pattern = fis.readLine()) != null) { patternsToSkip.add(pattern); } } catch (IOException ioe) { System.err.println(&quot;Caught exception while parsing the cached file &#39;&quot; + patternsFile + &quot;&#39; : &quot; + StringUtils.stringifyException(ioe)); } } } for (Path patternsFile : patternsFiles) { parseSkipFile(patternsFile); } } }</li>
</ol>
</li>
</ol>
</li>
</ol>
</li>
<li><ol>
<li><ol>
<li><ol>
<li><ol>
<li>53.</li>
</ol>
</li>
</ol>
</li>
</ol>
</li>
</ol>
</li>
<li><ol>
<li><ol>
<li>57.
public void map(LongWritable key,
Page 33
Copyright © 2008 The Apache Software Foundation. All rights reserved.
Map/Reduce Tutorial
Text value, OutputCollector<Text, IntWritable> output, Reporter reporter) throws IOException { 58. String line = (caseSensitive) ? value.toString() : value.toString().toLowerCase();</li>
</ol>
</li>
</ol>
</li>
<li><ol>
<li><ol>
<li>&quot;&quot;); 62. 63. 64. 65. 66. word.set(tokenizer.nextToken()); 67. 68. reporter.incrCounter(Counters.INPUT_WORDS, 1); 69. 70. 71. 72. if ((++numRecords % 100) == 0) { reporter.setStatus(&quot;Finished processing &quot; + numRecords + &quot; records &quot; + &quot;from the input file: &quot; + inputFile); } } } } output.collect(word, one); StringTokenizer tokenizer = new StringTokenizer(line); while (tokenizer.hasMoreTokens()) { } for (String pattern : patternsToSkip) { line = line.replaceAll(pattern,</li>
</ol>
</li>
</ol>
</li>
<li><ol>
<li>75.
Page 34
Copyright © 2008 The Apache Software Foundation. All rights reserved.
Map/Reduce Tutorial</li>
</ol>
</li>
<li><ol>
<li>public static class Reduce extends MapReduceBase implements Reducer<Text, IntWritable, Text, IntWritable> { public void reduce(Text key, Iterator<IntWritable> values, OutputCollector<Text, IntWritable> output, Reporter reporter) throws IOException { int sum = 0; while (values.hasNext()) { sum += values.next().get(); } output.collect(key, new IntWritable(sum)); } }
78.</li>
</ol>
</li>
<li><ol>
<li><ol>
<li><ol>
<li><ol>
<li><ol>
<li><ol>
<li><ol>
<li><ol>
<li><ol>
<li><ol>
<li><ol>
<li>91.
public int run(String[] args) throws Exception { JobConf conf = new JobConf(getConf(), WordCount.class); conf.setJobName(&quot;wordcount&quot;);
conf.setOutputKeyClass(Text.class); 92. conf.setOutputValueClass(IntWritable.class); 93. 94. 95. conf.setMapperClass(Map.class);
Page 35
Copyright © 2008 The Apache Software Foundation. All rights reserved.
Map/Reduce Tutorial
conf.setCombinerClass(Reduce.class); 96. conf.setReducerClass(Reduce.class); 97. 98. conf.setInputFormat(TextInputFormat.class); 99. conf.setOutputFormat(TextOutputFormat.class); 100. 101. 102. 103. 104. DistributedCache.addCacheFile(new Path(args[++i]).toUri(), conf); 105. conf.setBoolean(&quot;wordcount.skip.patterns&quot;, true); 106. 107. 108. 109. 110. 111. FileInputFormat.setInputPaths(conf, new Path(other_args.get(0))); 112. FileOutputFormat.setOutputPath(conf, new Path(other_args.get(1))); 113. } else { other_args.add(args[i]); } } List<String> other_args = new ArrayList<String>(); for (int i=0; i &lt; args.length; ++i) { if (&quot;-skip&quot;.equals(args[i])) {
Page 36
Copyright © 2008 The Apache Software Foundation. All rights reserved.
Map/Reduce Tutorial</li>
</ol>
</li>
</ol>
</li>
</ol>
</li>
</ol>
</li>
</ol>
</li>
</ol>
</li>
</ol>
</li>
</ol>
</li>
</ol>
</li>
</ol>
</li>
</ol>
</li>
<li><ol>
<li><ol>
<li><ol>
<li><ol>
<li>119.
JobClient.runJob(conf); return 0; }
public static void main(String[] args) throws Exception { int res = ToolRunner.run(new Configuration(), new WordCount(), args); System.exit(res); } }</li>
</ol>
</li>
</ol>
</li>
</ol>
</li>
</ol>
</li>
<li><ol>
<li><ol>
<li>123.
7.2. Sample Runs
Sample text-files as input: $ bin/hadoop dfs -ls /usr/joe/wordcount/input/ /usr/joe/wordcount/input/file01 /usr/joe/wordcount/input/file02 $ bin/hadoop dfs -cat /usr/joe/wordcount/input/file01 Hello World, Bye World! $ bin/hadoop dfs -cat /usr/joe/wordcount/input/file02 Hello Hadoop, Goodbye to hadoop. Run the application: $ bin/hadoop jar /usr/joe/wordcount.jar org.myorg.WordCount /usr/joe/wordcount/input /usr/joe/wordcount/output Output: $ bin/hadoop dfs -cat /usr/joe/wordcount/output/part-00000 Bye 1 Goodbye 1 Hadoop, 1 Hello 2
Page 37
Copyright © 2008 The Apache Software Foundation. All rights reserved.
Map/Reduce Tutorial
World! 1 World, 1 hadoop. 1 to 1 Notice that the inputs differ from the first version we looked at, and how they affect the outputs. Now, lets plug-in a pattern-file which lists the word-patterns to be ignored, via the DistributedCache. $ hadoop dfs -cat /user/joe/wordcount/patterns.txt . \, ! to Run it again, this time with more options: $ bin/hadoop jar /usr/joe/wordcount.jar org.myorg.WordCount -Dwordcount.case.sensitive=true /usr/joe/wordcount/input /usr/joe/wordcount/output -skip /user/joe/wordcount/patterns.txt As expected, the output: $ bin/hadoop dfs -cat /usr/joe/wordcount/output/part-00000 Bye 1 Goodbye 1 Hadoop 1 Hello 2 World 2 hadoop 1 Run it once more, this time switch-off case-sensitivity: $ bin/hadoop jar /usr/joe/wordcount.jar org.myorg.WordCount -Dwordcount.case.sensitive=false /usr/joe/wordcount/input /usr/joe/wordcount/output -skip /user/joe/wordcount/patterns.txt Sure enough, the output: $ bin/hadoop dfs -cat /usr/joe/wordcount/output/part-00000 bye 1
Page 38
Copyright © 2008 The Apache Software Foundation. All rights reserved.
Map/Reduce Tutorial
goodbye 1 hadoop 2 hello 2 world 2
7.3. Highlights
The second version of WordCount improves upon the previous one by using some features offered by the Map/Reduce framework: • Demonstrates how applications can access configuration parameters in the configure method of the Mapper (and Reducer) implementations (lines 28-43). • Demonstrates how the DistributedCache can be used to distribute read-only data needed by the jobs. Here it allows the user to specify word-patterns to skip while counting (line 104). • Demonstrates the utility of the Tool interface and the GenericOptionsParser to handle generic Hadoop command-line options (lines 87-116, 119). • Demonstrates how applications can use Counters (line 68) and how they can set application-specific status information via the Reporter instance passed to the map (and reduce) method (line 72). Java and JNI are trademarks or registered trademarks of Sun Microsystems, Inc. in the United States and other countries.
Page 39
Copyright © 2008 The Apache Software Foundation. All rights reserved.</li>
</ol>
</li>
</ol>
</li>
</ol>

      
    </div>
    
    
      
    
    
        <footer id="post-meta">
        <span class="categories">Posted in<span class="breadcrumb fa fa-folder"><li><a href="/categories/hadoop/">hadoop</a></li></span></span> | <span class="tags">Tagged <a href="/tags/hadoop/" class="label label-primary">hadoop</a></span> | <span class="time">recent updated:<time title="2014-03-07 09:54:36"datetime="2014-03-07 09:54:36"> mar. 7 2014</time></span> | <span class="comment-link">
<a href="http://itsolife.com/2014/02/02/2014-02-02-hadoop--mapred_tutorial/#comments" class="ds-thread-count comment-link" data-thread-key="2014-02-02-hadoop--mapred_tutorial" data-count-type="comments">暂无评论</a></span>
        </footer>
    
    <div class="clearfix"></div>
  </div>
</div>
</article>



  <article>
<div id="post" class="post well">
  <div class="post-content">
    <header class="well-sm">
      <i class="fa icon fa-5x pull-left"></i>
      <h1 class="title"><a href="/2014/02/02/2014-02-02-linux-redhat--centoslibxml2/">centos libxml2</a></h1>
      
        <span>Posted on<time datetime="2014-02-02T01:54:36.000Z"> <a href="/2014/02/02/2014-02-02-linux-redhat--centoslibxml2/">feb. 2 2014</a></time></span>      
    </header>
    
    
    <div class="entry">
      
        <h1 id="centos-libxml2">centos libxml2</h1>
<h2 id="introduction-to-libx-ml2">Introduction to libx‍ml2</h2>
<p>The libxml2 package contains          libraries and utilities used for parsing XML files.</p>
<p>This package is known to build and work properly using an LFS-7.4          platform.</p>
<h3 id="package-information">Package Information</h3>
<ul>
<li>Download (HTTP): <a href="http://xmlsoft.org/sources/libxml2-2.9.1.tar.gz" target="_blank"><a href="http://xmlsoft.org/sources/libxml2-2.9.1.tar.gz">http://xmlsoft.org/sources/libxml2-2.9.1.tar.gz</a></a></li>
<li>Download (FTP): <a href="ftp://xmlsoft.org/libxml2/libxml2-2.9.1.tar.gz">ftp://xmlsoft.org/libxml2/libxml2-2.9.1.tar.gz</a></li>
<li>Download MD5 sum: 9c0cfef285d5c4a5c80d00904ddab380</li>
<li>Download size: 5.0 MB</li>
<li>Estimated disk space required: 100 MB</li>
<li>Estimated build time: 0.6 SBU</li>
</ul>
<h3 id="additional-downloads">Additional Downloads</h3>
<ul>
<li>Optional Testsuite: <a href="http://www.w3.org/XML/Test/xmlts20130923.tar.gz" target="_blank"><a href="http://www.w3.org/XML/Test/xmlts20130923.tar.gz">http://www.w3.org/XML/Test/xmlts20130923.tar.gz</a></a>                - This enables <strong>make                check</strong> to do complete testing.</li>
</ul>
<h3 id="libxml2-dependencies">libxml2 Dependencies</h3>
<h3 id="recommended">Recommended</h3>
<p><a href="http://www.linuxfromscratch.org/blfs/view/svn/general/python2.html" title="Python-2.7.6" target="_blank">Python-2.7.6</a> (to build and install a          Python library module,          additionally it is required to run the full suite of tests)</p>
<p><img src="http://www.linuxfromscratch.org/blfs/view/svn/images/note.png" alt="[Note]">          </p>
<h3 id="note">Note</h3>
<p>Some packages which utilize libxml2 (such as GNOME Doc Utils) need the Python module installed to function properly            and some packages (such as MesaLib) will not build properly if            the Python module is not            available.</p>
<p>User Notes: <a href="http://wiki.linuxfromscratch.org/blfs/wiki/libxml2" target="_blank"><a href="http://wiki.linuxfromscratch.org/blfs/wiki/libxml2">http://wiki.linuxfromscratch.org/blfs/wiki/libxml2</a></a>        </p>
<h2 id="installation-of-libxml2">Installation of libxml2</h2>
<p>If you downloaded the testsuite, issue the following command:
tar xf ../xmlts20130923.tar.gz</p>
<p>Install libxml2 by running the          following commands:
./configure --prefix=/usr --disable-static --with-history &amp;&amp; make</p>
<p>To test the results, issue: <strong>make          check</strong>.</p>
<p>Now, as the</p>
<p>root
user:
make install</p>
<h2 id="command-explanations">Command Explanations</h2>
<p><em>
--disable-static
</em>: This          switch prevents installation of static versions of the libraries.</p>
<p>--with-history
: This switch enables          Readline support when running          <strong>xmlcatalog</strong> or          <strong>xmllint</strong> in shell          mode.</p>
<h2 id="contents">Contents</h2>
<p><strong>Installed Programs:</strong>              xml2-config, xmlcatalog and              xmllint            </p>
<p><strong>Installed Libraries:</strong>              libxml2.so and optionally, the              libxml2mod.so Python              module            </p>
<p><strong>Installed Directories:</strong>              /usr/include/libxml2,              /usr/share/doc/libxml2-2.9.1,              /usr/share/doc/libxml2-python-2.9.1 and              /usr/share/gtk-doc/html/libxml2            </p>
<h3 id="short-descriptions">Short Descriptions</h3>
<p><a href=""></a><strong>xml2-config</strong>                  </p>
<p>determines the compile and linker flags that should be                    used to compile and link programs that use</p>
<p>libxml2
.<a href=""></a><strong>xmlcatalog</strong>                  </p>
<p>is used to monitor and manipulate XML and SGML catalogs.<a href=""></a><strong>xmllint</strong>                  </p>
<p>parses XML files and outputs reports (based upon options)                    to detect errors in XML coding.<a href=""></a></p>
<p>libxml2.so</p>
<p>provides functions for programs to parse files that use                    the XML format.                   </p>

      
    </div>
    
    
      
    
    
        <footer id="post-meta">
        <span class="categories">Posted in<span class="breadcrumb fa fa-folder"><li><a href="/categories/linux/">linux</a></li></span><span class="breadcrumb"><li><a href="/categories/linux/">linux</a></li><li><a href="/categories/linux/redhat/">redhat</a></li></span></span> | <span class="tags">Tagged <a href="/tags/linux/" class="label label-primary">linux</a><a href="/tags/redhat/" class="label label-success">redhat</a></span> | <span class="time">recent updated:<time title="2014-03-07 09:54:36"datetime="2014-03-07 09:54:36"> mar. 7 2014</time></span> | <span class="comment-link">
<a href="http://itsolife.com/2014/02/02/2014-02-02-linux-redhat--centoslibxml2/#comments" class="ds-thread-count comment-link" data-thread-key="2014-02-02-linux-redhat--centoslibxml2" data-count-type="comments">暂无评论</a></span>
        </footer>
    
    <div class="clearfix"></div>
  </div>
</div>
</article>



  <article>
<div id="post" class="post well">
  <div class="post-content">
    <header class="well-sm">
      <i class="fa icon fa-5x pull-left"></i>
      <h1 class="title"><a href="/2014/02/02/2014-02-02-hadoop--hadoop/">hadoop</a></h1>
      
        <span>Posted on<time datetime="2014-02-02T01:54:36.000Z"> <a href="/2014/02/02/2014-02-02-hadoop--hadoop/">feb. 2 2014</a></time></span>      
    </header>
    
    
    <div class="entry">
      
        <h1 id="hadoop">hadoop</h1>
<p><img src="" alt=""></p>
<h1 id="hadoop">hadoop</h1>
<h2 id="table-of-contents">Table of Contents</h2>
<ul>
<li><p><a href="http://dirlt.com/hadoop.html#sec-1" target="_blank">1 hadoop</a></p>
</li>
<li><p><a href="http://dirlt.com/hadoop.html#sec-1-1" target="_blank">1.1 FAQ</a></p>
</li>
<li><p><a href="http://dirlt.com/hadoop.html#sec-1-1-1" target="_blank">1.1.1 Hadoop可以用来做什么</a></p>
</li>
<li><a href="http://dirlt.com/hadoop.html#sec-1-1-2" target="_blank">1.1.2 Hadoop包括哪些组件</a></li>
<li><a href="http://dirlt.com/hadoop.html#sec-1-1-3" target="_blank">1.1.3 CDH和Apache Hadoop的关系</a></li>
<li><a href="http://dirlt.com/hadoop.html#sec-1-1-4" target="_blank">1.1.4 CDH产品组件构成</a></li>
<li><p><a href="http://dirlt.com/hadoop.html#sec-1-1-5" target="_blank">1.1.5 CDH产品组件端口分布和配置</a></p>
</li>
<li><p><a href="http://dirlt.com/hadoop.html#sec-1-1-5-1" target="_blank">1.1.5.1 Hadoop HDFS</a></p>
</li>
<li><a href="http://dirlt.com/hadoop.html#sec-1-1-5-2" target="_blank">1.1.5.2 Hadoop MRv1</a></li>
<li><a href="http://dirlt.com/hadoop.html#sec-1-1-5-3" target="_blank">1.1.5.3 Hadoop YARN</a></li>
<li><a href="http://dirlt.com/hadoop.html#sec-1-1-5-4" target="_blank">1.1.5.4 HBase</a></li>
<li><a href="http://dirlt.com/hadoop.html#sec-1-1-5-5" target="_blank">1.1.5.5 Hive</a></li>
<li><a href="http://dirlt.com/hadoop.html#sec-1-1-5-6" target="_blank">1.1.5.6 Sqoop</a></li>
<li><a href="http://dirlt.com/hadoop.html#sec-1-1-5-7" target="_blank">1.1.5.7 Zookeeper</a></li>
<li><a href="http://dirlt.com/hadoop.html#sec-1-1-5-8" target="_blank">1.1.5.8 Hue</a></li>
<li><a href="http://dirlt.com/hadoop.html#sec-1-1-5-9" target="_blank">1.1.5.9 Ozzie</a></li>
<li><a href="http://dirlt.com/hadoop.html#sec-1-1-5-10" target="_blank">1.1.5.10 Ganglia</a></li>
<li><a href="http://dirlt.com/hadoop.html#sec-1-1-5-11" target="_blank">1.1.5.11 Kerberos</a></li>
<li><p><a href="http://dirlt.com/hadoop.html#sec-1-2" target="_blank">1.2 观点</a></p>
</li>
<li><p><a href="http://dirlt.com/hadoop.html#sec-1-2-1" target="_blank">1.2.1 Hadoop即将过时了吗？</a></p>
</li>
<li><a href="http://dirlt.com/hadoop.html#sec-1-2-2" target="_blank">1.2.2 Best Practices for Selecting Apache Hadoop Hardware</a></li>
<li><a href="http://dirlt.com/hadoop.html#sec-1-2-3" target="_blank">1.2.3 The dark side of Hadoop - BackType Technology</a></li>
<li><p><a href="http://dirlt.com/hadoop.html#sec-1-3" target="_blank">1.3 使用问题</a></p>
</li>
<li><p><a href="http://dirlt.com/hadoop.html#sec-1-3-1" target="_blank">1.3.1 CDH3u3搭建单节点集群</a></p>
</li>
<li><a href="http://dirlt.com/hadoop.html#sec-1-3-2" target="_blank">1.3.2 CDH4.2.0搭建单节点集群</a></li>
<li><a href="http://dirlt.com/hadoop.html#sec-1-3-3" target="_blank">1.3.3 CDH4.3.0</a></li>
<li><p><a href="http://dirlt.com/hadoop.html#sec-1-3-4" target="_blank">1.3.4 Configuration</a></p>
</li>
<li><p><a href="http://dirlt.com/hadoop.html#sec-1-3-4-1" target="_blank">1.3.4.1 .bash_profile</a></p>
</li>
<li><a href="http://dirlt.com/hadoop.html#sec-1-3-4-2" target="_blank">1.3.4.2 core-site.xml</a></li>
<li><a href="http://dirlt.com/hadoop.html#sec-1-3-4-3" target="_blank">1.3.4.3 hdfs-site.xml</a></li>
<li><a href="http://dirlt.com/hadoop.html#sec-1-3-4-4" target="_blank">1.3.4.4 mapred-site.xml</a></li>
<li><a href="http://dirlt.com/hadoop.html#sec-1-3-4-5" target="_blank">1.3.4.5 hadoop-env.sh</a></li>
<li><a href="http://dirlt.com/hadoop.html#sec-1-3-4-6" target="_blank">1.3.4.6 hbase-site.xml</a></li>
<li><a href="http://dirlt.com/hadoop.html#sec-1-3-4-7" target="_blank">1.3.4.7 hbase-env.sh</a></li>
<li><p><a href="http://dirlt.com/hadoop.html#sec-1-4" target="_blank">1.4 Hadoop权威指南</a></p>
</li>
<li><p><a href="http://dirlt.com/hadoop.html#sec-1-4-1" target="_blank">1.4.1 初识Hadoop</a></p>
</li>
<li><a href="http://dirlt.com/hadoop.html#sec-1-4-2" target="_blank">1.4.2 关于MapReduce</a></li>
<li><a href="http://dirlt.com/hadoop.html#sec-1-4-3" target="_blank">1.4.3 Hadoop分布式文件系统</a></li>
<li><a href="http://dirlt.com/hadoop.html#sec-1-4-4" target="_blank">1.4.4 Hadoop IO</a></li>
<li><a href="http://dirlt.com/hadoop.html#sec-1-4-5" target="_blank">1.4.5 MapReduce应用开发</a></li>
<li><a href="http://dirlt.com/hadoop.html#sec-1-4-6" target="_blank">1.4.6 MapReduce的工作机制</a></li>
<li><a href="http://dirlt.com/hadoop.html#sec-1-4-7" target="_blank">1.4.7 MapReduce的类型与格式</a></li>
<li><a href="http://dirlt.com/hadoop.html#sec-1-4-8" target="_blank">1.4.8 MapReduce的特性</a></li>
<li><a href="http://dirlt.com/hadoop.html#sec-1-4-9" target="_blank">1.4.9 构建Hadoop集群</a></li>
<li><a href="http://dirlt.com/hadoop.html#sec-1-4-10" target="_blank">1.4.10 管理Hadoop</a></li>
<li><p><a href="http://dirlt.com/hadoop.html#sec-1-5" target="_blank">1.5 Benchmark</a></p>
</li>
<li><p><a href="http://dirlt.com/hadoop.html#sec-1-5-1" target="_blank">1.5.1 TestDFSIO</a></p>
</li>
<li><a href="http://dirlt.com/hadoop.html#sec-1-5-2" target="_blank">1.5.2 TeraSort</a></li>
<li><a href="http://dirlt.com/hadoop.html#sec-1-5-3" target="_blank">1.5.3 nnbench</a></li>
<li><a href="http://dirlt.com/hadoop.html#sec-1-5-4" target="_blank">1.5.4 mrbench</a></li>
<li><a href="http://dirlt.com/hadoop.html#sec-1-5-5" target="_blank">1.5.5 hbase.PerformanceEvaluation</a></li>
</ul>
<h2 id="1-hadoop">1 hadoop</h2>
<p>参考资源</p>
<ul>
<li>Cloudera <a href="http://www.cloudera.com/" target="_blank"><a href="http://www.cloudera.com/">http://www.cloudera.com/</a></a></li>
<li>Apache Hadoop <a href="http://hadoop.apache.org/" target="_blank"><a href="http://hadoop.apache.org/">http://hadoop.apache.org/</a></a></li>
<li>Apache Hadoop r1.0.3 文档 <a href="http://hadoop.apache.org/common/docs/r1.0.3/" target="_blank"><a href="http://hadoop.apache.org/common/docs/r1.0.3/">http://hadoop.apache.org/common/docs/r1.0.3/</a></a></li>
<li>Apache Hadoop r1.0.3 中文文档 <a href="http://hadoop.apache.org/common/docs/r1.0.3/cn" target="_blank"><a href="http://hadoop.apache.org/common/docs/r1.0.3/cn">http://hadoop.apache.org/common/docs/r1.0.3/cn</a></a></li>
<li>CDH Downloads <a href="https://ccp.cloudera.com/display/SUPPORT/Downloads" target="_blank"><a href="https://ccp.cloudera.com/display/SUPPORT/Downloads">https://ccp.cloudera.com/display/SUPPORT/Downloads</a></a></li>
<li>CDH Documentation <a href="https://ccp.cloudera.com/display/DOC/Documentation" target="_blank"><a href="https://ccp.cloudera.com/display/DOC/Documentation">https://ccp.cloudera.com/display/DOC/Documentation</a></a></li>
<li>CDH Tutorial <a href="https://ccp.cloudera.com/display/SUPPORT/Hadoop+Tutorial" target="_blank"><a href="https://ccp.cloudera.com/display/SUPPORT/Hadoop+Tutorial">https://ccp.cloudera.com/display/SUPPORT/Hadoop+Tutorial</a></a></li>
</ul>
<h3 id="1-1-faq">1.1 FAQ</h3>
<h3 id="1-1-1-hadoop-">1.1.1 Hadoop可以用来做什么</h3>
<p>Why Hadoop? <a href="http://www.cloudera.com/why-hadoop/" target="_blank"><a href="http://www.cloudera.com/why-hadoop/">http://www.cloudera.com/why-hadoop/</a></a></p>
<p>TODO(dirlt):translate it!!!</p>
<p>Simply put, Hadoop can transform the way you store and process data throughout your enterprise. According to analysts, about 80% of the data in the world is unstructured, and until Hadoop, it was essentially unusable in any systematic way. With Hadoop, for the first time you can combine all your data and look at it as one.</p>
<ul>
<li>Make All Your Data Profitable. Hadoop enables you to gain insight from all the data you already have; to ingest the data flowing into your systems 24/7 and leverage it to make optimizations that were impossible before; to make decisions based on hard data, not hunches; to look at complete data, not samples; to look at years of transactions, not days or weeks. In short, Hadoop will change the way you run your organization.</li>
<li>Leverage All Types of Data, From All Types of Systems. Hadoop can handle all types of data from disparate systems: structured, unstructured, log files, pictures, audio files, communications records, email– just about anything you can think of. Even when different types of data have been stored in unrelated systems, you can dump it all into your Hadoop cluster before you even know how you might take advantage of it in the future.</li>
<li>Scale Beyond Anything You Have Today. The largest social network in the world is built on the same open-source technology as Hadoop, and now exceeds 100 petabytes. It’s unlikely your organization has that much data. As you need more capacity, you just add more commodity servers and Hadoop automatically incorporates the new storage and compute capacity.</li>
</ul>
<h3 id="1-1-2-hadoop-">1.1.2 Hadoop包括哪些组件</h3>
<p>TODO(dirlt):translate it!!!</p>
<p>Apache Hadoop包括了下面这些组件：</p>
<ul>
<li><a href="http://hadoop.apache.org/common/" target="_blank">Hadoop Common</a> The common utilities that support the other Hadoop subprojects.</li>
<li><a href="http://hadoop.apache.org/hdfs/" target="_blank">Hadoop Distributed File System(HDFS)</a> A distributed file system that provides high-throughput access to application data.</li>
<li><a href="http://hadoop.apache.org/mapreduce/" target="_blank">Hadoop MapReduce</a> A software framework for distributed processing of large data sets on compute clusters.</li>
</ul>
<p>和Apache Hadoop相关的组件有：</p>
<ul>
<li><a href="http://avro.apache.org/" target="_blank">Avro</a> A data serialization system.</li>
<li><a href="http://cassandra.apache.org/" target="_blank">Cassandra</a> A scalable multi-master database with no single points of failure.</li>
<li><a href="http://incubator.apache.org/chukwa/" target="_blank">Chukwa</a> A data collection system for managing large distributed systems.</li>
<li><a href="http://hbase.apache.org/" target="_blank">HBase</a> A scalable, distributed database that supports structured data storage for large tables.</li>
<li><a href="http://hive.apache.org/" target="_blank">Hive</a> A data warehouse infrastructure that provides data summarization and ad hoc querying.</li>
<li><a href="http://mahout.apache.org/" target="_blank">Mahout</a> A Scalable machine learning and data mining library.</li>
<li><a href="http://pig.apache.org/" target="_blank">Pig</a> A high-level data-flow language and execution framework for parallel computation.</li>
<li><a href="http://zookeeper.apache.org/" target="_blank">ZooKeeper</a> A high-performance coordination service for distributed applications.<h3 id="1-1-3-cdh-apache-hadoop-">1.1.3 CDH和Apache Hadoop的关系</h3>
</li>
</ul>
<p>CDH Hadoop FAQ <a href="https://ccp.cloudera.com/display/SUPPORT/Hadoop+FAQ" target="_blank"><a href="https://ccp.cloudera.com/display/SUPPORT/Hadoop+FAQ">https://ccp.cloudera.com/display/SUPPORT/Hadoop+FAQ</a></a></p>
<p>TODO(dirlt):translate it!!!</p>
<ul>
<li>What exactly is included in CDH? / Cloudera&#39;s Distribution Including Apache Hadoop (CDH) is a certified release of Apache Hadoop. We include some stable patches scheduled to be included in future releases, as well as some patches we have developed for our supported customers, and are in the process of contributing back to Apache.</li>
<li>What license is Cloudera&#39;s Distribution Including Apache Hadoop released under? / Just like Hadoop, Cloudera&#39;s Distribution Including Apache Hadoop is released under the Apache Public License version 2.</li>
<li>Is Cloudera forking Hadoop? / Absolutely not. Cloudera is committed to the Hadoop project and the principles of the Apache Software License and Foundation. We continue to work actively with current releases of Hadoop and deliver certified releases to the community as appropriate.</li>
<li>Does Cloudera contribute their changes back to Apache? / We do, and will continue to contribute all eligible changes back to Apache. We occasionally release code we know to be stable even if our contribution to Apache is still in progress. Some of our changes are not eligible for contribution, as they capture the Cloudera brand, or link to our tools and documentation, but these do not affect compatibility with core project.</li>
</ul>
<h3 id="1-1-4-cdh-">1.1.4 CDH产品组件构成</h3>
<p><a href="http://www.cloudera.com/content/cloudera/en/products/cdh.html" target="_blank"><a href="http://www.cloudera.com/content/cloudera/en/products/cdh.html">http://www.cloudera.com/content/cloudera/en/products/cdh.html</a></a></p>
<p>从这里可以下载CDH4组件 <a href="http://www.cloudera.com/content/cloudera-content/cloudera-docs/CDHTarballs/3.25.2013/CDH4-Downloadable-Tarballs/CDH4-Downloadable-Tarballs.html" target="_blank"><a href="http://www.cloudera.com/content/cloudera-content/cloudera-docs/CDHTarballs/3.25.2013/CDH4-Downloadable-Tarballs/CDH4-Downloadable-Tarballs.html">http://www.cloudera.com/content/cloudera-content/cloudera-docs/CDHTarballs/3.25.2013/CDH4-Downloadable-Tarballs/CDH4-Downloadable-Tarballs.html</a></a></p>
<p><img src="" alt="./images/cloudera-enterprise-diagram.png"></p>
<h3 id="1-1-5-cdh-">1.1.5 CDH产品组件端口分布和配置</h3>
<p>The CDH4 components, and third parties such as Kerberos, use the ports listed in the tables that follow. Before you deploy CDH4, make sure these ports are open on each system.</p>
<h3 id="1-1-5-1-hadoop-hdfs">1.1.5.1 Hadoop HDFS</h3>
<p>ServiceQualifierPortProtocolAccess RequirementConfigurationCommentDataNode50010TCPExternaldfs.datanode.addressDataNode HTTP server portDataNodeSecure1004TCPExternaldfs.datanode.addressDataNode50075TCPExternaldfs.datanode.http.addressDataNodeSecure1006TCPExternaldfs.datanode.http.addressDataNode50020TCPExternaldfs.datanode.ipc.addressNameNode8020TCPExternalfs.default.name or fs.defaultFSfs.default.name is deprecated (but still works)NameNode50070TCPExternaldfs.http.address or dfs.namenode.http-addressdfs.http.address is deprecated (but still works)NameNodeSecure50470TCPExternaldfs.https.address or dfs.namenode.https-addressdfs.https.address is deprecated (but still works)Sec NameNode50090TCPInternaldfs.secondary.http.address or dfs.namenode.secondary.http-addressdfs.secondary.http.address is deprecated (but still works)Sec NameNodeSecure50495TCPInternaldfs.secondary.https.addressJournalNode8485TCPInternaldfs.namenode.shared.edits.dirJournalNode8480TCPInternal</p>
<h3 id="1-1-5-2-hadoop-mrv1">1.1.5.2 Hadoop MRv1</h3>
<p>ServiceQualifierPortProtocolAccess RequirementConfigurationCommentJobTracker8021TCPExternalmapred.job.trackerJobTracker50030TCPExternalmapred.job.tracker.http.addressJobTrackerThrift Plugin9290TCPInternaljobtracker.thrift.addressRequired by Hue and Cloudera Manager Activity MonitorTaskTracker50060TCPExternalmapred.task.tracker.http.addressTaskTracker0TCPLocalhostmapred.task.tracker.report.addressCommunicating with child (umbilical)</p>
<h3 id="1-1-5-3-hadoop-yarn">1.1.5.3 Hadoop YARN</h3>
<p>ServiceQualifierPortProtocolAccess RequirementConfigurationCommentResourceManager8032TCPyarn.resourcemanager.addressResourceManager8030TCPyarn.resourcemanager.scheduler.addressResourceManager8031TCPyarn.resourcemanager.resource-tracker.addressResourceManager8033TCPyarn.resourcemanager.admin.addressResourceManager8088TCPyarn.resourcemanager.webapp.addressNodeManager8040TCPyarn.nodemanager.localizer.addressNodeManager8042TCPyarn.nodemanager.webapp.addressNodeManager8041TCPyarn.nodemanager.addressMapReduce JobHistory Server10020TCPmapreduce.jobhistory.addressMapReduce JobHistory Server19888TCPmapreduce.jobhistory.webapp.address</p>
<h3 id="1-1-5-4-hbase">1.1.5.4 HBase</h3>
<p>ServiceQualifierPortProtocolAccess RequirementConfigurationCommentMaster60000TCPExternalhbase.master.portIPCMaster60010TCPExternalhbase.master.info.portHTTPRegionServer60020TCPExternalhbase.regionserver.portIPCRegionServer60030TCPExternalhbase.regionserver.info.portHTTPHQuorumPeer2181TCPhbase.zookeeper.property.clientPortHBase-managed ZK modeHQuorumPeer2888TCPhbase.zookeeper.peerportHBase-managed ZK modeHQuorumPeer3888TCPhbase.zookeeper.leaderportHBase-managed ZK modeRESTREST Service8080TCPExternalhbase.rest.portThriftServerThrift Server9090TCPExternalPass -p <port> on CLIAvro server9090TCPExternalPass –port <port> on CLI</p>
<h3 id="1-1-5-5-hive">1.1.5.5 Hive</h3>
<p>ServiceQualifierPortProtocolAccess RequirementConfigurationCommentMetastore9083TCPExternalHiveServer10000TCPExternal</p>
<h3 id="1-1-5-6-sqoop">1.1.5.6 Sqoop</h3>
<p>ServiceQualifierPortProtocolAccess RequirementConfigurationCommentMetastore16000TCPExternalsqoop.metastore.server.portSqoop 2 server12000TCPExternal</p>
<h3 id="1-1-5-7-zookeeper">1.1.5.7 Zookeeper</h3>
<p>ServiceQualifierPortProtocolAccess RequirementConfigurationCommentServer (with CDH4 and/or Cloudera Manager 4)2181TCPExternalclientPortClient portServer (with CDH4 only)2888TCPInternalX in server.N=host:X:YPeerServer (with CDH4 only)3888TCPInternalY in server.N=host:X:YPeerServer (with CDH4 and Cloudera Manager 4)3181TCPInternalX in server.N=host:X:YPeerServer (with CDH4 and Cloudera Manager 4)4181TCPInternalY in server.N=host:X:YPeerZooKeeper FailoverController (ZKFC)8019TCPInternalUsed for HAZooKeeper JMX port9010TCPInternal</p>
<p>As JMX port, ZooKeeper will also use another randomly selected port for RMI. In order for Cloudera Manager to monitor ZooKeeper, you must open up all ports when the connection originates from the Cloudera Manager server.</p>
<h3 id="1-1-5-8-hue">1.1.5.8 Hue</h3>
<p>ServiceQualifierPortProtocolAccess RequirementConfigurationCommentServer8888TCPExternalBeeswax Server8002InternalBeeswax Metastore8003Internal</p>
<h3 id="1-1-5-9-ozzie">1.1.5.9 Ozzie</h3>
<p>ServiceQualifierPortProtocolAccess RequirementConfigurationCommentOozie Server11000TCPExternalOOZIE_HTTP_PORT in oozie-env.shHTTPOozie Server11001TCPlocalhostOOZIE_ADMIN_PORT in oozie-env.shShutdown port</p>
<h3 id="1-1-5-10-ganglia">1.1.5.10 Ganglia</h3>
<p>ServiceQualifierPortProtocolAccess RequirementConfigurationCommentganglia-gmond8649UDP/TCPInternalganglia-web80TCPExternalVia Apache httpd</p>
<h3 id="1-1-5-11-kerberos">1.1.5.11 Kerberos</h3>
<p>ServiceQualifierPortProtocolAccess RequirementConfigurationCommentKRB5 KDC ServerSecure88UDP/TCPExternalkdc_ports and kdc_tcp_ports in either the [kdcdefaults] or [realms] sections of kdc.confBy default only UDPKRB5 Admin ServerSecure749TCPInternalkadmind_port in the [realms] section of kdc.conf</p>
<h3 id="1-2-">1.2 观点</h3>
<h3 id="1-2-1-hadoop-">1.2.1 Hadoop即将过时了吗？</h3>
<p><a href="http://www.kuqin.com/database/20120715/322528.html" target="_blank"><a href="http://www.kuqin.com/database/20120715/322528.html">http://www.kuqin.com/database/20120715/322528.html</a></a></p>
<p>google提出的三个东西都是解决hadoop的软肋，最终目的还是需要解决大数据上面的实时性问题。</p>
<ul>
<li>增量索引过滤器（Percolator for incremental indexing）和频繁变化数据集分析。Hadoop是一台大型“机器”，当启动并全速运转时处理数据的性能惊人，你唯一需要操心的就是硬盘的传输速度跟不上。但是每次你准备启动分析数据时，都需要把所有的数据都过一遍，当数据集越来越庞大时，这个问题将导致分析时间无限延长。那么Google是如何解决让搜索结果返回速度越来越接近实时的呢？答案是用增量处理引擎Percolator代替GMR。通过只处理新增的、改动过的或删除的文档和使用二级指数来高效率建目录，返回查询结果。Percolator论文的作者写道：“将索引系统转换成增量系统…将文档处理延迟缩短了100倍。”这意味着索引web新内容的速度比用MapReduce快100倍！类似大型强子对撞机产生的数据将不断变大，Twitter也是如此。这也是为什么HBase中会新增触发流程，而Twitter Storm正在成为实时处理流数据的热门技术。</li>
<li>用于点对点分析的Dremel。Google和Hadoop生态系统都致力于让MapReduce成为可用的点对点分析工具。从Sawzall到Pig和Hive，创建了大量的界面层，但是尽管这让Hadoop看上去更像SQL系统，但是人们忘记了一个基本事实——MapReduce(以及Hadoop)是为组织数据处理任务开发的系统，诞生于工作流内核，而不是点对点分析。今天有大量的BI/分析查询都是点对点模式，属于互动和低延迟的分析。Hadoop的Map和Reduce工作流让很多分析师望而却步，而且工作启动和完成工作流运行的漫长周期对于很多互动性分析来说意味着糟糕的用户体验。于是，Google发明了Dremel（业界也称之为BigQuery产品）专用工具，可以让分析师数秒钟内就扫描成PB（Petabyte）的数据完成点到点查询，而且还能支持可视化。Google在Dremel的论文中声称：“Dremel能够在数秒内完成数万亿行数据的聚合查询，比MapReduce快上100倍！”</li>
<li>分析图数据的Pregel。Google MapReduce的设计初衷是分析世界上最大的数据图谱——互联网。但是在分析人际网络、电信设备、文档和其他一些图数据时就没有那么灵光了，例如MapReduce在计算单源最短路径（SSSP）时效率非常低下，已有的并行图算法库Parallel BGL或者CGMgraph又没有容错。于是Google开发了Pregel，一个可以在分布式通用服务器上处理PB级别图数据的大型同步处理应用。与Hadoop经常在处理图数据时产生指数级数据放大相比，Pregel能够自然高效地处理SSSP或PageRank等图算法，所用时间要短得多，代码也简洁得多。目前唯一能与Pregel媲美的开源选择是Giraph，这是一个早期的Apache孵化项目，调用了HDFS和Zookeeper。Githb上还有一个项目Golden Orb可用。</li>
</ul>
<h3 id="1-2-2-best-practices-for-selecting-apache-hadoop-hardware">1.2.2 Best Practices for Selecting Apache Hadoop Hardware</h3>
<p><a href="http://hortonworks.com/blog/best-practices-for-selecting-apache-hadoop-hardware/" target="_blank"><a href="http://hortonworks.com/blog/best-practices-for-selecting-apache-hadoop-hardware/">http://hortonworks.com/blog/best-practices-for-selecting-apache-hadoop-hardware/</a></a></p>
<p>RAID cards, redundant power supplies and other per-component reliability features are not needed. Buy error-correcting RAM and SATA drives with good MTBF numbers. Good RAM allows you to trust the quality of your computations. Hard drives are the largest source of failures, so buy decent ones.（不需要选购RAID，冗余电源或者是一些满足高可靠性组件，但是选择带有ECC的RAM以及good MTBF的SATA硬盘却是非常需要的。ECC RAM可以让你确保计算结果的正确性，而SATA故障是大部分故障的主要原因）</p>
<ul>
<li>On CPU: It helps to understand your workload, but for most systems I recommend sticking with medium clock speeds and no more than 2 sockets. Both your upfront costs and power costs rise quickly on the high-end. For many workloads, the extra performance per node is not cost-effective.（没有特别要求，普通频率，dual-socket？？？）</li>
<li>On Power: Power is a major concern when designing Hadoop clusters. It is worth understanding how much power the systems you are buying use and not buying the biggest and fastest nodes on the market.In years past we saw huge savings in pricing and significant power savings by avoiding the fastest CPUs, not buying redundant power supplies, etc. Nowadays, vendors are building machines for cloud data centers that are designed to reduce cost and power and that exclude a lot of the niceties that bulk up traditional servers. Spermicro, Dell and HP all have such product lines for cloud providers, so if you are buying in large volume, it is worth looking for stripped-down cloud servers. （根据自己的需要尽量减少能耗开销，撇去一些不需要的部件。而且现在很多厂商也在尽量减少不必要的部件）</li>
<li>On RAM: What you need to consider is the amount of RAM needed to keep the processors busy and where the knee in the cost curve resides. Right now 48GB seems like a pretty good number. You can get this much RAM at commodity prices on low-end server motherboards. This is enough to provide the Hadoop framework with lots of RAM (~4 GB) and still have plenty to run many processes. Don’t worry too much about RAM, you’ll find a use for it, often running more processes in parallel. If you don’t, the system will still use it to good effect, caching disk data and improving performance.（RAM方面的话越大越好，对于48GB的RAM来说普通的主板也是支持的。如果RAM用的上的话那么允许多个进程并行执行，如果暂时永不上的话可以做cache来提高速度）</li>
<li>On Disk: Look to buy high-capacity SATA drives, usually 7200RPM. Hadoop is storage hungry and seek efficient but it does not require fast, expensive hard drives. Keep in mind that with 12-drive systems you are generally getting 24 or 36 TB/node. Until recently, putting this much storage in a node was not practical because, in large clusters, disk failures are a regular occurrence and replicating 24+TB could swamp the network for long enough to really disrupt work and cause jobs to miss SLAs. The most recent release of Hadoop 0.20.204 is engineered to handle the failure of drives more elegantly, allowing machines to continue serving from their remaining drives. With these changes, we expect to see a lot of 12+ drive systems. In general, add disks for storage and not seeks. If your workload does not require huge amounts of storage, dropping disk count to 6 or 4 per box is a reasonable way to economize.（高容量SATA硬盘，最好是7.2KRPM，并且最好单机上面挂在12个硬盘。对于hadoop之前这种方式并不实际，因为磁盘非常容易损坏并且备份这24TB的数据非常耗时。而hadoop可以很好地解决这个问题。</li>
</ul>
<p>小集群来说的话，通常单个机器上面挂在4-6个disk即可）</p>
<ul>
<li>On Network: This is the hardest variable to nail down. Hadoop workloads vary a lot. The key is to buy enough network capacity to allow all nodes in your cluster to communicate with each other at reasonable speeds and for reasonable cost. For smaller clusters, I’d recommend at least 1GB all-to-all bandwidth, which is easily achieved by just connecting all of your nodes to a good switch. With larger clusters this is still a good target although based on workload you can probably go lower. In the very large data centers the Yahoo! built, they are seeing 2/<em>10GB per 20 node rack going up to a pair of central switches, with rack nodes connected with two 1GB links. As a rule of thumb, watch the ratio of network-to-computer cost and aim for network cost being somewhere around 20% of your total cost. Network costs should include your complete network, core switches, rack switches, any network cards needed, etc. We’ve been seeing InfiniBand and 10GB Ethernet networks to the node now. If you can build this cost effectively, that’s great. However, keep in mind that Hadoop grew up with commodity Ethernet, so understand your workload requirements before spending too much on the network.（这个主要还是看需求。通常来说网络整体开销占据所有开销的20%，包括核心交换机，机架之间的交换机以及网卡设备等。yahoo大集群的部署方式是rack之间使用2/</em>10GB的核心交换机工作，而20个节点的rack之间内部使用1GB链路）。<h3 id="1-2-3-the-dark-side-of-hadoop-backtype-technology">1.2.3 The dark side of Hadoop - BackType Technology</h3>
</li>
</ul>
<p><a href="http://web.archive.org/web/20110510125644/http://tech.backtype.com/the-dark-side-of-hadoop" target="_blank"><a href="http://web.archive.org/web/20110510125644/http://tech.backtype.com/the-dark-side-of-hadoop">http://web.archive.org/web/20110510125644/http://tech.backtype.com/the-dark-side-of-hadoop</a></a></p>
<p>谈到了一些在使用hadoop出现的一些问题，而这些问题是hadoop本身的。</p>
<ul>
<li>Critical configuration poorly documented 一些关键的参数和配置并没有很好地说明清楚。</li>
<li><p>Terrible with memory usage 内存使用上面存在问题。hadoop里面有一些非常sloppy的实现，比如chmod以及ln -s等操作，并没有调用fs API而是直接创建一个shell进程来完成。因为fork出一个shell进程需要申请同样大小的内存（虽然实现上是COW），但是这样造成jvm出现oom。解决的办法是开辟一定空间的swap The solution to these memory problems is to allocate a healthy amount of swap space for each machine to protect you from these memory glitches. We couldn&#39;t believe how much more stable everything became when we added swap space to our worker machines.</p>
</li>
<li><p>Thomas Jungblut&#39;s Blog: Dealing with &quot;OutOfMemoryError&quot; in Hadoop <a href="http://codingwiththomas.blogspot.jp/2011/07/dealing-with-outofmemoryerror-in-hadoop.html" target="_blank"><a href="http://codingwiththomas.blogspot.jp/2011/07/dealing-with-outofmemoryerror-in-hadoop.html">http://codingwiththomas.blogspot.jp/2011/07/dealing-with-outofmemoryerror-in-hadoop.html</a></a> 作者给出的解决办法就是修改hadoop的代码，通过调用Java API而不是使用ProcessBuilder来解决。</p>
</li>
<li><strong>NOTE(dirlt):出现OOM的话必须区分JVM还是Linux System本身的OOM。JVM出现OOM是抛出异常，而Linux出现OOM是会触发OOM killer</strong></li>
<li>Zombies hadoop集群出现一些zombie进程，而这些进程会一直持有内存直到大量zombie进程存在最后需要重启。造成这些zombie进程的原因通常是因为jvm oom（增加了swap之后就没有出现这个问题了），但是奇怪的是tasktracker作为这些process的parent，并不负责cleanup这些zombie进程而是依赖这些zombie进程的自己退出，这就是hadoop设计方面的问题。</li>
</ul>
<p>Making Hadoop easy to deploy, use, and operate should be the /#1 priority for the developers of Hadoop.</p>
<h3 id="1-3-">1.3 使用问题</h3>
<h3 id="1-3-1-cdh3u3-">1.3.1 CDH3u3搭建单节点集群</h3>
<p>搭建单节点集群允许我们在单机做一些模拟或者是测试，还是非常有意义的。如何操作的话可以参考链接 <a href="http://localhost/utils/hadoop-0.20.2-cdh3u3/docs/single_node_setup.html" target="_blank"><a href="http://localhost/utils/hadoop-0.20.2-cdh3u3/docs/single_node_setup.html">http://localhost/utils/hadoop-0.20.2-cdh3u3/docs/single_node_setup.html</a></a></p>
<p>这里稍微总结一下：</p>
<ul>
<li>首先安装ssh和rsync /# sudo apt-get install ssh &amp;&amp; sudo apt-get install rsync</li>
<li>本机建立好信任关系 /# cat ~/.ssh/id_dsa.pub &gt;&gt; ~/.ssh/authorized_keys</li>
<li>将{hadoop-package}/conf配置文件修改如下：</li>
<li><p>conf/core-site.xml</p>
<configuration>

   <property>
       <name>fs.default.name</name>

       <value>hdfs://localhost:9000</value>
   </property>

</li>
</ul>
<p></configuration></p>
<ul>
<li><p>conf/hdfs-site.xml</p>
<configuration>

   <property>
       <name>dfs.replication</name>

       <value>1</value>
   </property>

</li>
</ul>
<p></configuration></p>
<ul>
<li><p>conf/mapred-site.xml</p>
<configuration>

   <property>
       <name>mapred.job.tracker</name>

       <value>localhost:9001</value>
   </property>

</li>
</ul>
<p></configuration></p>
<ul>
<li>格式化namenode /# bin/hadoop namenode -format</li>
<li>启动hadoop集群 /# bin/start-all.sh</li>
<li>停止hadoop集群 /# bin/stop-all.sh</li>
<li><p>webconsole</p>
</li>
<li><p>NameNode - <a href="http://localhost:50070/" target="_blank"><a href="http://localhost:50070/">http://localhost:50070/</a></a></p>
</li>
<li>JobTracker - <a href="http://localhost:50030/" target="_blank"><a href="http://localhost:50030/">http://localhost:50030/</a></a></li>
</ul>
<h3 id="1-3-2-cdh4-2-0-">1.3.2 CDH4.2.0搭建单节点集群</h3>
<p>基本流程和CDH3u3是相同的，但是有一些差异我记录下来。</p>
<ul>
<li><p>配置文件</p>
</li>
<li><p>配置文件在etc/hadoop，包括环境配置脚本比如hadoop-env.sh</p>
</li>
<li>bin/sbin目录下面有hadoop集群启动停止工具 <strong>NOTE（dirlt）：不要使用它们</strong></li>
<li>libexec目录下面是公用的配置脚本</li>
<li>mapred-site.xml中jobtracker地址配置key修改为 mapred.jobtracker.address <strong>NOTE(dirlt):this for yarn.如果是mr1那么不用修改,依然是mapred.job.tracker</strong></li>
<li>hadoop-daemons.sh会使用/sbin/slaves.sh来在各个节点启动，但是 /<em>不知道什么原因，很多环境变量没有设置/</em> ，所以在slaves.sh执行ssh命令部分最开始增加了 source ~/.shrc; 来强制设置我的环境变量</li>
<li><strong>NOTE(dirlt):不要使用shell脚本来启动，而是直接使用类似hadoop namenode这种方式来启动单个机器上的实例</strong></li>
<li><p>公共组件</p>
</li>
<li><p>CDH4.2.0 native-library都放在了目录lib/native下面，而不是CDH3u3的lib/native/Linux-amd64-64下面，这点需要注意。</p>
</li>
<li>CDH4.2.0 没有自带libhadoop.so, 所以启动的时候都会出现 ”Unable to load native-hadoop library for your platform… using builtin-java classes where applicable“ 这个警告。需要自己编译放到lib/native目录下面。</li>
<li>CDH4.2.0 lib下面没有任何文件，所有的lib都在share/hadoop//*/lib下面，比如share/hadoop/common/lib. 这点和CDH3有差别，CDH3所有的jar都放在lib目录下面。使用 hadoop classpath 命令可以察看</li>
<li><p>环境变量</p>
</li>
<li><p>JAVA_LIBRARY_PATH用来设置native library path</p>
</li>
<li>HADOOP_CLASSPATH可以用来设置hadoop相关的classpath（比如使用hadoop-lzo等）</li>
<li><p>准备工作</p>
</li>
<li><p>使用hdfs namenode -format来做格式化 <strong>注意如果使用sudo apt-get来安装的话，是其他用户比如hdfs,impala,mapred,yarn来启动的，所以必须确保目录对于这些用户是可写的</strong></p>
</li>
<li>使用命令 hadoop org/apache/hadoop/examples/QuasiMonteCarlo 1 1 确定集群是否可以正常运行。<h3 id="1-3-3-cdh4-3-0">1.3.3 CDH4.3.0</h3>
</li>
</ul>
<p>基本流程和CDH4.2.0是相同的，但是存在一些差异我记录下来的。从4.3.0开始将mr1和mr2分开存放，还是一个比较大的区别的。这里我以使用mr1为例。</p>
<ul>
<li>在libexec/hadoop-config.sh添加source ~/.shrc 来强制设置环境变量。</li>
<li><p>mr1和mr2分开存放主要有</p>
</li>
<li><p>etc目录，hadoop and hadoop-mapreduce1</p>
</li>
<li>bin目录，bin and bin-mapreduce1</li>
<li><p>lib目录。如果需要使用mr1的话，那么将cp -r share/hadoop/mapreduce1/ .</p>
</li>
<li><p><strong>NOTE（dirlt）：似乎只需要最顶层的一些jar文件即可</strong></p>
</li>
<li>在bin/hadoop-config.sh添加source ~/.shrc 来强制设置环境变量。</li>
<li><strong>NOTE（dirlt）：不要使用start-dfs.sh这些脚本启动，似乎这些脚本会去读取master,slaves这些文件然后逐个上去ssh启动。直接使用hadoop namenode这种方式可以只启动单个机器上的实例</strong></li>
</ul>
<h3 id="1-3-4-configuration">1.3.4 Configuration</h3>
<h3 id="1-3-4-1-bash_profile">1.3.4.1 .bash_profile</h3>
<p>export HADOOP_HOME=$HOME/dirlt/hadoop-2.0.0-cdh4.3.0/</p>
<p>export HADOOP_CONF_DIR=$HADOOP_HOME/etc/hadoop
export HBASE_HOME=/home/alium_zhanyinan/dirlt/hbase-0.94.6-cdh4.3.0</p>
<p>export HBASE_CLASSPATH=$HBASE_HOME/hbase-0.94.6-cdh4.3.0-security.jar:$HBASE_HOME/conf
export ZK_HOME=/home/alium_zhanyinan/dirlt/zookeeper-3.4.5-cdh4.3.0</p>
<p>export ZK_CLASSPATH=$ZK_HOME/zookeeper-3.4.5-cdh4.3.0.jar
export HADOOP_CLASSPATH=$HADOOP_CLASSPATH:$HBASE_CLASSPATH:$ZK_CLASSPATH</p>
<p>export JAVA_HOME=/usr/java/default/</p>
<h3 id="1-3-4-2-core-site-xml">1.3.4.2 core-site.xml</h3>
<configuration>

  <property>
    <name>fs.default.name</name>

    <value>hdfs://umengds1.mob.cm3:8020</value>
  </property>


  <property>

    <name>fs.trash.interval</name>
    <value>1440</value>

  </property>
</configuration>


<h3 id="1-3-4-3-hdfs-site-xml">1.3.4.3 hdfs-site.xml</h3>
<configuration>

  <property>
    <name>dfs.name.dir</name>

    <value>/disk1/data/dfs/nn</value>
  </property>


  <property>

    <name>dfs.data.dir</name>
    <value>/disk1/data/dfs/dn</value>

  </property>


  <property>
    <name>fs.checkpoint.dir</name>

    <value>/disk1/data/dfs/snn</value>
  </property>


  <property>

    <name>dfs.replication</name>
    <value>3</value>

  </property>


  <property>
    <name>dfs.block.size</name>

    <value>134217728</value>
  </property>


  <property>

    <name>dfs.datanode.max.xcievers</name>
    <value>8192</value>

  </property>


  <property>
    <name>dfs.datanode.du.reserved</name>

    <value>21474836480</value>
  </property>


  <property>

    <name>dfs.namenode.handler.count</name>
    <value>64</value>

  </property>


  <property>
    <name>dfs.datanode.handler.count</name>

    <value>32</value>
  </property>


  <property>

    <name>dfs.client.read.shortcircuit</name>
    <value>true</value>

  </property>
</configuration>



<h3 id="1-3-4-4-mapred-site-xml">1.3.4.4 mapred-site.xml</h3>
<configuration>

  <property>
    <name>mapred.job.tracker</name>

    <value>umengds2.mob.cm3:8021</value>
  </property>


  <property>

    <name>mapred.system.dir</name>
    <value>/tmp/mapred/system</value>

  </property>


  <property>
    <name>mapreduce.jobtracker.staging.root.dir</name>

    <value>/user</value>
  </property>


  <property>

    <name>mapred.local.dir</name>
    <value>/disk1/data/mapred/local</value>

  </property>


  <property>
    <name>mapred.submit.replication</name>

    <value>3</value>
    <final>true</final>

  </property>


  <property>
    <name>mapred.tasktracker.map.tasks.maximum</name>

    <value>6</value>
  </property>

  <property>
    <name>mapred.tasktracker.reduce.tasks.maximum</name>

    <value>8</value>
  </property>


  <property>

    <name>mapred.child.java.opts</name>
    <value> -Xmx2048M -XX:-UseGCOverheadLimit</value>

  </property>


  <property>
    <name>mapred.job.tracker.handler.count</name>

    <value>64</value>
  </property>


  <property>

    <name>io.sort.mb</name>
    <value>256</value>

  </property>


  <property>
    <name>io.sort.factor</name>

    <value>64</value>
  </property>

</configuration>

<h3 id="1-3-4-5-hadoop-env-sh">1.3.4.5 hadoop-env.sh</h3>
<p>/# The maximum amount of heap to use, in MB. Default is 1000.</p>
<p>export HADOOP_HEAPSIZE=6000</p>
<p>/# Extra Java runtime options. Empty by default.
/# if [&quot;$HADOOP_OPTS&quot; == &quot;&quot; ]; then export HADOOP_OPTS=-server; else HADOOP_OPTS+=&quot; -server&quot;</p>
<p>; fi</p>
<p>/# Command specific options appended to HADOOP_OPTS when specified
export HADOOP_NAMENODE_OPTS=&quot;-Xmx12000m $HADOOP_NAMENODE_OPTS&quot;export HADOOP_SECONDARYNAMENODE_OPTS=&quot;-Xmx12000m $HADOOP_SECONDARYNAMENODE_OPTS&quot;export HADOOP_DATANODE_OPTS=&quot;-Xmx6000m $HADOOP_DATANODE_OPTS&quot;export HADOOP_BALANCER_OPTS=&quot;-Xmx3000m $HADOOP_BALANCER_OPTS&quot;export HADOOP_JOBTRACKER_OPTS=&quot;-Xmx12000m $HADOOP_JOBTRACKER_OPTS&quot;</p>
<h3 id="1-3-4-6-hbase-site-xml">1.3.4.6 hbase-site.xml</h3>
<configuration>

  <property>
    <name>hbase.cluster.distributed</name>

    <value>true</value>
  </property>


  <property>

    <name>hbase.rootdir</name>
    <value>hdfs://umengds1.mob.cm3:8020/hbase</value>

  </property>


  <property>
    <name>hbase.zookeeper.quorum</name>

    <value>umengds1.mob.cm3,umengds2.mob.cm3</value>
  </property>


  <property>

    <name>hbase.hregion.memstore.mslab.enabled</name>
    <value>true</value>

  </property>


  <property>
    <name>hbase.regionserver.handler.count</name>

    <value>128</value>
  </property>


  <property>

    <name>hbase.client.write.buffer</name>
    <value>4194304</value>

  </property>


  <property>
    <name>hbase.hregion.memstore.block.multiplier</name>

    <value>8</value>
  </property>


  <property>

    <name>hbase.server.thread.wakefrequency</name>
    <value>1000</value>

  </property>


  <property>
    <name>hbase.regionserver.lease.period</name>

    <value>600000</value>
  </property>


  <property>

    <name>hbase.hstore.blockingStoreFiles</name>
    <value>15</value>

  </property>


  <property>
    <name>hbase.hregion.max.filesize</name>

    <value>2147483648</value>
  </property>


  <property>

    <name>hbase.ipc.client.tcpnodelay</name>
    <value>true</value>

  </property>


  <property>
    <name>ipc.ping.interval</name>

    <value>10000</value>
  </property>


  <property>

    <name>hbase.hregion.majorcompaction</name>
    <value>0</value>

  </property>


  <property>
    <name>hbase.regionserver.checksum.verify</name>

    <value>true</value>
  </property>

</configuration>

<h3 id="1-3-4-7-hbase-env-sh">1.3.4.7 hbase-env.sh</h3>
<p>/# The maximum amount of heap to use, in MB. Default is 1000.</p>
<p>export HBASE_HEAPSIZE=14000</p>
<p>/# Extra Java runtime options.
/# Below are what we set by default. May only work with SUN JVM.</p>
<p>/# For more on why as well as other possible settings,
/# see <a href="http://wiki.apache.org/hadoop/PerformanceTuning" target="_blank">http://wiki.apache.org/hadoop/PerformanceTuning</a></p>
<p>/# export HBASE_OPTS=
&quot;-ea -XX:+UseConcMarkSweepGC -XX:+CMSIncrementalMode&quot;export HBASE_OPTS=&quot;-ea -XX:+UseParNewGC -XX:+UseConcMarkSweepGC -XX:+UseCMSCompactAtFullCollection -XX:CMSInitiatingOccupancyFraction=90&quot;</p>
<h3 id="1-4-hadoop-">1.4 Hadoop权威指南</h3>
<h3 id="1-4-1-hadoop">1.4.1 初识Hadoop</h3>
<p>古代，人们用牛来拉中午，当一头牛拉不动一根圆木的时候，他们不曾想过培育更大更壮的牛。同样，我们也不需要尝试开发超级计算机，而应试着结合使用更多计算机系统。</p>
<h3 id="1-4-2-mapreduce">1.4.2 关于MapReduce</h3>
<ul>
<li>设置HADOOP_CLASSPATH就可以直接使用hadoop CLASSNAME来在本地运行mapreduce程序。</li>
<li><p>hadoop jar $HADOOP_HOME/contrib/streaming/hadoop-streaming-0.20.2-cdh3u3.jar 可以用来启动streaming任务</p>
</li>
<li><p>使用stdin/stdout来作为输入和输出</p>
</li>
<li><p><strong>NOTE（dirlt）：倒是可以探索一下如何使用，但是觉得能力有限</strong></p>
</li>
<li><p>Input/Output Format</p>
</li>
<li>外围环境的访问比如访问hdfs以及hbase</li>
<li>程序打包。比如使用很多第三方库的话在其他机器上面没有部署。</li>
<li><p>hadoop pipes 可以用来启动pipes任务</p>
</li>
<li><p>Hadoop的Pipes是Hadoop MapReduce的C++接口代称</p>
</li>
<li>使用Unix Domain Socket来作为输入和输出</li>
<li><p><strong>NOTE（dirlt）：可能使用上面还是没有native mr或者是streaming方式方便</strong></p>
<h3 id="1-4-3-hadoop-">1.4.3 Hadoop分布式文件系统</h3>
</li>
<li><p>使用hadoop archive能够将大量小文档打包，存档文件之能够只读访问</p>
</li>
<li><p>使用hadoop archive -archiveName <file>.har -p <parent-path> src dst</p>
</li>
<li><p>存档过程使用mapreduce完成，输出结果为目录</p>
</li>
<li><p>part-0 表示存档内容文件，应该是使用一个reduce做聚合。</p>
</li>
<li>_index,_masterindex 是对存档内容文件的索引文件。</li>
<li><p>har(hadoop archive)文件系统是建立在其他文件系统上面的，比如hdfs或者是local fs.</p>
</li>
<li><p>hadoop fs -ls har:///file.har 那么访问的是默认的文件系统上面的file.har</p>
</li>
<li>如果想显示地访问hdfs文件系统的话，那么可以hadoop fs -ls har://hdfs-localhost:9000/file.har</li>
<li>如果想显示地访问本地文件系统的话，那么可以使用hadoop fs -ls har://file-localhost/file.har</li>
<li>hadoop fs -ls har://schema-<host>/<path> 是通用的访问方式</li>
</ul>
<h3 id="1-4-4-hadoop-io">1.4.4 Hadoop IO</h3>
<ul>
<li><p>文件系统</p>
</li>
<li><p>ChecksumFileSystem</p>
</li>
<li><p>使用decorator设计模式，底层filesystem称为RawFileSystem</p>
</li>
<li>对于每个文件filename都会创建.filename.crc文件存储校验和</li>
<li>计算crc的单位大小通过io.bytes.per.checksum来进行控制</li>
<li>读取文件如果出现错误的话，那么会抛出ChecksumException</li>
<li><p>考虑到存在多副本的情况，如果读取某个副本出错的话，期间那么会调用reportChecksumFailure方法</p>
</li>
<li><p><strong>NOTE（dirlt）：这个部分的代码不太好读，非常绕</strong></p>
</li>
<li><p>RawLocalFileSystem</p>
</li>
<li><p>本地文件系统</p>
</li>
<li><p>LocalFileSystem</p>
</li>
<li><p>RawLocalFileSystem + ChecksumFileSystem</p>
</li>
<li>reportChecksumFailure实现为将校验和存在问题的文件移动到bad_files边际文件夹（side directory）</li>
<li><p>DistributedFileSystem</p>
</li>
<li><p>分布式文件系统</p>
</li>
<li><p>ChecksumDistributedFileSystem</p>
</li>
<li><p>DistributedFileSystem + ChecksumFileSystem</p>
</li>
<li><p>压缩解压</p>
</li>
<li><p>DEFLATE org.apache.hadoop.io.compress.DefaultCodec 扩展名.defalte</p>
</li>
<li>Gzip org.apache.hadoop.io.compress.GzipCodec 扩展名.gz 使用DEFLATE算法但是增加了额外的文件头。</li>
<li>bzip2 org.apache.hadoop.io.compress.BZip2Codec 扩展名.bz2 自身支持文件切分，内置同步点。</li>
<li><p>LZO com.hadoop.compression.lzo.LzopCodec 扩展名.lzo 和lzop工具兼容，LZO算法增加了额外的文件头。</p>
</li>
<li><p>LzopCodec则是纯lzo格式的codec,使用.lzo_deflate作为文件扩展名</p>
</li>
<li>因为LZO代码库拥有GPL许可，因此没有办法包含在Apache的发行版本里面。</li>
<li><p>运行MapReduce时候可能需要针对不同压缩文件解压读取，就需要构造CompressionCodec对象，我们可以通过CompressionCodecFactory来构造这个对象</p>
</li>
<li><p>CompressionCodecFactory读取变量io.compression.codecs</p>
</li>
<li>然后根据输入文件的扩展名来选择使用何种codec.</li>
<li>getDefaultExtension</li>
<li><p>压缩和解压算法可能同时存在Java实现和原生实现</p>
</li>
<li><p>如果是原生实现的话通常是.so，那么需要设置java.library.path或者是在环境变量里面设置LD_LIBRARY_PATH</p>
</li>
<li>如果同时有原生实现和Java实现，我们想只是使用原生实现的话，那么可以设置hadoop.native.lib = false来禁用原生实现。</li>
<li><p>压缩算法涉及到对应的InputFormat,也就涉及到是否支持切分</p>
</li>
<li><p>对于一些不支持切分的文件，可能存在一些外部工具来建立索引，从而支持切分。</p>
</li>
<li><p>下面这些选项可以针对map结果以及mapreduce结果进行压缩</p>
</li>
<li><p>mapred.output.compress = true 将mapreduce结果做压缩</p>
</li>
<li>mapred.output.compression.codec mapreduce压缩格式</li>
<li>mapred.output.compress.type = BLOCK/RECORD 如果输出格式为SequenceFile的话，那么这个参数可以控制是块压缩还是记录压缩</li>
<li><strong>NOTE（dirlt）：我现在强烈感觉MR的中间结果存储格式为SequenceFile</strong></li>
<li><strong>NOTE（dirlt）：应该是IFile，但是是否共享了这个配置呢？</strong></li>
<li>mapred.compress.map.output = true 将map结果做压缩</li>
<li><p>mapred.map.output.compression.codec map压缩格式</p>
</li>
<li><p>序列化</p>
</li>
<li><p>Hadoop的序列化都是基于Writable实现的，WritableComparable则是同时继承Writable,Comparable<T>.</p>
</li>
<li><p>序列化对象需要实现RawComparator，接口为public int compare(byte[] b1, int s1, int l1, byte[] b2, int s2, int l2)进行二进制比较。</p>
</li>
<li><p>WritableComparator简化了这个实现，继承WritableComparator就实现了这个接口</p>
</li>
<li>但是这个接口实现起来非常naive，就是将两个byte stream反序列化然后调用对象的compareTo实现</li>
<li>如果想要提高效率的话，可以考虑通过直接比较两个byte stream来做优化。</li>
<li><p>基于文件的数据结构</p>
</li>
<li><p>SequenceFile 主要用来存储KV数据结构，多条记录之间会穿插一些同步标记，因此允许进行切分。</p>
</li>
<li><p>使用SequenceFileInputFormat和SequenceFileOutputFormat来读取和输出SequenceFile</p>
</li>
<li>hadoop fs -text 可以用来读取文件</li>
<li><p>mapred.output.compress.type = BLOCK/RECORD 可以用来控制压缩方式</p>
</li>
<li><p>如果没有使用压缩的话，那么格式为 recordLength(4byte) + keyLength(4byte) + key + value</p>
</li>
<li>如果使用记录压缩的话，那么格式为 recordLnegth(4byte) + keyLength(4byte) + key + compressedValue</li>
<li>如果使用块压缩的话，那么格式为 numberRecord(1-5byte) + keyLength(4byte) + compressedKeys + valueLength(4byte) + compressedValues.每个block之间会插入sync标记</li>
<li>块压缩大小可以使用io.seqfile.compress.blocksize来控制，默认1MB</li>
<li><p>MapFile 也是用来存储KV数据结构，但是可以认为已经按照了Key进行排序 <strong>NOTE（dirlt）：要求添加顺序就按照Key排序</strong></p>
</li>
<li><p>存储格式实际上也是SequenceFile，data，index都是。</p>
</li>
<li>底层会建立index，index在搜索的时候会加载到内存里面，这样可以减少data上的随机查询次数。</li>
<li>使用io.map.index.interval可以控制多少个item在index里面创建一个条目</li>
<li>使用io.map.index.skip = 0/1/2/n 可以控制skip几个index的item，如果为1的话那么表示只是使用1/2的索引。</li>
<li><p>从SequenceFile创建MapFile非常简单</p>
</li>
<li><p>首先使用sort将SequenceFile进行排序(可以使用hadoop example的sort）</p>
</li>
<li><p>然后调用hadoop MapFileFixer来建立索引</p>
<h3 id="1-4-5-mapreduce-">1.4.5 MapReduce应用开发</h3>
</li>
<li><p>Configuration用来读取配置文件，功能还是比较强大的，有变量替换的功能</p>
</li>
<li><property><name>…</name><value>…</value></property></li>
<li>如果使用<final>true</final>标记的话那么这个变量不允许被重置</li>
<li>变量替换可以使用${variable}</li>
<li><p>通过addResource来添加读取的配置文件</p>
</li>
<li><p>Hadoop集群有三种工作方式，分别为</p>
</li>
<li><p>standalone 使用单个JVM进程来模拟</p>
</li>
<li><p>如果不进行任何配置的话默认使用这个模式 <strong>NOTE（dirlt）：这个模式确实不错</strong></p>
</li>
<li>fs.default.name = file 本地文件系统</li>
<li>mapred.job.tracker = local</li>
<li><p>pseudo-distributed 本地启动单节点集群</p>
</li>
<li><p>fs.default.name = hdfs://localhost</p>
</li>
<li>mapred.job.tracker = localhost:8021</li>
<li><p>fully-distributed 完全分布式环境</p>
</li>
<li><p>fs.default.name = hdfs://<namenode></p>
</li>
<li><p>mapred.job.tracer = <jobtracker>:8021</p>
</li>
<li><p>使用hadoop启动MapReduce任务的常用参数</p>
</li>
</ul>
<ol>
<li>-D property=value 覆盖默认配置属性</li>
<li>-conf filename 添加配置文件</li>
<li>-fs uri 设置默认文件系统</li>
<li>-jt host:port 设置jobtracker</li>
<li>-files file,file2 这些文件可以在tasktracker工作目录下面访问</li>
<li>-archives archive,archive2 和files类似，但是是存档文件</li>
</ol>
<ul>
<li>突然觉得这个差别在files只能是平级结构，而archive可以是层级结构。</li>
<li>-libjars jar1,jar2 和files类似，通常这些JAR文件是MapReduce所需要的。</li>
</ul>
<p>如果希望运行时候动态创建集群的话，可以通过这几个类来创建</p>
<ul>
<li>MiniDFSCluster</li>
<li>MiniMRCluster</li>
<li>MiniHBaseCluster</li>
<li>MiniZooKeeperClutser</li>
<li><strong>NOTE(dirlt):都称为Mini???Cluster？</strong></li>
</ul>
<p>另外还有自带的ClusterMapReduceTestCase以及HBaseTestingUtility来帮助进行mapreduce的testcase. 这些类散步在hadoop,hbase,hadoop-test以及hbase-test里面。</p>
<p><strong>NOTE（dirlt）：但是个人觉得可能还是没有本地测试方便，不过倒是可以试试</strong></p>
<p>job，task and attempt</p>
<ul>
<li><p>jobID常见格式为 job_200904110811_0002</p>
</li>
<li><p>其中200904110811表示jobtracker从2009.04.11的08:11启动的</p>
</li>
<li>0002 表示第三个job,从0000开始计数。超过10000的话就不能够很好地排序</li>
<li><p>taskID常见格式为 task_200904110811_0002_m_000003</p>
</li>
<li><p>前面一串数字和jobID匹配，表示从属于这个job</p>
</li>
<li>m表示map任务，r表示reduce任务</li>
<li>000003表示这是第4个map任务。顺序是在初始化时候指定的，并不反应具体的执行顺序。</li>
<li><p>attemptID常见格式为 attempt_200904110811_0002_m_000003_0</p>
</li>
<li><p>前面一串数字和taskID匹配，表示从属与这个task</p>
</li>
<li>attempt出现的原因是因为一个task可能会因为失败重启或者是预测执行而执行多次</li>
<li>如果jobtracker重启而导致作业重启的话，那么做后面id从1000开始避免和原来的attempt冲突。</li>
</ul>
<p>作业调试</p>
<ul>
<li><p>相关配置</p>
</li>
<li><p>mapred.jobtracker.completeuserjobs.maximum 表示web页面下面展示completed jobs的个数，默认是100，超过的部分放到历史信息页。</p>
</li>
<li>mapred.jobtracker.restart.recover = true jobtracker重启之后自动恢复作业</li>
<li>hadoop.job.history.location 历史作业信息存放位置，超过30天删除，默认在_logs/history</li>
<li>hadoop.job.history.user.location 如果不为none那么历史作业信息在这里也会存在一份，不会删除。</li>
<li><p>相关命令</p>
</li>
<li><p>hadoop fs -getmerge <src> <dst> 能够将hdfs的src下面所有的文件merge合并成为一份文件并且copy到本地</p>
</li>
<li>hadoop job -history 察看作业历史</li>
<li>hadoop job -counter 察看作业计数器</li>
<li><p>相关日志</p>
</li>
<li><p>系统守护进程日志 写入HADOOP_LOG_DIR里面，可以用来监控namenode以及datanode的运行情况</p>
</li>
<li>MapReduce作业历史日志 _logs/history</li>
<li>MapReduce任务日志 写入HADOOP_LOG_DIR/userlogs里面，可以用来监控每个job的运行情况</li>
<li><p>分析任务</p>
</li>
<li><p>JobConf允许设置profile参数 <strong>NOTE（dirlt）：新的接口里面JobConf-&gt;JobContext-&gt;Job，Job没有这些接口，但是可以通过Configuration来设置</strong></p>
</li>
<li><p>setProfileEnabled 打开profile功能，默认false，属性 mapred.task.profile</p>
</li>
<li><p>setProfileParams 设置profile参数</p>
</li>
<li><p>属性 mapred.task.profile.params</p>
</li>
<li>默认使用hprof -agentlib:hprof=cpu=samples,heap=sites,force=n,thread=y,verbose=n,file=%s&quot;</li>
<li>其中%s会替换成为profile输出文件</li>
<li><strong>NOTE（dirlt）：其实这里似乎也可以设置成为jmxremote来通过jvisualvm来调试</strong></li>
<li><p>setProfileTaskRange(boolean,String)</p>
</li>
<li><p>参数1表示针对map还是reduce task做profile, true表示map, false表示reduce</p>
</li>
<li>参数2表示针对哪些tasks做优化，&quot;0-2&quot;表示针对0，1，2三个任务，默认也是&quot;0-2&quot;</li>
<li>map task对应属性mapred.task.profile.maps，reduce task对应属性mapred.task.profile.reduces</li>
<li><p>任务重现</p>
</li>
<li><p>首先将keep.failed.task.files设置为true,这样如果任务失败的话，那么这个任务的输入和输出都会保留下来</p>
</li>
<li><p>如果是map任务的话，那么输入分别会在本地保留</p>
</li>
<li>如果是reduce任务的话，那么对应的map任务输出会在本地保留</li>
<li>然后我们使用hadoop IsolationRunner job.xml来重新运行这个任务</li>
<li>可以修改HADOOP_OPTS添加远程调试选项来启动这个任务。</li>
<li>如果希望任务都保留而不仅仅是失败任务保留的话，那么可以设置 keep.task.files.pattern 为正则表达式（与保留的任务ID匹配）</li>
</ul>
<h3 id="1-4-6-mapreduce-">1.4.6 MapReduce的工作机制</h3>
<p>Hadoop运行MapReduce作业的工作原理</p>
<p><img src="" alt="./images/mapreduce-workflow-architecture.png"></p>
<p>其中有几点需要注意的：</p>
<ul>
<li>计算分片信息是在本地完成的，分片信息和其他resouce(包括jars,files,archives等）一起copy到HDFS上面，然后jobtracker直接读取分片信息。</li>
<li>提交的资源可以设置replication数目，高副本数目可以缓解tasktracker获取resource的压力。参数是mapred.submit.replication.</li>
<li>对于streaming以及pipes的实现，无非就是task并不直接执行任务，而是开辟另外一个子进程来运行streaming或者是pipes的程序。</li>
</ul>
<p><img src="" alt="./images/mapreduce-streamming-pipes.jpg"></p>
<p>进度和状态的更新</p>
<ul>
<li>map任务进度是已经处理输入的比例</li>
<li><p>reduce任务进度分为三个部分</p>
</li>
<li><p>shuffle 1/3</p>
</li>
<li>sort 1/3</li>
<li>reduce 1/3</li>
<li>也就是说如果刚运行完成sort的话，那么进度是2/3</li>
<li><p>状态的更新</p>
</li>
<li><p>触发事件</p>
</li>
<li><p>读取记录</p>
</li>
<li>输出记录</li>
<li>修改状态 reporter的setStatus</li>
<li>计数器修改</li>
<li>reporter的progress</li>
<li><p>子进程有单独线程每隔3秒检查progress位是否设置，如果设置的话那么和tasktracker发起心跳</p>
</li>
<li><p>通过mapred.task.timeout控制</p>
</li>
<li><p>tasktracker每隔5秒和jobtracker做心跳</p>
</li>
<li><p>心跳时间通过 mapred.tasktracker.expircy.interval 设置</p>
</li>
<li><p>jobClient定期会去jobtracker询问job是否完成</p>
</li>
<li><p>jobClient也可以设置属性job.end.notification.url,任务完成jobtracker会调用这个url</p>
</li>
<li>可以认为就是推拉方式的结合。</li>
</ul>
<p>失败检测和处理</p>
<ul>
<li><p>任务失败</p>
</li>
<li><p>子进程抛出异常的话，tasktracker将异常信息记录到日志文件然后标记失败</p>
</li>
<li>对于streaming任务的话非0退出表示出现问题，也可以使用stream.non.zero.exit.is.failure = false来规避（ <strong>这样是否就没有办法判断是否正常退出了？</strong> ）</li>
<li>如果长时间没有响应的话，没有和tasktracker有交互，那么也会认为失败。这个时间使用mapred.task.timeout控制，默认10min</li>
<li><p>如果任务失败的话，jobtracker会尝试进行多次重试</p>
</li>
<li><p>map重试次数通过 mapred.map.max.attempts 配置</p>
</li>
<li>reduce重试次数通过 mapre.reduce.max.attempts 配置</li>
<li><strong>任何任务重试超过4次的话那么会认为整个job失败</strong></li>
<li>另外需要区分KILLED状态和FAILED状态，对于KILLED状态可能是因为推测执行造成的，不会记录到failed attempts里面</li>
<li><p>如果我们希望允许少量任务失败的话，那么可以配置</p>
</li>
<li><p>mapred.max.map.failures.percent 允许map失败的最大比率</p>
</li>
<li>mapred.max.reduce.failures.percent 允许reduce失败的最大比率</li>
<li>如果一个job超过一定的task在某个tt上面运行失败的话，那么就会将这个tt加入到这个job的blacklist. mapred.max.tracker.failures = 4</li>
<li>如果job成功的话，检查运行task失败的tt并且标记，如果超过一定阈值的话，那么会将tt加入到全局的blacklist. mapred.max.tracker.blacklists = 4</li>
</ul>
<p>作业的调度</p>
<ul>
<li><p>fifo scheduler</p>
</li>
<li><p>可以通过mapred.job.priority或者是setJobPriority设置</p>
</li>
<li>当队列中有空闲的槽位需要执行任务时，从等待队列中选择优先级最高的作业</li>
<li>fair scheduler</li>
<li>capacity scheduler</li>
</ul>
<p>shuffle和排序</p>
<p><img src="" alt="./images/mapreduce-shuffle-sort.jpg"></p>
<p><img src="" alt="./images/mapreduce-shuffle-sort-2.png"></p>
<p>有下面这些参数控制shuffle和sort的过程 <strong>NOTE（dirlt）：书上倒是有很多参数，但是好多还是不太理解</strong></p>
<ul>
<li>io.sort.mb map输出缓存空间大小，默认是100MB. 建议设置10/* io.sort.factor.</li>
<li><p>io.sort.spill.percent 如果map输出超过了缓存空间大小的这个阈值的话，那么就会spill,默认是0.8</p>
</li>
<li><p>每次spill之前先会对这个文件进行排序，如果有combiner的话那么会在上面调用combiner</p>
</li>
<li>写磁盘是按照轮询的方式写到mapred.local.dir属性指定的目录下面</li>
<li>如果spill速度太慢的话，那么往缓存空间写入进程就会阻塞，直到spill腾出空间。</li>
<li><p>io.sort.factor 多路归并的数量，默认是10. 建议设置在25-32.</p>
</li>
<li><p>在map阶段，因为最终会存在多个spill文件，所以需要做多路归并。 <strong>TODO（dirlt）：如果归并数量少的话是否可能会多次merge？</strong></p>
</li>
<li>在reduce阶段的话，因为可能存在多路map输出的结果，所以需要做多路归并。</li>
<li>min.num.spill.for.combine 如果指定combiner并且spill次数超过这个值的话就会调用combine,默认为3</li>
<li>tasktracker.http.threads reduce通过HTTP接口来发起数据请求，这个就是HTTP接口相应线程数目，默认为40。 <strong>mapper as server</strong></li>
<li><p>mapred.reduce.parallel.copies reduce启动多少个线程去请求map输出，默认为5。 <strong>reducer as client</strong></p>
</li>
<li><p><strong>NOTE(dirlt):如果reduce和每个map都使用一个线程去请求输出结果的话，只要shuffle阶段没有出现network congestion，那么提高线程数量是有效果的</strong></p>
</li>
<li><strong>NOTE（dirlt）：可以设置到15-50</strong></li>
<li>mapred.reduce.copy.backoff = 300(s) reduce下载线程最大等待时间</li>
<li>mapred.job.shuffle.input.buffer.percent = 0.7 用来缓存shuffle数据的reduce task heap百分比</li>
<li>mapred.job.shuffle.merge.percent = 0.66 缓存的内存中多少百分比后开始做merge操作</li>
<li>mapred.job.reduce.input.buffer.percent = 0.0 sort完成后reduce计算阶段用来缓存数据的百分比. 默认来说不会使用任何内存来缓存，因此完全从磁盘上进行读取。</li>
</ul>
<p>任务的执行</p>
<ul>
<li><p>推测执行参数</p>
</li>
<li><p>如果某个任务执行缓慢的话会执行另外一个备份任务</p>
</li>
<li>mapred.map.tasks.speculative.execution true</li>
<li>mapred.reduce.tasks.speculative.execution true</li>
<li><p>JVM重用</p>
</li>
<li><p>一个JVM实例可以用来执行多个task.</p>
</li>
<li>mapred.job.reuse.jvm.num.tasks/setNumTasksToExecutePerJvm 单个JVM运行任务的最大数目</li>
<li>-1表示没有限制</li>
<li><p>任务执行环境</p>
</li>
<li><p>程序自身可以知道执行环境对于开发还是比较有帮助的</p>
</li>
<li><p>这些属性对于streaming可以通过环境变量获得</p>
</li>
<li><p><strong>对于streaming来说.替换成为_</strong></p>
</li>
<li>mapred.job.id string jobID</li>
<li>mapred.tip.id string taskID</li>
<li>mapred.task.id string attemptID</li>
<li>mapred.task.partition int 作业中任务编号</li>
<li>mapred.task.is.map boolean 是否为map</li>
<li>mapred.work.output.dir / FileOutputFormat.getWorkOutputPath 当前工作目录</li>
<li><p>杂项 <strong>NOTE（dirlt）：from misc articles</strong></p>
</li>
<li><p>mapred.job.map.capacity /# 最大同时运行map数量</p>
</li>
<li>mapred.job.reduce.capacity /# 最大同时运行reduce数量</li>
<li>mapred.job.queue.name /# 选择执行queue<h3 id="1-4-7-mapreduce-">1.4.7 MapReduce的类型与格式</h3>
</li>
</ul>
<p>MapReduce的类型</p>
<p>老API里面还有MapRunner这个类，这个类主要的作用是可以用来控制Mapper运行的方法，比如可以多线程来控制Mapper的运行。 但是在新API里面已经完全集成到Mapper实现里面来了，用户可以重写两个方法来完全控制mapper的运行</p>
<ul>
<li>map 如何处理kv</li>
<li><p>run 如何从context里面读取kv
protected void map(KEYIN key, VALUEIN value,</p>
<pre><code>             Context context) throws IOException, InterruptedException {
</code></pre><p>context.write((KEYOUT) key, (VALUEOUT) value);</p>
</li>
</ul>
<p>}
public void run(Context context) throws IOException, InterruptedException {</p>
<p>  setup(context);
  while (context.nextKeyValue()) {</p>
<pre><code>map(context.getCurrentKey(), context.getCurrentValue(), context);
</code></pre><p>  }</p>
<p>  cleanup(context);
}</p>
<p><strong>NOTE（dirlt）：觉得这个特性不是特别有用</strong></p>
<ul>
<li>mapred.input.format.class setInputFormat</li>
<li>mapred.mapoutput.key.class setMapOutputKeyClass</li>
<li>mapred.mapoutput.value.class setMapOutputValueClass</li>
<li>mapred.output.key.class setOutputKeyClass</li>
<li>mapred.output.value.class setOutputValueClass</li>
<li>mapred.mapper.class setMapperClass</li>
<li>mapred.map.runner.class setMapRunnerClass</li>
<li>mapred.combiner.class setCombinerClass</li>
<li>mapred.partitioner.class setPartitionerClass</li>
<li>mapred.output.key.comparator.class setOutputKeyComparatorClass</li>
<li>mapred.output.value.groupfn.class setOutputValueGroupingComparator</li>
<li>mapred.reducer.class setReducerClass</li>
<li>mapred.output.format.class setOutputFormat</li>
</ul>
<p>输入格式</p>
<p>对于InputFormat来说包含两个任务</p>
<ul>
<li>根据job描述来对输入进行切片（InputSplit）</li>
<li><p>根据切片信息来读取记录（RecordReader）
public abstract class InputFormat<K, V> {</p>
<p>public abstract
  List<InputSplit> getSplits(JobContext context</p>
<pre><code>                         ) throws IOException, InterruptedException;
</code></pre></li>
</ul>
<p>   public abstract
    RecordReader<K,V> createRecordReader(InputSplit split,</p>
<pre><code>                                     TaskAttemptContext context
                                    ) throws IOException,

                                             InterruptedException;
</code></pre><p>}</p>
<p>public abstract class InputSplit {
  public abstract long getLength() throws IOException, InterruptedException;</p>
<p>  public abstract</p>
<pre><code>String[] getLocations() throws IOException, InterruptedException;
</code></pre><p>}</p>
<p>public abstract class RecordReader<KEYIN, VALUEIN> implements Closeable {</p>
<p>  public abstract void initialize(InputSplit split,
                                  TaskAttemptContext context</p>
<pre><code>                              ) throws IOException, InterruptedException;
</code></pre><p>  public abstract
  boolean nextKeyValue() throws IOException, InterruptedException;</p>
<p>  public abstract</p>
<p>  KEYIN getCurrentKey() throws IOException, InterruptedException;</p>
<p>  public abstract
  VALUEIN getCurrentValue() throws IOException, InterruptedException;</p>
<p>  public abstract float getProgress() throws IOException, InterruptedException;</p>
<p>  public abstract void close() throws IOException;</p>
<p>}</p>
<p>下面是一些常见的InputFormat实现</p>
<ul>
<li><p>FileInputFormat</p>
</li>
<li><p>addInputPath或者是setInputPaths修改输入路径 mapred.input.dir</p>
</li>
<li><p>setInputPathFilter可以修改过滤器 mapred.input.path.Filter.class</p>
</li>
<li><p>基本实现会排除隐藏.或者是_开头文件。</p>
</li>
<li>自定义的过滤器是建立在默认过滤器的基础上的。</li>
<li><p>分片大小由下面三个参数控制</p>
</li>
<li><p>mapred.min.split.size 1</p>
</li>
<li>mapred.max.split.size MAX</li>
<li>dfs.block.size 64MB</li>
<li>算法是max(minSplitSize,min(maxSplitSize,blockSize))</li>
<li>isSplitable可以控制输入文件是否需要分片</li>
<li>CombineFileInputFormat 可以处理多个小文件输入，抽象类需要继承实现。</li>
<li><p>TextInputFormat</p>
</li>
<li><p>输入单位是行，key是LongWritable表示行偏移，value是Text表示行内容</p>
</li>
<li><p>KeyValueTextInputFormat</p>
</li>
<li><p>输入单位是行，按照key.value.seperator.in.input.line来进行分隔默认是\t</p>
</li>
<li>key和value的格式都是Text</li>
<li><p>NLineInputFormat</p>
</li>
<li><p>和TextInputFormat非常类似，大师使用多行输入默认为1行</p>
</li>
<li>通过mapred.line.input.format.linespermap来控制行数</li>
<li><p>XML</p>
</li>
<li><p>InputFormat使用StreamInputFormat,</p>
</li>
<li>设置RecordReader使用stream.recordreader.class来设置</li>
<li>RecordReader使用org.apache.hadoop.streaming.StreamXmlRecordReader</li>
<li><strong>NOTE（dirlt）：也有现成的XmlInputFormat的实现</strong></li>
<li>SequenceFileInputFormat</li>
<li><p>SequenceFileAsTextInputFormat</p>
</li>
<li><p>将输入的kv转换成为text对象适合streaming处理方式</p>
</li>
<li>SequenceFileAsBinaryInputFormat <strong>NOTE（dirlt）：似乎没有什么用！</strong></li>
<li>MultipleInputs</li>
<li>DBInputFormat/DBOutputFormat JDBC数据库输入输出</li>
<li>TableInputFormat/TableOutputFormat HBase输入输出</li>
</ul>
<p>输出格式</p>
<ul>
<li><p>TextOutputFormat</p>
</li>
<li><p>使用mpared.textoutputformat.seperator来控制kv的分隔，默认是\t</p>
</li>
<li>对应的输入格式为KeyValueTextInputFormat</li>
<li>可以使用NullWritable来忽略输出的k或者是v</li>
<li>SequenceFileOutputFormat</li>
<li>SequenceFileAsBinaryOutpuFormat <strong>NOTE（dirlt）：似乎没有什么用！</strong></li>
<li>MapFileOutputFormat</li>
<li>MultipleOutputFormat</li>
<li><p>MultipleOutputs</p>
</li>
<li><p>如果不像生成那写part-r-00000这些空文件的话，那么可以将OutputFormat设置成为NullOutputFormat</p>
</li>
<li>但是使用NullOutputFormat的话会没有输出目录，如果想保留目录的话那么可以使用LazyOutputFormat</li>
</ul>
<h3 id="1-4-8-mapreduce-">1.4.8 MapReduce的特性</h3>
<ul>
<li><p>计数器</p>
</li>
<li><p>streaming计数器和可以通过写stderr来提交</p>
</li>
<li><p>reporter:counter:<group>,<counter>,<amount></p>
</li>
<li>reporter:status:<message></li>
<li><p>连接</p>
</li>
<li><p>map端连接</p>
</li>
<li><p>必须确保多路输入文件的reduce数量相同以及键相同。</p>
</li>
<li>使用CompositeInputFormat来运行map端连接。</li>
<li><strong>NOTE（dirlt)；不过我稍微看了一下代码，实现上其实也是针对输入文件对每条记录读取，然后进行join包括inner或者是outer。感觉场景会有限，而且效率不会太高</strong></li>
<li><p>分布式缓存</p>
</li>
<li><p>使用-files以及-archives来添加缓存文件</p>
</li>
<li><p>也可以使用DistributedAPI来完成之间事情</p>
</li>
<li><p>addCacheFile/addCacheArchive</p>
</li>
<li>然后在task里面通过configuration的getLocalCacheFiles以及getLocalCacheArchives来获得这些缓存文件</li>
<li><p>工作原理</p>
</li>
<li><p>缓存文件首先被放到hdfs上面</p>
</li>
<li><p>task需要的话那么会尝试下载，之后会对这个缓存文件进行引用计数，如果为0那么删除</p>
</li>
<li><p>这也就意味着缓存文件可能会被多次下载</p>
</li>
<li>但是运气好的话多个task在一个node上面的话那么就不用重复下载</li>
<li>缓存文件存放在${mapred.local.dir}/taskTracker/archive下面，但是通过软连接指向工作目录</li>
<li>缓存大小通过local.cache.size来配置</li>
<li><p>MapReduce库类</p>
</li>
<li><p>ChainMapper/ChainReducer 能够在一个mapper以及reducer里面运行多次mapper以及reducer</p>
</li>
<li><p>ChainMapper 允许在Map阶段，多个mapper组成一个chain,然后连续进行调用</p>
</li>
<li>ChainReducer 允许在Reuduce阶段，reducer完成之后执行一个mapper chain.</li>
<li>最终达到的效果就是 M+ -&gt; R -&gt; M/* （1个或者是多个mapper, 一个reducer，然后0个或者是多个mapper)</li>
<li><p><strong>TODO(dirlt):这样做倒是可以将各个mapper组合起来用作adapter.</strong></p>
<h3 id="1-4-9-hadoop-">1.4.9 构建Hadoop集群</h3>
</li>
<li><p>很多教程说hadoop集群需要配置ssh,但是配置这个前提是你希望使用start-all.sh这个脚本来启动集群</p>
</li>
<li><p>我现在的公司使用apt-get来安装，使用cssh来登陆到所有的节点上面进行配置，因此没有配置这个信任关系</p>
</li>
<li><p>Hadoop配置</p>
</li>
<li><p>配置文件</p>
</li>
<li><p>hadoop-env.sh 环境变量脚本</p>
</li>
<li>core-site.xml core配置，包括hdfs以及mapred的IO配置等</li>
<li>hdfs-site.xml hadoop进程配置比如namenode以及datanode以及secondary namenode</li>
<li>mapred-site.xml mapred进程配置比如jobtracker以及tasktracker</li>
<li><p>masters 运行namenode（secondary namenode)的机器列表，每行一个, <strong>无需分发到各个节点</strong></p>
</li>
<li><p><strong>在本地启动primary namenode</strong></p>
</li>
<li><p>slaves 运行datanode以及tasktracker的机器列表，每行一个 <strong>无需分发到各个节点</strong></p>
</li>
<li><p><strong>在本地启动jobtracker</strong></p>
</li>
<li>hadoop-metrics.properties 对hadoop做监控的配置文件</li>
<li>log4j.properties 日志配置文件</li>
<li>这些文件在conf目录下面有，如果想使用不同的文件也可以使用-config来另行指定</li>
<li><strong>NOTE(dirlt):所以从上面这个脚本来看，还是具有一定的局限性的</strong></li>
<li><p>hadoop-env.sh</p>
</li>
<li><p>HADOOP_HEAPSIZE = 1000MB 守护进程大小</p>
</li>
<li>HADOOP_NAMENODE_OPTS</li>
<li>HADOOP_SECONDARYNAMENODE_OPTS</li>
<li>HADOOP_IDENT_STRING 用户名称标记，默认为${USER}</li>
<li>HADOOP_LOG_DIR hadoop日志文件，默认是HADOOP_INSTALL/logs</li>
<li><p>core-site.xml</p>
</li>
<li><p>io.file.buffer.size IO操作缓冲区大小，默认是4KB <strong>这个需要提高</strong></p>
</li>
<li><p>hdfs-site.xml</p>
</li>
<li><p>fs.default.name</p>
</li>
<li>hadoop.tmp.dir hadoop临时目录，默认是在/tmp/hadoop-${user.name}</li>
<li>dfs.name.dir namenode数据目录，一系列的目录，namenode内容会同时备份在所有指定的目录中。默认为${hadoop.tmp.dir}/dfs/name</li>
<li>dfs.data.dir datanode数据目录，一系列的目录，循环将数据写在各个目录里面。默认是${hadoop.tmp.dir}/dfs/data</li>
<li>fs.checkpoint.dir secondarynamenode数据目录，一系列目录，所有目录都会写一份。默认为${hadoop.tmp.dir}/dfs/namesecondary</li>
<li>dfs.namenode.handler.count namenode上用来处理请求的线程数目</li>
<li>dfs.datanode.ipc.address 0.0.0.0:50020 datanode的RPC接口，主要和namenode交互</li>
<li>dfs.datanode.address 0.0.0.0:50010 datanode的data block传输接口，主要和client交互</li>
<li>dfs.datanode.http.address 0.0.0.0:50075 datanode的HTTP接口，和user交互</li>
<li>dfs.datanode.handler.count datanode上用来处理请求的线程数目</li>
<li>dfs.datanode.max.xcievers datanode允许最多同时打开的文件数量</li>
<li>dfs.http.address 0.0.0.0:50070 namenode的HTTP接口</li>
<li>dfs.secondary.http.address 0.0.0.0:50090 secondard namenode的HTTP接口</li>
<li>dfs.datanode.dns.interface default 绑定的NIC，默认是绑定默认的NIC比如eth0</li>
<li>dfs.hosts / dfs.hosts.exclude 加入的datanode以及排除的datanode</li>
<li>dfs.replication = 3 副本数目</li>
<li>dfs.block.size = 64MB</li>
<li>dfs.datanode.du.reserved 默认datanode会使用目录所在磁盘所有空间，这个值可以保证有多少空间被reserved的</li>
<li><p>fs.trash.interval 单位分钟，如果不为0的话，那么删除文件会移动到回收站，超过这个单位时间的文件才会完全删除。</p>
</li>
<li><p>回收站位置/home/${user]/.Trash <strong>NOTE(dirlt):回收站这个功能只是对fs shell有效。fs shell remove时候会构造Trash这个类来处理删除文件的请求。如果调用Java API的话那么会直接删除文件</strong></p>
</li>
<li>haddop fs -expunge 强制删除</li>
<li><strong>NOTE（dirlt）：grep代码发现只有NameNode在TrashEmptier里面构造了Trash这个类，因此这个配置之需要在nn上配置即可，决定多久定期删除垃圾文件</strong></li>
<li><p>fs.trash.checkpoint.interval 单位分钟，namenode多久检查一次文件是否需要删除。</p>
</li>
<li><p><strong>NOTE（dirlt）：似乎没有这个参数。如果没有这个参数的话，那么两次检查时长应该是由参数fs.trasn.interval来决定</strong></p>
</li>
<li><p>mapred-site.xml</p>
</li>
<li><p>mapred.job.tracker</p>
</li>
<li>mapred.local.dir MR中间数据存储，一系列目录，分散写到各个目录下面，默认为${hadoop.tmp.dir}/mapred/local</li>
<li>mapred.system.dir MR运行期间存储，比如存放jar或者是缓存文件等。默认${hadoop.tmp.dir}/mapred/system</li>
<li>mapred.tasktracker.map.tasks.maximum = 2 单个tasktracker最多多少map任务</li>
<li>mapred.tasktracker.reduce.tasks.maximum = 2 单个tasktracker最多多少个reduce任务</li>
<li>mapred.tasktracker.dns.interface default 绑定的NIC，默认是绑定默认的NIC比如eth0</li>
<li>mapred.child.ulimit 单个tasktracker允许子进程占用的最大内存空间。通常为2-3/* mapred.child.java.opts.</li>
<li><p>mapred.child.java.opts = -Xmx200m 每个子JVM进程200M. <strong>NOTE（dirlt）：这个是在提交机器上面设置的，而不是每个tasktracker上面设置的，每个job可以不同</strong></p>
</li>
<li><p>不一定支持将map/reduce的jvm参数分开设置 <a href="http://hadoop-common.472056.n3.nabble.com/separate-JVM-flags-for-map-and-reduce-tasks-td743351.html" target="_blank"><a href="http://hadoop-common.472056.n3.nabble.com/separate-JVM-flags-for-map-and-reduce-tasks-td743351.html">http://hadoop-common.472056.n3.nabble.com/separate-JVM-flags-for-map-and-reduce-tasks-td743351.html</a></a></p>
</li>
<li><strong>NOTE（dirlt）：个人折中思路是限制内存大小为1G，然后大内存机器允许同时执行map/reduce数量上限提高，通过增加job的map/reduce数量来提高并发增加性能</strong></li>
<li><p><strong>NOTE（dirlt）：我grep了一下cdh3u3的代码，应该是将map/reduce的jvm参数分开进行了设置</strong></p>
</li>
<li><p>mapred.map.child.java.opts</p>
</li>
<li>mapred.reduce.child.java.opts</li>
<li>mapred.task.tracker.report.address 127.0.0.1:0 tasktracker启动子进程通信的端口，0表示使用任意端口</li>
<li>mapred.task.tracker.expiry.interval 600(sec) tt和jt之间的心跳间隔</li>
<li>mapred.job.tracker.handler.count. jobtracker用来处理请求的线程数目。</li>
<li>mapred.job.tracker.http.address 0.0.0.0:50030 jobtracker的HTTP接口</li>
<li>mapred.task.tracker.http.address 0.0.0.0:50060 tasktrackder的HTTP接口</li>
<li>mapred.hosts / mapred.hosts.exclude 加入的tasktracker以及排除的tasktracker.</li>
<li><p>Hadoop Benchmark <strong>NOTE（dirlt）：try it out</strong></p>
</li>
<li><p>在hadoop安装目录下面有jar可以来做基准测试</p>
</li>
<li>TestDFSIO测试HDFS的IO性能</li>
<li>Sort测试MapReduce性能</li>
<li>MRBench多次运行一个小作业来检验小作业能否快速相应</li>
<li>NNBench测试namenode硬件的负载</li>
</ul>
<h3 id="1-4-10-hadoop">1.4.10 管理Hadoop</h3>
<ul>
<li><p>永久性数据结构</p>
</li>
<li><p>namenode的目录结构</p>
</li>
<li><p>current表示当前的namenode数据（对于辅助节点上这个数据并不是最新的）</p>
</li>
<li><p>previous.checkpoint表示secondarynamenode完成checkpoint的数据（和current可能存在一些编辑差距）</p>
</li>
<li><p>hadoop dfsadmin -saveNamespace 可以强制创建检查点,仅仅在安全模式下面运行</p>
</li>
<li><p>辅助namenode每隔5分钟会检查</p>
</li>
<li><p>如果超过fs.checkpoint.period = 3600（sec），那么会创建检查点</p>
</li>
<li>如果编辑日志大小超过fs.checkpoint.size = 64MB,同样也会创建检查点</li>
<li>除了将文件copy到namenode之外，在辅助节点上面可以使用选项-importCheckpoint来载入</li>
<li><p>VERSION Java属性文件</p>
</li>
<li><p>namespaceID 每次格式化都会重新生成一个ID，这样可以防止错误的datanode加入</p>
</li>
<li>cTime namenode存储系统创建时间，对于刚格式化的存储系统为0.对于升级的话会更新到最新的时间戳</li>
<li>storageType NAME_NODE or DATA_NODE</li>
<li>layoutVersion 负整数表示hdfs文件系统布局版本号，对于hadoop升级的话这个版本号可能不会变化</li>
<li>edits 编辑日志文件</li>
<li>fsimage 镜像文件</li>
<li>fstime ???</li>
<li><p>datanode的目录结构</p>
</li>
<li><p>blk<em><id>以及blk</em><id>.meta 表示块数据以及对应的元信息，元数据主要包括校验和等内容</p>
</li>
<li>如果datanode文件非常多的话，超过dfs.datanode.numblocks = 64的话，那么会创建一个目录单独存放，最终结果就是形成树存储结构。</li>
<li>dfs.data.dir目录是按照round-robin的算法选择的。</li>
<li><p>安全模式</p>
</li>
<li><p>namenode启动的时候会尝试合并edit数据并且新建一个checkpoint，然后进入安全模式，在这个模式内文件系统是只读的</p>
</li>
<li>可以通过hadoop dfsadmin -safemode来操作安全模式</li>
<li><p>当达到下面几个条件的时候会离开安全模式</p>
</li>
<li><p>整个系统的副本数目大于某个阈值的副本数目比率超过一个阈值之后，然后继续等待一段时间就会离开安全模式</p>
</li>
<li>dfs.replication.min = 1 副本数目阈值</li>
<li>dfs.safemode.threshold.pct = 0.999 比率阈值</li>
<li>dfs.safemode.extension = 30000(ms) 等待时间</li>
<li><p>工具</p>
</li>
<li><p>dfsadmin</p>
</li>
<li>fsck</li>
<li><p>scanner</p>
</li>
<li><p>DataBlockScanner每隔一段时间会扫描本地的data block检查是否出现校验和问题</p>
</li>
<li>时间间隔是dfs.datanode.scan.period.hours = 504默认三周</li>
<li>可以通过页面访问每个datanode的block情况 <a href="http://localhost:50075/blockScannerReport" target="_blank"><a href="http://localhost:50075/blockScannerReport">http://localhost:50075/blockScannerReport</a></a></li>
<li>加上listblocks参数可以看每个block情况 <a href="http://localhost:50075/blockScannerReport?listblocks" target="_blank"><a href="http://localhost:50075/blockScannerReport?listblocks">http://localhost:50075/blockScannerReport?listblocks</a></a> <strong>NOTE（dirlt）：可能会很大</strong></li>
<li><p>balancer</p>
</li>
<li><p>通过start-balancer.sh来启动,集群中只允许存在一个均衡器</p>
</li>
<li>均衡的标准是datanode的利用率和集群平均利用率的插值，如果超过某个阈值就会进行block movement</li>
<li>-threshold可以执行阈值，默认为10%</li>
<li>dfs.balance.bandwidthPerSec = 1024 /* 1024 用于balance的带宽上限。</li>
<li><p>监控</p>
</li>
<li><p>日志</p>
</li>
<li><p>jobtracker的stack信息（thread-dump）<a href="http://localhost:50030/stacks" target="_blank"><a href="http://localhost:50030/stacks">http://localhost:50030/stacks</a></a></p>
</li>
<li><p>度量</p>
</li>
<li><p>度量从属于特性的上下文(context),包括下面几个</p>
</li>
<li><p>dfs</p>
</li>
<li>mapred</li>
<li>rpc</li>
<li>jvm</li>
<li><p>下面是几种常见的context</p>
</li>
<li><p>FileContext 度量写到文件</p>
</li>
<li>GangliaContext 度量写到ganglia <strong>(这个似乎比较靠谱）</strong></li>
<li>CompositeContext 组合context</li>
<li>度量可以从hadoop-metrics.properties进行配置</li>
</ul>
<h3 id="1-5-benchmark">1.5 Benchmark</h3>
<ul>
<li>Benchmarking and Stress Testing an Hadoop Cluster with TeraSort, TestDFSIO &amp; Co. - Michael G. Noll <a href="http://www.michael-noll.com/blog/2011/04/09/benchmarking-and-stress-testing-an-hadoop-cluster-with-terasort-testdfsio-nnbench-mrbench/" target="_blank"><a href="http://www.michael-noll.com/blog/2011/04/09/benchmarking-and-stress-testing-an-hadoop-cluster-with-terasort-testdfsio-nnbench-mrbench/">http://www.michael-noll.com/blog/2011/04/09/benchmarking-and-stress-testing-an-hadoop-cluster-with-terasort-testdfsio-nnbench-mrbench/</a></a></li>
<li>intel-hadoop/HiBench · GitHub <a href="https://github.com/intel-hadoop/HiBench" target="_blank"><a href="https://github.com/intel-hadoop/HiBench">https://github.com/intel-hadoop/HiBench</a></a></li>
<li>HBase Performance Testing at hstack <a href="http://hstack.org/hbase-performance-testing/" target="_blank"><a href="http://hstack.org/hbase-performance-testing/">http://hstack.org/hbase-performance-testing/</a></a></li>
<li>Performance testing / Benchmarking a HBase cluster – Sujee Maniyam <a href="http://sujee.net/tech/articles/hadoop/hbase-performance-testing/" target="_blank"><a href="http://sujee.net/tech/articles/hadoop/hbase-performance-testing/">http://sujee.net/tech/articles/hadoop/hbase-performance-testing/</a></a></li>
<li>new Put(&quot;lars&quot;.toBytes(&quot;UTF-8&quot;)) : Performance testing HBase using YCSB <a href="http://blog.lars-francke.de/2010/08/16/performance-testing-hbase-using-ycsb/" target="_blank"><a href="http://blog.lars-francke.de/2010/08/16/performance-testing-hbase-using-ycsb/">http://blog.lars-francke.de/2010/08/16/performance-testing-hbase-using-ycsb/</a></a></li>
<li>Hbase/PerformanceEvaluation - Hadoop Wiki <a href="http://wiki.apache.org/hadoop/Hbase/PerformanceEvaluation" target="_blank"><a href="http://wiki.apache.org/hadoop/Hbase/PerformanceEvaluation">http://wiki.apache.org/hadoop/Hbase/PerformanceEvaluation</a></a></li>
</ul>
<h3 id="1-5-1-testdfsio">1.5.1 TestDFSIO</h3>
<p>测试hdfs吞吐
hdfs@hadoop1:~$ hadoop jar /usr/lib/hadoop/hadoop-test-0.20.2-cdh3u3.jar TestDFSIO</p>
<p>Usage: TestDFSIO [genericOptions] -read | -write | -append | -clean [-nrFiles N] [-fileSize Size[B|KB|MB|GB|TB]] [-resFile resultFileName] [-bufferSize Bytes] [-rootDir]%</p>
<ul>
<li>read / write / append / clean 操作类型 <strong>append和write执行效率差别不大，但是write会创建新文件所以使用比较方便</strong> (default read)</li>
<li>nrFiles 文件数目(default 1) <strong>启动相同数量的map</strong></li>
<li>fileSize 每个文件大小(1MB)</li>
<li>resFile 结果报告文件(TestDFSIO_results.log)</li>
<li>bufferSize write buffer size(单次write写入大小）（1000000 bytes)</li>
<li><p>rootDir 操作文件根目录（/benchmarks/TestDFSIO/）
----- TestDFSIO ----- : write</p>
<pre><code>     Date &amp; time: Thu Apr 25 19:14:21 CST 2013
 Number of files: 2
</code></pre></li>
</ul>
<p>Total MBytes processed: 2.0
     Throughput mb/sec: 7.575757575757576</p>
<p>Average IO rate mb/sec: 7.61113977432251
IO rate std deviation: 0.5189420757292891</p>
<pre><code>Test exec time sec: 14.565
</code></pre><p>----- TestDFSIO ----- : read
           Date &amp; time: Thu Apr 25 19:15:13 CST 2013</p>
<pre><code>   Number of files: 2
</code></pre><p>Total MBytes processed: 2.0</p>
<pre><code> Throughput mb/sec: 27.77777777777778
</code></pre><p>Average IO rate mb/sec: 28.125</p>
<p>IO rate std deviation: 3.125
    Test exec time sec: 14.664</p>
<ul>
<li>throughtput = sum(filesize) / sum(time)</li>
<li>avaerage io rate = sum(filesize/time) / n</li>
<li>io rate std deviation<h3 id="1-5-2-terasort">1.5.2 TeraSort</h3>
</li>
</ul>
<p>通过排序测试MR执行效率 <strong>我看了一下代码map/reduce都有CPU操作，并且这个也非常依靠shuffle/copy.因此这个测试应该会是比较全面的</strong>
hdfs@hadoop1:~$ hadoop jar /usr/lib/hadoop/hadoop-examples-0.20.2-cdh3u3.jar <command></p>
<ul>
<li><p>teragen 产生排序数据</p>
</li>
<li><number of 100-byte rows>
</li>
<li><p>10 bytes key(random characters)</p>
</li>
<li>10 bytes rowid(right justified row id as a int)</li>
<li>78 bytes filler</li>
<li>\r\n</li>
<li><output dir></li>
<li><p>terasort 对数据排序</p>
</li>
<li><input dir></li>
<li><output dir></li>
<li>teravalidate 对排序数据做验证</li>
</ul>
<p>可以使用hadoop job -history all <job-output-dir>来观察程序运行数据，也可以通过web page来分析。</p>
<h3 id="1-5-3-nnbench">1.5.3 nnbench</h3>
<p>测试nn负载能力
➜  ~HADOOP_HOME  hadoop jar hadoop-test-0.20.2-cdh3u3.jar nnbench</p>
<p>NameNode Benchmark 0.4
Usage: nnbench <options></p>
<p>Options:
        -operation <Available operations are create_write open_read rename delete. This option is mandatory></p>
<pre><code>     /* NOTE: The open_read, rename and delete operations assume that the files they operate on, are already available. The create_write operation must be run before running the other operations.
    -maps &lt;number of maps. default is 1. This is not mandatory&gt;

    -reduces &lt;number of reduces. default is 1. This is not mandatory&gt;
    -startTime &lt;time to start, given in seconds from the epoch. Make sure this is far enough into the future, so all maps (operations) will start at the same time&gt;. default is launch time + 2 mins. This is not mandatory

    -blockSize &lt;Block size in bytes. default is 1. This is not mandatory&gt;
    -bytesToWrite &lt;Bytes to write. default is 0. This is not mandatory&gt;

    -bytesPerChecksum &lt;Bytes per checksum for the files. default is 1. This is not mandatory&gt;
    -numberOfFiles &lt;number of files to create. default is 1. This is not mandatory&gt;

    -replicationFactorPerFile &lt;Replication factor for the files. default is 1. This is not mandatory&gt;
    -baseDir &lt;base DFS path. default is /becnhmarks/NNBench. This is not mandatory&gt;

    -readFileAfterOpen &lt;true or false. if true, it reads the file and reports the average time to read. This is valid with the open_read operation. default is false. This is not mandatory&gt;
    -help: Display the help statement
</code></pre><ul>
<li>startTime 作用是为了能够让所有的map同时启动以便对nn造成压力
➜  ~HADOOP_HOME  hadoop jar hadoop-test-0.20.2-cdh3u3.jar nnbench -operation create_write -bytesToWrite 0 -numberOfFiles 1200</li>
</ul>
<p>➜  ~HADOOP_HOME  hadoop jar hadoop-test-0.20.2-cdh3u3.jar nnbench -operation open_read</p>
<p>结果报告文件是 NNBench_results.log</p>
<p>-------------- NNBench -------------- :</p>
<pre><code>                           Version: NameNode Benchmark 0.4
                       Date &amp; time: 2013-04-25 19:41:02,873


                    Test Operation: create_write

                        Start time: 2013-04-25 19:40:21,70
                       Maps to run: 1

                    Reduces to run: 1
                Block Size (bytes): 1

                    Bytes to write: 0
                Bytes per checksum: 1

                   Number of files: 1200
                Replication factor: 1

        Successful file operations: 1200


    /# maps that missed the barrier: 0
                      /# exceptions: 0


           TPS: Create/Write/Close: 75
</code></pre><p>Avg exec time (ms): Create/Write/Close: 26.526666666666667
            Avg Lat (ms): Create/Write: 13.236666666666666</p>
<pre><code>               Avg Lat (ms): Close: 13.164166666666667


             RAW DATA: AL Total /#1: 15884
             RAW DATA: AL Total /#2: 15797

          RAW DATA: TPS Total (ms): 31832
   RAW DATA: Longest Map Time (ms): 31832.0

               RAW DATA: Late maps: 0
         RAW DATA: /# of exceptions: 0
</code></pre><p>-------------- NNBench -------------- :</p>
<pre><code>                           Version: NameNode Benchmark 0.4
                       Date &amp; time: 2013-04-25 19:44:42,354


                    Test Operation: open_read

                        Start time: 2013-04-25 19:44:31,921
                       Maps to run: 1

                    Reduces to run: 1
                Block Size (bytes): 1

                    Bytes to write: 0
                Bytes per checksum: 1

                   Number of files: 1
                Replication factor: 1

        Successful file operations: 1


    /# maps that missed the barrier: 0
                      /# exceptions: 0


                    TPS: Open/Read: 500

     Avg Exec time (ms): Open/Read: 2.0
                Avg Lat (ms): Open: 2.0


             RAW DATA: AL Total /#1: 2

             RAW DATA: AL Total /#2: 0
          RAW DATA: TPS Total (ms): 2

   RAW DATA: Longest Map Time (ms): 2.0
               RAW DATA: Late maps: 0

         RAW DATA: /# of exceptions: 0
</code></pre><ul>
<li>maps that missed the barrier 从代码上分析是，在等待到start time期间中,如果sleep出现异常的话。</li>
<li>exceptions 表示在操作文件系统时候的exception数量</li>
<li>TPS transactions per second</li>
<li>exec（execution） 执行时间</li>
<li>lat（latency） 延迟时间</li>
<li>late maps 和 maps missed the barrier是一个概念。</li>
</ul>
<p>对于后面RAW DATA部分的话，从代码上看，就是为了计算出上面那些指标的，所以没有必要关注。</p>
<h3 id="1-5-4-mrbench">1.5.4 mrbench</h3>
<p>测试运行small mr jobs执行效率，主要关注响应时间。
MRBenchmark.0.0.2</p>
<p>Usage: mrbench [-baseDir <base DFS path for output/input, default is /benchmarks/MRBench>] [-jar <local path to job jar file containing Mapper and Reducer implementations, default is current jar file>] [-numRuns <number of times to run the job, default is 1>] [-maps <number of maps for each run, default is 2>] [-reduces <number of reduces for each run, default is 1>] [-inputLines <number of input lines to generate, default is 1>] [-inputType <type of input to generate, one of ascending (default), descending, random>] [-verbose]</p>
<ul>
<li>baseDir 输入输出目录</li>
<li>jar 通常不需要指定，用默认即可。</li>
<li>inputLines 输入条数</li>
<li>inputType 输入是否有序
hdfs@hadoop1:~$ hadoop jar /usr/lib/hadoop/hadoop-test-0.20.2-cdh3u3.jar mrbench -verbose</li>
</ul>
<p>结果直接输出在终端上面，</p>
<p>Total MapReduce jobs executed: 1</p>
<p>Total lines of data per job: 1
Maps per job: 2</p>
<p>Reduces per job: 1
Total milliseconds for task: 1 = 16452</p>
<p>DataLines       Maps    Reduces AvgTime (milliseconds)
1               2       1       16452</p>
<p>可以看到每个任务平均执行时间在16.452s.</p>
<h3 id="1-5-5-hbase-performanceevaluation">1.5.5 hbase.PerformanceEvaluation</h3>
<p>hdfs@hadoop1:~$ hbase org.apache.hadoop.hbase.PerformanceEvaluation</p>
<p>Usage: java org.apache.hadoop.hbase.PerformanceEvaluation \
  [--miniCluster] [--nomapred] [--rows=ROWS] <command> <nclients></p>
<p>Options:</p>
<p> miniCluster     Run the test on an HBaseMiniCluster
 nomapred        Run multiple clients using threads (rather than use mapreduce)</p>
<p> rows            Rows each client runs. Default: One million
 flushCommits    Used to determine if the test should flush the table.  Default: false</p>
<p> writeToWAL      Set writeToWAL on puts. Default: True</p>
<p>Command:
 filterScan      Run scan test using a filter to find a specific row based on it&#39;s value (make sure to use --rows=20)</p>
<p> randomRead      Run random read test
 randomSeekScan  Run random seek and scan 100 test</p>
<p> randomWrite     Run random write test
 scan            Run scan test (read every row)</p>
<p> scanRange10     Run random seek scan with both start and stop row (max 10 rows)
 scanRange100    Run random seek scan with both start and stop row (max 100 rows)</p>
<p> scanRange1000   Run random seek scan with both start and stop row (max 1000 rows)
 scanRange10000  Run random seek scan with both start and stop row (max 10000 rows)</p>
<p> sequentialRead  Run sequential read test
sequentialWrite Run sequential write test</p>
<p>Args:</p>
<p> nclients        Integer. Required. Total number of clients (and HRegionServers)
                 running: 1 &lt;= value &lt;= 500</p>
<p>Examples:
To run a single evaluation client:</p>
<p>$ bin/hbase org.apache.hadoop.hbase.PerformanceEvaluation sequentialWrite 1</p>
<p>从参数上看还是比较直接的。benchmark每个client通常对应10个mapper, 每个client操作<rows>个row,因此每个mapper操作<rows>/10个row,每个row大约1000bytes.</p>
<ul>
<li>filterScan 随机生成value，然后从头开始scan直到equal</li>
<li>randomRead 随机选取key读取</li>
<li>randomSeekScan 从某个随机位置开始scan最多100个</li>
<li>randomWrite 随即生成key写入</li>
<li>scan 每次scan 1个row，start随机</li>
<li>scan<num> 每次scan num个row，start随机</li>
<li>seqRead 顺序地读取每个key</li>
<li>seqWrite 顺序地写入每个key</li>
<li><strong>NOTE(dirlt):这里的key都非常简单，10个字符的数字，printf(&quot;%010d&quot;,row)</strong>
hdfs@hadoop1:~$ time hbase org.apache.hadoop.hbase.PerformanceEvaluation --rows=1000 sequentialWrite 2</li>
</ul>
<p>13/04/25 23:47:56 INFO mapred.JobClient:   HBase Performance Evaluation
13/04/25 23:47:56 INFO mapred.JobClient:     Row count=2000</p>
<p>13/04/25 23:47:56 INFO mapred.JobClient:     Elapsed time in milliseconds=258</p>
<p>输出结果是在counter里面，这里面row count = 2000, 占用时间为258 ms.
Date: 2013-12-15T10:28+0800</p>
<p><a href="http://orgmode.org/" target="_blank">Org</a> version 7.9.2 with <a href="http://www.gnu.org/software/emacs/" target="_blank">Emacs</a> version 24
<a href="http://validator.w3.org/check?uri=referer" target="_blank">Validate XHTML 1.0</a>
来源： <a href="[http://dirlt.com/hadoop.html/#sec-1-1-4](http://dirlt.com/hadoop.html#sec-1-1-4)">[http://dirlt.com/hadoop.html/#sec-1-1-4](http://dirlt.com/hadoop.html#sec-1-1-4)</a> </p>

      
    </div>
    
    
      
    
    
        <footer id="post-meta">
        <span class="categories">Posted in<span class="breadcrumb fa fa-folder"><li><a href="/categories/hadoop/">hadoop</a></li></span></span> | <span class="tags">Tagged <a href="/tags/hadoop/" class="label label-primary">hadoop</a></span> | <span class="time">recent updated:<time title="2014-03-07 09:54:36"datetime="2014-03-07 09:54:36"> mar. 7 2014</time></span> | <span class="comment-link">
<a href="http://itsolife.com/2014/02/02/2014-02-02-hadoop--hadoop/#comments" class="ds-thread-count comment-link" data-thread-key="2014-02-02-hadoop--hadoop" data-count-type="comments">暂无评论</a></span>
        </footer>
    
    <div class="clearfix"></div>
  </div>
</div>
</article>





<ul id="pagination" class="pagination pagination-lg">
  <li><a class="extend prev" href="/page/106/">&laquo;</a></li><li><a class="page-number" href="/">1</a></li><li><a class="page-number" href="/page/2/">2</a></li><li><span class="space">&hellip;</span></li><li><a class="page-number" href="/page/104/">104</a></li><li><a class="page-number" href="/page/105/">105</a></li><li><a class="page-number" href="/page/106/">106</a></li><li class="active"><li><span class="page-number current">107</span></li><li><a class="page-number" href="/page/108/">108</a></li><li><a class="page-number" href="/page/109/">109</a></li><li><a class="page-number" href="/page/110/">110</a></li><li><span class="space">&hellip;</span></li></li><li><a class="page-number" href="/page/164/">164</a></li><li><a class="page-number" href="/page/165/">165</a></li><li><a class="extend next" href="/page/108/">&raquo;</a></li>
  <div class="clearfix"></div>
</ul></div><!--wapper-->
       </div><!-- ID main-col END -->
       <aside id="sidebar" class="alignright col-sx-6 col-sm-4 col-md-3 col-lg-3">
<div id="widget_search" class="widget panel panel-primary">
    <form action="//google.com/search" method="get" accept-charset="utf-8">
  <div class="input-group">
    <input class="form-control" id="searchbox" type="search" name="q" results="0" placeholder="search">
    <span class="input-group-btn">
      <button class="btn btn-default" type="submit">Go!</button>
    </span>
    <input type="hidden" name="q" value="site:itsolife.com">
  </div>
</form>
</div>

<div id="widget_category" class="widget panel panel-primary">
  <div class="panel-heading">category</div>  <div data-src='category' class='ajax_widgets'>正在加载...</div>
</div>

<div id="widget_recent_posts" class="widget panel panel-primary">
  <div class="panel-heading">recent posts</div>  <div data-src='recent_posts' class='ajax_widgets'>正在加载...</div></div>

<div id="widget_tagcloud" class="widget panel panel-primary">
  <div class="panel-heading">tagcloud</div>  <div data-src='tagcloud' class='ajax_widgets'>正在加载...</div></div>

<div id="widget_latest_update_posts" class="widget panel panel-primary">
  <div class="panel-heading">最近更新</div>  <div data-src='latest_update_posts' class='ajax_widgets'>正在加载...</div></div>

<div id="widget_recent_comments" class="widget panel panel-primary">
  <div class="panel-heading">recent comments</div>  

<div class="list-group-item ds-recent-comments" data-num-items="6" data-show-avatars="1" data-show-time="1" data-show-title="1" data-show-admin="1" data-excerpt-length="50"></div>



</div>

</aside>
       <div class="clearfix"></div>
     </div><!-- row END -->
  </div>
  <footer id="footer" class="container">
  <div class="panel panel-info">
  <section id='footer_widget'></section>  <div class="panel-footer">
  <div id="site-info">
    <span class='author'>
  
  &copy; 2014 RobinChia
  
    &nbsp;&nbsp;</span>
  
  <span id='analytics-51la'></span><span id='analytics-google'>
<script type="text/javascript">
  var _gaq = _gaq || [];
  _gaq.push(['_setAccount', 'UA-48559895-1']);
  _gaq.push(['_trackPageview']);
  _js2load.push({src:('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js'});
</script></span><span id='analytics-cnzz'>
<script type="text/javascript">var cnzz_protocol = (("https:" == document.location.protocol) ? " https://" : " http://");document.write(unescape("%3Cspan id='cnzz_stat_icon_5774006'%3E%3C/span%3E%3Cscript src='" + cnzz_protocol + "s17.cnzz.com/stat.php%3Fid%3D5774006%26show%3Dpic' type='text/javascript'%3E%3C/script%3E"));</script>
</span><span id='analytics-baidu'>
<script>
var _hmt = _hmt || [];
_js2load.push({src:"//hm.baidu.com/hm.js?1442f50724afc42380b51f097c43082c"});
</script>
</span>  </div>
  <div id="copyright">Blog powered by <a href='http://zespia.tw/hexo/'><strong>hexo</strong></a> Theme <strong><a href='https://github.com/chenall/hexo-theme-chenall'>chenall</a></strong>(Some change in it)<span class="pull-right"> 更新时间: <em>2014-03-23 21:54:38</em></span></div>
</div>
<div class="clearfix"></div>


  </div>
  </footer>
  
        <script src="http://cdn.bootcss.com/jquery/1.10.2/jquery.min.js"></script>        
        <script src="http://cdn.staticfile.org/twitter-bootstrap/3.1.0/js/bootstrap.min.js"></script>        
                
        <script src="http://cdn.bootcss.com/prettify/r298/prettify.min.js"></script>    <script type="text/javascript">
   var lang=["bsh", "c", "cc", "cpp", "cs", "csh", "cyc", "cv", "htm", "html",
    "java", "js", "m", "mxml", "perl", "pl", "pm", "py", "rb", "sh",
    "xhtml", "xml", "xsl"];
   var pretty_base='';
   $('script').each(function(){
	var c = $(this).attr('src');
	if (!c)
	    return;
	if (c.match(/(\/)?prettify(\.min)?\.js/i))
	{
	    var index = c.lastIndexOf('/');
	    if (index != -1)
		pretty_base = c.substr(0,index + 1);
	    return false;
	}
   })
   $('pre code').each(function(){
	var c = $(this).attr('class')
	if (!c)
	    return;
	c = c.match(/\s?(lang\-\w+)/i);
	if (c && lang.indexOf(c[1]) == -1)
	{
	    lang.push(c[1]);
	    $.getScript(pretty_base + c[1] + '.min.js');
	}
   })

    $(window).load(function(){
       $("pre").addClass("prettyprint");
       prettyPrint();
    })
</script>    
            <script type="text/javascript">
var duoshuoQuery = {short_name:"robinchia"};
_js2load.push({src:'http://static.duoshuo.com/embed.js',charset:'UTF-8'});
</script>
    
            <!--wumii_relatedItems-->
    
        <script src="http://cdn.bootcss.com/fancybox/2.1.5/jquery.fancybox.min.js"></script>    <script type="text/javascript">
(function($){
  $('.entry').each(function(i){
    $(this).find('img').each(function(){
      var alt = this.alt;

      if (alt){
        $(this).before('<span class="caption">' + alt + '</span>');
      }

      $(this).wrap('<a href="' + this.src + '" title="' + alt + '" class="fancybox" rel="fancybox' + i + '" />');
    });
  });

  $('.fancybox').fancybox();
})(jQuery);
</script>    
        <script src="http://cdn.bootcss.com/mathjax/2.3/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>    <script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      processEscapes: true
    }
  });
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>
    
<script type="text/javascript">
$('.ajax_widgets').each(function(){var src=$(this).attr('data-src');if(src)$(this).load('/widgets/'+src+'.html');});
$.each(_js2load,function(index,obj){loadjs(obj.src,obj.charset)});
</script>

<div id="scroll2top">
<img src="/scroll2top/arrow.png"/>
</div>
<script src="/scroll2top/scroll2top.min.js"></script>
<div id="winterland">
  <canvas></canvas>
</div>
<script src="/js/winterland.min.js"></script>

  </body>
</html>
