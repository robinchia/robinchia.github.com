
<!DOCTYPE HTML>
<html>
<head>
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<title>The 107 Page | It so life</title>
<meta name="author" content="RobinChia">

<meta name="description" content="It so life">


<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">


<meta property="og:site_name" content="It so life"/>

<!--[if IE]><style>.testIE.IE{display:inline;}</style><![endif]-->
<!--[if lte IE 7]><link rel="stylesheet" href="/css/ie7.css" type="text/css"><![endif]-->
<!--[if (lt IE 9)&(gt IE 7)]><style>.testIE.IE8{display:inline;}</style><![endif]-->
<!--[if gt IE 8]><style>.testIE.IE9{display:inline;}</style><![endif]-->

<link href="/favicon.png" rel="icon">
<link rel="alternate" href="/atom.xml" title="It so life Feed" type="application/atom+xml">

<link rel="stylesheet" href="http://cdn.staticfile.org/twitter-bootstrap/3.1.0/css/bootstrap.min.css" type="text/css">
<link rel="stylesheet" href="http://cdn.bootcss.com/font-awesome/4.0.3/css/font-awesome.min.css" type="text/css">
<link rel="stylesheet" href="http://cdn.bootcss.com/prettify/r298/prettify.min.css" type="text/css">
<link rel="stylesheet" href="http://cdn.bootcss.com/fancybox/2.1.5/jquery.fancybox.min.css" type="text/css">
<link rel="stylesheet" href="/css/style.css" type="text/css">
<!--[if lt IE 9]>
   <style>article,aside,dialog,footer,header,section,footer,nav,figure,menu{display:block}</style>
   <script src="http://cdn.staticfile.org/html5shiv/r29/html5.min.js"></script>
   <script src="http://cdn.staticfile.org/respond.js/1.4.2/respond.min.js"></script>
<link href="http://cdn.staticfile.org/respond.js/1.4.2/respond-proxy.html" id="respond-proxy" rel="respond-proxy" />
<link href="/js/respond.proxy.gif" id="respond-redirect" rel="respond-redirect" />
<script src="/js/respond.proxy.js"></script>
   <script src="http://cdn.bootcss.com/selectivizr/1.0.2/selectivizr-min.js"></script>
<![endif]-->
<script type="text/javascript">
function loadjs(c,d){var a=document.createElement("script");a.async=!0;a.type="text/javascript";a.src=c;a.charset=d||"gbk";(document.getElementsByTagName("HEAD")[0]||document.getElementsByTagName("BODY")[0]).appendChild(a)};
var _protocol = (("https:" == document.location.protocol) ? " https://" : " http://");
var _js2load = [];
</script>

</head>
<body>
      <header id="header" class="container"><nav id="main-nav" class="navbar navbar-default navbar-fixed-top " role="navigation">
  <div class="container">
    <div class="navbar-header">
    <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".navbar-collapse">
      <span class="sr-only">Toggle navigation</span>
      <span class="icon-bar"></span>
      <span class="icon-bar"></span>
      <span class="icon-bar"></span>
    </button>
    <a class="navbar-brand" href="/">It so life</a>
    </div>
    <div  class="collapse navbar-collapse">
      <ul  class="nav navbar-nav">
  
        <li><a href="/" title="Home">Home</a></li>      
        <li><a href="/about/" title="About">About</a></li>      
        <li><a href="/archives/" title="Archives">Archives</a></li>      
      <li class='dropdown'>
        <a class='dropdown-toggle' data-toggle='dropdown' href='#'>website<b class='caret'></b></a>
        <ul class='dropdown-menu pure-menu-selected'>
    
          <li><a href="//groups.google.com/forum/#!forum/pongba" title="TopLanguage">TopLanguage</a></li>    
          <li><a href="//itpub.net/" title="ITPub">ITPub</a></li>    
          <li><a href="//blog.jobbole.com/" title="Bole">Bole</a></li>    
          <li><a href="//nosql-database.org/" title="nosql">nosql</a></li>    
          <li><a href="//gitimmersion.googol.im/" title="Git">Git</a></li>    
        </ul>
      </li>
    
      </ul>
      <ul class="nav navbar-nav navbar-right">
      
        <li><a href="/atom.xml">RSS</a></li>
      
      
        <li><a href="https://twitter.com/robinchia">twitter</a></li>
      
      
      
      
        <li><a href="https://github.com/robinchia">github</a></li>
      
      </ul>
    </div>
  </div>
</nav>
<div class="clearfix"></div>
</header>
  <div id='content' class="container">
     <div class="page-header-wrapper">
      <!--[if lt IE 9]><div class="alert alert-warning alert-dismissable"><button type="button" class="close" data-dismiss="alert" aria-hidden="true">&times;</button><strong>提示:</strong>您的浏览器版本太低了,建议升级到 <strong><a href="http://windows.microsoft.com/zh-cn/internet-explorer/download-ie" title="IE9">IE9</a></strong> 以上,本站使用<a href="https://www.google.com/intl/zh-CN/chrome/">Chrome浏览器</a>可以获得最好的显示效果.</div><![endif]-->
      <div class="page-header"><h1 align="center"><big>It so life</big> </h1>
        <h5 align="center"><big>love as life</big></h5>
      </div>
     </div>
     <div class="row">
       <div id="main-col" class="alignleft col-sx-12 col-sm-8 col-md-9 col-lg-9">
      <section id='header_widget'></section>
          <div id="wrapper"><article>
<div id="post" class="post well">
  <div class="post-content">
    <header class="well-sm">
      <i class="fa icon fa-5x pull-left"></i>
      <h1 class="title"><a href="/2014/02/02/2014-02-02-hadoop--漫步云中网络/">漫步云中网络</a></h1>
      
        <span>Posted on<time datetime="2014-02-02T01:54:36.000Z"> <a href="/2014/02/02/2014-02-02-hadoop--漫步云中网络/">feb. 2 2014</a></time></span>      
    </header>
    
    
    <div class="entry">
      
        <h1 id="-">漫步云中网络</h1>
<p><a href="http://www.ibm.com/developerworks/cn/cloud/library/1209_zhanghua_openstacknetwork/#author1" target="_blank">张 华</a>, 高级软件工程师, IBM</p>
<p><a href="http://www.ibm.com/developerworks/cn/cloud/library/1209_zhanghua_openstacknetwork/#author2" target="_blank">龚 永生</a>, 资深软件工程师, IBM</p>
<p><strong>简介：</strong> 在生产环境中，云中的网络通常被划分为公共网络、管理网络和服务网络。本文首先通过三个小试验向您介绍了如何通过 TAP/TUN、NAT、Linux Bridge、VLAN 等技术实现云中网络的一般原理。有了这些基础，相信您会对接下来介绍的一个具体的 OpenStack 云的示例网络配置倍感亲切。同理，这些基础也将助您在其他云中网络中轻松漫步。
来源： <a href="[http://www.ibm.com/developerworks/cn/cloud/library/1209_zhanghua_openstacknetwork/](http://www.ibm.com/developerworks/cn/cloud/library/1209_zhanghua_openstacknetwork/)">[http://www.ibm.com/developerworks/cn/cloud/library/1209_zhanghua_openstacknetwork/](http://www.ibm.com/developerworks/cn/cloud/library/1209_zhanghua_openstacknetwork/)</a> </p>
<p>阅读本文前，最好能先了解以下的知识：</p>
<ul>
<li>了解 OpenStack 将有助于对本文的理解。本文讲解的是 Linux 虚拟网络中的一般原理方法 , 虽不仅限于应用在 OpenStack 之中 , 但本文的实验是以 OpenStack 为基础的。OpenStack 是一个开源的 IaaS 云 , 您可以从 devstack 脚本 (<a href="http://devstack.org/" target="_blank"><a href="http://devstack.org/">http://devstack.org/</a></a>) 开始熟悉它。</li>
<li>了解 QEMU 也将有助于对本文的理解。QEMU 是一种支持多种 CPU 的机器模拟器 , 本文采用 QEMU 来创建虚拟机验证本文中的试验。</li>
</ul>
<p><a href="">什么是云？</a></p>
<p>什么是云？我的理解是，为多租户提供各层次上的服务（如操作系统层、中间件层、应用软件层等）的可动态水平扩展的服务器集群称之为云。所以云具有大规模、高可扩展性、按需服务、自动化、节能环保、高可靠性等特点。下图１从软件堆栈视角勾画了云的架构：
<a href=""><strong>图 1. 云的架构</strong></a>
<img src="" alt="图 1. 云的架构"> </p>
<ul>
<li><strong>IaaS, Infrastructure as a Service，基础设施即服务：</strong>您可以简单理解为将可伸缩的操作系统（虚机或实机）实例作为基础设施服务卖给多租户，然后按需计算费用。当然，将操作系统作为基础设施服务只是 IaaS 中的一种，且是最主要的一种，我怕大家概念混淆所以就只重点提了这种。实际上，只要是基础设施提供服务了从概念上讲都应该叫 IaaS，比如说关系型数据库，如果是集群部署的话，它也是基础设施提供服务了，也应该叫 IaaS。这类产品如 IBM 的 Smart Cloud Entry，如开源的 OpenStack。</li>
<li><strong>PaaS, Platform as a Service, 平台即服务：</strong>您可以简单理解为将可伸缩的中间件资源作为平台服务卖给多租户，然后按需计算费用。举个例子，如果 SaaS 应用程序的并发瞬间加大的话，PaaS 可以自动实时地启动一个由 IaaS 提供的操作系统实例，然后自动在它上面部署中间件应用服务器（如 IBM 的 WebSphere），最后再部署一套该 SaaS 应用实例，并自动将它们纳入到负载均衡体系之中，从而实现平台服务的自动伸缩，这就是 PaaS。这类产品如 IBM 的 IWD，如 Google 的 App Engine。</li>
<li><strong>SaaS, Software as a Service, 软件即服务：</strong>您可以简单理解为可伸缩的分布式软件作为软件服务为用户提供某种在线服务，如视频服务，地图服务等。</li>
<li><strong>XaaS, X as a Server, 一切即服务：</strong>只要是给多租户按需提供服务都可以叫 XaaS, 像在 OpenStack 中，将网络部分代码单独抽出来组成 Quantum 工程，就可以叫网络即服务（NaaS, Network as a Service)；像使用 xCat 自动部署裸机可以叫裸机即服务（MaaS, Bare-metal as a Service)。</li>
</ul>
<p><a href="http://www.ibm.com/developerworks/cn/cloud/library/1209_zhanghua_openstacknetwork/#ibm-pcon" target="_blank">回页首</a></p>
<p><a href="">什么是云中网络？</a></p>
<p>在传统的数据中心中，每个网口对应唯一一个物理机；有了云，一台物理网卡可能会承载多个虚拟网卡。物理网卡与虚拟网卡之间的关系无外乎就是下列三种情况：</p>
<ol>
<li>一对一，一个物理网卡对应对一个虚拟网卡，是下面一对多情况的一种特例</li>
<li>一对多，一个物理网卡对应多个虚拟网卡，是本文要介绍的情况</li>
<li>多对一，多个物理网对应一个虚拟网卡，即我们常说的 Bonding，用作负载均衡
<a href=""><strong>图 2. 虚拟网络的主要内容</strong></a>
<img src="" alt="图 2. 虚拟网络的主要内容"> </li>
</ol>
<p>上图 2 显示了虚拟网络的主要内容：</p>
<ol>
<li>目前，对网络的虚拟化主要集中在第 2 层和第 3 层</li>
<li>在 Linux 中，第 2 层通常使用 TAP 设备来实现虚拟网卡，使用 Linux Bridge 来实现虚拟交换机</li>
<li>在 Linux 中，第 3 层通常是基于 Iptable 的 NAT，路由及转发</li>
<li>对于网络隔离，可以采用传统的基于 802.1Q 协议的 VLAN 技术，但这受限于 VLAN ID 大小范围的限制，并且需要手动地在各物理交换机上配置 VLAN；也可以采用虚拟交换机软件，如 Openvswitch，它可以自动创建 GRE 隧道来避免手动去为物理交换机配置 VLAN。</li>
</ol>
<p>下面将结合一个生产环境中的网络实例来讲解如何实现一个虚拟网络。</p>
<p><a href="http://www.ibm.com/developerworks/cn/cloud/library/1209_zhanghua_openstacknetwork/#ibm-pcon" target="_blank">回页首</a></p>
<p><a href="">云中网络实验</a></p>
<p>在生产环境中，按通用做法一般将云中网络划分为三大部分，公共网络、管理&amp;存储网络、服务网络。</p>
<p><strong>公共网络：</strong>用于云向外部租户提供 API 调用或者访问</p>
<p><strong>管理网络：</strong>用于云中各物理机之间的通信</p>
<p><strong>存储网络：</strong>用于 iSCSI 服务端与客户端之间的流量，一般与管理网络同</p>
<p><strong>服务网络：</strong>虚机内部使用的网络</p>
<p>为了将上述网络的实现原理讲清楚，我们选择了两台物理机做实验，并将采用 NAT、Linux Bridge、VLAN 技术分步实现一个典型的 OpenStack 云的网络拓扑。当然，这种网络的原理是通用的，并不仅限于 OpenStack 云。</p>
<ol>
<li>台式机 (node1)， 双有线网卡， 将作为控制节点、存储节点及一个计算节点</li>
<li>笔记本 (node2)，一有线网卡，将作为一个计算节点</li>
<li>路由器，家中 ADSL 宽带出口</li>
<li>交换机 , 用于连接各物理机</li>
</ol>
<p>值得一提的是，如果采用了 VLAN 技术进行网络隔离，且想要两台物理机上的虚机能够互访的话，交换机必须是支持 VLAN 的，且需要手动将交换机相应的端口配置成 Trunk 模式。因为我没有支持 VLAN 的物理交换机，在本实验中，我是采用直连线直接连接两台实验机器的。</p>
<p>下图 3 显示了实验网络拓扑：
<a href=""><strong>图 3. OpenStack 实验网络拓扑</strong></a>
<img src="" alt=""> </p>
<p><strong>公共网络：</strong>192.168.99.0/24 网段，外网用户通过公共网络上提供的服务来访问云。注意：在实际的生产环境中，公共网络一般采用外网 IP，因为我没有外网 IP，所以用 192.168.99.0 网段模拟。将台式机的一有线网卡 eth1 与 TP-Link 路由器相连即可。</p>
<p><strong>管理＆存储网络：</strong>172.16.99.0/24 网段，管理网络用于 OpenStack 各组件以及 DB、MQ 之间进行通信；存储网络用于存储节点和需要使用外部存储的计算节点之间的通信。将台式机的另一有线网卡 eth0 和笔记本电脑的有线网卡 eth0 连接到交换机即可。</p>
<p><strong>服务网络：</strong>10.0.0.0/8 网段，用于虚机内部。</p>
<p>两个节点的基本网络配置如下：
<a href=""><strong>清单 1. node1 的基本网络配置</strong></a></p>
<p>root@node1:/home/hua/# cat /etc/network/interfaces
auto lo</p>
<p>iface lo inet loopback
auto eth1</p>
<p>iface eth1 inet dhcp
up iptables -t nat -A POSTROUTING -s 172.16.99.0/24 -o eth1 -j MASQUERADE</p>
<p>auto eth0
iface eth0 inet static</p>
<p>address 172.16.99.108
netmask 255.255.255.0</p>
<p>network 172.16.99.0
broadcast 172.16.99.255</p>
<p><a href=""><strong>清单 2. node2 的基本网络配置</strong></a>
[</p>
<p>cat /etc/sysconfig/network-scripts/ifcfg-eth0
DEVICE=eth0</p>
<p>HWADDR=00:21:86:94:63:27
ONBOOT=yes</p>
<p>BOOTPROTO=static
USERCTL=yes</p>
<p>PEERDNS=yes
IPV6INIT=no</p>
<p>NM_CONTROLLED=yes
TYPE=Ethernet</p>
<p>NETMASK=255.255.255.0
IPADDR=172.16.99.109</p>
<p>NETWORK=172.16.99.0
GATEWAY=172.16.99.108</p>
<p>DNS1=202.106.195.68
DNS2=202.106.46.151</p>
<p>我虽然只用了两台物理机来模拟实际生产环境的部署模型， 但文中的这种部署结构是典型的，如果是大规模部署的话，只需要将控制节点上的每一个进程（如 DB、MQ、glance、keystone、nova-api、nova-schedule、nova-network 等）分布部署在每一台物理机即可。想要进一步的 HA 的话，可以：</p>
<ul>
<li>将 DB 配置成集群模式</li>
<li>将 MQ 配置成集群模式</li>
<li>采用 multi-host 模式，将 nova-network 同时安装在计算节点 (nova-compute) 上</li>
<li>将 nova-api、nova-schedule 这些无状态的服务也同时部署在计算节点上，再加上负载均衡器分发负载</li>
<li>采用多网卡做 Bonding
]()</li>
</ul>
<p><a href=""></a><a href="">NAT</a></p>
<p>node2 可以通过 NAT 方式访问外网，数据流向如下：</p>
<ol>
<li>node2 中需设置网关指向 node1 的 eth0，例： GATEWAY=172.16.99.108</li>
<li>在 node1 中打开 ipv4 转发功能，这样，node1 会相当于一台路由器，在 eth0 收到来自 node2 的数据之后，会将数据包转发到其他网卡 eth1， sysctl -w net.ipv4.ip_forward=1</li>
<li>在 node1 上设置 NAT 规则，这样，从 node2（172.16.99.0/24 网段）发出的数据包看来起就像从 node1 的 eth1 发出的一样： iptables -t nat -A POSTROUTING -s 172.16.99.0/24 -o eth1 -j MASQUERADE</li>
</ol>
<p><a href="">Linux Bridge</a></p>
<p>网桥 ( Bridge ) 工作在二层，了解链路层协议，按帧转发数据。就是交我们常说的交换机，所以连接到网桥的设备处于同一网段。
<a href=""><strong>图 4. 网桥示例</strong></a>
<img src="" alt="图 4. 网桥示例"> </p>
<p>上图 4 显示了 node1 网桥中的 VM1 与 node2 网桥中的 VM2 是如何通信的。在 openstack 中，这是典型的 multi-host 模式，即每一个计算节点均部署了网络服务来提供网关服务。Linux Bridge 充当了交换机的功能，而将 sysctl -w net.ipv4.ip_forward 设置为 1 也相当于 node1 同时充当了一个路由器（路由器的实质就是一个具有多个网卡的机器，因为它的多网卡同时具有这些不同网段的 IP 地址，所以它能将一个网络的流量路由转发到另一个网络）。</p>
<p>网桥，交换机，是用来连接两个 LAN 的。 是根据 MAC 与端口的映射进行转发的，而在虚机的网卡都是知道的，若从转发数据库知道目的 MAC 地址，以太网帧就只会正确的网桥端口传输，否则，就会扩散到网桥设备的所有端口。</p>
<p>因为网桥工作在第二层，所以 eth0.1, tap0, tap1 这些网卡均不需要设置 IP（因为对于上层路由器来说，它们是同一个子网，只需要将 IP 设置在 br1 上即可）。同时， 对 Linux 而言，网桥是虚拟设备，因此，除非将一个或多个真实设备绑定到网桥设备上，否则它就无法接收或传输任何东西。所以需要将一个真实设备（如 eth0）或者真实设备的 vlan 接口（如 eth0.1) 加入到网桥。对于前一种情况，将 eth0 加入到网桥之后，eth0 便不再具有 IP，这时候它与 tap0 这些虚拟网卡均通过 br1 处于 10.0.1.0/24 网络，当然我们也可以为网桥 br1 设置一个别名让它也具有 172.16.99.0/24 网管网段的 IP 地址。）</p>
<p>下面，我们来实现这个示例网桥，在 node1 与 node2 上分别执行下述脚本（对重要命令的描述请参见注释）：
<a href=""><strong>清单 3. node1 与 node2 的 Linux Bridge 配置脚本</strong></a></p>
<p>/#!/bin/sh
TAP=tap0</p>
<p>BRIDGE=br1
IFACE=eth0</p>
<p><strong>MANAGE_IP=172.16.99.108</strong></p>
<p>SERVICE_IP=10.0.1.1
GATEWAY=10.0.1.1</p>
<p>BROADCAST=10.0.1.255
/# 设置物理网卡为混杂模式</p>
<p>ifdown $IFACE
ifconfig $IFACE 0.0.0.0 promisc up</p>
<p>/# 创建网桥，并物理网卡加入网桥，同时设置网桥的 IP 为服务网络网段
brctl addbr $BRIDGE</p>
<p>brctl addif $BRIDGE $IFACE
brctl stp $BRIDGE on</p>
<p>ifconfig $BRIDGE $SERVICE_IP netmask 255.255.255.0 broadcast $BROADCAST
route add default gw $GATEWAY</p>
<p>/# 在网桥上设置多 IP，让它同时具有管理网段的 IP
ifconfig ${BRIDGE}:0 $MANAGE_IP netmask 255.255.255.0 broadcast 172.16.99.255</p>
<p>注意，上述黑体的一句在 node2 中需要作相应修改，其余不变，如下：
MANAGE_IP=172.16.99.109</p>
<p><a href="">VLAN</a></p>
<p>图 4 同样适用于 VLAN 网络，下面我们来实现它。在 node1 与 node2 上分别执行下述脚本：
<a href=""><strong>清单 4. node1 与 node2 的 VLAN 配置脚本</strong></a></p>
<p><strong>MAC=c8:3a:35:d7:86:da</strong></p>
<p>IP=10.0.1.1/24
ip link add link eth1 name eth1.1 type vlan id 1</p>
<p>ip link set eth1.1 up
brctl addbr br1</p>
<p>brctl setfd br1 0
brctl stp br1 on</p>
<p>ip link set br1 address $MAC
ip link set br1 up</p>
<p>brctl addif br1 eth1.1
ip addr add $IP dev br1</p>
<p>注意，上述黑体的一句在 node2 中需要作相应修改，其余不变，如下：
MAC= c8:3a:35:d7:86:db</p>
<p><a href="">测试</a></p>
<p>我们采用 QEMU 创建虚拟机来进行测试，其中网络部分的配置为：
  <interface type='bridge'></p>
<p>   <mac address='52:54:00:00:01:89'/>
 　　  <source bridge='br1'/></p>
<p> 　　  <address type='pci' domain='0x0000' bus='0x00' slot='0x03' function='0x0'/>
  </interface></p>
<p>我们使用一个现成的镜像，下载地址：
curl <a href="http://wiki.qemu.org/download/linux-0.2.img.bz2" target="_blank">http://wiki.qemu.org/download/linux-0.2.img.bz2</a> -o /bak/kvmimages/linux-0.2.img</p>
<p>在 node1 与 node2 上分别用下列配置定义两个虚机，注意，下面打粗体的部分（<mac address='52:54:00:00:01:89'/>）在两个节点中请设置不一样的值。</p>
<p>cat /etc/libvirt/qemu/test.xml
<a href=""><strong>清单 5. node1 与 node2 的虚机定义文件</strong></a></p>
<domain type='qemu'>
 <strong><name>VM1</name></strong>

<uuid></uuid>
<memory>393216</memory>

<currentMemory>393216</currentMemory>
<vcpu>1</vcpu>

<os>
<type arch='i686' machine='pc-1.0'>hvm</type>

<boot dev='hd'/>
</os>

<features>
<acpi/>

</features>
<clock offset='utc'/>

<on_poweroff>destroy</on_poweroff>
<on_reboot>restart</on_reboot>

<on_crash>destroy</on_crash>
<devices>

<emulator>/usr/bin/qemu-system-i386</emulator>
<disk type='block' device='disk'>

<driver name='qemu' type='raw'/>
<source dev='/bak/kvmimages/linux-0.2.img'/>

<target dev='hda' bus='ide'/>
<address type='drive' controller='0' bus='0' unit='0'/>

</disk>
<controller type='ide' index='0'>

<address type='pci' domain='0x0000' bus='0x00' slot='0x01' function='0x1'/>
</controller>

<interface type='bridge'>
 <strong><mac address='52:54:00:00:01:89'/></strong>

<source bridge='br1'/>
<address type='pci' domain='0x0000' bus='0x00' slot='0x03' function='0x0'/>

</interface>
<input type='tablet' bus='usb'/>

<input type='mouse' bus='ps2'/>
<graphics type='vnc' port='-1' autoport='yes' listen='127.0.0.1'>

<listen type='address' address='127.0.0.1'/>
</graphics>

<video>
<model type='cirrus' vram='9216' heads='1'/>

<address type='pci' domain='0x0000' bus='0x00' slot='0x02' function='0x0'/>
</video>

<memballoon model='virtio'>
<address type='pci' domain='0x0000' bus='0x00' slot='0x04' function='0x0'/>

</memballoon>
</devices>

</domain>

<p>然后用上述配置创建虚机（virsh define /etc/libvirt/qemu/test.xml ），接着启动虚机（ virsh start test ），最后设置虚机的 IP 和默认网关， 如下：</p>
<p><strong>VM1：</strong></p>
<p>ifconfig eth0 10.0.1.2 netmask 255.255.255.0 broadcast 10.0.1.255
route add default gw 10.0.1.1</p>
<p><strong>VM3：</strong>
ifconfig eth0 10.0.1.3 netmask 255.255.255.0 broadcast 10.0.1.255</p>
<p>route add default gw 10.0.1.1</p>
<p>这时候在一虚机上 ping 另一虚机 , 如果能够 ping 通，成功。若想要 ping 外网的话，还需在 /etc/resolv.conf 文件中添加域名，如下图 5 所示：
<a href=""><strong>图 5. 验证实验是否成功</strong></a>
<img src="" alt="图 5. 验证实验是否成功"> </p>
<p><a href="http://www.ibm.com/developerworks/cn/cloud/library/1209_zhanghua_openstacknetwork/#ibm-pcon" target="_blank">回页首</a></p>
<p><a href="">OpenStack 云中网络拓扑配置示例</a></p>
<p>在掌握了上述基本原理之后，应该不难理解下面 OpenStack 云中的网络拓扑。</p>
<p>在 OpenStack 中，目前存在着 nova-network 与 quantum 两种网络组件。nova-network 仅支持下列三种网络拓扑：</p>
<ul>
<li>FlatManager, 不支持 VLAN，也不支持 DHCP 的扁平网络</li>
<li>FlatDHCPManager，不支持 VLAN，但支持 DHCP 的扁平网络</li>
<li>VlanManager，支持 VLAN，也支持 DHCP 的 VLAN 隔离网络</li>
</ul>
<p>如今 nova-network 的代码已经全部挪到了 Quantum 工程中，但在网络拓扑方面，二者的原理是一致的，所以下面只给出一个典型的 nova-network 的网络配置，有了上面的基础，现在看这段配置是否会感到很亲切呢？
<a href=""><strong>清单 6. /etc/nova/nova.conf 中的网络配置示例</strong></a></p>
<p>/#/#/#/#/# nova-network /#/#/#/#/#
network_manager=nova.network.manager.VlanManager</p>
<p>public_interface=eth1
vlan_interface=eth0</p>
<p>network_host=node1
fixed_range=10.0.0.0/8</p>
<p>network_size=1024
dhcpbridge_flagfile=/etc/nova/nova.conf</p>
<p>dhcpbridge=/usr/bin/nova-dhcpbridge
force_dhcp_release=True</p>
<p>fixed_ip_disassociate_timeout=30
my_ip=172.16.99.108</p>
<p>routing_source_ip=192.168.99.108</p>
<p>相关参数说明如下：</p>
<ul>
<li>network_manager，目前支持 VlanManager、FlatManager、FlatDHCPManager 三种拓扑</li>
<li>public_interface, 接外网的物理网卡 , floating ip 功能需要用到它</li>
<li>valn_interface, 用于划分 VLAN 的物理网卡</li>
<li>fixed_range, 服务网络，即虚机内部所用的网络地址</li>
<li>my_ip，管理网络，用于安装 Openstack 组件的物理机之间的通信。例如：本实验中的控制节点同时具有外网网络地址 192.168.99.108 与管理网络地址 172.16.99.108，另一计算节点的管理网络地址为 172.16.99.109，所以 my_ip 应该设置为 172.16.99.108</li>
<li>routing_source_ip, NAT 映射后的公共网络 IP，设置了此参数，会自动执行 NAT 命令： iptables -t nat -A POSTROUTING -s 172.16.99.0/24 -o eth1 -j SNAT --to 192.168.99.108</li>
</ul>
<p>显然，如果没有区分公共网络与管理网络，即它们处于同一网段的话，并不需要配置 my_ip 及 routing_source_ip 两个参数。</p>
<p><a href="http://www.ibm.com/developerworks/cn/cloud/library/1209_zhanghua_openstacknetwork/#ibm-pcon" target="_blank">回页首</a></p>
<p><a href="">结论</a></p>
<p>云中网络一般被划分为公共网络、管理网络 &amp; 存储网络与服务网络三大类。虚拟网络拓扑一般有 NAT、Bridge、VLAN 三种情形。我们手工一步一步地通过 NAT、Bridge、VLAN 三个试验简单实现了一个上述典型的云中网络。原理都是相通的，您再看 OpenStack 云中网络或才其他云的网络时都会倍感亲切。</p>
<p><a href="">参考资料</a></p>
<p><strong>学习</strong></p>
<ul>
<li>参考 <a href="http://devstack.org/" target="_blank">Devstack 官网</a>，您可以从 devstack 脚本快速上手 Openstack。 </li>
<li>参考 <a href="http://docs.openstack.org/" target="_blank">Openstack 官网</a>，您可以获得更多关于 Openstack 的知识。 </li>
<li>参考 <a href="http://wiki.openstack.org/Quantum-Linux-Bridge-Plugin/" target="_blank">Quantum Wiki</a>, 您可以获得关于 Linux Bridge 网络的一般原理。 </li>
<li>“<a href="http://www.ibm.com/developerworks/cn/cloud/library/cl-openstack-cloud/" target="_blank">使用 OpenStack 实现云计算和存储</a>”（developerWorks，2012 年 9 月）：Infrastructure as a Service (IaaS) 云平台种类繁多，例如像 Nebula 和 Eucalyptus 这样为人熟知的解决方案。而此领域的一个新来者已展示了其不俗的增长，不仅包括用户数量的增长，还包括支持公司的数量的大量增长。在本文中，我们将了解这个开源平台 OpenStack，发现它是否真的是一种开源云操作系统。</li>
<li><p><a href="https://www.ibm.com/developerworks/cn/cloud/index.html" target="_blank">developerWorks 云计算站点</a> 提供了有关云计算的更新资源，包括</p>
</li>
<li><p>云计算 <a href="http://www.ibm.com/developerworks/cn/cloud/newto.html" target="_blank">简介</a>。</p>
</li>
<li>更新的 <a href="http://www.ibm.com/developerworks/cn/cloud/resources.html" target="_blank">技术文章和教程，以及网络广播</a>，让您的开发变得轻松，<a href="http://www.ibm.com/developerworks/cn/cloud/events.html" target="_blank">专家研讨会和录制会议</a> 帮助您成为高效的云开发人员。</li>
<li>连接转为云计算设计的 <a href="http://www.ibm.com/developerworks/cn/cloud/products.html" target="_blank">IBM 产品下载和信息</a>。</li>
<li>关于 <a href="http://www.ibm.com/developerworks/cn/cloud/collaborate.html" target="_blank">社区最新话题</a> 的活动聚合。</li>
<li>加入<a href="https://www.ibm.com/developerworks/mydeveloperworks/groups/service/html/communityview?communityUuid=e69e9c57-3210-49b0-a603-ca597c736557" target="_blank">云计算讨论组</a>，了解和讨论云计算的最新技术、解决方案、趋势等内容。 </li>
</ul>
<p><strong>讨论</strong></p>
<ul>
<li>加入 <a href="http://www.ibm.com/developerworks/cn/community/" target="_blank">developerWorks 中文社区</a>。查看开发人员推动的博客、论坛、组和维基，并与其他 developerWorks 用户交流。</li>
</ul>
<p><a href="">作者简介</a>
<a href=""></a>张华，IBM 高级软件工程师热衷于技术钻研，拥有丰富的搜索引擎、应用服务器、互联网、云计算领域的行业经验，精通 Java、JavaEE、Linux、Network 等技术，目前正从事 OpenStack 相关的工作。</p>
<p><a href=""></a>龚永生，IBM 资深软件工程师，热衷于开源软件，具有多年的 Linux，Java 和 JavaEE 经验。目前是 OpenStack 项目的积极贡献者。</p>

      
    </div>
    
    
      
    
    
        <footer id="post-meta">
        <span class="categories">Posted in<span class="breadcrumb fa fa-folder"><li><a href="/categories/hadoop/">hadoop</a></li></span></span> | <span class="tags">Tagged <a href="/tags/hadoop/" class="label label-primary">hadoop</a></span> | <span class="time">recent updated:<time title="2014-03-07 09:54:36"datetime="2014-03-07 09:54:36"> mar. 7 2014</time></span> | <span class="comment-link">
<a href="http://itsolife.com/2014/02/02/2014-02-02-hadoop--漫步云中网络/#comments" class="ds-thread-count comment-link" data-thread-key="2014-02-02-hadoop--漫步云中网络" data-count-type="comments">暂无评论</a></span>
        </footer>
    
    <div class="clearfix"></div>
  </div>
</div>
</article>



  <article>
<div id="post" class="post well">
  <div class="post-content">
    <header class="well-sm">
      <i class="fa icon fa-5x pull-left"></i>
      <h1 class="title"><a href="/2014/02/02/2014-02-02-hadoop--Hadoop集群_Hadoop安装配置/">Hadoop集群_Hadoop安装配置</a></h1>
      
        <span>Posted on<time datetime="2014-02-02T01:54:36.000Z"> <a href="/2014/02/02/2014-02-02-hadoop--Hadoop集群_Hadoop安装配置/">feb. 2 2014</a></time></span>      
    </header>
    
    
    <div class="entry">
      
        <h1 id="hadoop-_hadoop-">Hadoop集群_Hadoop安装配置</h1>
<h1 id="-hadoop-5-_hadoop-http-www-cnblogs-com-xia520pi-archive-2012-05-16-2503949-html-"><a href="http://www.cnblogs.com/xia520pi/archive/2012/05/16/2503949.html" target="_blank">Hadoop集群（第5期）_Hadoop安装配置</a></h1>
<h2 id="1-">1、集群部署介绍</h2>
<h3 id="1-1-hadoop-">1.1 Hadoop简介</h3>
<p><img src="" alt="">　　Hadoop是Apache软件基金会旗下的一个开源分布式计算平台。以Hadoop分布式文件系统（HDFS，Hadoop Distributed Filesystem）和MapReduce（Google MapReduce的开源实现）为<strong>核心</strong>的Hadoop为用户提供了系统底层细节透明的分布式基础架构。</p>
<p>对于Hadoop的集群来讲，可以分成两大类角色：Master和Salve。一个<strong>HDFS</strong>集群是由一个NameNode和若干个DataNode组成的。其中NameNode作为主服务器，管理文件系统的命名空间和客户端对文件系统的访问操作；集群中的DataNode管理存储的数据。<strong>MapReduce</strong>框架是由一个单独运行在主节点上的JobTracker和运行在每个集群从节点的TaskTracker共同组成的。主节点负责调度构成一个作业的所有任务，这些任务分布在不同的从节点上。主节点监控它们的执行情况，并且重新执行之前的失败任务；从节点仅负责由主节点指派的任务。当一个Job被提交时，JobTracker接收到提交作业和配置信息之后，就会将配置信息等分发给从节点，同时调度任务并监控TaskTracker的执行。</p>
<p>从上面的介绍可以看出，HDFS和MapReduce共同组成了Hadoop分布式系统体系结构的核心。<strong>HDFS</strong>在集群上实现分布式文件系统，<strong>MapReduce</strong>在集群上实现了分布式计算和任务处理。HDFS在MapReduce任务处理过程中提供了文件操作和存储等支持，MapReduce在HDFS的基础上实现了任务的分发、跟踪、执行等工作，并收集结果，二者相互作用，完成了Hadoop分布式集群的主要任务。</p>
<h3 id="1-2-">1.2 环境说明</h3>
<p>集群中包括4个节点：1个Master，3个Salve，节点之间局域网连接，可以相互ping通，具体集群信息可以查看&quot;<strong>Hadoop集群（第2期）</strong>&quot;。节点IP地址分布如下：</p>
<p><strong>机器名称</strong></p>
<p><strong>IP地址</strong>Master.Hadoop</p>
<p>192.168.1.2Salve1.Hadoop</p>
<p>192.168.1.3Salve2.Hadoop</p>
<p>192.168.1.4Salve3.Hadoop</p>
<p>192.168.1.5</p>
<p>四个节点上均是CentOS6.0系统，并且有一个相同的用户<strong>hadoop</strong>。Master机器主要配置NameNode和JobTracker的角色，负责总管分布式数据和分解任务的执行；3个Salve机器配置DataNode和TaskTracker的角色，负责分布式数据存储以及任务的执行。其实应该还应该有1个Master机器，用来作为<strong>备用</strong>，以防止Master服务器<strong>宕机</strong>，还有一个备用马上启用。后续经验积累一定阶段后<strong>补上</strong>一台备用Master机器。</p>
<h3 id="1-3-">1.3 网络配置</h3>
<p>Hadoop集群要按照<strong>1.2小节</strong>表格所示进行配置，我们在&quot;<strong>Hadoop集群（第1期）</strong>&quot;的CentOS6.0安装过程就按照提前规划好的主机名进行安装和配置。如果实验室后来人在安装系统时，没有配置好，不要紧，没有必要重新安装，在安装完系统之后仍然可以根据后来的规划对机器的主机名进行修改。</p>
<p>下面的例子我们将以Master机器为例，即主机名为&quot;Master.Hadoop&quot;，IP为&quot;192.168.1.2&quot;进行一些主机名配置的相关操作。其他的Slave机器以此为依据进行修改。</p>
<p><strong>1）查看当前机器名称</strong></p>
<p>用下面命令进行显示机器名称，如果跟规划的不一致，要按照下面进行修改。</p>
<p>hostname</p>
<p><img src="" alt=""></p>
<p>上图中，用&quot;hostname&quot;查&quot;Master&quot;机器的名字为&quot;Master.Hadoop&quot;，与我们预先规划的一致。</p>
<p><strong>2）修改当前机器名称</strong></p>
<p><strong>假定</strong>我们发现我们的机器的主机名不是我们想要的，通过对&quot;<strong>/etc/sysconfig/network</strong>&quot;文件修改其中&quot;<strong>HOSTNAME</strong>&quot;后面的值，改成我们规划的名称。</p>
<p>这个&quot;<strong>/etc/sysconfig/network</strong>&quot;文件是定义hostname和是否利用网络的不接触网络设备的对系统全体定义的文件。</p>
<p><strong>设定形式</strong>：设定值=值</p>
<p>&quot;/etc/sysconfig/network&quot;的<strong>设定项目</strong>如下：</p>
<p>NETWORKING 是否利用网络</p>
<p>GATEWAY 默认网关</p>
<p>IPGATEWAYDEV 默认网关的接口名</p>
<p>HOSTNAME 主机名</p>
<p>DOMAIN 域名</p>
<p>用下面命令进行修改当前机器的主机名（<strong>备注：</strong>修改系统文件一般用<strong>root</strong>用户）</p>
<p>vim /etc/sysconfig/network</p>
<p><img src="" alt=""></p>
<p>通过上面的命令我们从&quot;/etc/sysconfig/network&quot;中找到&quot;HOSTNAME&quot;进行修改，查看内容如下：</p>
<p><img src="" alt=""></p>
<p><strong>3）修改当前机器IP</strong></p>
<p><strong>假定</strong>我们的机器连IP在当时安装机器时都没有配置好，那此时我们需要对&quot;<strong>ifcfg-eth0</strong>&quot;文件进行配置，该文件位于&quot;<strong>/etc/sysconfig/network-scripts</strong>&quot;文件夹下。</p>
<p>在这个目录下面，存放的是网络接口（网卡）的制御脚本文件（控制文件），ifcfg- eth0是默认的第一个网络接口，如果机器中有多个网络接口，那么名字就将依此类推ifcfg-eth1，ifcfg-eth2，ifcfg- eth3，……。</p>
<p>这里面的文件是相当重要的，涉及到网络能否正常工作。</p>
<p>设定形式：设定值=值</p>
<p>设定项目项目如下：</p>
<p>DEVICE 接口名（设备,网卡）</p>
<p>BOOTPROTO IP的配置方法（static:固定IP， dhcpHCP， none:手动）</p>
<p>HWADDR MAC地址</p>
<p>ONBOOT 系统启动的时候网络接口是否有效（yes/no）</p>
<p>TYPE 网络类型（通常是Ethemet）</p>
<p>NETMASK 网络掩码</p>
<p><strong>IPADDR</strong> IP地址</p>
<p>IPV6INIT IPV6是否有效（yes/no）</p>
<p>GATEWAY 默认网关IP地址</p>
<p>查看&quot;/etc/sysconfig/network-scripts/ifcfg-eth0&quot;内容，如果IP不复核，就行修改。</p>
<p><img src="" alt=""></p>
<p>如果上图中IP与规划不相符，用下面命令进行修改：</p>
<p>vim /etc/sysconfig/network-scripts/ifcgf-eth0</p>
<p>修改完之后可以用&quot;ifconfig&quot;进行查看。</p>
<p><img src="" alt=""></p>
<p><strong>4）配置hosts文件（必须）</strong></p>
<p>&quot;<strong>/etc/hosts</strong>&quot;这个文件是用来配置主机将用的<strong>DNS</strong>服务器信息，是记载LAN内接续的各主机的对应[HostName和IP]用的。当用户在进行网络连接时，首先查找该文件，寻找对应主机名（或域名）对应的IP地址。</p>
<p>我们要测试两台机器之间知否连通，一般用&quot;ping 机器的IP&quot;，如果想用&quot;ping 机器的主机名&quot;发现找不见该名称的机器，解决的办法就是修改&quot;<strong>/etc/hosts</strong>&quot;这个文件，通过把LAN内的各主机的IP地址和HostName的<strong>一一对应</strong>写入这个文件的时候，就可以解决问题。</p>
<p>例如：机器为&quot;Master.Hadoop:192.168.1.2&quot;对机器为&quot;Salve1.Hadoop:192.168.1.3&quot;用命令&quot;ping&quot;记性连接测试。测试结果如下：</p>
<p><img src="" alt=""></p>
<p>从上图中的值，直接对IP地址进行测试，能够ping通，但是对主机名进行测试，发现没有ping通，提示&quot;unknown host——未知主机&quot;，这时查看&quot;Master.Hadoop&quot;的&quot;/etc/hosts&quot;文件内容。</p>
<p><img src="" alt=""></p>
<p>发现里面没有&quot;192.168.1.3 Slave1.Hadoop&quot;内容，故而本机器是无法对机器的主机名为&quot;Slave1.Hadoop&quot; 解析。</p>
<p>在进行<strong>Hadoop集群</strong>配置中，需要在&quot;/etc/hosts&quot;文件中添加集群中所有机器的IP与主机名，这样Master与所有的Slave机器之间不仅可以通过IP进行通信，而且还可以通过主机名进行通信。所以在所有的机器上的&quot;/etc/hosts&quot;文件<strong>末尾</strong>中都要添加如下内容：</p>
<p>192.168.1.2 Master.Hadoop</p>
<p>192.168.1.3 Slave1.Hadoop</p>
<p>192.168.1.4 Slave2.Hadoop</p>
<p>192.168.1.5 Slave3.Hadoop</p>
<p>用以下命令进行添加：</p>
<p>vim /etc/hosts</p>
<p><img src="" alt=""></p>
<p>添加结果如下：</p>
<p><img src="" alt=""></p>
<p>现在我们在进行对机器为&quot;Slave1.Hadoop&quot;的主机名进行ping通测试，看是否能测试成功。</p>
<p><img src="" alt=""></p>
<p>从上图中我们已经能用主机名进行ping通了，说明我们刚才添加的内容，在局域网内能进行DNS解析了，那么现在剩下的事儿就是在其余的Slave机器上进行相同的配置。然后进行测试。（<strong>备注：</strong>当设置SSH无密码验证后，可以&quot;scp&quot;进行复制，然后把原来的&quot;hosts&quot;文件执行覆盖即可。）</p>
<h3 id="1-4-">1.4 所需软件</h3>
<p><strong>1）JDK软件</strong></p>
<p>下载地址：<a href="http://www.oracle.com/technetwork/java/javase/index.html" target="_blank"><a href="http://www.oracle.com/technetwork/java/javase/index.html">http://www.oracle.com/technetwork/java/javase/index.html</a></a></p>
<p>JDK版本：jdk-6u31-linux-i586.bin</p>
<p><strong>2）Hadoop软件</strong></p>
<p>下载地址：<a href="http://hadoop.apache.org/common/releases.html" target="_blank"><a href="http://hadoop.apache.org/common/releases.html">http://hadoop.apache.org/common/releases.html</a></a></p>
<p>Hadoop版本：hadoop-1.0.0.tar.gz</p>
<h3 id="1-5-vsftp-">1.5 VSFTP上传</h3>
<p>在&quot;<strong>Hadoop集群（第3期）</strong>&quot;讲了VSFTP的安装及配置，如果没有安装VSFTP可以按照该文档进行安装。如果安装好了，就可以通过<strong>FlashFXP.exe</strong>软件把我们下载的JDK6.0和Hadoop1.0软件上传到&quot;<strong>Master.Hadoop:192.168.1.2</strong>&quot;服务器上。</p>
<p><img src="" alt=""></p>
<p>刚才我们用一般用户（hadoop）通过FlashFXP软件把所需的两个软件上传了跟目下，我们通过命令查看下一下是否已经上传了。</p>
<p><img src="" alt=""></p>
<p>从图中，我们的所需软件已经准备好了。</p>
<h2 id="2-ssh-">2、SSH无密码验证配置</h2>
<p>Hadoop运行过程中需要管理远端Hadoop守护进程，在Hadoop启动以后，NameNode是通过SSH（Secure Shell）来启动和停止各个DataNode上的各种守护进程的。这就必须在节点之间执行指令的时候是不需要输入密码的形式，故我们需要配置SSH运用无密码公钥认证的形式，这样NameNode使用SSH无密码登录并启动DataName进程，同样原理，DataNode上也能使用SSH无密码登录到NameNode。</p>
<h3 id="2-1-ssh-">2.1 安装和启动SSH协议</h3>
<p>在&quot;Hadoop集群（第1期）&quot;安装CentOS6.0时，我们选择了一些基本安装包，所以我们需要两个服务：ssh和rsync已经安装了。可以通过下面命令查看结果显示如下：</p>
<p>rpm –qa | grep openssh</p>
<p>rpm –qa | grep rsync</p>
<p><img src="" alt=""></p>
<p><strong>假设</strong>没有安装ssh和rsync，可以通过下面命令进行安装。</p>
<p>yum install ssh 安装SSH协议</p>
<p>yum install rsync （rsync是一个远程数据同步工具，可通过LAN/WAN快速同步多台主机间的文件）</p>
<p>service sshd restart 启动服务</p>
<p>确保所有的服务器都安装，上面命令执行完毕，各台机器之间可以通过密码验证相互登。</p>
<h3 id="2-2-master-salve">2.2 配置Master无密码登录所有Salve</h3>
<p><strong>1）SSH无密码原理</strong></p>
<p>Master（NameNode | JobTracker）作为客户端，要实现无密码公钥认证，连接到服务器Salve（DataNode | Tasktracker）上时，需要在Master上生成一个密钥对，包括一个公钥和一个私钥，而后将公钥复制到所有的Slave上。当Master通过SSH连接Salve时，Salve就会生成一个随机数并用Master的公钥对随机数进行加密，并发送给Master。Master收到加密数之后再用私钥解密，并将解密数回传给Slave，Slave确认解密数无误之后就允许Master进行连接了。这就是一个公钥认证过程，其间不需要用户手工输入密码。重要过程是将客户端Master复制到Slave上。</p>
<p><strong>2）Master机器上生成密码对</strong></p>
<p>在Master节点上执行以下命令：</p>
<p>ssh-keygen –t rsa –P &#39;&#39;</p>
<p>这条命是生成其<strong>无密码密钥对</strong>，询问其保存路径时<strong>直接回车</strong>采用默认路径。生成的密钥对：id_rsa和id_rsa.pub，默认存储在&quot;<strong>/home/hadoop/.ssh</strong>&quot;目录下。</p>
<p><img src="" alt=""></p>
<p>查看&quot;/home/hadoop/&quot;下是否有&quot;.ssh&quot;文件夹，且&quot;.ssh&quot;文件下是否有两个刚生产的无密码密钥对。</p>
<p><img src="" alt=""></p>
<p>接着在Master节点上做如下配置，把id_rsa.pub追加到授权的key里面去。</p>
<p>cat ~/.ssh/id_rsa.pub &gt;&gt; ~/.ssh/authorized_keys</p>
<p><img src="" alt=""></p>
<p>在验证前，需要做两件事儿。第一件事儿是修改文件&quot;<strong>authorized_keys</strong>&quot;权限（<strong>权限的设置非常重要，因为不安全的设置安全设置，会让你不能使用RSA功能</strong>），另一件事儿是用root用户设置&quot;<strong>/etc/ssh/sshd_config</strong>&quot;的内容。使其无密码登录有效。</p>
<p><strong>1）修改文件&quot;authorized_keys&quot;</strong></p>
<p>chmod 600 ~/.ssh/authorized_keys</p>
<p><img src="" alt=""></p>
<p><strong>备注：</strong>如果不进行设置，在验证时，扔提示你输入密码，在这里花费了将近半天时间来查找原因。在网上查到了几篇不错的文章，把作为&quot;<strong>Hadoop集群_第5期副刊_JDK和SSH无密码配置</strong>&quot;来帮助额外学习之用。</p>
<p><strong>2）设置SSH配置</strong></p>
<p>用<strong>root</strong>用户登录服务器修改SSH配置文件&quot;/etc/ssh/sshd_config&quot;的下列内容。</p>
<p><img src="" alt=""></p>
<p>RSAAuthentication yes /# 启用 RSA 认证</p>
<p>PubkeyAuthentication yes /# 启用公钥私钥配对认证方式</p>
<p>AuthorizedKeysFile .ssh/authorized_keys /# 公钥文件路径（和上面生成的文件同）</p>
<p>设置完之后记得<strong>重启SSH服务</strong>，才能使刚才设置有效。</p>
<p>service sshd restart</p>
<p><strong>退出root登录</strong>，使用<strong>hadoop</strong>普通用户验证是否成功。</p>
<p>ssh localhost</p>
<p><img src="" alt=""></p>
<p>从上图中得知无密码登录本级已经设置完毕，接下来的事儿是把<strong>公钥</strong>复制<strong>所有</strong>的Slave机器上。使用下面的命令格式进行复制公钥：</p>
<p>scp ~/.ssh/id_rsa.pub 远程用户名@远程服务器IP:~/</p>
<p>例如：</p>
<p>scp ~/.ssh/id_rsa.pub hadoop@192.168.1.3:~/</p>
<p>上面的命令是<strong>复制</strong>文件&quot;<strong>id_rsa.pub</strong>&quot;到服务器IP为&quot;<strong>192.168.1.3</strong>&quot;的用户为&quot;<strong>hadoop</strong>&quot;的&quot;<strong>/home/hadoop/</strong>&quot;下面。</p>
<p>下面就针对IP为&quot;192.168.1.3&quot;的Slave1.Hadoop的节点进行配置。</p>
<p><strong>1）把Master.Hadoop上的公钥复制到Slave1.Hadoop上</strong></p>
<hr>
<p><img src="" alt=""></p>
<p>从上图中我们得知，已经把文件&quot;id_rsa.pub&quot;传过去了，因为并没有建立起无密码连接，所以在连接时，仍然要提示输入输入Slave1.Hadoop服务器用户hadoop的密码。为了确保确实已经把文件传过去了，用SecureCRT登录Slave1.Hadoop:192.168.1.3服务器，查看&quot;/home/hadoop/&quot;下是否存在这个文件。</p>
<p><img src="" alt=""></p>
<p>从上面得知我们已经成功把公钥复制过去了。</p>
<p><strong>2）在&quot;/home/hadoop/&quot;下创建&quot;.ssh&quot;文件夹</strong></p>
<p>这一步<strong>并不是必须</strong>的，如果在Slave1.Hadoop的&quot;/home/hadoop&quot;<strong>已经存在</strong>就不需要创建了，因为我们之前并没有对Slave机器做过无密码登录配置，所以该文件是不存在的。用下面命令进行创建。（<strong>备注：</strong>用hadoop登录系统，如果不涉及系统文件修改，一般情况下都是用我们之前建立的普通用户hadoop进行执行命令。）</p>
<p>mkdir ~/.ssh</p>
<p>然后是修改文件夹&quot;<strong>.ssh</strong>&quot;的用户权限，把他的权限修改为&quot;<strong>700</strong>&quot;，用下面命令执行：</p>
<p>chmod 700 ~/.ssh</p>
<p><strong>备注：</strong>如果不进行，即使你按照前面的操作设置了&quot;authorized_keys&quot;权限，并配置了&quot;/etc/ssh/sshd_config&quot;，还重启了sshd服务，在Master能用&quot;ssh localhost&quot;进行无密码登录，但是对Slave1.Hadoop进行登录仍然需要输入密码，就是因为&quot;.ssh&quot;文件夹的权限设置不对。这个文件夹&quot;.ssh&quot;在配置SSH无密码登录时系统自动生成时，权限自动为&quot;700&quot;，如果是自己手动创建，它的组权限和其他权限都有，这样就会导致RSA无密码远程登录失败。</p>
<p><img src="" alt=""></p>
<p><img src="" alt=""></p>
<p>对比上面两张图，发现文件夹&quot;.ssh&quot;权限已经变了。</p>
<p><strong>3）追加到授权文件&quot;authorized_keys&quot;</strong></p>
<p>到目前为止Master.Hadoop的公钥也有了，文件夹&quot;.ssh&quot;也有了，且权限也修改了。这一步就是把Master.Hadoop的公钥<strong>追加</strong>到Slave1.Hadoop的授权文件&quot;authorized_keys&quot;中去。使用下面命令进行追加并修改&quot;authorized_keys&quot;文件权限：</p>
<p>cat ~/id_rsa.pub &gt;&gt; ~/.ssh/authorized_keys</p>
<p>chmod 600 ~/.ssh/authorized_keys</p>
<p><img src="" alt=""></p>
<p><strong>4）用root用户修改&quot;/etc/ssh/sshd_config&quot;</strong></p>
<p><strong>**具体步骤参考前面Master.Hadoop的&quot;</strong>设置SSH配置**&quot;，具体分为两步：第1是修改配置文件；第2是重启SSH服务。</p>
<p><strong>5）用Master.Hadoop使用SSH无密码登录Slave1.Hadoop</strong></p>
<p>当前面的步骤设置完毕，就可以使用下面命令格式进行SSH无密码登录了。</p>
<p>ssh 远程服务器IP</p>
<p><img src="" alt=""></p>
<p>从上图我们主要3个地方，第1个就是SSH无密码登录命令，第2、3个就是登录前后&quot;<strong>@</strong>&quot;后面的<strong>机器名</strong>变了，由&quot;<strong>Master</strong>&quot;变为了&quot;<strong>Slave1</strong>&quot;，这就说明我们已经成功实现了SSH无密码登录了。</p>
<p>最后记得把&quot;/home/hadoop/&quot;目录下的&quot;id_rsa.pub&quot;文件删除掉。</p>
<p>rm –r ~/id_rsa.pub</p>
<p><img src="" alt=""></p>
<p>到此为止，我们经过前5步已经实现了从&quot;Master.Hadoop&quot;到&quot;Slave1.Hadoop&quot;SSH无密码登录，下面就是重复上面的步骤把剩余的两台（Slave2.Hadoop和Slave3.Hadoop）Slave服务器进行配置。<strong>这样</strong>，我们就完成了&quot;配置Master无密码登录所有的Slave服务器&quot;。</p>
<h3 id="2-3-slave-master">2.3 配置所有Slave无密码登录Master</h3>
<p>和Master无密码登录所有Slave原理一样，就是把Slave的公钥<strong>追加</strong>到Master的&quot;.ssh&quot;文件夹下的&quot;authorized_keys&quot;中，记得是<strong>追加（&gt;&gt;）</strong>。</p>
<p>为了说明情况，我们现在就以&quot;Slave1.Hadoop&quot;无密码登录&quot;Master.Hadoop&quot;为例，进行一遍操作，也算是<strong>巩固</strong>一下前面所学知识，剩余的&quot;Slave2.Hadoop&quot;和&quot;Slave3.Hadoop&quot;就按照这个示例进行就可以了。</p>
<p>首先创建&quot;Slave1.Hadoop&quot;自己的公钥和私钥，并把自己的公钥追加到&quot;authorized_keys&quot;文件中。用到的命令如下：</p>
<p>ssh-keygen –t rsa –P &#39;&#39;</p>
<p>cat ~/.ssh/id_rsa.pub &gt;&gt; ~/.ssh/authorized_keys</p>
<p><img src="" alt=""></p>
<p>接着是用命令&quot;<strong>scp</strong>&quot;复制&quot;Slave1.Hadoop&quot;的公钥&quot;id_rsa.pub&quot;到&quot;Master.Hadoop&quot;的&quot;/home/hadoop/&quot;目录下，并<strong>追加</strong>到&quot;Master.Hadoop&quot;的&quot;authorized_keys&quot;中。</p>
<p><strong>1）在&quot;Slave1.Hadoop&quot;服务器的操作</strong></p>
<p>用到的命令如下：</p>
<p>scp ~/.ssh/id_rsa.pub hadoop@192.168.1.2:~/</p>
<p><img src="" alt=""></p>
<hr>
<p><strong>2）在&quot;Master.Hadoop&quot;服务器的操作</strong></p>
<p>用到的命令如下：</p>
<p>cat ~/id_rsa.pub &gt;&gt; ~/.ssh/authorized_keys</p>
<p><img src="" alt=""></p>
<p>然后删除掉刚才复制过来的&quot;id_rsa.pub&quot;文件。</p>
<p><img src="" alt=""></p>
<p>最后是测试从&quot;Slave1.Hadoop&quot;到&quot;Master.Hadoop&quot;无密码登录。</p>
<p><img src="" alt=""></p>
<p>从上面结果中可以看到已经成功实现了，再试下从&quot;Master.Hadoop&quot;到&quot;Slave1.Hadoop&quot;无密码登录。</p>
<p><img src="" alt=""></p>
<p>至此&quot;Master.Hadoop&quot;与&quot;Slave1.Hadoop&quot;之间可以互相无密码登录了，剩下的就是按照上面的步骤把剩余的&quot;Slave2.Hadoop&quot;和&quot;Slave3.Hadoop&quot;与&quot;Master.Hadoop&quot;之间建立起无密码登录。这样，Master能无密码验证登录每个Slave，每个Slave也能无密码验证登录到Master。</p>
<h2 id="3-java-">3、Java环境安装</h2>
<p>所有的机器上都要安装JDK，现在就先在Master服务器安装，然后其他服务器按照步骤重复进行即可。安装JDK以及配置环境变量，需要以&quot;<strong>root</strong>&quot;的身份进行。</p>
<h3 id="3-1-jdk">3.1 安装JDK</h3>
<p>首先用<strong>root</strong>身份登录&quot;Master.Hadoop&quot;后在&quot;/usr&quot;下创建&quot;java&quot;文件夹，再把用FTP上传到&quot;/home/hadoop/&quot;下的&quot;jdk-6u31-linux-i586.bin&quot;复制到&quot;/usr/java&quot;文件夹中。</p>
<p>mkdir /usr/java</p>
<p>cp /home/hadoop/ jdk-6u31-linux-i586.bin /usr/java</p>
<p><img src="" alt=""></p>
<p>接着<strong>进入</strong>&quot;<strong>/usr/java</strong>&quot;目录<strong>下</strong>通过下面命令使其JDK获得可执行权限，并安装JDK。</p>
<p>chmod +x jdk-6u31-linux-i586.bin</p>
<p>./jdk-6u31-linux-i586.bin</p>
<p><img src="" alt=""></p>
<p>按照上面几步进行操作，最后点击&quot;<strong>Enter</strong>&quot;键开始安装，安装完会提示你按&quot;<strong>Enter</strong>&quot;键退出，然后查看&quot;<strong>/usr/java</strong>&quot;下面会发现多了一个名为&quot;<strong>jdk1.6.0_31</strong>&quot;文件夹，说明我们的JDK安装结束，删除&quot;jdk-6u31-linux-i586.bin&quot;文件，进入下一个&quot;配置环境变量&quot;环节。</p>
<p><img src="" alt=""></p>
<h3 id="3-2-">3.2 配置环境变量</h3>
<p>编辑&quot;/etc/profile&quot;文件，在后面添加Java的&quot;JAVA_HOME&quot;、&quot;CLASSPATH&quot;以及&quot;PATH&quot;内容。</p>
<p><strong>1）编辑&quot;/etc/profile&quot;文件</strong></p>
<p>vim /etc/profile</p>
<p><img src="" alt=""></p>
<p><strong>2）添加Java环境变量</strong></p>
<p>在&quot;<strong>/etc/profile</strong>&quot;文件的<strong>尾部</strong>添加以下内容：</p>
<p>/# set java environment</p>
<p>export JAVA_HOME=/usr/java/jdk1.6.0_31/</p>
<p>export JRE_HOME=/usr/java/jdk1.6.0_31/jre</p>
<p>export CLASSPATH=.:$CLASSPATH:$JAVA_HOME/lib:$JRE_HOME/lib</p>
<p>export PATH=$PATH:$JAVA_HOME/bin:$JRE_HOME/bin</p>
<p>或者</p>
<p>/# set java environment</p>
<p>export JAVA_HOME=/usr/java/jdk1.6.0_31</p>
<p>export CLASSPATH=.:$CLASSPATH:$JAVA_HOME/lib:$JAVA_HOME/jre/lib</p>
<p>export PATH=$PATH:$JAVA_HOME/bin:$JAVA_HOME/jre/bin</p>
<p>以上两种意思一样，那么我们就选择<strong>第2种</strong>来进行设置。</p>
<p><img src="" alt=""></p>
<p><strong>3）使配置生效</strong></p>
<p>保存并退出，执行下面命令使其配置立即生效。</p>
<p>source /etc/profile</p>
<p><img src="" alt=""></p>
<h3 id="3-3-">3.3 验证安装成功</h3>
<p>配置完毕并生效后，用下面命令判断是否成功。</p>
<p>java -version</p>
<p><img src="" alt=""></p>
<p>从上图中得知，我们以确定JDK已经安装成功。</p>
<h3 id="3-4-">3.4 安装剩余机器</h3>
<p>这时用<strong>普通用户hadoop</strong>通过下面命令格式把&quot;Master.Hadoop&quot;文件夹&quot;/home/hadoop/&quot;的JDK复制到其他Slave的&quot;/home/hadoop/&quot;下面，剩下的事儿就是在其余的Slave服务器上按照上图的步骤安装JDK。</p>
<p>scp /home/hadoop/jdk-6u31-linux-i586.bin 远程用户名@远程服务器IP:~/</p>
<p>或者</p>
<p>scp ~/jdk-6u31-linux-i586.bin 远程用户名@远程服务器IP:~/</p>
<p><strong>备注：</strong>&quot;<strong>~</strong>&quot;代表<strong>当前</strong>用户的主目录，当<strong>前用户为hadoop</strong>，所以&quot;<strong>~</strong>&quot;代表&quot;<strong>/home/hadoop</strong>&quot;。</p>
<p><strong>例如：</strong>把JDK从&quot;Master.Hadoop&quot;复制到&quot;Slave1.Hadoop&quot;的命令如下。</p>
<p>scp ~/jdk-6u31-linux-i586 hadoop@192.168.1.3:~/</p>
<p><img src="" alt=""></p>
<p>然后查看&quot;Slave1.Hadoop&quot;的&quot;/home/hadoop&quot;查看是否已经复制成功了。</p>
<p><img src="" alt=""></p>
<p>从上图中得知，我们已经成功复制了，现在我们就用<strong>最高权限用户root</strong>进行安装了。其他的与这个一样。</p>
<h2 id="4-hadoop-">4、Hadoop集群安装</h2>
<p>所有的机器上都要安装hadoop，现在就先在Master服务器安装，然后其他服务器按照步骤重复进行即可。安装和配置hadoop需要以&quot;<strong>root</strong>&quot;的身份进行。</p>
<h3 id="4-1-hadoop">4.1 安装hadoop</h3>
<p>首先用<strong>root</strong>用户登录&quot;Master.Hadoop&quot;机器，查看我们之前用FTP上传至&quot;/home/Hadoop&quot;上传的&quot;<strong>hadoop-1.0.0.tar.gz</strong>&quot;。</p>
<p><img src="" alt=""></p>
<p>接着把&quot;<strong>hadoop-1.0.0.tar.gz</strong>&quot;复制到&quot;/usr&quot;目录下面。</p>
<p>cp /home/hadoop/hadoop-1.0.0.tar.gz /usr</p>
<p><img src="" alt=""></p>
<p>下一步进入&quot;/usr&quot;目录下，用下面命令把&quot;<strong>hadoop-1.0.0.tar.gz</strong>&quot;进行解压，并将其命名为&quot;hadoop&quot;，把该文件夹的<strong>读权限</strong>分配给普通用户<strong>hadoop</strong>，然后删除&quot;<strong>hadoop-1.0.0.tar.gz</strong>&quot;安装包。</p>
<p>cd /usr /#进入&quot;/usr&quot;目录</p>
<p>tar –zxvf hadoop-1.0.0.tar.gz /#解压&quot;<strong>hadoop-1.0.0.tar.gz</strong>&quot;安装包</p>
<p>mv hadoop-1.0.0 hadoop /#将&quot;hadoop-1.0.0&quot;文件夹<strong>重命名</strong>&quot;hadoop&quot;</p>
<p>chown <strong>–R</strong> hadoop:hadoop hadoop /#<strong>将文件夹&quot;hadoop&quot;读权限分配给hadoop用户</strong></p>
<p>rm –rf hadoop-1.0.0.tar.gz /#删除&quot;<strong>hadoop-1.0.0.tar.gz</strong>&quot;安装包</p>
<p><img src="" alt=""></p>
<p>解压后，并重命名。</p>
<p><img src="" alt=""></p>
<p>把&quot;/usr/hadoop&quot;<strong>读权</strong>限分配给<strong>hadoop</strong>用户（<strong>非常重要</strong>）</p>
<p><img src="" alt=""></p>
<p>删除&quot;<strong>hadoop-1.0.0.tar.gz</strong>&quot;安装包</p>
<p><img src="" alt=""></p>
<p>最后在&quot;<strong>/usr/hadoop</strong>&quot;下面创建<strong>tmp</strong>文件夹，把Hadoop的安装路径添加到&quot;<strong>/etc/profile</strong>&quot;中，修改&quot;/etc/profile&quot;文件（配置java环境变量的文件），将以下语句添加到<strong>末尾</strong>，并使其有效：</p>
<p>/# set hadoop path</p>
<p>export HADOOP_HOME=/usr/hadoop</p>
<p>export PATH=$PATH :$HADOOP_HOME/bin</p>
<p><strong>1）在&quot;/usr/hadoop&quot;创建&quot;tmp&quot;文件夹</strong></p>
<p>mkdir /usr/hadoop/tmp</p>
<p><img src="" alt=""></p>
<p><strong>2）配置&quot;/etc/profile&quot;</strong></p>
<p>vim /etc/profile</p>
<p><img src="" alt=""></p>
<p>配置后的文件如下：</p>
<p><img src="" alt=""></p>
<p><strong>3）重启&quot;/etc/profile&quot;</strong></p>
<p>source /etc/profile</p>
<p><img src="" alt=""></p>
<h3 id="4-2-hadoop">4.2 配置hadoop</h3>
<p><strong>1）配置hadoop-env.sh</strong></p>
<p>该&quot;<strong>hadoop-env.sh</strong>&quot;文件位于&quot;<strong>/usr/hadoop/conf</strong>&quot;目录下。</p>
<p><img src="" alt=""></p>
<p>在文件的末尾添加下面内容。</p>
<p>/# set java environment</p>
<p>export JAVA_HOME=/usr/java/jdk1.6.0_31</p>
<p><img src="" alt=""></p>
<p>Hadoop配置文件在conf目录下，之前的版本的配置文件主要是Hadoop-default.xml和Hadoop-site.xml。由于Hadoop发展迅速，代码量急剧增加，代码开发分为了core，hdfs和map/reduce三部分，配置文件也被分成了三个core-site.xml、hdfs-site.xml、mapred-site.xml。core-site.xml和hdfs-site.xml是站在HDFS角度上配置文件；core-site.xml和mapred-site.xml是站在MapReduce角度上配置文件。</p>
<p><strong>2）配置core-site.xml文件</strong></p>
<p>修改Hadoop核心配置文件core-site.xml，这里配置的是HDFS的地址和端口号。</p>
<configuration>

<property>

<name>hadoop.tmp.dir</name>

<value>/usr/hadoop/tmp</value>

（<strong>备注：</strong>请先在 /usr/hadoop 目录下建立 tmp 文件夹）

<description>A base for other temporary directories.</description>

</property>

<!-- file system properties -->

<property>

<name>fs.default.name</name>

<value>hdfs://<strong>192.168.1.2</strong>:<strong>9000</strong></value>

</property>

</configuration>

<p><strong>备注：</strong>如没有配置hadoop.tmp.dir参数，此时系统默认的临时目录为：/tmp/hadoo-hadoop。而这个目录在每次重启后都会被干掉，必须重新执行format才行，否则会出错。</p>
<p>用下面命令进行编辑：</p>
<p><img src="" alt=""></p>
<p>编辑结果显示如下：</p>
<p><img src="" alt=""></p>
<p><strong>3）配置hdfs-site.xml文件</strong></p>
<p>修改Hadoop中HDFS的配置，配置的备份方式默认为3。</p>
<configuration>

<property>

<name>dfs.replication</name>

<value><strong>1</strong></value>

(<strong>备注：</strong>replication 是数据副本数量，默认为3，salve少于3台就会报错)

</property>

<configuration>

用下面命令进行编辑：

<img src="" alt="">

编辑结果显示如下：

<img src="" alt="">

<strong>4）配置mapred-site.xml文件</strong>

修改Hadoop中MapReduce的配置文件，配置的是JobTracker的地址和端口。

<configuration>

<property>

<name>mapred.job.tracker</name>

<value><a href="http://**192.168.1.2**:**9001**" target="_blank">http://**192.168.1.2**:**9001**</a></value>

</property>

</configuration>

<p>用下面命令进行编辑：</p>
<p><img src="" alt=""></p>
<p>编辑结果显示如下：</p>
<p><img src="" alt=""></p>
<p><strong>5）配置masters文件</strong></p>
<p>有两种方案：</p>
<p>（1）第一种</p>
<p>修改localhost为Master.Hadoop</p>
<p>（2）第二种</p>
<p>去掉&quot;localhost&quot;，加入Master机器的IP：192.168.1.2</p>
<p>为保险起见，启用第二种，因为万一忘记配置&quot;/etc/hosts&quot;局域网的DNS失效，这样就会出现意想不到的错误，但是一旦IP配对，网络畅通，就能通过IP找到相应主机。</p>
<p>用下面命令进行修改：</p>
<p><img src="" alt=""></p>
<p>编辑结果显示如下：</p>
<p><img src="" alt=""></p>
<p><strong>6）配置slaves文件（Master主机特有</strong>）</p>
<p>有两种方案：</p>
<p>（1）第一种</p>
<p>去掉&quot;localhost&quot;，每行只添加一个主机名，把剩余的Slave主机名都填上。</p>
<p>例如：添加形式如下</p>
<p>Slave1.Hadoop</p>
<p>Slave2.Hadoop</p>
<p>Slave3.Hadoop</p>
<p>（2）第二种</p>
<p>去掉&quot;localhost&quot;，加入集群中所有Slave机器的IP，也是每行一个。</p>
<p>例如：添加形式如下</p>
<p>192.168.1.3</p>
<p>192.168.1.4</p>
<p>192.168.1.5</p>
<p>原因和添加&quot;masters&quot;文件一样，选择第二种方式。</p>
<p>用下面命令进行修改：</p>
<p><img src="" alt=""></p>
<p>编辑结果如下：</p>
<p><img src="" alt=""></p>
<p>现在在Master机器上的Hadoop配置就结束了，剩下的就是配置Slave机器上的Hadoop。</p>
<p><strong>一种方式</strong>是按照上面的步骤，把Hadoop的安装包在用<strong>普通用户hadoop</strong>通过&quot;<strong>scp</strong>&quot;复制到其他机器的&quot;/home/hadoop&quot;目录下，然后根据实际情况进行安装配置，<strong>除了第6步，那是Master特有的</strong>。用下面命令格式进行。（<strong>备注：</strong>此时切换到普通用户hadoop）</p>
<p>scp ~/hadoop-1.0.0.tar.gz hadoop@服务器IP:~/</p>
<p>例如：从&quot;Master.Hadoop&quot;到&quot;Slave1.Hadoop&quot;复制Hadoop的安装包。</p>
<p><img src="" alt=""></p>
<p><strong>另一种方式</strong>是将 Master上配置好的hadoop所在文件夹&quot;<strong>/usr/hadoop</strong>&quot;复制到所有的Slave的&quot;/usr&quot;目录下（实际上Slave机器上的slavers文件是不必要的， 复制了也没问题）。用下面命令格式进行。（<strong>备注：</strong>此时用户可以为hadoop也可以为root）</p>
<p>scp <strong>-r</strong> /usr/hadoop <strong>root</strong>@服务器IP:/usr/</p>
<p>例如：从&quot;Master.Hadoop&quot;到&quot;Slave1.Hadoop&quot;复制配置Hadoop的文件。</p>
<p><img src="" alt=""></p>
<p>上图中以root用户进行复制，当然不管是用户root还是hadoop，虽然Master机器上的&quot;/usr/hadoop&quot;文件夹用户hadoop有权限，但是Slave1上的hadoop用户却没有&quot;/usr&quot;权限，所以没有创建文件夹的权限。所以无论是哪个用户进行拷贝，右面都是&quot;root@机器IP&quot;格式。因为我们只是建立起了hadoop用户的SSH无密码连接，所以用root进行&quot;scp&quot;时，扔提示让你输入&quot;Slave1.Hadoop&quot;服务器用户root的密码。</p>
<p>查看&quot;Slave1.Hadoop&quot;服务器的&quot;/usr&quot;目录下是否已经存在&quot;hadoop&quot;文件夹，确认已经复制成功。查看结果如下：</p>
<p><img src="" alt=""></p>
<p>从上图中知道，hadoop文件夹确实已经复制了，但是我们发现hadoop权限是root，所以我们现在要给&quot;Slave1.Hadoop&quot;服务器上的用户hadoop添加对&quot;/usr/hadoop&quot;读权限。</p>
<p>以<strong>root</strong>用户登录&quot;Slave1.Hadoop&quot;，执行下面命令。</p>
<p>chown <strong>-R</strong> hadoop:hadoop（<strong>用户名：用户组</strong>） hadoop（<strong>文件夹</strong>）</p>
<p><img src="" alt=""></p>
<p>接着在&quot;Slave1 .Hadoop&quot;上修改&quot;/etc/profile&quot;文件（配置 java 环境变量的文件），将以下语句添加到末尾，并使其有效（source /etc/profile）：</p>
<p>/# set hadoop environment</p>
<p>export HADOOP_HOME=/usr/hadoop</p>
<p>export PATH=$PATH :$HADOOP_HOME/bin</p>
<p>如果不知道怎么设置，可以查看前面&quot;Master.Hadoop&quot;机器的&quot;/etc/profile&quot;文件的配置，到此为此在一台Slave机器上的Hadoop配置就结束了。剩下的事儿就是照葫芦画瓢把剩余的几台Slave机器按照《<strong>从&quot;Master.Hadoop&quot;到&quot;Slave1.Hadoop&quot;复制Hadoop的安装包。</strong>》这个例子进行部署Hadoop。</p>
<h3 id="4-3-">4.3 启动及验证</h3>
<p><strong>1）格式化HDFS文件系统</strong></p>
<p>在&quot;Master.Hadoop&quot;上使用<strong>普通</strong>用户<strong>hadoop</strong>进行操作。（<strong>备注：</strong>只需一次，下次启动不再需要格式化，只需 start-all.sh）</p>
<p>hadoop namenode -format</p>
<p>某些书上和网上的某些资料中用下面命令执行。</p>
<p><img src="" alt=""></p>
<p>我们在看好多文档包括有些书上，按照他们的hadoop环境变量进行配置后，并立即使其生效，但是执行发现没有找见&quot;bin/hadoop&quot;这个命令。</p>
<p><img src="" alt=""></p>
<p>其实我们会发现我们的环境变量配置的是&quot;<strong>$HADOOP_HOME/bin</strong>&quot;，我们已经把bin包含进入了，所以执行时，加上&quot;bin&quot;反而找不到该命令，除非我们的hadoop坏境变量如下设置。</p>
<p>/# set hadoop path</p>
<p>export HADOOP_HOME=/usr/hadoop</p>
<p>export PATH=$PATH :<strong>$HADOOP_HOME</strong>:<strong>$HADOOP_HOME/bin</strong></p>
<p>这样就能直接使用&quot;bin/hadoop&quot;也可以直接使用&quot;hadoop&quot;，现在不管哪种情况，hadoop命令都能找见了。我们也没有必要重新在设置hadoop环境变量了，只需要记住执行Hadoop命令时不需要在前面加&quot;bin&quot;就可以了。</p>
<p><img src="" alt=""></p>
<p><img src="" alt=""></p>
<p>从上图中知道我们已经成功格式话了，但是美中不足就是出现了一个<strong>警告</strong>，从网上的得知这个警告并不影响hadoop执行，但是也有办法解决，详情看后面的&quot;常见问题FAQ&quot;。</p>
<p><strong>2）启动hadoop</strong></p>
<p>在启动前关闭集群中所有机器的防火墙，不然会出现datanode开后又自动关闭。</p>
<p>service iptables stop</p>
<p>使用下面命令启动。</p>
<p>start-all.sh</p>
<p><img src="" alt=""></p>
<p>执行结果如下：</p>
<p><img src="" alt=""></p>
<p>可以通过以下启动日志看出，首先启动namenode 接着启动datanode1，datanode2，…，然后启动secondarynamenode。再启动jobtracker，然后启动tasktracker1，tasktracker2，…。</p>
<p>启动 hadoop成功后，在 Master 中的 tmp 文件夹中生成了 dfs 文件夹，在Slave 中的 tmp 文件夹中均生成了 dfs 文件夹和 mapred 文件夹。</p>
<p>查看Master中&quot;/usr/hadoop/tmp&quot;文件夹内容</p>
<p><img src="" alt=""></p>
<p>查看Slave1中&quot;/usr/hadoop/tmp&quot;文件夹内容。</p>
<p><img src="" alt=""></p>
<p><strong>3）验证hadoop</strong></p>
<p>（1）验证方法一：用&quot;jps&quot;命令</p>
<p>在Master上用 java自带的小工具<strong>jps</strong>查看进程。</p>
<p><img src="" alt=""></p>
<p>在Slave1上用jps查看进程。</p>
<p><img src="" alt=""></p>
<p>如果在查看Slave机器中发现&quot;DataNode&quot;和&quot;TaskTracker&quot;没有起来时，先查看一下日志的，如果是&quot;namespaceID&quot;不一致问题，采用&quot;常见问题FAQ6.2&quot;进行解决，如果是&quot;No route to host&quot;问题，采用&quot;常见问题FAQ6.3&quot;进行解决。</p>
<p>（2）验证方式二：用&quot;hadoop dfsadmin -report&quot;</p>
<p>用这个命令可以查看Hadoop集群的状态。</p>
<p>Master服务器的状态：</p>
<p><img src="" alt=""></p>
<p>Slave服务器的状态</p>
<p><img src="" alt=""></p>
<h3 id="4-4-">4.4 网页查看集群</h3>
<p><strong>1）访问&quot;http:192.168.1.2:50030&quot;</strong></p>
<p><img src="" alt=""></p>
<p>2）访问&quot;<strong>http:192.168.1.2:50070</strong>&quot;</p>
<p><img src="" alt=""></p>
<h2 id="5-faq">5、常见问题FAQ</h2>
<h3 id="5-1-warning-hadoop_home-is-deprecated-">5.1 关于 Warning: $HADOOP_HOME is deprecated.</h3>
<p>hadoop 1.0.0版本，安装完之后敲入hadoop命令时，<strong>老</strong>是提示这个警告：</p>
<p>Warning: $HADOOP_HOME is deprecated.</p>
<p>经查hadoop-1.0.0/bin/hadoop脚本和&quot;hadoop-config.sh&quot;脚本，发现脚本中对HADOOP_HOME的环境变量设置做了判断，笔者的环境根本不需要设置HADOOP_HOME环境变量。</p>
<p>解决方案一：编辑&quot;/etc/profile&quot;文件，去掉HADOOP_HOME的变量设定，重新输入hadoop fs命令，警告消失。</p>
<p>解决方案二：编辑&quot;/etc/profile&quot;文件，添加一个环境变量，之后警告消失：</p>
<p>export HADOOP_HOME_WARN_SUPPRESS=1</p>
<p>解决方案三：编辑&quot;hadoop-config.sh&quot;文件，把下面的&quot;if - fi&quot;功能注释掉。</p>
<p><img src="" alt=""></p>
<p>我们这里本着不动Hadoop原配置文件的前提下，采用&quot;<strong>方案二</strong>&quot;，在&quot;/etc/profile&quot;文件添加上面内容，并用命令&quot;source /etc/profile&quot;使之有效。</p>
<p><strong>1）切换至root用户</strong></p>
<p><img src="" alt=""></p>
<p><strong>2）添加内容</strong></p>
<p><img src="" alt=""></p>
<p><strong>3）重新生效</strong></p>
<p><img src="" alt=""></p>
<h3 id="5-2-no-datanode-to-stop-">5.2 解决&quot;no datanode to stop&quot;问题</h3>
<p>当我停止Hadoop时发现如下信息：</p>
<p><img src="" alt=""></p>
<p>原因：每次namenode format会重新创建一个namenodeId，而tmp/dfs/data下包含了上次format下的id，namenode format清空了namenode下的数据，但是没有清空datanode下的数据，导致启动时失败，所要做的就是每次fotmat前，清空tmp一下的所有目录。</p>
<p><strong>第一种解决方案如下：</strong></p>
<p><strong>1）先删除&quot;/usr/hadoop/tmp&quot;</strong></p>
<p>rm -rf /usr/hadoop/tmp</p>
<p><strong>2）创建&quot;/usr/hadoop/tmp&quot;文件夹</strong></p>
<p>mkdir /usr/hadoop/tmp</p>
<p><strong>3）删除&quot;/tmp&quot;下以&quot;hadoop&quot;开头文件</strong></p>
<p>rm -rf /tmp/hadoop/*</p>
<p><strong>4）重新格式化hadoop</strong></p>
<p>hadoop namenode -format</p>
<p><strong>5）启动hadoop</strong></p>
<p>start-all.sh</p>
<p>使用第一种方案，有种不好处就是原来集群上的重要数据全没有了。假如说Hadoop集群已经运行了一段时间。建议采用第二种。</p>
<p><strong>第二种方案如下：</strong></p>
<p>1）修改每个Slave的namespaceID使其与Master的namespaceID一致。</p>
<p>或者</p>
<p>2）修改Master的namespaceID使其与Slave的namespaceID一致。</p>
<p>该&quot;namespaceID&quot;位于&quot;<strong>/usr/hadoop/tmp/dfs/data/current/VERSION</strong>&quot;文件中，前面<strong>蓝色</strong>的可能根据实际情况变化，但后面<strong>红色</strong>是不变的。</p>
<p>例如：查看&quot;Master&quot;下的&quot;<strong>VERSION</strong>&quot;文件</p>
<p><img src="" alt=""></p>
<p>本人建议采用<strong>第二种</strong>，这样方便快捷，而且还能防止误删。</p>
<h3 id="5-3-slave-datanode-">5.3 Slave服务器中datanode启动后又自动关闭</h3>
<p>查看日志发下如下错误。</p>
<p><strong>ERROR</strong> org.apache.hadoop.hdfs.server.datanode.DataNode: java.io.IOException: Call to ... failed on local exception: java.net.NoRouteToHostException: <strong>No route to host</strong></p>
<p>解决方案是：关闭防火墙</p>
<p>service iptables stop</p>
<h3 id="5-4-hdfs-">5.4 从本地往hdfs文件系统上传文件</h3>
<p>出现如下错误：</p>
<p>INFO hdfs.DFSClient: Exception in createBlockOutputStream java.io.IOException: <strong>Bad connect ack with firstBadLink</strong></p>
<p>INFO hdfs.DFSClient: Abandoning block blk_-1300529705803292651_37023</p>
<p>WARN hdfs.DFSClient: DataStreamer Exception: java.io.IOException: <strong>Unable to create new block.</strong></p>
<p>解决方案是：</p>
<p><strong>1）关闭防火墙</strong></p>
<p>service iptables stop</p>
<p><strong>2）禁用selinux</strong></p>
<p>编辑 &quot;<strong>/etc/selinux/config</strong>&quot;文件，设置&quot;<strong>SELINUX</strong>=<strong>disabled</strong>&quot;</p>
<h3 id="5-5-">5.5 安全模式导致的错误</h3>
<p>出现如下错误：</p>
<p>org.apache.hadoop.dfs.SafeModeException: <strong>Cannot delete ..., Name node is in safe mode</strong></p>
<p>在分布式文件系统启动的时候，开始的时候会有安全模式，当分布式文件系统处于安全模式的情况下，文件系统中的内容不允许修改也不允许删除，直到安全模式结束。安全模式主要是为了系统启动的时候检查各个DataNode上数据块的有效性，同时根据策略必要的复制或者删除部分数据块。运行期通过命令也可以进入安全模式。在实践过程中，系统启动的时候去修改和删除文件也会有安全模式不允许修改的出错提示，只需要等待一会儿即可。</p>
<p>解决方案是：关闭安全模式</p>
<p>hadoop dfsadmin -safemode leave</p>
<h3 id="5-6-exceeded-max_failed_unique_fetches">5.6 解决Exceeded MAX_FAILED_UNIQUE_FETCHES</h3>
<p>出现错误如下：</p>
<p>Shuffle Error: Exceeded MAX_FAILED_UNIQUE_FETCHES; bailing-out</p>
<p>程序里面需要打开多个文件，进行分析，系统一般默认数量是1024，（用ulimit -a可以看到）对于正常使用是够了，但是对于程序来讲，就太少了。</p>
<p>解决方案是：修改2个文件。</p>
<p><strong>1）&quot;/etc/security/limits.conf&quot;</strong></p>
<p>vim /etc/security/limits.conf</p>
<p>加上：</p>
<p>soft nofile 102400</p>
<p>hard nofile 409600</p>
<p><strong>2）&quot;/etc/pam.d/login&quot;</strong></p>
<p>vim /etc/pam.d/login</p>
<p>添加：</p>
<p>session required /lib/security/pam_limits.so</p>
<p>针对第一个问题我纠正下答案：</p>
<p>这是reduce预处理阶段shuffle时获取已完成的map的输出失败次数超过上限造成的，上限默认为5。引起此问题的方式可能会有很多种，比如网络连接不正常，连接超时，带宽较差以及端口阻塞等。通常框架内网络情况较好是不会出现此错误的。</p>
<h3 id="5-7-too-many-fetch-failures-">5.7 解决&quot;Too many fetch-failures&quot;</h3>
<p>出现这个问题主要是结点间的连通不够全面。</p>
<p>解决方案是：</p>
<p><strong>1）检查&quot;/etc/hosts&quot;</strong></p>
<p>要求本机ip 对应 服务器名</p>
<p>要求要包含所有的服务器ip +服务器名</p>
<p><strong>2）检查&quot;.ssh/authorized_keys&quot;</strong></p>
<p>要求包含所有服务器（包括其自身）的public key</p>
<h3 id="5-8-">5.8 处理速度特别的慢</h3>
<p>出现<strong>map</strong>很<strong>快</strong>，但是<strong>reduce</strong>很<strong>慢</strong>，而且反复出现&quot;<strong>reduce=0%</strong>&quot;。</p>
<p>解决方案如下：</p>
<p>结合解决方案5.7，然后修改&quot;conf/hadoop-env.sh&quot;中的&quot;export HADOOP_HEAPSIZE=4000&quot;</p>
<h3 id="5-9-hadoop-outofmemoryerror-">5.9解决hadoop OutOfMemoryError问题</h3>
<p>出现这种异常，明显是jvm内存不够得原因。</p>
<p>解决方案如下：要修改所有的datanode的jvm内存大小。</p>
<p>Java –Xms 1024m -Xmx 4096m</p>
<p>一般jvm的最大内存使用应该为总内存大小的一半，我们使用的8G内存，所以设置为4096m，这一值可能依旧不是最优的值。</p>
<h3 id="5-10-namenode-in-safe-mode">5.10 Namenode in safe mode</h3>
<p>解决方案如下：</p>
<p>bin/hadoop dfsadmin -safemode leave</p>
<h3 id="5-11-io-">5.11 IO写操作出现问题</h3>
<p>0-1246359584298, infoPort=50075, ipcPort=50020):Got exception while serving blk_-5911099437886836280_1292 to /172.16.100.165:</p>
<p>java.net.SocketTimeoutException: 480000 millis timeout while waiting for channel to be ready for write. ch : java.nio.channels.SocketChannel[connected local=/</p>
<p>172.16.100.165:50010 remote=/172.16.100.165:50930]</p>
<p>at org.apache.hadoop.net.SocketIOWithTimeout.waitForIO(SocketIOWithTimeout.java:185)</p>
<p>at org.apache.hadoop.net.SocketOutputStream.waitForWritable(SocketOutputStream.java:159)</p>
<p>……</p>
<p>It seems there are many reasons that it can timeout, the example given in HADOOP-3831 is a slow reading client.</p>
<p>解决方案如下：</p>
<p>在hadoop-site.xml中设置dfs.datanode.socket.write.timeout=0</p>
<h3 id="5-12-status-of-255-error">5.12 status of 255 error</h3>
<p>错误类型：</p>
<p>java.io.IOException: Task process exit with nonzero status of 255.</p>
<p>at org.apache.hadoop.mapred.TaskRunner.run(TaskRunner.java:424)</p>
<p>错误原因：</p>
<p>Set mapred.jobtracker.retirejob.interval and mapred.userlog.retain.hours to higher value. By default, their values are 24 hours. These might be the reason for failure, though I&#39;m not sure restart.</p>
<p>解决方案如下：单个datanode</p>
<p>如果一个datanode 出现问题，解决之后需要重新加入cluster而不重启cluster，方法如下：</p>
<p>bin/hadoop-daemon.sh start datanode</p>
<p>bin/hadoop-daemon.sh start jobtracker</p>
<h2 id="6-linux-">6、用到的Linux命令</h2>
<h3 id="6-1-chmod-">6.1 chmod命令详解</h3>
<p><strong>使用权限：</strong>所有使用者</p>
<p><strong>使用方式：</strong>chmod [-cfvR] [--help] [--version] mode file...</p>
<p><strong>说明：</strong></p>
<p>Linux/Unix 的档案存取权限分为三级 : 档案拥有者、群组、其他。利用 chmod 可以藉以控制档案如何被他人所存取。</p>
<p>mode ：权限设定字串，格式如下 ：[ugoa...][[+-=][rwxX]...][,...]，其中u 表示该档案的拥有者，g 表示与该档案的拥有者属于同一个群体(group)者，o 表示其他以外的人，a 表示这三者皆是。</p>
<ul>
<li>表示增加权限、- 表示取消权限、= 表示唯一设定权限。</li>
</ul>
<p>r 表示可读取，w 表示可写入，x 表示可执行，X 表示只有当该档案是个子目录或者该档案已经被设定过为可执行。</p>
<p>-c : 若该档案权限确实已经更改，才显示其更改动作</p>
<p>-f : 若该档案权限无法被更改也不要显示错误讯息</p>
<p>-v : 显示权限变更的详细资料</p>
<p>-R : 对目前目录下的所有档案与子目录进行相同的权限变更(即以递回的方式逐个变更)</p>
<p>--help : 显示辅助说明</p>
<p>--version : 显示版本</p>
<p><strong>范例：</strong></p>
<p>将档案 file1.txt 设为所有人皆可读取</p>
<p>chmod ugo+r file1.txt</p>
<p>将档案 file1.txt 设为所有人皆可读取</p>
<p>chmod a+r file1.txt</p>
<p>将档案 file1.txt 与 file2.txt 设为该档案拥有者，与其所属同一个群体者可写入，但其他以外的人则不可写入</p>
<p>chmod ug+w,o-w file1.txt file2.txt</p>
<p>将 ex1.py 设定为只有该档案拥有者可以执行</p>
<p>chmod u+x ex1.py</p>
<p>将目前目录下的所有档案与子目录皆设为任何人可读取</p>
<p>chmod -R a+r /*</p>
<p>此外chmod也可以用数字来表示权限如 chmod 777 file</p>
<p><strong>语法为：</strong>chmod abc file</p>
<p>其中a,b,c各为一个数字，分别表示User、Group、及Other的权限。</p>
<p>r=4，w=2，x=1</p>
<p>若要rwx属性则4+2+1=7；</p>
<p>若要rw-属性则4+2=6；</p>
<p>若要r-x属性则4+1=7。</p>
<p><strong>范例：</strong></p>
<p>chmod a=rwx file 和 chmod 777 file 效果相同</p>
<p>chmod ug=rwx,o=x file 和 chmod 771 file 效果相同</p>
<p>若用chmod 4755 filename可使此程式具有<strong>root</strong>的权限</p>
<h3 id="6-2-chown-">6.2 chown命令详解</h3>
<p><strong>使用权限：</strong>root</p>
<p><strong>使用方式：</strong>chown [-cfhvR] [--help] [--version] user[:group] file...</p>
<p><strong>说明：</strong></p>
<p>Linux/Unix 是多人多工作业系统，所有的档案皆有拥有者。利用 chown 可以将档案的拥有者加以改变。一般来说，这个指令只有是由系统管理者(root)所使用，一般使用者没有权限可以改变别人的档案拥有者，也没有权限可以自己的档案拥有者改设为别人。只有系统管理者(root)才有这样的权限。</p>
<p>user : 新的档案拥有者的使用者</p>
<p>IDgroup : 新的档案拥有者的使用者群体(group)</p>
<p>-c : 若该档案拥有者确实已经更改，才显示其更改动作</p>
<p>-f : 若该档案拥有者无法被更改也不要显示错误讯息</p>
<p>-h : 只对于连结(link)进行变更，而非该 link 真正指向的档案</p>
<p>-v : 显示拥有者变更的详细资料</p>
<p>-R : 对目前目录下的所有档案与子目录进行相同的拥有者变更(即以递回的方式逐个变更)</p>
<p>--help : 显示辅助说明</p>
<p>--version : 显示版本</p>
<p><strong>范例：</strong></p>
<p>将档案 file1.txt 的拥有者设为 users 群体的使用者 jessie</p>
<p>chown jessie:users file1.txt</p>
<p>将目前目录下的所有档案与子目录的拥有者皆设为 users 群体的使用者 lamport</p>
<p>chown -R lamport:users /*</p>
<p>-rw------- (600) -- 只有属主有读写权限。</p>
<p>-rw-r--r-- (644) -- 只有属主有读写权限；而属组用户和其他用户只有读权限。</p>
<p>-rwx------ (700) -- 只有属主有读、写、执行权限。</p>
<p>-rwxr-xr-x (755) -- 属主有读、写、执行权限；而属组用户和其他用户只有读、执行权限。</p>
<p>-rwx--x--x (711) -- 属主有读、写、执行权限；而属组用户和其他用户只有执行权限。</p>
<p>-rw-rw-rw- (666) -- 所有用户都有文件读、写权限。这种做法不可取。</p>
<p>-rwxrwxrwx (777) -- 所有用户都有读、写、执行权限。更不可取的做法。</p>
<p>以下是对目录的两个普通设定：</p>
<p>drwx------ (700) - 只有属主可在目录中读、写。</p>
<p>drwxr-xr-x (755) - 所有用户可读该目录，但只有属主才能改变目录中的内容</p>
<p>suid的代表数字是4，比如4755的结果是-rwsr-xr-x</p>
<p>sgid的代表数字是2，比如6755的结果是-rwsr-sr-x</p>
<p>sticky位代表数字是1，比如7755的结果是-rwsr-sr-t</p>
<h3 id="6-3-scp-">6.3 scp命令详解</h3>
<p>scp是 secure copy的缩写，scp是linux系统下基于ssh登陆进行安全的远程文件拷贝命令。linux的scp命令可以在linux服务器之间复制文件和目录。</p>
<p><strong>scp命令的用处：</strong></p>
<p>scp在网络上不同的主机之间复制文件，它使用ssh安全协议传输数据，具有和ssh一样的验证机制，从而安全的远程拷贝文件。</p>
<p><strong>scp命令基本格式：</strong></p>
<p>scp [-1246BCpqrv] [-c cipher] [-F ssh_config] [-i identity_file]</p>
<p>[-l limit] [-o ssh_option] [-P port] [-S program]</p>
<p>[[user@]host1:]file1 [...] [[user@]host2:]file2</p>
<p>scp命令的参数说明：</p>
<p>-1 强制scp命令使用协议ssh1</p>
<p>-2 强制scp命令使用协议ssh2</p>
<p>-4 强制scp命令只使用IPv4寻址</p>
<p>-6 强制scp命令只使用IPv6寻址</p>
<p>-B 使用批处理模式（传输过程中不询问传输口令或短语）</p>
<p>-C 允许压缩。（将-C标志传递给ssh，从而打开压缩功能）</p>
<p>-p 保留原文件的修改时间，访问时间和访问权限。</p>
<p>-q 不显示传输进度条。</p>
<p>-r 递归复制整个目录。</p>
<p>-v 详细方式显示输出。scp和ssh(1)会显示出整个过程的调试信息。这些信息用于调试连接，验证和配置问题。</p>
<p>-c cipher 以cipher将数据传输进行加密，这个选项将直接传递给ssh。</p>
<p>-F ssh_config 指定一个替代的ssh配置文件，此参数直接传递给ssh。</p>
<p>-i identity_file 从指定文件中读取传输时使用的密钥文件，此参数直接传递给ssh。</p>
<p>-l limit 限定用户所能使用的带宽，以Kbit/s为单位。</p>
<p>-o ssh_option 如果习惯于使用ssh_config(5)中的参数传递方式，</p>
<p>-P port 注意是大写的P, port是指定数据传输用到的端口号</p>
<p>-S program 指定加密传输时所使用的程序。此程序必须能够理解ssh(1)的选项。</p>
<p><strong>scp命令的实际应用</strong></p>
<p><strong>1）从本地服务器复制到远程服务器</strong></p>
<p><strong>(1) 复制文件：</strong></p>
<p>命令格式：</p>
<p>scp local_file remote_username@remote_ip:remote_folder</p>
<p>或者</p>
<p>scp local_file remote_username@remote_ip:remote_file</p>
<p>或者</p>
<p>scp local_file remote_ip:remote_folder</p>
<p>或者</p>
<p>scp local_file remote_ip:remote_file</p>
<p>第1,2个指定了用户名，命令执行后需要输入用户密码，第1个仅指定了远程的目录，文件名字不变，第2个指定了文件名</p>
<p>第3,4个没有指定用户名，命令执行后需要输入用户名和密码，第3个仅指定了远程的目录，文件名字不变，第4个指定了文件名</p>
<p><strong>实例：</strong></p>
<p>scp /home/linux/soft/scp.zip root@www.mydomain.com:/home/linux/others/soft</p>
<p>scp /home/linux/soft/scp.zip root@www.mydomain.com:/home/linux/others/soft/scp2.zip</p>
<p>scp /home/linux/soft/scp.zip www.mydomain.com:/home/linux/others/soft</p>
<p>scp /home/linux/soft/scp.zip www.mydomain.com:/home/linux/others/soft/scp2.zip</p>
<p><strong>(2) 复制目录：</strong></p>
<p>命令格式：</p>
<p>scp -r local_folder remote_username@remote_ip:remote_folder</p>
<p>或者</p>
<p>scp -r local_folder remote_ip:remote_folder</p>
<p>第1个指定了用户名，命令执行后需要输入用户密码；</p>
<p>第2个没有指定用户名，命令执行后需要输入用户名和密码；</p>
<p><strong>例子：</strong></p>
<p>scp -r /home/linux/soft/ root@www.mydomain.com:/home/linux/others/</p>
<p>scp -r /home/linux/soft/ www.mydomain.com:/home/linux/others/</p>
<p>上面 命令 将 本地 soft 目录 复制 到 远程 others 目录下，即复制后远程服务器上会有/home/linux/others/soft/ 目录。</p>
<p><strong>2）从远程服务器复制到本地服务器</strong></p>
<p>从远程复制到本地的scp命令与上面的命令雷同，只要将从本地复制到远程的命令后面2个参数互换顺序就行了。</p>
<p><strong>例如：</strong></p>
<p>scp root@www.mydomain.com:/home/linux/soft/scp.zip /home/linux/others/scp.zip</p>
<p>scp www.mydomain.com:/home/linux/soft/ -r /home/linux/others/</p>
<p>linux系统下scp命令中很多参数都和ssh1有关，还需要看到更原汁原味的参数信息，可以运行man scp 看到更细致的英文说明。</p>
<p>文章下载地址：<a href="http://files.cnblogs.com/xia520pi/HadoopCluster_Vol.5.rar" target="_blank"><a href="http://files.cnblogs.com/xia520pi/HadoopCluster_Vol.5.rar">http://files.cnblogs.com/xia520pi/HadoopCluster_Vol.5.rar</a></a>
来源： &lt;<a href="http://www.cnblogs.com/xia520pi/archive/2012/05/16/2503949.html" target="_blank">Hadoop集群（第5期）_Hadoop安装配置 - 虾皮 - 博客园</a>&gt; </p>

      
    </div>
    
    
      
    
    
        <footer id="post-meta">
        <span class="categories">Posted in<span class="breadcrumb fa fa-folder"><li><a href="/categories/hadoop/">hadoop</a></li></span></span> | <span class="tags">Tagged <a href="/tags/hadoop/" class="label label-primary">hadoop</a></span> | <span class="time">recent updated:<time title="2014-03-07 09:54:36"datetime="2014-03-07 09:54:36"> mar. 7 2014</time></span> | <span class="comment-link">
<a href="http://itsolife.com/2014/02/02/2014-02-02-hadoop--Hadoop集群_Hadoop安装配置/#comments" class="ds-thread-count comment-link" data-thread-key="2014-02-02-hadoop--Hadoop集群_Hadoop安装配置" data-count-type="comments">暂无评论</a></span>
        </footer>
    
    <div class="clearfix"></div>
  </div>
</div>
</article>



  <article>
<div id="post" class="post well">
  <div class="post-content">
    <header class="well-sm">
      <i class="fa icon fa-5x pull-left"></i>
      <h1 class="title"><a href="/2014/02/02/2014-02-02-hadoop--基于Hadoop220的高可用性集群搭建步骤（64位）/">基于Hadoop 2.2.0的高可用性集群搭建步骤（64位）</a></h1>
      
        <span>Posted on<time datetime="2014-02-02T01:54:36.000Z"> <a href="/2014/02/02/2014-02-02-hadoop--基于Hadoop220的高可用性集群搭建步骤（64位）/">feb. 2 2014</a></time></span>      
    </header>
    
    
    <div class="entry">
      
        <h1 id="-hadoop-2-2-0-64-">基于Hadoop 2.2.0的高可用性集群搭建步骤（64位）</h1>
<p> 内容概要: CentSO_64bit集群搭建， hadoop2.2(64位)编译，安装，配置以及测试步骤</p>
<p>新版亮点:基于yarn计算框架和高可用性DFS的第一个稳定版本。</p>
<p>注1：官网只提供32位release版本, 若机器为64位，需要手动编译。</p>
<p>注2：目前网上传的2.2版本的安装步骤几乎都有问题，没有一个版本是完全正确的。若不懂新框架内部机制，不要照抄网传的版本。</p>
<p><strong>0. 编译前的准备</strong></p>
<p><strong>虚拟机vmware准备，64bit CentOS准备</strong></p>
<p><strong>节点ip</strong></p>
<p>cluster1   172.16.102. 201</p>
<p>cluster2  172.16.102. 202</p>
<p>cluster3  172.16.102. 203</p>
<p>cluster4  172.16.102. 204</p>
<p><strong>各节点职能划分</strong>     </p>
<p>cluster1        resourcemanager, nodemanager,</p>
<p>proxyserver,historyserver, datanode, namenode,</p>
<p>cluster2        datanode, nodemanager</p>
<p>cluster3 datanode, nodemanager</p>
<p>cluster4 datanode, nodemanager</p>
<p><strong>说明</strong></p>
<p>以下关于修改hostname, 设置静态ip, ssh免登陆，关闭防火墙等步骤，放在文章末尾，这些内容并不是本文讨论的重点。</p>
<p><strong>1. hadoop2.2**</strong>编译**
1说明：标准的</p>
<p>bash</p>
<p>提示符，root用户为</p>
<p>&#39;/#&#39;</p>
<p>，普通用户为</p>
<p>&#39;%&#39;</p>
<p>，由于博客编辑器的缘故，</p>
<p>&#39;/#&#39;</p>
<p>提示符会被默认为comment, 因此在这篇博文中不再区分root和普通user的提示符， 默认全部为</p>
<p>&#39;$&#39;</p>
<p>因为我们安装的CentOS是64bit的，而官方release的hadoop2.2.0版本没有对应的64bit安装包，故需要自行编译。</p>
<p>首先需要去oracle下载64位jdk:
1</p>
<p>2
$</p>
<p>su</p>
<p>root</p>
<p>$wget http:</p>
<p>//download</p>
<p>.oracle.com</p>
<p>/otn-pub/java/jdk/7u45-b18/jdk-7u45-linux-x64</p>
<p>.</p>
<p>tar</p>
<p>.gz</p>
<p>注: prompt（提示符）为%默认为当前用户， /#则为root,注意以下各步骤中的prompt类型。</p>
<p>下面为hadoop编译步骤（注：中间部分的文本框里内容提要只是一些补充说明，不要执行框里的命令）</p>
<p>1.1 BOTPROTO改为”dhcp”
1</p>
<p>2
$</p>
<p>su</p>
<p>root</p>
<p>$</p>
<p>sed</p>
<p>–i  s</p>
<p>/static/dhcp/g</p>
<p>/etc/sysconfig/network-scripts/ifcfg-eth0</p>
<p>/#servicenetwork restart</p>
<p>1.2 下载hadoop2.2.0 源码</p>
<p>1</p>
<p>2
3$</p>
<p>su</p>
<p>grid</p>
<p>$</p>
<p>cd</p>
<p>~
wget  http:</p>
<p>//apache</p>
<p>.dataguru.cn</p>
<p>/hadoop/common/stable/hadoop-2</p>
<p>.2.0-src.</p>
<p>tar</p>
<p>.gz</p>
<p>1.3 安装maven</p>
<p>1</p>
<p>2
3</p>
<p>4
5$</p>
<p>su</p>
<p>root</p>
<p>$</p>
<p>cd</p>
<p>/opt
wget http:</p>
<p>//apache</p>
<p>.fayea.com</p>
<p>/apache-mirror/maven/maven-3/3</p>
<p>.1.1</p>
<p>/binaries/apache-maven-3</p>
<p>.1.1-bin.</p>
<p>tar</p>
<p>.gz</p>
<p>$</p>
<p>tar</p>
<p>zxvf apache-maven-3.1.1-bin.</p>
<p>tar</p>
<p>.gz
$</p>
<p>cd</p>
<p>apache-maven-3.1.1</p>
<p><em>**</em>修改系统环境变量有两种方式，修改/etc/profile， 或者在/etc/profile.d/下添加定制的shell文件，</p>
<p>鉴于profile文件的重要性，尽量不要在profile文件里添加内容，官方建议采用第二种，以保证profile文件的绝对安全。</p>
<p>下面采用第二种方式：</p>
<p>首先，创建一个简单shell脚脚本并添加相关内容进去：
1</p>
<p>2
$</p>
<p>cd</p>
<p>/etc/profile</p>
<p>.d/</p>
<p>$</p>
<p>touch</p>
<p>maven.sh     其次，maven.sh里添加内容如下：1</p>
<p>2
3/#environmentvariable settings for mavn2</p>
<p>export</p>
<p>MAVEN_HOME=</p>
<p>&#39;/opt/apache-maven-3.1.1&#39;</p>
<p>3
export</p>
<p>PATH=$MAVEN_HOME</p>
<p>/bin</p>
<p>:$PATH</p>
<p>最后，source一下</p>
<p>1$</p>
<p>source</p>
<p>/etc/profile</p>
<p><br><br>$</p>
<p>source</p>
<p>/etc/profile1</p>
<p>2
3&lt;em</p>
<p>id</p>
<p>=</p>
<p>&quot;__mceDel&quot;</p>
<blockquote>
<p>$mvn -version</p>
</blockquote>
<p>Apache Maven 3.1.1
&lt;</p>
<p>/em</p>
<p>&gt;</p>
<p>1.4 安装protobuf</p>
<p>注意apache官方网站上的提示“NOTE: You will need protoc 2.5.0 installed.”
1</p>
<p>2
3</p>
<p>4
5</p>
<p>6
$</p>
<p>su</p>
<p>root</p>
<p>$</p>
<p>cd</p>
<p>/opt
wget https:</p>
<p>//protobuf</p>
<p>.googlecode.com</p>
<p>/files/protobuf-2</p>
<p>.5.0.</p>
<p>tar</p>
<p>.bz2</p>
<p>$</p>
<p>tar</p>
<p>xvf protobuf-2.5.0.</p>
<p>tar</p>
<p>.bz2 (注意压缩文件后缀， maven安装包是—</p>
<p>gzip</p>
<p>文件，解压时需加–z)
$</p>
<p>cd</p>
<p>protobuf-2.5.0</p>
<p>.</p>
<p>/configure</p>
<p>安装protobuf时提示报错”configure: error: C++ preprocessor &quot;/lib/cpp&quot; failssanity check”</p>
<p>安装gcc
1$yum</p>
<p>install</p>
<p>gcc</p>
<p>1.5 编译hadoop</p>
<p>首先从官网下载hadoop2.2.0source code:
1</p>
<p>2
3$</p>
<p>su</p>
<p>grid;</p>
<p>$</p>
<p>cd</p>
<p>~grid/
wget http:</p>
<p>//apache</p>
<p>.dataguru.cn</p>
<p>/hadoop/common/stable/hadoop-2</p>
<p>.2.0-src.</p>
<p>tar</p>
<p>.gz</p>
<p>好了，痛苦的编译过程来了。</p>
<p>解压之：
1</p>
<p>2
$</p>
<p>tar</p>
<p>zxvf hadoop-2.2.0-src.</p>
<p>tar</p>
<p>.gz</p>
<p>$</p>
<p>cd</p>
<p>hadoop-2.2.0-src</p>
<p>1.6 给maven指定国内镜像源</p>
<p>1.6.1. 切换root权限, 修改/opt/apache-maven-3.1.1/conf/settings.xml
1</p>
<p>2
$</p>
<p>su</p>
<p>root</p>
<p>$vim</p>
<p>/opt/apache-maven-3</p>
<p>.1.1</p>
<p>/conf/settings</p>
<p>.xml</p>
<p>修改1. 在<mirrors>…</mirrors>里添加国内源（注意，保留原本就有的<mirrors>...</mirrors>）：</p>
<p><a href="&quot;复制代码&quot;"><img src="http://common.cnblogs.com/images/copycode.gif" alt="复制代码"></a>
1  <mirrors> 2     <mirror>3         <id>nexus-osc</id>4         <mirrorOf>/*</mirrorOf>5     <name>Nexusosc</name>6     <url><a href="http://maven.oschina.net/content/groups/public/" target="_blank">http://maven.oschina.net/content/groups/public/</a></url>7     </mirror>8 </mirrors></p>
<p><a href="&quot;复制代码&quot;"><img src="http://common.cnblogs.com/images/copycode.gif" alt="复制代码"></a></p>
<p>修改2. 在<profiles>标签中增加以下内容（保留原来的<profiles>…</profiles>， jdk版本根据用户的情况填写）</p>
<p><a href="&quot;复制代码&quot;"><img src="http://common.cnblogs.com/images/copycode.gif" alt="复制代码"></a>
 1  <profile> 2       <id>jdk-1.4</id> 3       <activation> 4         <jdk>1.4</jdk> 5       </activation> 6       <repositories> 7         <repository> 8           <id>nexus</id> 9           <name>local private nexus</name>10           <url><a href="http://maven.oschina.net/content/groups/public/">http://maven.oschina.net/content/groups/public/</a></url>11           <releases>12             <enabled>true</enabled>13           </releases>14           <snapshots>15             <enabled>false</enabled>16           </snapshots>17         </repository>18       </repositories>19       <pluginRepositories>20         <pluginRepository>21           <id>nexus</id>22           <name>local private nexus</name>23           <url><a href="http://maven.oschina.net/content/groups/public/" target="_blank">http://maven.oschina.net/content/groups/public/</a></url>24           <releases>25             <enabled>true</enabled>26           </releases>27           <snapshots>28             <enabled>false</enabled>29           </snapshots>30         </pluginRepository>31       </pluginRepositories>32     </profile>33 </profiles></p>
<p><a href="&quot;复制代码&quot;"><img src="http://common.cnblogs.com/images/copycode.gif" alt="复制代码"></a></p>
<p>   1.6.2 将刚才修改的配置文件拷到当前用户home目录下</p>
<p><a href="&quot;复制代码&quot;"><img src="http://common.cnblogs.com/images/copycode.gif" alt="复制代码"></a>
$ su grid $ sudo cp /opt/apache-maven-3.1.1/conf/settings.xml ~/.m2//#若提示该用户不在sudoers里，执行以下步骤： $ su root  /#在sudoers里第99行添加当前用户（下面行号不要加）： $ cat /etc/sudoers98  root     ALL=(ALL)       ALL99  grid     ALL=(ALL)       ALL</p>
<p><a href="&quot;复制代码&quot;"><img src="http://common.cnblogs.com/images/copycode.gif" alt="复制代码"></a></p>
<p>1.7 现在执行官方的clean步骤： $mvn clean install –DskipTests</p>
<p>漫长的等待后发现安装一切正常。</p>
<p>1.8 安装3个依赖包
1</p>
<p>2
3Cmake</p>
<p>ncurses-devel
openssl-devel</p>
<p>执行以下步骤：</p>
<p>1</p>
<p>2
3</p>
<p>4
$</p>
<p>su</p>
<p>root</p>
<p>$yum</p>
<p>install</p>
<p>ncurses-devel
$yum</p>
<p>install</p>
<p>openssl-devel</p>
<p>$yum</p>
<p>install</p>
<p>cmake89</p>
<p>以上安装完成后，切回用户grid：</p>
<p>1</p>
<p>2
$</p>
<p>su</p>
<p>grid</p>
<p>$</p>
<p>cd</p>
<p>~</p>
<p>/hadoop-2</p>
<p>.2.0-src</p>
<p><em>**</em>1.9 所有依赖已安装完毕，开始编译</p>
<p>1$mvn package-Pdist,native -DskipTests -Dtar</p>
<p>漫长的等待后，编译成功，查看结果：</p>
<p><img src="http://m1.img.libdd.com/farm4/d/2013/1114/11/160569527B88184A5CFD1B4C6D260A60_B500_900_500_214.png" alt=""></p>
<p>一切正常。至此，hadoop2.2.0编译完成。</p>
<p><em>**</em>1.10 验证</p>
<p>下面验证编译结果是否符合预期, 注意我们当前是在目录~/hadoop-2.2.0-src下，
1</p>
<p>2
3$</p>
<p>cd</p>
<p>hadoop-dist/</p>
<p>$</p>
<p>ls
pom.xml  target</p>
<p>以上为maven编译的配置文件</p>
<p>1</p>
<p>2
3</p>
<p>4
5</p>
<p>6
7$</p>
<p>cd</p>
<p>target</p>
<p>$</p>
<p>ls</p>
<p>-sF
total 276M</p>
<p>4.0K antrun/                    92M hadoop-2.2.0.</p>
<p>tar</p>
<p>.gz            4.0K maven-archiver/
4.0K dist-layout-stitching.sh  4.0K hadoop-dist-2.2.0.jar          4.0K</p>
<p>test</p>
<p>-</p>
<p>dir</p>
<p>/</p>
<p>4.0K dist-</p>
<p>tar</p>
<p>-stitching.sh     184M hadoop-dist-2.2.0-javadoc.jar
4.0K hadoop-2.2.0/             4.0K javadoc-bundle-options/</p>
<p>以上为maven编译后自动生成的目录文件，进入hadoop-2.2.0:</p>
<p>1</p>
<p>2
3$</p>
<p>cd</p>
<p>hadoop-2.2.023</p>
<p>$</p>
<p>ls
bin  etc  include  lib libexec  sbin  share</p>
<p>这才是和官方release2.2.0版本（官方只有32bit版本）的相同的目录结构。</p>
<p>1.10.1 下面主要验证两项：</p>
<p><em>**</em>a.验证版本号
1</p>
<p>2
3</p>
<p>4
5</p>
<p>6
7$bin</p>
<p>/hadoop</p>
<p>version</p>
<p>Hadoop 2.2.0
Subversion Unknown -r Unknown</p>
<p>Compiled by grid on 2013-11-06T13:51Z
Compiled with protoc 2.5.0</p>
<p>From</p>
<p>source</p>
<p>with checksum 79e53ce7994d1628b240f09af91e1af4
This</p>
<p>command</p>
<p>was run using</p>
<p>/home/grid/hadoop-2</p>
<p>.2.0-src</p>
<p>/hadoop-dist/target/hadoop-2</p>
<p>.2.0</p>
<p>/share/hadoop/common/hadoop-common-2</p>
<p>.2.0.jar</p>
<p>可以看到hadoop版本号，编译工具（protoc2.5.0版本号与官方要求一致）以及编译日期.</p>
<p><em>**</em>b.验证hadoop lib的位数
1</p>
<p>2
3</p>
<p>4
5</p>
<p>6
$</p>
<p>file</p>
<p>lib</p>
<p>//native/</p>
<p>/*</p>
<p>lib</p>
<p>//native/libhadoop</p>
<p>.a:        current ar archive
lib</p>
<p>//native/libhadooppipes</p>
<p>.a:   current ar archive</p>
<p>lib</p>
<p>//native/libhadoop</p>
<p>.so:       symbolic link to `libhadoop.so.1.0.0&#39;
lib</p>
<p>//native/libhadoop</p>
<p>.so.1.0.0:<strong>ELF 64-bitLSB&lt;</p>
<p>/strong</p>
<blockquote>
<p>shared object, x86-64, version 1 (SYSV), dynamically linked, notstripped1011 lib</p>
</blockquote>
<p>//native/libhadooputils</p>
<p>.a:   current ar archive1213 lib</p>
<p>//native/libhdfs</p>
<p>.a:          current ar archive1415 lib</p>
<p>//native/libhdfs</p>
<p>.so:         symbolic link to `libhdfs.so.0.0.0&#39;</p>
<p>lib</p>
<p>//native/libhdfs</p>
<p>.so.0.0.0:   <strong>ELF 64-bit LSB shared object&lt;</p>
<p>/strong</p>
<blockquote>
<p>, x86-64,version 1 (SYSV), dynamically linked, not stripped</p>
</blockquote>
<p>看到黑色的“ELF-64bit LSB”证明64bit hadoop2.2.0初步编译成功，查看我们之前的hadoop0.20.3版本，会发现lib//native/libhadoop.so.1.0.0是32bit，这是不正确的！。^_^</p>
<p><strong>2. hadoop2.2**</strong>配置**</p>
<p><strong>**</strong>2.1 home设置**</p>
<p>为了和MRv1区别， 2.2版本的home目录直接命名为yarn:
1</p>
<p>2
3</p>
<p>4
$</p>
<p>su</p>
<p>hadoop</p>
<p>$</p>
<p>cd</p>
<p>~
$</p>
<p>mkdir</p>
<p>–p yarn</p>
<p>/yarn_data67</p>
<p>$</p>
<p>cp</p>
<p>–a ~hadoop</p>
<p>/hadoop-2</p>
<p>.2.0-src</p>
<p>/hadoop-dist/target/hadoop-2</p>
<p>.2.0  ~hadoop</p>
<p>/yarn</p>
<p><strong>2.2 环境变量设置</strong></p>
<p>~/.bashrc里添加新环境变量：</p>
<p>1</p>
<p>2
3</p>
<p>4
5</p>
<p>6
7</p>
<p>8
9</p>
<p>10
11</p>
<p>12
/# javaenv</p>
<p>export</p>
<p>JAVA_HOME=</p>
<p>&quot;/usr/java/jdk1.7.0_45&quot;</p>
<p>exportPATH=</p>
<p>&quot;$JAVA_HOME/bin:$PATH&quot;
/# hadoopvariable settings</p>
<p>export</p>
<p>HADOOP_HOME=</p>
<p>&quot;$HOME/yarn/hadoop-2.2.0&quot;
export</p>
<p>HADOOP_PREFIX=</p>
<p>&quot;$HADOOP_HOME/&quot;</p>
<p>export</p>
<p>YARN_HOME=$HADOOP_HOME
export</p>
<p>HADOOP_MAPRED_HOME=</p>
<p>&quot;$HADOOP_HOME&quot;</p>
<p>export</p>
<p>HADOOP_COMMON_HOME=</p>
<p>&quot;$HADOOP_HOME&quot;
export</p>
<p>HADOOP_HDFS_HOME=</p>
<p>&quot;$HADOOP_HOME&quot;</p>
<p>export</p>
<p>HADOOP_CONF_DIR=</p>
<p>&quot;$HADOOP_HOME/etc/hadoop/&quot;
export</p>
<p>YARN_CONF_DIR=$HADOOP_CONF_DIR</p>
<p>export</p>
<p>PATH=</p>
<p>&quot;$HADOOP_HOME/bin:$HADOOP_HOME/sbin:$PATH&quot;</p>
<p>以上操作注意2点：</p>
<ol>
<li>jdk一定要保证是64bit的</li>
</ol>
<p>2.HADOOP_PREFIX极其重要，主要是为了兼容MRv1，优先级最高（比 如寻找conf目录，即使我们配置了HADOOP_CONF_DIR,启动脚本依然会优先从$HADOOP_PREFIX/conf/里查找），一定要保 证此变量正确配置(也可不设置，则默认使用HADOOP_HOME/etc/hadoop/下的配置文件)</p>
<p><strong>**</strong>2.3 改官方启动脚本的bug**</p>
<p><em>**</em>说明：此版本虽然是release稳定版，但是依然有非常弱智的bug存在。</p>
<p>正常情况下，启动守护进程$YARN_HOME/sbin/hadoop-daemons.sh中可以指定node.</p>
<p>我们看下该启动脚本的说明：
1</p>
<p>2
$sbin</p>
<p>/hadoop-daemons</p>
<p>.sh</p>
<p>Usage:hadoop-daemons.sh [--config confdir] [--hosts hostlistfile] [start|stop]</p>
<p>command</p>
<p>args...</p>
<p>可以看到--hosts可以指定需要启动的存放节点名的文件名：</p>
<p>但这是无效的，此脚本调用的$YARN_HOME/libexec/hadoop-config.sh脚本有bug.</p>
<p>先建一个文件datanodefile 添加datanode节点，放入conf/文件夹下，然后执行一下启动脚本：
1</p>
<p>2
$sbin</p>
<p>/hadoop-daemons</p>
<p>.sh --hosts datanodefile start datanode</p>
<p>at:</p>
<p>/home/grid/yarn/hadoop-2</p>
<p>.2.0</p>
<p>/etc/hadoop//126571</p>
<p>:No such</p>
<p>file</p>
<p>or directory</p>
<p>分析脚本，定位到嵌套脚本$YARN_HOME/libexec/hadoop-config.sh第96行：</p>
<p>196        </p>
<p>export</p>
<p>HADOOP_SLAVES=</p>
<p>&quot;${HADOOP_CONF_DIR}/$$1&quot;</p>
<p>看到红色部分是不对的，应该修改为：</p>
<p>196        </p>
<p>export</p>
<p>HADOOP_SLAVES=</p>
<p>&quot;${HADOOP_CONF_DIR}/$1&quot;</p>
<p>备注1：此版本11月初发布至今，网上教程不论中文还是英文，均未提及此错误。</p>
<p>备注2：$YARN_HOME/libexec/hadoop-config.sh中分析hostfile的逻辑非常脑残：
1</p>
<p>2
3</p>
<p>4
593    </p>
<p>if</p>
<p>[</p>
<p>&quot;--hosts&quot;</p>
<p>=</p>
<p>&quot;$1&quot;</p>
<p>]</p>
<p>94    </p>
<p>then
95        </p>
<p>shift</p>
<p>96        </p>
<p>export</p>
<p>HADOOP_SLAVES=</p>
<p>&quot;${HADOOP_CONF_DIR}/%$1&quot;
97        </p>
<p>shift</p>
<p>因此，用户只能将自己的hostfile放在${HADOOP_CONF_DIR}/ 下面，放在其它地方是无效的。</p>
<p>备注3：按照$YARN_HOME/libexec/hadoop-config.sh脚本的逻辑，还有一种方式指定host
1$hadoop-daemons.sh –hostnames cluster1 start datanode</p>
<p>注意，因为bash脚本区分输入参数的分割符为\t或\s，所以限制了此种方式只能指定一个单独的节点</p>
<p>总结以上分析和修改步骤：
1</p>
<p>2
3</p>
<p>4
5$</p>
<p>cd</p>
<p>$YARN_HOME</p>
<p>/libexec/</p>
<p>$vim hadoop-config.sh
/#修改第96行代码为：</p>
<p>export</p>
<p>HADOOP_SLAVES=</p>
<p>&quot;${HADOOP_CONF_DIR}/$1&quot;
/#保存退出vim</p>
<p><strong>**</strong>2.4 配置文件设置**</p>
<p><a href="&quot;复制代码&quot;"><img src="http://common.cnblogs.com/images/copycode.gif" alt="复制代码"></a>
 1 <!--$YARN_HOME/etc/hadoop/core-site.xml--> 2                                                                                                                                                                                                                                  3 &lt;?xml version=&quot;1.0&quot;?&gt; 4 &lt;?xml-stylesheet type=&quot;text/xsl&quot; href=&quot;configuration.xsl&quot;?&gt; 5                                                                                                                                                                                                                                  6 <configuration> 7 <property> 8 <name>fs.defaultFS</name> 9   <value>hdfs://cluster1:9000</value>10 </property>11                                                                                                                                                                                                                                 12 <property>13 <name>hadoop.tmp.dir</name>14   <value>/home/grid/hadoop-2.2.0-src/hadoop-dist/target/hadoop-2.2.0//yarn_data/tmp/hadoop-grid</value>15 </property>16 </configuration>17   备注1：注意fs.defaultFS为新的变量，代替旧的：fs.default.name18 19  备注2：tmp文件夹放在我们刚才新建的$HOME/yarn/yarn_data/下面。20 21     <!--$YARN_HOME/etc/hadoop/hdfs-site.xml-->22 <configuration>23     <property>24     <name>dfs.replication</name>25     <value>3</value>26     </property>27 </configuration></p>
<p><a href="&quot;复制代码&quot;"><img src="http://common.cnblogs.com/images/copycode.gif" alt="复制代码"></a></p>
<p><em>**</em>备注1. 新：dfs.namenode.name.dir，旧：dfs.name.dir，新：dfs.datanode.name.dir，旧：dfs.data.dir</p>
<p><em>**</em>备注2. dfs.replication确定 data block的副本数目，hadoop基于rackawareness(机架感知)默认复制3份分block,（同一个rack下两个，另一个rack下一 份，按照最短距离确定具体所需block, 一般很少采用跨机架数据块，除非某个机架down了）</p>
<p><a href="&quot;复制代码&quot;"><img src="http://common.cnblogs.com/images/copycode.gif" alt="复制代码"></a>
 1     <!--$YARN_HOME/etc/hadoop/yarn-site.xml--> 2                                                                                                                                                                                                                             3 <configuration> 4    <property> 5      <name>yarn.nodemanager.aux-services</name> 6      <value>mapreduce_shuffle</value> 7   </property> 8                                                                                                                                                                                                                             9   <property>10      <name>yarn.resourcemanager.address</name>11      <value>cluster1:8032</value>12   </property>13                                                                                                                                                                                                                            14   <property>15       <name>yarn.resourcemanager.resource-tracker.address</name>16       <value>cluster1:8031</value>17   </property>18                                                                                                                                                                                                                            19   <property>20       <name>yarn.resourcemanager.admin.address</name>21       <value>cluster1:8033</value>22   </property>23                                                                                                                                                                                                                            24   <property>25       <name>yarn.resourcemanager.scheduler.address</name>26       <value>cluster1:8030</value>27   </property>28                                                                                                                                                                                                                            29   <property>30       <name>yarn.nodemanager.loacl-dirs</name>31       <value>/home/grid/hadoop-2.2.0-src/hadoop-dist/target//hadoop-2.2.0/yarn_data/mapred/nodemanager</value>32       <final>true</final>33   </property>34                                                                                                                                                                                                                            35   <property>36       <name>yarn.web-proxy.address</name>37       <value>cluster1:8888</value>38   </property>39                                                                                                                                                                                                                            40   <property>41      <name>yarn.nodemanager.aux-services.mapreduce.shuffle.class</name>42      <value>org.apache.hadoop.mapred.ShuffleHandler</value>43   </property>44 </configuration></p>
<p><a href="&quot;复制代码&quot;"><img src="http://common.cnblogs.com/images/copycode.gif" alt="复制代码"></a></p>
<p><strong>在此特意提醒：</strong></p>
<p>(1) 目前网上盛传的各种hadoop2.2的基于分布式集群的安装教程大部分都是错的,hadoop2.2配置里最重要的也莫过于yarn-site.xml里的变量了吧？</p>
<p>(2)不同于单机安装，基于集群的搭建必须设置</p>
<p>yarn-site.xml可选变量yarn.web-proxy.address
和</p>
<p>yarn.nodemanager.loacl-dirs
外的其它所有变量！后续会专门讨论yarn-site.xml里各个端口配置的含义。</p>
<p><a href="&quot;复制代码&quot;"><img src="http://common.cnblogs.com/images/copycode.gif" alt="复制代码"></a>
1 <!--$YARN_HOME/etc/hadoop/mapred-site.xml-->2                                                                                                                                                                                                            3 <configuration>4     <property>5         <name>mapreduce.framework.name</name>6         <value>yarn</value>7     </property>8 </configuration></p>
<p><a href="&quot;复制代码&quot;"><img src="http://common.cnblogs.com/images/copycode.gif" alt="复制代码"></a></p>
<p>备注1：新的计算框架取消了实体上的jobtracker, 故不需要再指定mapreduce.jobtracker.addres，而是要指定一种框架，这里选择yarn. 备注2：hadoop2.2.还支持第三方的计算框架，但没怎么关注过。
          配置好以后将$HADOOP_HOME下的所有文件，包括hadoop目录分别copy到其它3个节点上。</p>
<p><strong>2.5**</strong>各节点功能规划**</p>
<p>确保在每主机的/etc/hosts里添加了所有node的域名解析表（i.e.cluster1   198.0.0.1）；iptables已关闭；/etc/sysconfig/network-script/ifcfg-eth0里 BOTPROTO=static；/etc/sysconfig/network文件里已设置了各台主机的hostname, 静态ip地址，且已经重启过每台机器；jdk和hadoop都为64bit；ssh免登陆已配置；完成以上几项后，就可以启动hadoop2.2.0了。</p>
<p>注意到从头到尾都没有提到Master, Slave,也没有提到namenode,datanode,是因为，新的计算框架，新的hdfs中不存在物理上的Master节点，所有的节点都是等价的。</p>
<p>各节点职能划分在篇首已有说明， 在此重提一下：
1</p>
<p>2
3</p>
<p>4
cluster1    resourcemanager, nodemanager, proxyserver,historyserver, datanode, namenode,</p>
<p>cluster2    datanode, nodemanager
cluster3    datanode, nodemanager</p>
<p>cluster4    datanode, nodemanager</p>
<p><strong>2.6 hdfs 格式化</strong></p>
<p>1$bin</p>
<p>/hdfs</p>
<p>namenode –</p>
<p>format</p>
<p>(注意：hadoop 2.2.0的格式化步骤和旧版本不一样，旧的为 $YARN_HOME/bin/hadoop namenode –format)</p>
<p><strong>2.7 hadoop整体启动</strong></p>
<p><em>**</em>启动方式（1）分别登录各自主机开启</p>
<p>在cluster1节点上，分别启动resourcemanager,nodemanager, proxyserver, historyserver, datanode, namenode,</p>
<p>在cluster2, cluster3, cluster4 节点主机上，分别启动datanode,nodemanager</p>
<p><strong>备注</strong>：如果resourcemanager是 独立的，则除了resourcemanager,其余每一个节点都需要一个nodemanager，我们可以在$YARN_HOME/etc /hadoop/下新建一个nodehosts, 在里面添加所有的除了resourcemanager外的所有node，因为此处我们配置的resourcemanager和namenode是同一台主 机，所以此处也需要添加nodemanager</p>
<p>执行步骤如下：
1</p>
<p>2
3</p>
<p>4
5</p>
<p>6
7</p>
<p>8
9</p>
<p>10
11</p>
<p>12
13$</p>
<p>hostname</p>
<p>/#查看host名字</p>
<p>cluster1
$sbin</p>
<p>/hadoop-daemon</p>
<p>.sh  --script hdfs start namenode</p>
<p>/# 启动namenode</p>
<p>$sbin</p>
<p>/hadoop-daemon</p>
<p>.sh --script hdfs start datanode</p>
<p>/# 启动datanode
$sbin</p>
<p>/yarndaemon</p>
<p>.shstart nodemanager</p>
<p>/#启动nodemanager</p>
<p>$sbin</p>
<p>/yarn-daemon</p>
<p>.sh   start resourcemanager</p>
<p>/# 启动resourcemanager
$sbin</p>
<p>/yarn-daemon</p>
<p>.shstart proxyserver</p>
<p>/# 启动web App proxy, 作用类似jobtracker,若yarn-site.xml里没有设置yarn.web-proxy.address的host和端口，或者设置了和 resourcemanager相同的host和端口，则hadoop默认proxyserver和resourcemanager共享 host:port</p>
<p>$sbin</p>
<p>/mr-jobhistory-daemon</p>
<p>.sh   start historyserver</p>
<p>/#你懂得
$</p>
<p>ssh</p>
<p>cluster2 </p>
<p>/#登录cluster2</p>
<p>$</p>
<p>hostname</p>
<p>/#查看host名字cluster2
$sbin</p>
<p>/yarndaemon</p>
<p>.shstart nodemanager</p>
<p>/# 启动nodemanager</p>
<p>$sbin</p>
<p>/hadoop-daemon</p>
<p>.sh  --script hdfs start datanode</p>
<p>/# 启动datanode
$</p>
<p>ssh</p>
<p>cluster3 </p>
<p>/#登录cluster3.../# cluster2, cluster3, cluster4启动方式和cluster2一样。</p>
<p>启动方式（2）使用hadoop自带的批处理脚本开启</p>
<p>Step1.确认已登录cluster1:
1$</p>
<p>hostname</p>
<p>cluster1</p>
<p>在$YARN_HOME/etc/hadoop/下新建namenodehosts,添加所有namenode节点</p>
<p>1</p>
<p>2</p>
<p>$</p>
<p>cat</p>
<p>$YARN_HOME</p>
<p>/etc/hadoop/namenodehosts</p>
<p>cluster1</p>
<p>在$YARN_HOME/etc/hadoop/下新建datanodehosts,添加所有datanode节点</p>
<p>1</p>
<p>2
3</p>
<p>4
$</p>
<p>cat</p>
<p>$YARN_HOME</p>
<p>/etc/hadoop/datanodehosts</p>
<p>cluster2
cluster3</p>
<p>cluster4</p>
<p>在$YARN_HOME/etc/hadoop/下新建nodehosts,添加所有datanode和namenode节点</p>
<p>1</p>
<p>2
3</p>
<p>4
5$</p>
<p>cat</p>
<p>$YARN_HOME</p>
<p>/etc/hadoop/datanodehosts</p>
<p>cluster1
cluster2</p>
<p>cluster3
cluster4</p>
<p><strong>备注</strong>：以上的hostfile名字是随便起的，可以是任意的file1,file2,file3, 但是必须放在$YARN_HOME/etc/hadoop/下面！</p>
<p>Step2.执行脚本
1</p>
<p>2
3</p>
<p>4
5</p>
<p>6
$sbin</p>
<p>/hadoop-daemons</p>
<p>.sh--hosts namenodehosts --script  hdfsstart  namenode</p>
<p>$sbin</p>
<p>/hadoop-daemons</p>
<p>.sh--hosts datanodehosts --script  hdfsstart  datanode
$sbin</p>
<p>/yarn-daemons</p>
<p>.sh--hostnames cluster1 start resourcemanager</p>
<p>$sbin</p>
<p>/yarn-daemons</p>
<p>.sh--hosts allnodehosts start nodemanager
$sbin</p>
<p>/yarn-daemons</p>
<p>.sh--hostnames cluster1 start proxyserver</p>
<p>$sbin</p>
<p>/mr-jobhistory-daemon</p>
<p>.sh   start  historyserver</p>
<p><strong>在这里不得不纠正一个其他教程上关于对hadoop2.2做相关配置和运行时的错误!</strong></p>
<p>我们在core-site.xml指定了default filesystem为cluster1, 并指定了其节点ip（或节点名）和端口号， 这意味着若启动hadoop时不额外添加配置（启动hadoop时命令行里加--conf指定用户配置文件的存放位置)，则默认的namenode就一 个，那就是cluster1, 如果随意指定namenode必然会出现错误！</p>
<p>如果你要还要再启动一个新的namenode(以cluster3为例)，则必须如下操作：</p>
<p>a. 新建一个core-site.xml，添加fs.defaultFS的value为hdfs://cluster3:9000</p>
<p>b. command：
1$sbin</p>
<p>/hadoop-daemon</p>
<p>.sh --hostnames cluster3 --conf you_conf_dir --script hdfs --start namenode</p>
<p>我们之前启动的namenode为cluster1, 假如要查看放在此文件系统根目录下的文件input_file，则它的完整路径为 hdfs://cluster1:9000/input_file</p>
<p>Step3.查看启动情况</p>
<p>在cluster1上：
1</p>
<p>2
3</p>
<p>4
5</p>
<p>6
$jps</p>
<p>22411 NodeManager
23356 Jps</p>
<p>22292 ResourceManager
22189 DataNode</p>
<p>22507 WebAppProxyServer</p>
<p>在cluster2上</p>
<p>1</p>
<p>2
3</p>
<p>4
$jps</p>
<p>8147 DataNode
8355 Jps</p>
<p>8234 NodeManagercluster3，cluster4上和cluster2一样。</p>
<p><em>**</em>Step4.查看各节点状态以及yarncluster运行状态</p>
<p>（1）查看各节点状态</p>
<p>FireFox进入： <a href="http://cluster1:50070(cluster1为namenode所在节点" target="_blank">http://cluster1:50070(cluster1为namenode所在节点</a>)</p>
<p>在主页面（第一张图）上点击Live Node，查看（第二张图）上各Live Node状态：</p>
<p><img src="http://m1.img.libdd.com/farm4/d/2013/1114/14/5D0856F11A09F93C0BD97246AE6DEE43_B500_900_500_218.png" alt=""></p>
<p>（2）查看resourcemanager上cluster运行状态</p>
<p>Firefox进入：<a href="http://cluster2:8088(node1为resourcemanager所在节点" target="_blank">http://cluster2:8088(node1为resourcemanager所在节点</a>)</p>
<p><img src="http://m3.img.libdd.com/farm4/d/2013/1114/14/5B46215F81D52CF3C3DB80D2DEFF8770_B500_900_500_195.png" alt=""></p>
<p><strong> </strong>Step5. Cluster上MapReduce测试</p>
<p>现提供3个test cases</p>
<p><strong>Test Case 1 testimated_value_of_pi</strong></p>
<p>command:
$sbin/yarnjar $YARN_HOME/share/hadoop//mapreduce/hadoop-mapreduce-examples-2.2.0.jar \pi 101000000</p>
<p>console输出：</p>
<p><a href="&quot;复制代码&quot;"><img src="http://common.cnblogs.com/images/copycode.gif" alt="复制代码"></a>
Number of Maps  =10                        Samples per Map = 1000000                         Wrote input for Map /#0                          Wrote input for Map /#1                          Wrote input for Map /#2                         Wrote input for Map /#3                        Wrote input for Map /#4                       Wrote input for Map /#5                         Wrote input for Map /#6                        Wrote input for Map /#7                        Wrote input for Map /#8                        Wrote input for Map /#9                        Starting Job                        13/11/06 23:20:07 INFO Configuration.deprecation: mapred.map.tasksis deprecated. Instead, use mapreduce.job.maps                        13/11/06 23:20:07 INFO Configuration.deprecation:mapred.output.key.class is deprecated. Instead, usemapreduce.job.output.key.class                         13/11/06 23:20:07 INFO Configuration.deprecation:mapred.working.dir is deprecated. Instead, use mapreduce.job.working.dir                         13/11/06 23:20:11 INFO mapreduce.JobSubmitter: Submittingtokens for job: job_1383806445149_0001                         13/11/06 23:20:15 INFO impl.YarnClientImpl: Submittedapplication application_1383806445149_0001 to ResourceManager at /0.0.0.0:8032                       13/11/06 23:20:16 INFO mapreduce.Job: The url to trackthe job: <a href="http://Node1:8088/proxy/application_1383806445149_0001/" target="_blank">http://Node1:8088/proxy/application_1383806445149_0001/</a>                        13/11/06 23:20:16 INFO mapreduce.Job: Running job:job_1383806445149_0001                         13/11/06 23:21:09 INFO mapreduce.Job: Jobjob_1383806445149_0001 running in uber mode : false                        13/11/06 23:21:10 INFO mapreduce.Job:  map 0% reduce 0%                          13/11/06 23:24:28 INFO mapreduce.Job:  map 20% reduce 0%                         13/11/06 23:24:30 INFO mapreduce.Job:  map 30% reduce 0%                          13/11/06 23:26:56 INFO mapreduce.Job:  map 57% reduce 0%                          13/11/06 23:26:58 INFO mapreduce.Job:  map 60% reduce 0%                          13/11/06 23:28:33 INFO mapreduce.Job:  map 70% reduce 20%                         13/11/06 23:28:35 INFO mapreduce.Job:  map 80% reduce 20%                         13/11/06 23:28:39 INFO mapreduce.Job:  map 80% reduce 27%                         13/11/06 23:30:06 INFO mapreduce.Job:  map 90% reduce 27%                         13/11/06 23:30:09 INFO mapreduce.Job:  map 100% reduce 27%                         13/11/06 23:30:12 INFO mapreduce.Job:  map 100% reduce 33%                         13/11/06 23:30:25 INFO mapreduce.Job:  map 100% reduce 100%                          13/11/06 23:30:54 INFO mapreduce.Job: Jobjob_1383806445149_0001 completed successfully                          13/11/06 23:31:10 INFO mapreduce.Job: Counters: 43                                    File SystemCounters                                             FILE:Number of bytes read=226                                             FILE:Number of bytes written=879166                                              FILE:Number of read operations=0                                             FILE:Number of large read operations=0                                            FILE:Number of write operations=0                                             HDFS:Number of bytes read=2590                                               HDFS:Number of bytes written=215                                               HDFS:Number of read operations=43                                               HDFS:Number of large read operations=0                                                 HDFS:Number of write operations=3                                    JobCounters                                              Launchedmap tasks=10                                                Launchedreduce tasks=1                                               Data-localmap tasks=10                                                Totaltime spent by all maps in occupied slots (ms)=1349359                                             Totaltime spent by all reduces in occupied slots (ms)=190811                                    Map-ReduceFramework                                             Mapinput records=10                                              Mapoutput records=20                                                Mapoutput bytes=180                                                 Mapoutput materialized bytes=280                                                Inputsplit bytes=1410                                                 Combineinput records=0                                                Combineoutput records=0                                                  Reduceinput groups=2                                                 Reduceshuffle bytes=280                                              Reduceinput records=20                                                Reduceoutput records=0                                                 SpilledRecords=40                                                 ShuffledMaps =10                                                  FailedShuffles=0                                                MergedMap outputs=10                                                 GCtime elapsed (ms)=45355                                                 CPUtime spent (ms)=29860                                                 Physicalmemory (bytes) snapshot=1481818112                                                 Virtualmemory (bytes) snapshot=9214468096                                                 Totalcommitted heap usage (bytes)=1223008256                                        ShuffleErrors                                              BAD_ID=0                                              CONNECTION=0                                             IO_ERROR=0                                              WRONG_LENGTH=0                                               WRONG_MAP=0                                            WRONG_REDUCE=0                                    File InputFormat Counters                                              BytesRead=1180                                    File OutputFormat Counters                                             BytesWritten=97                           13/11/06 23:31:15 INFO mapred.ClientServiceDelegate:Application state is completed. FinalApplicationStatus=SUCCEEDED. Redirectingto job history server                           Job Finished in 719.041 seconds                           Estimated value of Pi is 3.14158440000000000000</p>
<p><a href="&quot;复制代码&quot;"><img src="http://common.cnblogs.com/images/copycode.gif" alt="复制代码"></a></p>
<p><strong>说明</strong>：可以看到最后输出值为该job使用了 10个maps, job id为job<em>1383806445149_000, 最后计算得Pi的值为13.14158440000000000000， job Id分配原则为job</em>年月日时分<em>job序列号，序列号从0开始，上限值为1000， task id分配原则为job</em>年月日时分<em>job序列号_task序列号_m, job</em>年月日时分_job序列号_task序列号_r, m代表map taskslot , r代表reduce task slot, task 序列号从0开始，上限值为1000.</p>
<p><strong>Test Case 2 random_writting</strong>
/#command line $sbin/yarnjar $YARN_HOME/share/hadoop//mapreduce/hadoop-mapreduce-examples-2.2.0.jar \randomwriter/user/grid/test/test_randomwriter/out</p>
<p><a href="&quot;复制代码&quot;"><img src="http://common.cnblogs.com/images/copycode.gif" alt="复制代码"></a></p>
<p>/#Console输出摘录：                                                                                                                                                    Running 10 maps.Job started: Wed Nov 0623:42:17 PST 201313/11/0623:42:17 INFO client.RMProxy: Connecting toResourceManager at /0.0.0.0:803213/11/0623:42:19 INFO mapreduce.JobSubmitter: number ofsplits:1013/11/0623:42:20 INFO mapreduce.JobSubmitter: Submittingtokens for job: job_1383806445149_000213/11/0623:42:21 INFO impl.YarnClientImpl: Submittedapplication application_1383806445149_0002 to ResourceManager at /0.0.0.0:803213/11/0623:42:21 INFO mapreduce.Job: The url to trackthe job: <a href="http://Master:8088/proxy/application_1383806445149_0002/13/11/0623:42:21" target="_blank">http://Master:8088/proxy/application_1383806445149_0002/13/11/0623:42:21</a> INFO mapreduce.Job: Running job:job_1383806445149_000213/11/0623:42:40 INFO mapreduce.Job: Jobjob_1383806445149_0002 running in uber mode : false13/11/0623:42:40 INFO mapreduce.Job:  map 0% reduce 0%                  13/11/0623:55:02 INFO mapreduce.Job:  map 10% reduce 0%                    13/11/0623:55:14 INFO mapreduce.Job:  map 20% reduce 0%                  13/11/0623:55:42 INFO mapreduce.Job:  map 30% reduce 0%                    13/11/0700:06:55 INFO mapreduce.Job:  map 40% reduce 0%                    13/11/0700:07:10 INFO mapreduce.Job:  map 50% reduce 0%                   13/11/0700:07:36 INFO mapreduce.Job:  map 60% reduce 0%                    13/11/0700:13:47 INFO mapreduce.Job:  map 70% reduce 0%                     13/11/0700:13:54 INFO mapreduce.Job:  map 80% reduce 0%                     13/11/0700:13:58 INFO mapreduce.Job:  map 90% reduce 0%                    13/11/0700:16:29 INFO mapreduce.Job:  map 100% reduce 0%                    13/11/0700:16:37 INFO mapreduce.Job: Jobjob_1383806445149_0002 completed successfully        File OutputFormat Counters                  BytesWritten=10772852496Job ended: Thu Nov 0700:16:40 PST 2013The job took 2062 seconds.</p>
<p><a href="&quot;复制代码&quot;"><img src="http://common.cnblogs.com/images/copycode.gif" alt="复制代码"></a></p>
<p><strong>说明</strong>：电脑存储空间足够的话，可以从hdfs里down下来看看。</p>
<p>现只能看一看输出文件存放的具体形式：</p>
<p><a href="&quot;复制代码&quot;"><img src="http://common.cnblogs.com/images/copycode.gif" alt="复制代码"></a>
./bin/hadoopfs -ls /user/grid/test/test_randomwriter/out/Found 11items-rw-r--r--   2 grid supergroup          02013-11-0700:16/user/grid/test/test_randomwriter/out/_SUCCESS-rw-r--r--   2 grid supergroup 10772782142013-11-0623:54 /user/grid/test/test_randomwriter/out/part-m-00000                                                                                                                                                   -rw-r--r--   2 grid supergroup 10772827512013-11-0623:55 /user/grid/test/test_randomwriter/out/part-m-00001                                                                                                                                                   -rw-r--r--   2 grid supergroup 10772802982013-11-0623:55 /user/grid/test/test_randomwriter/out/part-m-00002                                                                                                                                                   -rw-r--r--   2 grid supergroup 10773031522013-11-0700:07 /user/grid/test/test_randomwriter/out/part-m-00003                                                                                                                                                   -rw-r--r--   2 grid supergroup 10772842402013-11-0700:06 /user/grid/test/test_randomwriter/out/part-m-00004                                                                                                                                                   -rw-r--r--   2 grid supergroup 10772866042013-11-0700:07 /user/grid/test/test_randomwriter/out/part-m-00005                                                                                                                                                   -rw-r--r--   2 grid supergroup 10772843362013-11-0700:13 /user/grid/test/test_randomwriter/out/part-m-00006                                                                                                                                                   -rw-r--r--   2 grid supergroup 10772848292013-11-0700:13 /user/grid/test/test_randomwriter/out/part-m-00007                                                                                                                                                   -rw-r--r--   2 grid supergroup 10772897062013-11-0700:13 /user/grid/test/test_randomwriter/out/part-m-00008                                                                                                                                                   -rw-r--r--   2 grid supergroup 10772783662013-11-0700:16 /user/grid/test/test_randomwriter/out/part-m-00009</p>
<p><a href="&quot;复制代码&quot;"><img src="http://common.cnblogs.com/images/copycode.gif" alt="复制代码"></a></p>
<p><strong>Test Case3 word_count</strong></p>
<p>（1）Locaol上创建文件：
$mkdirinput%echo ‘hello,world’ &gt;&gt; input/file1.in$echo ‘hello, ruby’ &gt;&gt; input/file2.in</p>
<p>（2）上传到hdfs上：
./bin/hadoop fs -mkdir -p /user/grid/test/test_wordcount/./bin/hadoop fs –put input/user/grid/test/test_wordcount/in</p>
<p>（3）用yarn新计算框架运行mapreduce：</p>
<p><a href="&quot;复制代码&quot;"><img src="http://common.cnblogs.com/images/copycode.gif" alt="复制代码"></a>
/#command line $bin/yarn jar$YARN_HOME/share/hadoop//mapreduce/hadoop-mapreduce-examples-2.2.0.jarwordcount  /user/grid/test/test_wordcount/in/user/grid/test/test_wordcount/out                                                                                                                                             /#ConSole输出摘录：3/11/0700:35:03 INFO client.RMProxy:Connecting to ResourceManager at /0.0.0.0:803213/11/0700:35:05 INFO input.FileInputFormat:Total input paths to process : 213/11/0700:35:05 INFO mapreduce.JobSubmitter:number of splits:213/11/0700:35:06 INFO mapreduce.JobSubmitter:Submitting tokens for job: job_1383806445149_000313/11/0700:35:08 INFO impl.YarnClientImpl:Submitted application application_1383806445149_0003 to ResourceManager at /0.0.0.0:803213/11/0700:35:08 INFO mapreduce.Job: The urlto track the job: <a href="http://Master:8088/proxy/application_1383806445149_0003/13/11/0700:35:08" target="_blank">http://Master:8088/proxy/application_1383806445149_0003/13/11/0700:35:08</a> INFO mapreduce.Job: Runningjob: job_1383806445149_000313/11/0700:35:25 INFO mapreduce.Job: Jobjob_1383806445149_0003 running in uber mode : false13/11/0700:35:25 INFO mapreduce.Job:  map 0% reduce 0%                                                                                                                                              13/11/0700:37:50 INFO mapreduce.Job:  map 33% reduce 0%                                                                                                                                              13/11/0700:37:54 INFO mapreduce.Job:  map 67% reduce 0%                                                                                                                                              13/11/0700:37:55 INFO mapreduce.Job:  map 83% reduce 0%                                                                                                                                              13/11/0700:37:58 INFO mapreduce.Job:  map 100% reduce 0%                                                                                                                                              13/11/0700:38:51 INFO mapreduce.Job:  map 100% reduce 100%                                                                                                                                              13/11/0700:38:54 INFO mapreduce.Job: Jobjob_1383806445149_0003 completed successfully13/11/0700:38:56 INFO mapreduce.Job:Counters: 43</p>
<p><a href="&quot;复制代码&quot;"><img src="http://common.cnblogs.com/images/copycode.gif" alt="复制代码"></a></p>
<p><strong>说明</strong>：查看word count的计算结果：
1</p>
<p>2
3</p>
<p>4
$bin</p>
<p>/hadoop</p>
<p>fs -</p>
<p>cat</p>
<p>/user/grid/test//test_wordcount/out/</p>
<p>/*</p>
<p>hadoop 1
hello  1</p>
<p>ruby</p>
<p><strong>补充</strong>：因为新的YARN为了保持与MRv1框 架的旧版本兼容性，很多老的API还是可以用，但是会有INFO。此处通过修改$YARN_HOME/etc/hadoop /log4j.properties可以turn offconfiguration deprecation warnings.</p>
<p>建议去掉第138行的注释（可选），确保错误级别为WARN（默认为INFO级别，详见第20行：hadoop.root.logger=INFO,console）：
1138 log4j.logger.org.apache.hadoop.conf.Configuration.deprecation=WARN</p>
<p>附文：</p>
<p><strong>集群搭建、配置步骤（基于CentOS_64bit）</strong></p>
<p><strong>0. 说明</strong></p>
<p>大体规划如下：</p>
<p>虚拟机： VMware-workstation-full-8.0.3-703057（VMware10中文版不完整，此版本内含vmware tools，为设定共享文件夹所必须）</p>
<p>电脑1，VMWare,内装2个虚拟系统，(cluster1, cluster2)</p>
<p>电脑2,，VMware内装2个虚拟系统，(cluster3, cluster4)</p>
<p>虚拟主机： CentOS x86 64bit</p>
<p>局域网IP设置：<em>**</em>
1</p>
<p>2
3</p>
<p>4</p>
<p>cluster1  172.16.102. 201</p>
<p>cluster2   172.16.102. 202
cluster3   172.16.102. 203</p>
<p>cluster4   172.16.102. 204</p>
<p>网关</p>
<p>1172.16.102.254</p>
<p><strong>1. Linux集群安装</strong></p>
<p>(1) 准备</p>
<p>Vmware: VMware-workstation-full-8.0.3-703057(此安装包自带VMWare Tools)</p>
<p>Linux:CentOS.iso</p>
<p>(2) VMWare配置</p>
<p>VMWare以及所有安装的虚拟机均为桥接</p>
<p>Step1. 配置VMWare 联网方式： Editor-&gt;Virtual Network Editor-&gt;选择Bridged， 点击确定</p>
<p>Step2. 安装虚拟机</p>
<p>Step3.配置各虚拟机接网方式：右键已安装虚拟机-&gt;点击NetworkAdapter, 选择桥接，确定</p>
<p>Step4. 为所有安装好的虚拟系统设置一个共享目录(类似FTP，但是设置共享目录更方便) ：右键已安装虚拟机-&gt;点击Virtual Machine Settings对话框上部Options， 选择Shared Folder， 在本地新建SharedFolder并添加进来,确定。</p>
<p>(3) linux下网卡以及IP配置</p>
<p>以下配置在三个虚拟系统里均相同, 以cluster1为例：</p>
<p>配置前需切换为root</p>
<p>Step1. 修改主机名， 设置为开启网络</p>
<p>配置/etc/sysconfig/network：
1</p>
<p>2
3[root@localhost ~]</p>
<p>/# cat /etc/sysconfig/network</p>
<p>NETWORKING=</p>
<p>yes
HOSTNAME=cluster1</p>
<p>Step2.修改当前机器的IP， 默认网关， IP分配方式， 设置网络接口为系统启动时有效：</p>
<p>a.查看配置前的ip:
1</p>
<p>2
3</p>
<p>4
[root@localhost ~]</p>
<p>/# ifconfig</p>
<p>eth0      Link encap:Ethernet  HWaddr 00:0C:29:E1:FB:95 
inet addr:172.16.102.3  Bcast:172.16.102.255  Mask:255.255.255.0</p>
<p>inet6 addr: fe80::20c:29ff:fee1:fb95</p>
<p>/64</p>
<p>Scope:Lin</p>
<p>b.配置/etc/sysconfig/network-scripts/ifcfg-eth0</p>
<p>注意以下几项：</p>
<p>BOOTPROTO=&quot;static&quot; (IP分配方式, 设为static则主机ip即为IPADDR里所设，若为”dhcp”, 此时只有局域网有效，则vmware会启动自带虚拟”dhcp”, 动态分配ip， 此时可以连接互联网 )</p>
<p><a href="&quot;复制代码&quot;"><img src="http://common.cnblogs.com/images/copycode.gif" alt="复制代码"></a>
IPV6INIT=&quot;no&quot;  /#你懂得 IPADDR=&quot;172.16.102.201&quot; /#ip地址 GETEWAY=&quot;172.16.102.254&quot; /#默认网关地址 ONBOOT=&quot;yes&quot; /#系统启动时网络接口有效 [root@localhost ~]/# cat /etc/sysconfig/network-scripts/ifcfg-eth0  DEVICE=&quot;eth0&quot;BOOTPROTO=&quot;static&quot;HWADDR=&quot;00:0C:29:E1:FB:95&quot;BOOTPROTO=&quot;dhcp&quot;IPV6INIT=&quot;no&quot;IPADDR=&quot;172.16.102.201&quot;GETEWAY=&quot;172.16.102.254&quot;ONBOOT=&quot;yes&quot;NM_CONTROLLED=&quot;yes&quot;TYPE=&quot;Ethernet&quot;UUID=&quot;79612b26-326b-472c-94af-9ab151fc2831</p>
<p><a href="&quot;复制代码&quot;"><img src="http://common.cnblogs.com/images/copycode.gif" alt="复制代码"></a></p>
<p>c.使当前设置生效：
$service network restart $ifdown eth0 /#关闭默认网卡 $ifup eth/#重新启用默认网卡 $service network restart; ifdown eth0; ifup eth0</p>
<p>d.查看新设置的ip:
$ifconfigeth0      Link encap:Ethernet  HWaddr 00:0C:29:E1:FB:95  inet addr:192.168.1.200  Bcast:192.168.1.255  Mask:255.255.255.0inet6 addr: fe80::20c:29ff:fee1:fb95/64 Scope:Link UP BROADCAST RUNNING MULTICAST  MTU:1500  Metric:1</p>
<p>Step3. 修改hosts文件，此文件会被ＤＮＳ解析，类似linux里的alias, 设置后以后，hostname就是ip地址， 两者可以互换。</p>
<p>配置/etc/hosts, 添加三行如下， （注意，此3行在3个虚拟主机里都相同，切必须全部都要加上）</p>
<p><a href="&quot;复制代码&quot;"><img src="http://common.cnblogs.com/images/copycode.gif" alt="复制代码"></a>
[root@localhost ~]/# cat /etc/hosts127.0.0.1   localhost localhost.localdomain localhost4 localhost4.localdomain4 ::1         localhost localhost.localdomain localhost6 localhost6.localdomain6 cluster1   172.16.102. 201cluster2   172.16.102. 202cluster3   172.16.102. 203cluster4   172.16.102. 204</p>
<p><a href="&quot;复制代码&quot;"><img src="http://common.cnblogs.com/images/copycode.gif" alt="复制代码"></a></p>
<p>Step4. 按照以上3个步骤分别设置两外两个虚拟主机，</p>
<p>配置完以后必须按照之前的三个命令分别重启 network, ifeth0:
$service network restart $ifdown eth0 $ifup eth0</p>
<p>重新初始化以后查看各自主机的ip配置是否正确。</p>
<p>在任意一台主机上执行： ping cluster1; ping cluster2; ping cluster3; ping cluster4</p>
<p>若配置正确，一定可以ping通，否则，请检查各个主机的/etc/hosts文件是否已经添加新的映射！至此，linux集群已成功配置。</p>
<p><strong>2. 设置 ssh免登陆</strong></p>
<p>a. 新建一个用户</p>
<p>在三台主机上分别以root权限新建一个用户，此处默认为grid:</p>
<p>cluster1,cluster2, cluster3, cluster4上：
$useradd –m grid $passwd grid 1qaz!QAZ</p>
<p>注意一定要保证4台主机上有相同的用户名， 以实现同一个用户在4台主机上登录。</p>
<p>b. 在cluster1上生成RSA密钥</p>
<p>切换回user: grid
$su grid $cd ~</p>
<p>生成密钥对：</p>
<p>1</p>
<p>2
3$</p>
<p>ssh</p>
<p>-keygen –t rsa</p>
<p>/#一路回车到最后（ 此处生成无需密码的秘钥-公钥对）。
/#上一个步骤 ssh-keygen –t rsa会在grid的home目录下生成一个.ssh文件夹</p>
<p>之后：</p>
<p>$cd ~/.ssh/$cp id_rsa.pub authorized_keys</p>
<p>c. 在另外3个主机上的grid用户home目录下也声称相应的密钥对, 并在.ssh目录下生成一个</p>
<p>authorized_keys
文件</p>
<p>d. 想办法将4台主机grid用户下刚生成的</p>
<p>authorized_keys
里的内容整合成一个完整的</p>
<p>authorized_keys.</p>
<p>比如将4个authorized_keys里的内容全部复制到cluster1上的authorized_keys里， 然后：
$chmod 600 authorized_keys $scp .ssh/authorized_keys  cluster2:/home/grid/.ssh/$scp .ssh/authorized_keys  cluster3:/home/grid/.ssh/$scp .ssh/authorized_keys  cluster4:/home/grid/.ssh/</p>
<p>若要求输入密码，则输入之前新建用户grid时设置的密码， 一路回车到最后.</p>
<p>e. 下面尝试ssh无秘钥登录：</p>
<p>在cluster1主机上：ssh cluster2， 依次尝试登陆cluster2, cluster3, cluster4</p>
<p>若均可可以免密码登录，则直接跳到下一步骤，否则，请看下面的解决方案：</p>
<p>可能出现的问题有3，</p>
<p>第一种可能，.ssh文件夹非自动生成，而是你手动新建的，若如此，会出现.ssh的安全上下文问题， 解决方案， 在三个主机上以grid用户，删除刚才生成的.ssh文件夹，重新进入home目录，务必用 /# ssh-keygen –t rsa 生成秘钥， 此过程ssh程序会自动在home目录下成成一个.ssh文件夹</p>
<p>第二种可能, authorized_keys权限不一致。 在各自.ssh目录下：ls–alauthorizedkeys,查看此文件的权限，一定为600(−rw−−−−−−−),即只对当前用户grid开放可读可写权限，若不是，则修改authorizedkeys文件权限chmod 600 authorized_keys</p>
<p>若经过以上两步还不行，则执行以下命令，重新启动ssh-agent， 且加入当前用户秘钥id_rsa
$ssh-add ~grid/.ssh/id_rsa</p>
<p>经过以上三步，一定可以实现grid从cluster1到其它3个节点的免秘钥登录。</p>
<p>因为hadoop2.2新架构的缘故，我们还应该设置为每一个节点到其它任意节点免登陆。</p>
<p>具体步骤:</p>
<p>1.在将cluster2, cluster3, cluster4上各自的.ssh/id_rsa.pub 分别复制到cluster1的.ssh/下： scp id_rsa.pub cluster1:/home/grid/.ssh/tmp2, scp id_rsa.pub cluster1:/home/grid/.ssh/tmp3 ...</p>
<p>2.将cluster1节点/home/grid/.ssh/下的tmp2, tmp3, tmp4分别appand到authorized_keys里， 并请将各自节点上的id_rsa.pub也append到各自节点的authorized_keys里，以实现本地登陆（类似： ssh localhost)</p>
<p>至此，ssh免秘钥登陆设置完成。</p>
<p><strong>3. 安装jdk</strong></p>
<p>Step1. 记得之前在安装cluster1时在VMWare里设置的共享目录吗？</p>
<p>在Windows下将Hadoop安装包，jdk安装包Copy到共享目录下，然后在linux下从/mnt/hgfs/Data-shared/下cp到/home/grid/下，直接执行jdk安装包，不需要解压。</p>
<p>注意：若在安装过程中提示”Extracting… install.sfx 5414: /lib/ld-linux.50.2 No such file or directory, 则执行以下命令:</p>
<p>a.我们之前为了测试自定义的ip是否有效，已将各台主机的ip分配方式设为了BOOTPROTO=”static”, 这种方式是无法连入外部网络的，所以此时为了安装缺省的包，切换到root, 修改/etc/sysconfig/network-script/ifcfg-eth0，将BOOTPROTO=”static” 修改为BOOTPROTO=”dhcp”,</p>
<p>b.重启网络服务和网卡: 在root权限下:
$service network restart; ifdown eth0; ifup eth0 $yum install glibc.i686</p>
<p>d.切换回grid,重新安装jdk</p>
<p>$cd /usr/java/  /#注意必须进入java文件夹，因为java安装包默认安装在当前目录下 $./jdk-6u25-linux-i586.bin /#安装jdk</p>
<p>安装完以后，记得将eth0文件修改回来：</p>
<p>$sed -i s/dhcp/static/g /etc/sysconfig/network-scripts/ifcfg-eth0 /#此处用sed直接替换，若不放心也可以用编辑器修改 /#service network restart; ifdown eth0;ifup eth0</p>
<p>至此，jdk已在linux下安装完毕。</p>
<p>最后，将java安装好的路径append到$PATH变量里（处于个人习惯，新环境变量一律添加到所需用户的.bashrc文件里， 即只对当前用户有效）
$su grid $vim ~/.bashrc （修改.bashrc文件，添加如下两行：） $tail -2 ~/.bashrc export JAVA_HOME=&quot;/usr/java/jdk1.6.0_25/&quot;export PATH=&quot;${PATH}:${JAVA_HOME}/bin&quot;</p>
<p>测试一下java是否可以正常启动：</p>
<p>$source ~/.bashrc $which java $/usr/java/jdk1.6.0_25/bin/java</p>
<p>至此，jdk安装完毕。</p>
<p>用同样的方式在另外3台虚拟主机上安装jdk, 提示：先复制到home下
$scp -r /mnt/hgfs/Data-shared/Archive/jdk-6u25-linux-i586.bin  cluster2:/home/grid/$scp -r /mnt/hgfs/Data-shared/Archive/jdk-6u25-linux-i586.bin  cluster3:/home/grid/$scp -r /mnt/hgfs/Data-shared/Archive/jdk-6u25-linux-i586.bin  cluster4:/home/grid/</p>
<p>再切root, copy到/usr/java下， cd /usr/java, 然后再安装.</p>

      
    </div>
    
    
      
    
    
        <footer id="post-meta">
        <span class="categories">Posted in<span class="breadcrumb fa fa-folder"><li><a href="/categories/hadoop/">hadoop</a></li></span></span> | <span class="tags">Tagged <a href="/tags/hadoop/" class="label label-primary">hadoop</a></span> | <span class="time">recent updated:<time title="2014-03-07 09:54:36"datetime="2014-03-07 09:54:36"> mar. 7 2014</time></span> | <span class="comment-link">
<a href="http://itsolife.com/2014/02/02/2014-02-02-hadoop--基于Hadoop220的高可用性集群搭建步骤（64位）/#comments" class="ds-thread-count comment-link" data-thread-key="2014-02-02-hadoop--基于Hadoop220的高可用性集群搭建步骤（64位）" data-count-type="comments">暂无评论</a></span>
        </footer>
    
    <div class="clearfix"></div>
  </div>
</div>
</article>



  <article>
<div id="post" class="post well">
  <div class="post-content">
    <header class="well-sm">
      <i class="fa icon fa-5x pull-left"></i>
      <h1 class="title"><a href="/2014/02/02/2014-02-02-hadoop--mapreduce-osdi04/">mapreduce</a></h1>
      
        <span>Posted on<time datetime="2014-02-02T01:54:36.000Z"> <a href="/2014/02/02/2014-02-02-hadoop--mapreduce-osdi04/">feb. 2 2014</a></time></span>      
    </header>
    
    
    <div class="entry">
      
        <h1 id="mapreduce-osdi04">mapreduce-osdi04</h1>
<p>MapReduce: Simpliﬁed Data Processing on Large Clusters
Jeffrey Dean and Sanjay Ghemawat
jeff@google.com, sanjay@google.com
Google, Inc.
Abstract
MapReduce is a programming model and an associated implementation for processing and generating large data sets. Users specify a map function that processes a key/value pair to generate a set of intermediate key/value pairs, and a reduce function that merges all intermediate values associated with the same intermediate key. Many real world tasks are expressible in this model, as shown in the paper. Programs written in this functional style are automatically parallelized and executed on a large cluster of commodity machines. The run-time system takes care of the details of partitioning the input data, scheduling the program’s execution across a set of machines, handling machine failures, and managing the required inter-machine communication. This allows programmers without any experience with parallel and distributed systems to easily utilize the resources of a large distributed system. Our implementation of MapReduce runs on a large cluster of commodity machines and is highly scalable: a typical MapReduce computation processes many terabytes of data on thousands of machines. Programmers ﬁnd the system easy to use: hundreds of MapReduce programs have been implemented and upwards of one thousand MapReduce jobs are executed on Google’s clusters every day.
1 Introduction
Over the past ﬁve years, the authors and many others at Google have implemented hundreds of special-purpose computations that process large amounts of raw data, such as crawled documents, web request logs, etc., to compute various kinds of derived data, such as inverted indices, various representations of the graph structure of web documents, summaries of the number of pages crawled per host, the set of most frequent queries in a To appear in OSDI 2004
given day, etc. Most such computations are conceptually straightforward. However, the input data is usually large and the computations have to be distributed across hundreds or thousands of machines in order to ﬁnish in a reasonable amount of time. The issues of how to parallelize the computation, distribute the data, and handle failures conspire to obscure the original simple computation with large amounts of complex code to deal with these issues. As a reaction to this complexity, we designed a new abstraction that allows us to express the simple computations we were trying to perform but hides the messy details of parallelization, fault-tolerance, data distribution and load balancing in a library. Our abstraction is inspired by the map and reduce primitives present in Lisp and many other functional languages. We realized that most of our computations involved applying a map operation to each logical “record” in our input in order to compute a set of intermediate key/value pairs, and then applying a reduce operation to all the values that shared the same key, in order to combine the derived data appropriately. Our use of a functional model with userspeciﬁed map and reduce operations allows us to parallelize large computations easily and to use re-execution as the primary mechanism for fault tolerance. The major contributions of this work are a simple and powerful interface that enables automatic parallelization and distribution of large-scale computations, combined with an implementation of this interface that achieves high performance on large clusters of commodity PCs. Section 2 describes the basic programming model and gives several examples. Section 3 describes an implementation of the MapReduce interface tailored towards our cluster-based computing environment. Section 4 describes several reﬁnements of the programming model that we have found useful. Section 5 has performance measurements of our implementation for a variety of tasks. Section 6 explores the use of MapReduce within Google including our experiences in using it as the basis 1
for a rewrite of our production indexing system. Section 7 discusses related and future work.
2.2 Types
Even though the previous pseudo-code is written in terms of string inputs and outputs, conceptually the map and reduce functions supplied by the user have associated types: map reduce (k1,v1) (k2,list(v2)) → list(k2,v2) → list(v2)
2 Programming Model
The computation takes a set of input key/value pairs, and produces a set of output key/value pairs. The user of the MapReduce library expresses the computation as two functions: Map and Reduce. Map, written by the user, takes an input pair and produces a set of intermediate key/value pairs. The MapReduce library groups together all intermediate values associated with the same intermediate key I and passes them to the Reduce function. The Reduce function, also written by the user, accepts an intermediate key I and a set of values for that key. It merges together these values to form a possibly smaller set of values. Typically just zero or one output value is produced per Reduce invocation. The intermediate values are supplied to the user’s reduce function via an iterator. This allows us to handle lists of values that are too large to ﬁt in memory.
I.e., the input keys and values are drawn from a different domain than the output keys and values. Furthermore, the intermediate keys and values are from the same domain as the output keys and values. Our C++ implementation passes strings to and from the user-deﬁned functions and leaves it to the user code to convert between strings and appropriate types.
2.3 More Examples
Here are a few simple examples of interesting programs that can be easily expressed as MapReduce computations. Distributed Grep: The map function emits a line if it matches a supplied pattern. The reduce function is an identity function that just copies the supplied intermediate data to the output. Count of URL Access Frequency: The map function processes logs of web page requests and outputs URL, 1 . The reduce function adds together all values for the same URL and emits a URL, total count pair. Reverse Web-Link Graph: The map function outputs target, source pairs for each link to a target URL found in a page named source. The reduce function concatenates the list of all source URLs associated with a given target URL and emits the pair: target, list(source) Term-Vector per Host: A term vector summarizes the most important words that occur in a document or a set of documents as a list of word, f requency pairs. The map function emits a hostname, term vector pair for each input document (where the hostname is extracted from the URL of the document). The reduce function is passed all per-document term vectors for a given host. It adds these term vectors together, throwing away infrequent terms, and then emits a ﬁnal hostname, term vector pair. 2
2.1 Example
Consider the problem of counting the number of occurrences of each word in a large collection of documents. The user would write code similar to the following pseudo-code:
map(String key, String value): // key: document name // value: document contents for each word w in value: EmitIntermediate(w, &quot;1&quot;); reduce(String key, Iterator values): // key: a word // values: a list of counts int result = 0; for each v in values: result += ParseInt(v); Emit(AsString(result));
The map function emits each word plus an associated count of occurrences (just ‘1’ in this simple example). The reduce function sums together all counts emitted for a particular word. In addition, the user writes code to ﬁll in a mapreduce speciﬁcation object with the names of the input and output ﬁles, and optional tuning parameters. The user then invokes the MapReduce function, passing it the speciﬁcation object. The user’s code is linked together with the MapReduce library (implemented in C++). Appendix A contains the full program text for this example. To appear in OSDI 2004
User Program
(1) fork (1) fork (1) fork
Master
(2) assign map
(2) assign reduce
worker split 0 split 1 split 2 split 3 split 4 worker Input files Map phase Intermediate files (on local disks) Reduce phase Output files
(3) read (5) remote read
worker worker
(6) write
output file 0 output file 1
worker
(4) local write
Figure 1: Execution overview Inverted Index: The map function parses each document, and emits a sequence of word, document ID pairs. The reduce function accepts all pairs for a given word, sorts the corresponding document IDs and emits a word, list(document ID) pair. The set of all output pairs forms a simple inverted index. It is easy to augment this computation to keep track of word positions. Distributed Sort: The map function extracts the key from each record, and emits a key, record pair. The reduce function emits all pairs unchanged. This computation depends on the partitioning facilities described in Section 4.1 and the ordering properties described in Section 4.2. large clusters of commodity PCs connected together with switched Ethernet [4]. In our environment: (1) Machines are typically dual-processor x86 processors running Linux, with 2-4 GB of memory per machine. (2) Commodity networking hardware is used – typically either 100 megabits/second or 1 gigabit/second at the machine level, but averaging considerably less in overall bisection bandwidth. (3) A cluster consists of hundreds or thousands of machines, and therefore machine failures are common. (4) Storage is provided by inexpensive IDE disks attached directly to individual machines. A distributed ﬁle system [8] developed in-house is used to manage the data stored on these disks. The ﬁle system uses replication to provide availability and reliability on top of unreliable hardware. (5) Users submit jobs to a scheduling system. Each job consists of a set of tasks, and is mapped by the scheduler to a set of available machines within a cluster.
3 Implementation
Many different implementations of the MapReduce interface are possible. The right choice depends on the environment. For example, one implementation may be suitable for a small shared-memory machine, another for a large NUMA multi-processor, and yet another for an even larger collection of networked machines. This section describes an implementation targeted to the computing environment in wide use at Google: To appear in OSDI 2004
3.1 Execution Overview
The Map invocations are distributed across multiple machines by automatically partitioning the input data 3
into a set of M splits. The input splits can be processed in parallel by different machines. Reduce invocations are distributed by partitioning the intermediate key space into R pieces using a partitioning function (e.g., hash(key) mod R). The number of partitions (R) and the partitioning function are speciﬁed by the user. Figure 1 shows the overall ﬂow of a MapReduce operation in our implementation. When the user program calls the MapReduce function, the following sequence of actions occurs (the numbered labels in Figure 1 correspond to the numbers in the list below): 1. The MapReduce library in the user program ﬁrst splits the input ﬁles into M pieces of typically 16 megabytes to 64 megabytes (MB) per piece (controllable by the user via an optional parameter). It then starts up many copies of the program on a cluster of machines. 2. One of the copies of the program is special – the master. The rest are workers that are assigned work by the master. There are M map tasks and R reduce tasks to assign. The master picks idle workers and assigns each one a map task or a reduce task. 3. A worker who is assigned a map task reads the contents of the corresponding input split. It parses key/value pairs out of the input data and passes each pair to the user-deﬁned Map function. The intermediate key/value pairs produced by the Map function are buffered in memory. 4. Periodically, the buffered pairs are written to local disk, partitioned into R regions by the partitioning function. The locations of these buffered pairs on the local disk are passed back to the master, who is responsible for forwarding these locations to the reduce workers. 5. When a reduce worker is notiﬁed by the master about these locations, it uses remote procedure calls to read the buffered data from the local disks of the map workers. When a reduce worker has read all intermediate data, it sorts it by the intermediate keys so that all occurrences of the same key are grouped together. The sorting is needed because typically many different keys map to the same reduce task. If the amount of intermediate data is too large to ﬁt in memory, an external sort is used. 6. The reduce worker iterates over the sorted intermediate data and for each unique intermediate key encountered, it passes the key and the corresponding set of intermediate values to the user’s Reduce function. The output of the Reduce function is appended to a ﬁnal output ﬁle for this reduce partition. To appear in OSDI 2004</p>
<ol>
<li>When all map tasks and reduce tasks have been completed, the master wakes up the user program. At this point, the MapReduce call in the user program returns back to the user code. After successful completion, the output of the mapreduce execution is available in the R output ﬁles (one per reduce task, with ﬁle names as speciﬁed by the user). Typically, users do not need to combine these R output ﬁles into one ﬁle – they often pass these ﬁles as input to another MapReduce call, or use them from another distributed application that is able to deal with input that is partitioned into multiple ﬁles.
3.2 Master Data Structures
The master keeps several data structures. For each map task and reduce task, it stores the state (idle, in-progress, or completed), and the identity of the worker machine (for non-idle tasks). The master is the conduit through which the location of intermediate ﬁle regions is propagated from map tasks to reduce tasks. Therefore, for each completed map task, the master stores the locations and sizes of the R intermediate ﬁle regions produced by the map task. Updates to this location and size information are received as map tasks are completed. The information is pushed incrementally to workers that have in-progress reduce tasks.
3.3 Fault Tolerance
Since the MapReduce library is designed to help process very large amounts of data using hundreds or thousands of machines, the library must tolerate machine failures gracefully. Worker Failure The master pings every worker periodically. If no response is received from a worker in a certain amount of time, the master marks the worker as failed. Any map tasks completed by the worker are reset back to their initial idle state, and therefore become eligible for scheduling on other workers. Similarly, any map task or reduce task in progress on a failed worker is also reset to idle and becomes eligible for rescheduling. Completed map tasks are re-executed on a failure because their output is stored on the local disk(s) of the failed machine and is therefore inaccessible. Completed reduce tasks do not need to be re-executed since their output is stored in a global ﬁle system. When a map task is executed ﬁrst by worker A and then later executed by worker B (because A failed), all 4
workers executing reduce tasks are notiﬁed of the reexecution. Any reduce task that has not already read the data from worker A will read the data from worker B. MapReduce is resilient to large-scale worker failures. For example, during one MapReduce operation, network maintenance on a running cluster was causing groups of 80 machines at a time to become unreachable for several minutes. The MapReduce master simply re-executed the work done by the unreachable worker machines, and continued to make forward progress, eventually completing the MapReduce operation. Master Failure It is easy to make the master write periodic checkpoints of the master data structures described above. If the master task dies, a new copy can be started from the last checkpointed state. However, given that there is only a single master, its failure is unlikely; therefore our current implementation aborts the MapReduce computation if the master fails. Clients can check for this condition and retry the MapReduce operation if they desire. Semantics in the Presence of Failures When the user-supplied map and reduce operators are deterministic functions of their input values, our distributed implementation produces the same output as would have been produced by a non-faulting sequential execution of the entire program. We rely on atomic commits of map and reduce task outputs to achieve this property. Each in-progress task writes its output to private temporary ﬁles. A reduce task produces one such ﬁle, and a map task produces R such ﬁles (one per reduce task). When a map task completes, the worker sends a message to the master and includes the names of the R temporary ﬁles in the message. If the master receives a completion message for an already completed map task, it ignores the message. Otherwise, it records the names of R ﬁles in a master data structure. When a reduce task completes, the reduce worker atomically renames its temporary output ﬁle to the ﬁnal output ﬁle. If the same reduce task is executed on multiple machines, multiple rename calls will be executed for the same ﬁnal output ﬁle. We rely on the atomic rename operation provided by the underlying ﬁle system to guarantee that the ﬁnal ﬁle system state contains just the data produced by one execution of the reduce task. The vast majority of our map and reduce operators are deterministic, and the fact that our semantics are equivalent to a sequential execution in this case makes it very To appear in OSDI 2004
easy for programmers to reason about their program’s behavior. When the map and/or reduce operators are nondeterministic, we provide weaker but still reasonable semantics. In the presence of non-deterministic operators, the output of a particular reduce task R1 is equivalent to the output for R1 produced by a sequential execution of the non-deterministic program. However, the output for a different reduce task R2 may correspond to the output for R2 produced by a different sequential execution of the non-deterministic program. Consider map task M and reduce tasks R1 and R2 . Let e(Ri ) be the execution of Ri that committed (there is exactly one such execution). The weaker semantics arise because e(R1 ) may have read the output produced by one execution of M and e(R2 ) may have read the output produced by a different execution of M .
3.4 Locality
Network bandwidth is a relatively scarce resource in our computing environment. We conserve network bandwidth by taking advantage of the fact that the input data (managed by GFS [8]) is stored on the local disks of the machines that make up our cluster. GFS divides each ﬁle into 64 MB blocks, and stores several copies of each block (typically 3 copies) on different machines. The MapReduce master takes the location information of the input ﬁles into account and attempts to schedule a map task on a machine that contains a replica of the corresponding input data. Failing that, it attempts to schedule a map task near a replica of that task’s input data (e.g., on a worker machine that is on the same network switch as the machine containing the data). When running large MapReduce operations on a signiﬁcant fraction of the workers in a cluster, most input data is read locally and consumes no network bandwidth.
3.5 Task Granularity
We subdivide the map phase into M pieces and the reduce phase into R pieces, as described above. Ideally, M and R should be much larger than the number of worker machines. Having each worker perform many different tasks improves dynamic load balancing, and also speeds up recovery when a worker fails: the many map tasks it has completed can be spread out across all the other worker machines. There are practical bounds on how large M and R can be in our implementation, since the master must make O(M + R) scheduling decisions and keeps O(M ∗ R) state in memory as described above. (The constant factors for memory usage are small however: the O(M ∗ R) piece of the state consists of approximately one byte of data per map task/reduce task pair.) 5
Furthermore, R is often constrained by users because the output of each reduce task ends up in a separate output ﬁle. In practice, we tend to choose M so that each individual task is roughly 16 MB to 64 MB of input data (so that the locality optimization described above is most effective), and we make R a small multiple of the number of worker machines we expect to use. We often perform MapReduce computations with M = 200, 000 and R = 5, 000, using 2,000 worker machines.
3.6 Backup Tasks
One of the common causes that lengthens the total time taken for a MapReduce operation is a “straggler”: a machine that takes an unusually long time to complete one of the last few map or reduce tasks in the computation. Stragglers can arise for a whole host of reasons. For example, a machine with a bad disk may experience frequent correctable errors that slow its read performance from 30 MB/s to 1 MB/s. The cluster scheduling system may have scheduled other tasks on the machine, causing it to execute the MapReduce code more slowly due to competition for CPU, memory, local disk, or network bandwidth. A recent problem we experienced was a bug in machine initialization code that caused processor caches to be disabled: computations on affected machines slowed down by over a factor of one hundred. We have a general mechanism to alleviate the problem of stragglers. When a MapReduce operation is close to completion, the master schedules backup executions of the remaining in-progress tasks. The task is marked as completed whenever either the primary or the backup execution completes. We have tuned this mechanism so that it typically increases the computational resources used by the operation by no more than a few percent. We have found that this signiﬁcantly reduces the time to complete large MapReduce operations. As an example, the sort program described in Section 5.3 takes 44% longer to complete when the backup task mechanism is disabled.
the intermediate key. A default partitioning function is provided that uses hashing (e.g. “hash(key) mod R”). This tends to result in fairly well-balanced partitions. In some cases, however, it is useful to partition data by some other function of the key. For example, sometimes the output keys are URLs, and we want all entries for a single host to end up in the same output ﬁle. To support situations like this, the user of the MapReduce library can provide a special partitioning function. For example, using “hash(Hostname(urlkey)) mod R” as the partitioning function causes all URLs from the same host to end up in the same output ﬁle.
4.2 Ordering Guarantees
We guarantee that within a given partition, the intermediate key/value pairs are processed in increasing key order. This ordering guarantee makes it easy to generate a sorted output ﬁle per partition, which is useful when the output ﬁle format needs to support efﬁcient random access lookups by key, or users of the output ﬁnd it convenient to have the data sorted.
4.3 Combiner Function
In some cases, there is signiﬁcant repetition in the intermediate keys produced by each map task, and the userspeciﬁed Reduce function is commutative and associative. A good example of this is the word counting example in Section 2.1. Since word frequencies tend to follow a Zipf distribution, each map task will produce hundreds or thousands of records of the form <the, 1>. All of these counts will be sent over the network to a single reduce task and then added together by the Reduce function to produce one number. We allow the user to specify an optional Combiner function that does partial merging of this data before it is sent over the network. The Combiner function is executed on each machine that performs a map task. Typically the same code is used to implement both the combiner and the reduce functions. The only difference between a reduce function and a combiner function is how the MapReduce library handles the output of the function. The output of a reduce function is written to the ﬁnal output ﬁle. The output of a combiner function is written to an intermediate ﬁle that will be sent to a reduce task. Partial combining signiﬁcantly speeds up certain classes of MapReduce operations. Appendix A contains an example that uses a combiner.
4 Reﬁnements
Although the basic functionality provided by simply writing Map and Reduce functions is sufﬁcient for most needs, we have found a few extensions useful. These are described in this section.
4.1 Partitioning Function
The users of MapReduce specify the number of reduce tasks/output ﬁles that they desire (R). Data gets partitioned across these tasks using a partitioning function on To appear in OSDI 2004
4.4 Input and Output Types
The MapReduce library provides support for reading input data in several different formats. For example, “text” 6
mode input treats each line as a key/value pair: the key is the offset in the ﬁle and the value is the contents of the line. Another common supported format stores a sequence of key/value pairs sorted by key. Each input type implementation knows how to split itself into meaningful ranges for processing as separate map tasks (e.g. text mode’s range splitting ensures that range splits occur only at line boundaries). Users can add support for a new input type by providing an implementation of a simple reader interface, though most users just use one of a small number of predeﬁned input types. A reader does not necessarily need to provide data read from a ﬁle. For example, it is easy to deﬁne a reader that reads records from a database, or from data structures mapped in memory. In a similar fashion, we support a set of output types for producing data in different formats and it is easy for user code to add support for new output types.
the signal handler sends a “last gasp” UDP packet that contains the sequence number to the MapReduce master. When the master has seen more than one failure on a particular record, it indicates that the record should be skipped when it issues the next re-execution of the corresponding Map or Reduce task.
4.7 Local Execution
Debugging problems in Map or Reduce functions can be tricky, since the actual computation happens in a distributed system, often on several thousand machines, with work assignment decisions made dynamically by the master. To help facilitate debugging, proﬁling, and small-scale testing, we have developed an alternative implementation of the MapReduce library that sequentially executes all of the work for a MapReduce operation on the local machine. Controls are provided to the user so that the computation can be limited to particular map tasks. Users invoke their program with a special ﬂag and can then easily use any debugging or testing tools they ﬁnd useful (e.g. gdb).
4.5 Side-effects
In some cases, users of MapReduce have found it convenient to produce auxiliary ﬁles as additional outputs from their map and/or reduce operators. We rely on the application writer to make such side-effects atomic and idempotent. Typically the application writes to a temporary ﬁle and atomically renames this ﬁle once it has been fully generated. We do not provide support for atomic two-phase commits of multiple output ﬁles produced by a single task. Therefore, tasks that produce multiple output ﬁles with cross-ﬁle consistency requirements should be deterministic. This restriction has never been an issue in practice.
4.8 Status Information
The master runs an internal HTTP server and exports a set of status pages for human consumption. The status pages show the progress of the computation, such as how many tasks have been completed, how many are in progress, bytes of input, bytes of intermediate data, bytes of output, processing rates, etc. The pages also contain links to the standard error and standard output ﬁles generated by each task. The user can use this data to predict how long the computation will take, and whether or not more resources should be added to the computation. These pages can also be used to ﬁgure out when the computation is much slower than expected. In addition, the top-level status page shows which workers have failed, and which map and reduce tasks they were processing when they failed. This information is useful when attempting to diagnose bugs in the user code.
4.6 Skipping Bad Records
Sometimes there are bugs in user code that cause the Map or Reduce functions to crash deterministically on certain records. Such bugs prevent a MapReduce operation from completing. The usual course of action is to ﬁx the bug, but sometimes this is not feasible; perhaps the bug is in a third-party library for which source code is unavailable. Also, sometimes it is acceptable to ignore a few records, for example when doing statistical analysis on a large data set. We provide an optional mode of execution where the MapReduce library detects which records cause deterministic crashes and skips these records in order to make forward progress. Each worker process installs a signal handler that catches segmentation violations and bus errors. Before invoking a user Map or Reduce operation, the MapReduce library stores the sequence number of the argument in a global variable. If the user code generates a signal, To appear in OSDI 2004
4.9 Counters
The MapReduce library provides a counter facility to count occurrences of various events. For example, user code may want to count total number of words processed or the number of German documents indexed, etc. To use this facility, user code creates a named counter object and then increments the counter appropriately in the Map and/or Reduce function. For example: 7
Input (MB/s)
Counter/<em> uppercase; uppercase = GetCounter(&quot;uppercase&quot;); map(String name, String contents): for each word w in contents: if (IsCapitalized(w)): uppercase-&gt;Increment(); EmitIntermediate(w, &quot;1&quot;);
30000 20000 10000 0 20 40 60 80 100
The counter values from individual worker machines are periodically propagated to the master (piggybacked on the ping response). The master aggregates the counter values from successful map and reduce tasks and returns them to the user code when the MapReduce operation is completed. The current counter values are also displayed on the master status page so that a human can watch the progress of the live computation. When aggregating counter values, the master eliminates the effects of duplicate executions of the same map or reduce task to avoid double counting. (Duplicate executions can arise from our use of backup tasks and from re-execution of tasks due to failures.) Some counter values are automatically maintained by the MapReduce library, such as the number of input key/value pairs processed and the number of output key/value pairs produced. Users have found the counter facility useful for sanity checking the behavior of MapReduce operations. For example, in some MapReduce operations, the user code may want to ensure that the number of output pairs produced exactly equals the number of input pairs processed, or that the fraction of German documents processed is within some tolerable fraction of the total number of documents processed.
Seconds
Figure 2: Data transfer rate over time disks, and a gigabit Ethernet link. The machines were arranged in a two-level tree-shaped switched network with approximately 100-200 Gbps of aggregate bandwidth available at the root. All of the machines were in the same hosting facility and therefore the round-trip time between any pair of machines was less than a millisecond. Out of the 4GB of memory, approximately 1-1.5GB was reserved by other tasks running on the cluster. The programs were executed on a weekend afternoon, when the CPUs, disks, and network were mostly idle.
5.2 Grep
The grep program scans through 1010 100-byte records, searching for a relatively rare three-character pattern (the pattern occurs in 92,337 records). The input is split into approximately 64MB pieces (M = 15000), and the entire output is placed in one ﬁle (R = 1). Figure 2 shows the progress of the computation over time. The Y-axis shows the rate at which the input data is scanned. The rate gradually picks up as more machines are assigned to this MapReduce computation, and peaks at over 30 GB/s when 1764 workers have been assigned. As the map tasks ﬁnish, the rate starts dropping and hits zero about 80 seconds into the computation. The entire computation takes approximately 150 seconds from start to ﬁnish. This includes about a minute of startup overhead. The overhead is due to the propagation of the program to all worker machines, and delays interacting with GFS to open the set of 1000 input ﬁles and to get the information needed for the locality optimization.
5 Performance
In this section we measure the performance of MapReduce on two computations running on a large cluster of machines. One computation searches through approximately one terabyte of data looking for a particular pattern. The other computation sorts approximately one terabyte of data. These two programs are representative of a large subset of the real programs written by users of MapReduce – one class of programs shufﬂes data from one representation to another, and another class extracts a small amount of interesting data from a large data set.
5.3 Sort
The sort program sorts 1010 100-byte records (approximately 1 terabyte of data). This program is modeled after the TeraSort benchmark [10]. The sorting program consists of less than 50 lines of user code. A three-line Map function extracts a 10-byte sorting key from a text line and emits the key and the 8
5.1 Cluster Conﬁguration
All of the programs were executed on a cluster that consisted of approximately 1800 machines. Each machine had two 2GHz Intel Xeon processors with HyperThreading enabled, 4GB of memory, two 160GB IDE To appear in OSDI 2004
20000
Input (MB/s)
Input (MB/s)
15000 10000 5000 0 500 1000
Input (MB/s)
Done
20000 15000 10000 5000 0 500 1000
20000
Done
Done
15000 10000 5000 0 500 1000
Shuffle (MB/s)
15000 10000 5000 0 500 1000
Shuffle (MB/s)
500 1000
20000
Shuffle (MB/s)
20000 15000 10000 5000 0
20000 15000 10000 5000 0 500 1000
Output (MB/s)
Output (MB/s)
15000 10000 5000 0 500 1000
15000 10000 5000 0 500 1000
Output (MB/s)
20000
20000
20000 15000 10000 5000 0 500 1000
Seconds
Seconds
Seconds
(a) Normal execution
(b) No backup tasks
(c) 200 tasks killed
Figure 3: Data transfer rates over time for different executions of the sort program original text line as the intermediate key/value pair. We used a built-in Identity function as the Reduce operator. This functions passes the intermediate key/value pair unchanged as the output key/value pair. The ﬁnal sorted output is written to a set of 2-way replicated GFS ﬁles (i.e., 2 terabytes are written as the output of the program). As before, the input data is split into 64MB pieces (M = 15000). We partition the sorted output into 4000 ﬁles (R = 4000). The partitioning function uses the initial bytes of the key to segregate it into one of R pieces. Our partitioning function for this benchmark has builtin knowledge of the distribution of keys. In a general sorting program, we would add a pre-pass MapReduce operation that would collect a sample of the keys and use the distribution of the sampled keys to compute splitpoints for the ﬁnal sorting pass. Figure 3 (a) shows the progress of a normal execution of the sort program. The top-left graph shows the rate at which input is read. The rate peaks at about 13 GB/s and dies off fairly quickly since all map tasks ﬁnish before 200 seconds have elapsed. Note that the input rate is less than for grep. This is because the sort map tasks spend about half their time and I/O bandwidth writing intermediate output to their local disks. The corresponding intermediate output for grep had negligible size. The middle-left graph shows the rate at which data is sent over the network from the map tasks to the reduce tasks. This shufﬂing starts as soon as the ﬁrst map task completes. The ﬁrst hump in the graph is for To appear in OSDI 2004 the ﬁrst batch of approximately 1700 reduce tasks (the entire MapReduce was assigned about 1700 machines, and each machine executes at most one reduce task at a time). Roughly 300 seconds into the computation, some of these ﬁrst batch of reduce tasks ﬁnish and we start shufﬂing data for the remaining reduce tasks. All of the shufﬂing is done about 600 seconds into the computation. The bottom-left graph shows the rate at which sorted data is written to the ﬁnal output ﬁles by the reduce tasks. There is a delay between the end of the ﬁrst shufﬂing period and the start of the writing period because the machines are busy sorting the intermediate data. The writes continue at a rate of about 2-4 GB/s for a while. All of the writes ﬁnish about 850 seconds into the computation. Including startup overhead, the entire computation takes 891 seconds. This is similar to the current best reported result of 1057 seconds for the TeraSort benchmark [18]. A few things to note: the input rate is higher than the shufﬂe rate and the output rate because of our locality optimization – most data is read from a local disk and bypasses our relatively bandwidth constrained network. The shufﬂe rate is higher than the output rate because the output phase writes two copies of the sorted data (we make two replicas of the output for reliability and availability reasons). We write two replicas because that is the mechanism for reliability and availability provided by our underlying ﬁle system. Network bandwidth requirements for writing data would be reduced if the underlying ﬁle system used erasure coding [14] rather than replication. 9
In Figure 3 (b), we show an execution of the sort program with backup tasks disabled. The execution ﬂow is similar to that shown in Figure 3 (a), except that there is a very long tail where hardly any write activity occurs. After 960 seconds, all except 5 of the reduce tasks are completed. However these last few stragglers don’t ﬁnish until 300 seconds later. The entire computation takes 1283 seconds, an increase of 44% in elapsed time.
Number of instances in source tree
5.4 Effect of Backup Tasks
1000 800 600 400 200 0 2003/03 2003/06 2003/09 2003/12 2004/03 2004/06 2004/09
5.5 Machine Failures
In Figure 3 (c), we show an execution of the sort program where we intentionally killed 200 out of 1746 worker processes several minutes into the computation. The underlying cluster scheduler immediately restarted new worker processes on these machines (since only the processes were killed, the machines were still functioning properly). The worker deaths show up as a negative input rate since some previously completed map work disappears (since the corresponding map workers were killed) and needs to be redone. The re-execution of this map work happens relatively quickly. The entire computation ﬁnishes in 933 seconds including startup overhead (just an increase of 5% over the normal execution time).
Figure 4: MapReduce instances over time
Number of jobs Average job completion time Machine days used Input data read Intermediate data produced Output data written Average worker machines per job Average worker deaths per job Average map tasks per job Average reduce tasks per job Unique map implementations Unique reduce implementations Unique map/reduce combinations 29,423 634 secs 79,186 days 3,288 TB 758 TB 193 TB 157 1.2 3,351 55 395 269 426
6 Experience
We wrote the ﬁrst version of the MapReduce library in February of 2003, and made signiﬁcant enhancements to it in August of 2003, including the locality optimization, dynamic load balancing of task execution across worker machines, etc. Since that time, we have been pleasantly surprised at how broadly applicable the MapReduce library has been for the kinds of problems we work on. It has been used across a wide range of domains within Google, including: • large-scale machine learning problems, • clustering problems for the Google News and Froogle products, • extraction of data used to produce reports of popular queries (e.g. Google Zeitgeist), • extraction of properties of web pages for new experiments and products (e.g. extraction of geographical locations from a large corpus of web pages for localized search), and • large-scale graph computations. To appear in OSDI 2004
Table 1: MapReduce jobs run in August 2004 Figure 4 shows the signiﬁcant growth in the number of separate MapReduce programs checked into our primary source code management system over time, from 0 in early 2003 to almost 900 separate instances as of late September 2004. MapReduce has been so successful because it makes it possible to write a simple program and run it efﬁciently on a thousand machines in the course of half an hour, greatly speeding up the development and prototyping cycle. Furthermore, it allows programmers who have no experience with distributed and/or parallel systems to exploit large amounts of resources easily. At the end of each job, the MapReduce library logs statistics about the computational resources used by the job. In Table 1, we show some statistics for a subset of MapReduce jobs run at Google in August 2004.
6.1 Large-Scale Indexing
One of our most signiﬁcant uses of MapReduce to date has been a complete rewrite of the production index10
ing system that produces the data structures used for the Google web search service. The indexing system takes as input a large set of documents that have been retrieved by our crawling system, stored as a set of GFS ﬁles. The raw contents for these documents are more than 20 terabytes of data. The indexing process runs as a sequence of ﬁve to ten MapReduce operations. Using MapReduce (instead of the ad-hoc distributed passes in the prior version of the indexing system) has provided several beneﬁts: • The indexing code is simpler, smaller, and easier to understand, because the code that deals with fault tolerance, distribution and parallelization is hidden within the MapReduce library. For example, the size of one phase of the computation dropped from approximately 3800 lines of C++ code to approximately 700 lines when expressed using MapReduce. • The performance of the MapReduce library is good enough that we can keep conceptually unrelated computations separate, instead of mixing them together to avoid extra passes over the data. This makes it easy to change the indexing process. For example, one change that took a few months to make in our old indexing system took only a few days to implement in the new system. • The indexing process has become much easier to operate, because most of the problems caused by machine failures, slow machines, and networking hiccups are dealt with automatically by the MapReduce library without operator intervention. Furthermore, it is easy to improve the performance of the indexing process by adding new machines to the indexing cluster.
7 Related Work
Many systems have provided restricted programming models and used the restrictions to parallelize the computation automatically. For example, an associative function can be computed over all preﬁxes of an N element array in log N time on N processors using parallel preﬁx computations [6, 9, 13]. MapReduce can be considered a simpliﬁcation and distillation of some of these models based on our experience with large real-world computations. More signiﬁcantly, we provide a fault-tolerant implementation that scales to thousands of processors. In contrast, most of the parallel processing systems have only been implemented on smaller scales and leave the details of handling machine failures to the programmer. Bulk Synchronous Programming [17] and some MPI primitives [11] provide higher-level abstractions that To appear in OSDI 2004
make it easier for programmers to write parallel programs. A key difference between these systems and MapReduce is that MapReduce exploits a restricted programming model to parallelize the user program automatically and to provide transparent fault-tolerance. Our locality optimization draws its inspiration from techniques such as active disks [12, 15], where computation is pushed into processing elements that are close to local disks, to reduce the amount of data sent across I/O subsystems or the network. We run on commodity processors to which a small number of disks are directly connected instead of running directly on disk controller processors, but the general approach is similar. Our backup task mechanism is similar to the eager scheduling mechanism employed in the Charlotte System [3]. One of the shortcomings of simple eager scheduling is that if a given task causes repeated failures, the entire computation fails to complete. We ﬁx some instances of this problem with our mechanism for skipping bad records. The MapReduce implementation relies on an in-house cluster management system that is responsible for distributing and running user tasks on a large collection of shared machines. Though not the focus of this paper, the cluster management system is similar in spirit to other systems such as Condor [16]. The sorting facility that is a part of the MapReduce library is similar in operation to NOW-Sort [1]. Source machines (map workers) partition the data to be sorted and send it to one of R reduce workers. Each reduce worker sorts its data locally (in memory if possible). Of course NOW-Sort does not have the user-deﬁnable Map and Reduce functions that make our library widely applicable. River [2] provides a programming model where processes communicate with each other by sending data over distributed queues. Like MapReduce, the River system tries to provide good average case performance even in the presence of non-uniformities introduced by heterogeneous hardware or system perturbations. River achieves this by careful scheduling of disk and network transfers to achieve balanced completion times. MapReduce has a different approach. By restricting the programming model, the MapReduce framework is able to partition the problem into a large number of ﬁnegrained tasks. These tasks are dynamically scheduled on available workers so that faster workers process more tasks. The restricted programming model also allows us to schedule redundant executions of tasks near the end of the job which greatly reduces completion time in the presence of non-uniformities (such as slow or stuck workers). BAD-FS [5] has a very different programming model from MapReduce, and unlike MapReduce, is targeted to 11
the execution of jobs across a wide-area network. However, there are two fundamental similarities. (1) Both systems use redundant execution to recover from data loss caused by failures. (2) Both use locality-aware scheduling to reduce the amount of data sent across congested network links. TACC [7] is a system designed to simplify construction of highly-available networked services. Like MapReduce, it relies on re-execution as a mechanism for implementing fault-tolerance.
8 Conclusions
The MapReduce programming model has been successfully used at Google for many different purposes. We attribute this success to several reasons. First, the model is easy to use, even for programmers without experience with parallel and distributed systems, since it hides the details of parallelization, fault-tolerance, locality optimization, and load balancing. Second, a large variety of problems are easily expressible as MapReduce computations. For example, MapReduce is used for the generation of data for Google’s production web search service, for sorting, for data mining, for machine learning, and many other systems. Third, we have developed an implementation of MapReduce that scales to large clusters of machines comprising thousands of machines. The implementation makes efﬁcient use of these machine resources and therefore is suitable for use on many of the large computational problems encountered at Google. We have learned several things from this work. First, restricting the programming model makes it easy to parallelize and distribute computations and to make such computations fault-tolerant. Second, network bandwidth is a scarce resource. A number of optimizations in our system are therefore targeted at reducing the amount of data sent across the network: the locality optimization allows us to read data from local disks, and writing a single copy of the intermediate data to local disk saves network bandwidth. Third, redundant execution can be used to reduce the impact of slow machines, and to handle machine failures and data loss.
David Kramer, Shun-Tak Leung, and Josh Redstone for their work in developing GFS. We would also like to thank Percy Liang and Olcan Sercinoglu for their work in developing the cluster management system used by MapReduce. Mike Burrows, Wilson Hsieh, Josh Levenberg, Sharon Perl, Rob Pike, and Debby Wallach provided helpful comments on earlier drafts of this paper. The anonymous OSDI reviewers, and our shepherd, Eric Brewer, provided many useful suggestions of areas where the paper could be improved. Finally, we thank all the users of MapReduce within Google’s engineering organization for providing helpful feedback, suggestions, and bug reports.
References
[1] Andrea C. Arpaci-Dusseau, Remzi H. Arpaci-Dusseau, David E. Culler, Joseph M. Hellerstein, and David A. Patterson. High-performance sorting on networks of workstations. In Proceedings of the 1997 ACM SIGMOD International Conference on Management of Data, Tucson, Arizona, May 1997. [2] Remzi H. Arpaci-Dusseau, Eric Anderson, Noah Treuhaft, David E. Culler, Joseph M. Hellerstein, David Patterson, and Kathy Yelick. Cluster I/O with River: Making the fast case common. In Proceedings of the Sixth Workshop on Input/Output in Parallel and Distributed Systems (IOPADS ’99), pages 10–22, Atlanta, Georgia, May 1999. [3] Arash Baratloo, Mehmet Karaul, Zvi Kedem, and Peter Wyckoff. Charlotte: Metacomputing on the web. In Proceedings of the 9th International Conference on Parallel and Distributed Computing Systems, 1996. [4] Luiz A. Barroso, Jeffrey Dean, and Urs H¨ lzle. Web o search for a planet: The Google cluster architecture. IEEE Micro, 23(2):22–28, April 2003. [5] John Bent, Douglas Thain, Andrea C.Arpaci-Dusseau, Remzi H. Arpaci-Dusseau, and Miron Livny. Explicit control in a batch-aware distributed ﬁle system. In Proceedings of the 1st USENIX Symposium on Networked Systems Design and Implementation NSDI, March 2004. [6] Guy E. Blelloch. Scans as primitive parallel operations. IEEE Transactions on Computers, C-38(11), November 1989. [7] Armando Fox, Steven D. Gribble, Yatin Chawathe, Eric A. Brewer, and Paul Gauthier. Cluster-based scalable network services. In Proceedings of the 16th ACM Symposium on Operating System Principles, pages 78– 91, Saint-Malo, France, 1997. [8] Sanjay Ghemawat, Howard Gobioff, and Shun-Tak Leung. The Google ﬁle system. In 19th Symposium on Operating Systems Principles, pages 29–43, Lake George, New York, 2003.
Acknowledgements
Josh Levenberg has been instrumental in revising and extending the user-level MapReduce API with a number of new features based on his experience with using MapReduce and other people’s suggestions for enhancements. MapReduce reads its input from and writes its output to the Google File System [8]. We would like to thank Mohit Aron, Howard Gobioff, Markus Gutschke, To appear in OSDI 2004
12
[9] S. Gorlatch. Systematic efﬁcient parallelization of scan and other list homomorphisms. In L. Bouge, P. Fraigniaud, A. Mignotte, and Y. Robert, editors, Euro-Par’96. Parallel Processing, Lecture Notes in Computer Science 1124, pages 401–408. Springer-Verlag, 1996. [10] Jim Gray. Sort benchmark home page. <a href="http://research.microsoft.com/barc/SortBenchmark/" target="_blank">http://research.microsoft.com/barc/SortBenchmark/</a>. [11] William Gropp, Ewing Lusk, and Anthony Skjellum. Using MPI: Portable Parallel Programming with the Message-Passing Interface. MIT Press, Cambridge, MA, 1999. [12] L. Huston, R. Sukthankar, R. Wickremesinghe, M. Satyanarayanan, G. R. Ganger, E. Riedel, and A. Ailamaki. Diamond: A storage architecture for early discard in interactive search. In Proceedings of the 2004 USENIX File and Storage Technologies FAST Conference, April 2004. [13] Richard E. Ladner and Michael J. Fischer. Parallel preﬁx computation. Journal of the ACM, 27(4):831–838, 1980. [14] Michael O. Rabin. Efﬁcient dispersal of information for security, load balancing and fault tolerance. Journal of the ACM, 36(2):335–348, 1989. [15] Erik Riedel, Christos Faloutsos, Garth A. Gibson, and David Nagle. Active disks for large-scale data processing. IEEE Computer, pages 68–74, June 2001. [16] Douglas Thain, Todd Tannenbaum, and Miron Livny. Distributed computing in practice: The Condor experience. Concurrency and Computation: Practice and Experience, 2004. [17] L. G. Valiant. A bridging model for parallel computation. Communications of the ACM, 33(8):103–111, 1997. [18] Jim Wyllie. Spsort: How to sort a terabyte quickly. <a href="http://alme1.almaden.ibm.com/cs/spsort.pdf" target="_blank">http://alme1.almaden.ibm.com/cs/spsort.pdf</a>.
if (start &lt; i) Emit(text.substr(start,i-start),&quot;1&quot;); } } }; REGISTER_MAPPER(WordCounter); // User’s reduce function class Adder : public Reducer { virtual void Reduce(ReduceInput/</em> input) { // Iterate over all entries with the // same key and add the values int64 value = 0; while (!input-&gt;done()) { value += StringToInt(input-&gt;value()); input-&gt;NextValue(); } // Emit sum for input-&gt;key() Emit(IntToString(value)); } }; REGISTER_REDUCER(Adder); int main(int argc, char/<em>/</em> argv) { ParseCommandLineFlags(argc, argv); MapReduceSpecification spec; // Store list of input files into &quot;spec&quot; for (int i = 1; i &lt; argc; i++) { MapReduceInput/<em> input = spec.add_input(); input-&gt;set_format(&quot;text&quot;); input-&gt;set_filepattern(argv[i]); input-&gt;set_mapper_class(&quot;WordCounter&quot;); } // Specify the output files: // /gfs/test/freq-00000-of-00100 // /gfs/test/freq-00001-of-00100 // ... MapReduceOutput/</em> out = spec.output(); out-&gt;set_filebase(&quot;/gfs/test/freq&quot;); out-&gt;set_num_tasks(100); out-&gt;set_format(&quot;text&quot;); out-&gt;set_reducer_class(&quot;Adder&quot;); // Optional: do partial sums within map // tasks to save network bandwidth out-&gt;set_combiner_class(&quot;Adder&quot;); // Tuning parameters: use at most 2000 // machines and 100 MB of memory per task spec.set_machines(2000); spec.set_map_megabytes(100); spec.set_reduce_megabytes(100); // Now run it MapReduceResult result; if (!MapReduce(spec, &amp;result)) abort(); // Done: ’result’ structure contains info // about counters, time taken, number of // machines used, etc. return 0; }
A
Word Frequency
This section contains a program that counts the number of occurrences of each unique word in a set of input ﬁles speciﬁed on the command line.
/#include &quot;mapreduce/mapreduce.h&quot; // User’s map function class WordCounter : public Mapper { public: virtual void Map(const MapInput&amp; input) { const string&amp; text = input.value(); const int n = text.size(); for (int i = 0; i &lt; n; ) { // Skip past leading whitespace while ((i &lt; n) &amp;&amp; isspace(text[i])) i++; // Find word end int start = i; while ((i &lt; n) &amp;&amp; !isspace(text[i])) i++;
To appear in OSDI 2004
13</li>
</ol>

      
    </div>
    
    
      
    
    
        <footer id="post-meta">
        <span class="categories">Posted in<span class="breadcrumb fa fa-folder"><li><a href="/categories/hadoop/">hadoop</a></li></span></span> | <span class="tags">Tagged <a href="/tags/hadoop/" class="label label-primary">hadoop</a></span> | <span class="time">recent updated:<time title="2014-03-07 09:54:36"datetime="2014-03-07 09:54:36"> mar. 7 2014</time></span> | <span class="comment-link">
<a href="http://itsolife.com/2014/02/02/2014-02-02-hadoop--mapreduce-osdi04/#comments" class="ds-thread-count comment-link" data-thread-key="2014-02-02-hadoop--mapreduce-osdi04" data-count-type="comments">暂无评论</a></span>
        </footer>
    
    <div class="clearfix"></div>
  </div>
</div>
</article>



  <article>
<div id="post" class="post well">
  <div class="post-content">
    <header class="well-sm">
      <i class="fa icon fa-5x pull-left"></i>
      <h1 class="title"><a href="/2014/02/02/2014-02-02-zhiya-IT外企--IT外企那点儿事1：外企也就那么回事/">IT外企那点儿事(1)：外企也就那么回事</a></h1>
      
        <span>Posted on<time datetime="2014-02-02T01:54:36.000Z"> <a href="/2014/02/02/2014-02-02-zhiya-IT外企--IT外企那点儿事1：外企也就那么回事/">feb. 2 2014</a></time></span>      
    </header>
    
    
    <div class="entry">
      
        <h1 id="it-1-">IT外企那点儿事(1)：外企也就那么回事</h1>
<p><a href=""></a></p>
<h1 id="-http-www-cnblogs-com-forfuture1978-"><a href="http://www.cnblogs.com/forfuture1978/" target="_blank">觉先</a></h1>
<p>  <a href="http://www.cnblogs.com/" target="_blank">博客园</a> :: <a href="http://www.cnblogs.com/forfuture1978/" target="_blank">首页</a> :: <a href="http://q.cnblogs.com/" target="_blank">博问</a> :: <a href="http://home.cnblogs.com/ing/" target="_blank">闪存</a> :: <a href="http://www.cnblogs.com/forfuture1978/admin/EditPosts.aspx?opt=1" target="_blank">新随笔</a> :: <a href="http://space.cnblogs.com/msg/send/%e8%a7%89%e5%85%88" target="_blank">联系</a> :: <a href="http://www.cnblogs.com/forfuture1978/rss" target="_blank">订阅</a> <a href="http://www.cnblogs.com/forfuture1978/rss" target="_blank"><img src="" alt="订阅"></a> :: <a href="http://www.cnblogs.com/forfuture1978/admin/EditPosts.aspx" target="_blank">管理</a> :: <img src="" alt="">   130 随笔 :: 0 文章 :: 544 评论 :: 0 引用
<a href="">&lt;</a>2010年4月<a href="">&gt;</a>日一二三四五六28293031123<a href="http://www.cnblogs.com/forfuture1978/archive/2010/04/04.html" target="_blank">4</a>56<a href="http://www.cnblogs.com/forfuture1978/archive/2010/04/07.html" target="_blank">7</a><a href="http://www.cnblogs.com/forfuture1978/archive/2010/04/08.html" target="_blank">8</a>9101112<a href="http://www.cnblogs.com/forfuture1978/archive/2010/04/13.html" target="_blank">13</a>14<a href="http://www.cnblogs.com/forfuture1978/archive/2010/04/15.html" target="_blank">15</a>161718192021<a href="http://www.cnblogs.com/forfuture1978/archive/2010/04/22.html" target="_blank">22</a>2324<a href="http://www.cnblogs.com/forfuture1978/archive/2010/04/25.html" target="_blank">25</a>26<a href="http://www.cnblogs.com/forfuture1978/archive/2010/04/27.html" target="_blank">27</a>28<a href="http://www.cnblogs.com/forfuture1978/archive/2010/04/29.html" target="_blank">29</a><a href="http://www.cnblogs.com/forfuture1978/archive/2010/04/30.html" target="_blank">30</a>12345678</p>
<h3 id="-">公告</h3>
<p>昵称：<a href="http://home.cnblogs.com/u/forfuture1978/" target="_blank">觉先</a>
园龄：<a href="http://home.cnblogs.com/u/forfuture1978/" title="入园时间：2009-12-10" target="_blank">3年7个月</a>
荣誉：<a href="http://www.cnblogs.com/expert/" target="_blank">推荐博客</a>
粉丝：<a href="http://home.cnblogs.com/u/forfuture1978/followers/" target="_blank">560</a>
关注：<a href="http://home.cnblogs.com/u/forfuture1978/followees/" target="_blank">3</a></p>
<p><a href="">+加关注</a></p>
<h3 id="-">搜索</h3>
<h3 id="-">常用链接</h3>
<ul>
<li><a href="http://www.cnblogs.com/forfuture1978/MyPosts.html" target="_blank">我的随笔</a></li>
<li><a href="http://www.cnblogs.com/forfuture1978/MyComments.html" target="_blank">我的评论</a></li>
<li><a href="http://www.cnblogs.com/forfuture1978/OtherPosts.html" title="我发表过评论的随笔" target="_blank">我的参与</a></li>
<li><a href="http://www.cnblogs.com/forfuture1978/RecentComments.html" target="_blank">最新评论</a></li>
<li><a href="http://www.cnblogs.com/forfuture1978/tag/" target="_blank">我的标签</a></li>
</ul>
<h3 id="-">随笔分类</h3>
<ul>
<li><a href="http://www.cnblogs.com/forfuture1978/category/300670.html" target="_blank">Hadoop原理与代码分析(7)</a></li>
<li><a href="http://www.cnblogs.com/forfuture1978/category/300669.html" target="_blank">IT外企那点儿事(12)</a></li>
<li><a href="http://www.cnblogs.com/forfuture1978/category/345798.html" target="_blank">Java(2)</a></li>
<li><a href="http://www.cnblogs.com/forfuture1978/category/345797.html" target="_blank">Linux(14)</a></li>
<li><a href="http://www.cnblogs.com/forfuture1978/category/300665.html" target="_blank">Lucene原理与代码分析(38)</a></li>
<li><a href="http://www.cnblogs.com/forfuture1978/category/300666.html" target="_blank">长尾理论(16)</a></li>
<li><a href="http://www.cnblogs.com/forfuture1978/category/345794.html" target="_blank">管理学(10)</a></li>
<li><a href="http://www.cnblogs.com/forfuture1978/category/345800.html" target="_blank">经济学(4)</a></li>
<li><a href="http://www.cnblogs.com/forfuture1978/category/345796.html" target="_blank">算法(1)</a></li>
<li><a href="http://www.cnblogs.com/forfuture1978/category/345795.html" target="_blank">闲话IT业(3)</a></li>
<li><a href="http://www.cnblogs.com/forfuture1978/category/300667.html" target="_blank">心理学与管理学效应(9)</a></li>
<li><a href="http://www.cnblogs.com/forfuture1978/category/300668.html" target="_blank">组织行为学(15)</a></li>
</ul>
<h3 id="-">随笔档案</h3>
<ul>
<li><a href="http://www.cnblogs.com/forfuture1978/archive/2012/11.html" target="_blank">2012年11月 (3)</a></li>
<li><a href="http://www.cnblogs.com/forfuture1978/archive/2012/01.html" target="_blank">2012年1月 (5)</a></li>
<li><a href="http://www.cnblogs.com/forfuture1978/archive/2011/12.html" target="_blank">2011年12月 (6)</a></li>
<li><a href="http://www.cnblogs.com/forfuture1978/archive/2011/10.html" target="_blank">2011年10月 (3)</a></li>
<li><a href="http://www.cnblogs.com/forfuture1978/archive/2011/09.html" target="_blank">2011年9月 (1)</a></li>
<li><a href="http://www.cnblogs.com/forfuture1978/archive/2010/11.html" target="_blank">2010年11月 (8)</a></li>
<li><a href="http://www.cnblogs.com/forfuture1978/archive/2010/10.html" target="_blank">2010年10月 (1)</a></li>
<li><a href="http://www.cnblogs.com/forfuture1978/archive/2010/09.html" target="_blank">2010年9月 (1)</a></li>
<li><a href="http://www.cnblogs.com/forfuture1978/archive/2010/08.html" target="_blank">2010年8月 (1)</a></li>
<li><a href="http://www.cnblogs.com/forfuture1978/archive/2010/07.html" target="_blank">2010年7月 (1)</a></li>
<li><a href="http://www.cnblogs.com/forfuture1978/archive/2010/06.html" target="_blank">2010年6月 (6)</a></li>
<li><a href="http://www.cnblogs.com/forfuture1978/archive/2010/05.html" target="_blank">2010年5月 (22)</a></li>
<li><a href="http://www.cnblogs.com/forfuture1978/archive/2010/04.html" target="_blank">2010年4月 (18)</a></li>
<li><a href="http://www.cnblogs.com/forfuture1978/archive/2010/03.html" target="_blank">2010年3月 (8)</a></li>
<li><a href="http://www.cnblogs.com/forfuture1978/archive/2010/02.html" target="_blank">2010年2月 (39)</a></li>
<li><a href="http://www.cnblogs.com/forfuture1978/archive/2010/01.html" target="_blank">2010年1月 (1)</a></li>
<li><a href="http://www.cnblogs.com/forfuture1978/archive/2009/12.html" target="_blank">2009年12月 (6)</a></li>
</ul>
<h3 id="-">相册</h3>
<ul>
<li><a href="http://www.cnblogs.com/forfuture1978/gallery/247104.html" target="_blank">IT外企那点儿事</a></li>
</ul>
<h3 id="-">最新评论</h3>
<ul>
<li><a href="http://www.cnblogs.com/forfuture1978/archive/2012/11/26/2788610.html#2727561" target="_blank">1. Re:IT外企那点儿事(12)：也说跳槽</a></li>
<li>楼主怎么之后没有更新hadoop的相关信息了呢？是没有再研究了吗？</li>
<li>--lyeoswu</li>
<li><a href="http://www.cnblogs.com/forfuture1978/archive/2010/06/13/1757479.html#2713121" target="_blank">2. Re:Lucene 原理与代码分析完整版</a></li>
<li>提个建议，你生成的pdf中没有目录，影响阅读，用office转制的过程中其实设置一下即可，方便大众嘛~，还望能发我一份，谢谢！
sendreams@hotmail.com</li>
<li>--sendreams</li>
<li><a href="http://www.cnblogs.com/forfuture1978/archive/2012/11/26/2788610.html#2712415" target="_blank">3. Re:IT外企那点儿事(12)：也说跳槽</a></li>
<li><a href="&quot;查看所回复的评论&quot;">@</a>mojunbin
现在这公司，本来做的Siverlight，我进去后没多久就转JAVA了，最近在公司折腾JAVA的一些东西，业余时间玩玩游戏，看看CLR、并折腾linux。现在观点有所转变，觉得学技术更多的是为了扩宽思维、提高眼界</li>
<li>--峰顶飞龙</li>
<li><a href="http://www.cnblogs.com/forfuture1978/archive/2012/11/26/2788610.html#2711923" target="_blank">4. Re:IT外企那点儿事(12)：也说跳槽</a></li>
<li><a href="&quot;查看所回复的评论&quot;">@</a>峰顶飞龙
您的经历和我差不多，呵呵。不晓得现在兄弟在搞C/C++呢？</li>
<li>--mojunbin</li>
<li><a href="http://www.cnblogs.com/forfuture1978/archive/2010/11/23/1884967.html#2708814" target="_blank">5. Re:Hadoop学习总结之五：Hadoop的运行痕迹</a></li>
<li>楼主你好，在远程调试MapReduce时，本地代码进入不了自定义的job类，而是进入到Credentials class中，此类在hadoop-core-1.0.4.jar中，请问楼主在调试过程可否遇到此问题？</li>
<li>--彭莉珊</li>
</ul>
<h3 id="-">阅读排行榜</h3>
<ul>
<li><a href="http://www.cnblogs.com/forfuture1978/archive/2010/08/04/1791660.html" target="_blank">1. IT外企那点儿事(8)：又是一年加薪时(26799)</a></li>
<li><a href="http://www.cnblogs.com/forfuture1978/archive/2010/06/13/1757479.html" target="_blank">2. Lucene 原理与代码分析完整版(25616)</a></li>
<li><a href="http://www.cnblogs.com/forfuture1978/archive/2010/02/23/1671909.html" target="_blank">3. 从技术生命周期看IT历史(20878)</a></li>
<li><a href="http://www.cnblogs.com/forfuture1978/archive/2009/12/21/1628546.html" target="_blank">4. 101个著名的管理学及心理学效应(20828)</a></li>
<li><a href="http://www.cnblogs.com/forfuture1978/archive/2010/11/14/1877086.html" target="_blank">5. Hadoop学习总结之三：Map-Reduce入门(18681)</a></li>
</ul>
<h3 id="-">评论排行榜</h3>
<ul>
<li><a href="http://www.cnblogs.com/forfuture1978/archive/2010/06/13/1757479.html" target="_blank">1. Lucene 原理与代码分析完整版(68)</a></li>
<li><a href="http://www.cnblogs.com/forfuture1978/archive/2010/05/05/1727644.html" target="_blank">2. IT外企那点儿事(4)：激动人心的入职演讲(39)</a></li>
<li><a href="http://www.cnblogs.com/forfuture1978/archive/2010/05/13/1734162.html" target="_blank">3. IT外企那点儿事(6)：管理路线和技术路线(37)</a></li>
<li><a href="http://www.cnblogs.com/forfuture1978/archive/2010/08/04/1791660.html" target="_blank">4. IT外企那点儿事(8)：又是一年加薪时(35)</a></li>
<li><a href="http://www.cnblogs.com/forfuture1978/archive/2012/11/26/2788610.html" target="_blank">5. IT外企那点儿事(12)：也说跳槽(33)</a></li>
</ul>
<h3 id="-">推荐排行榜</h3>
<ul>
<li><a href="http://www.cnblogs.com/forfuture1978/archive/2010/06/13/1757479.html" target="_blank">1. Lucene 原理与代码分析完整版(55)</a></li>
<li><a href="http://www.cnblogs.com/forfuture1978/archive/2010/05/03/1726200.html" target="_blank">2. IT外企那点儿事(3)：奇怪的面试(39)</a></li>
<li><a href="http://www.cnblogs.com/forfuture1978/archive/2010/08/04/1791660.html" target="_blank">3. IT外企那点儿事(8)：又是一年加薪时(36)</a></li>
<li><a href="http://www.cnblogs.com/forfuture1978/archive/2012/11/26/2788610.html" target="_blank">4. IT外企那点儿事(12)：也说跳槽(34)</a></li>
<li><a href="http://www.cnblogs.com/forfuture1978/archive/2010/05/13/1734162.html" target="_blank">5. IT外企那点儿事(6)：管理路线和技术路线(27)</a></li>
</ul>
<p><a href="http://www.cnblogs.com/forfuture1978/archive/2010/04/30/1725341.html" target="_blank">IT外企那点儿事(1)：外企也就那么回事</a></p>
<p>外企，一个听起来似乎充满光环的名字，每年众多大学毕业生向往的地方。</p>
<p>说起外企，总能让人联想到很多令人心动的名词：高薪，人性化，浮动工作制，年假，完善的流程，各种福利如：旅游，室内乒乓球台，健身房，按摩椅，小食品，酸奶……</p>
<p>然而真正进入了外企，时间长了，也就发现，其实外企也就那么回事。</p>
<h2 id="-">高薪</h2>
<p>所谓高薪，严格意义上来讲是高起薪，也即刚毕业的时候每个企业公开的秘密，同学们总能够从师哥师姐那里打听到这个数字，有的企业甚至爆出较去年惊人的数字来做宣传。一个个光鲜的数字吸引着尚未毕业的大学生们，宣讲会的人数是基本和这个数字成正比的。</p>
<p>然而由于大多数的外企，由于规模比较大，机构也相对的稳定，高起薪的背后是稳定的加薪，每年7%~10%是常道，20%则是皇恩浩荡了，除非你能够取得整个Team都认可的成就，然而如果不幸参与的项目是一个多年的产品，至多是修改一些Bug或者增加一些边边角角的功能，又有多少这样的机会呢？大约在下看到的是这样的，也许并不符合所有外企的情形。</p>
<p>于是当毕业生中的佼佼者很幸运的加入大的外企的时候，不如你的同学只有默默的加入了不算太大的民企。</p>
<p>这一直是你引以为豪的资本，并总在同学聚会的时候大说特说你们公司的薪水，福利，在你的同学抱怨民企的加班声中附和着，心中却莫名的产生了一种优越感。</p>
<p>这种优越感使得你进一步沉浸在美好的外企生活中，却发现越来越没有那么优越了。三年，五年，你一次次的听说你的同学升职了，又升职了，而你还是一个普通的engineer，因为外企的升职基本是由严格的年限的，有时候多少有些按资排辈的味道。你一次一次听说你的同学加薪了，又加薪了，薪水直逼你当前的薪水，甚至在五年的关头超过你。</p>
<p>你越来越发现你的同学逐渐的掌握了一个系统前前后后的模块，能够完整的负责起一个项目的时候，你却还是螺丝钉，每天接受外国人的指示，在yes, ok, no problem, i am 100% agree的声音中继续做你的螺丝钉般的小功能。</p>
<p>我不知道十年后会如何，在参加了多次的开发者大会后，我发现几乎所有的外企的演讲者都是外国人，中国的演讲者则多来自本土的创业企业，当听着他们如数家珍的谈着自己的创业企业如何一步步做大，系统如何一步步改进，直到今天的架构，他们外企的同学能有这种机会吗？</p>
<h2 id="-">人性化</h2>
<p>所谓人性化，用外企的语言就是我们是很Open的。</p>
<p>Open体现在很多方面，诸如高管的办公室的门始终是开着的，你可以在任何时刻走到任何的高官的办公室里发表自己的看法，只是你必须保证，当你满怀激情的走进高官的办公室，关上门，半个小时后同样满怀激情走出办公室，你的顶头上司对你没有看法，即便你确实没有说什么，仅仅谈论了一下午餐而已。</p>
<p>所以除非高层主动安排和你谈话，尽量不要没事跑到高层那里，在你的顶头上司控制范围之外和他的上司进行私密的谈话，要知道有一种关系叫表面上支持，心中的隔阂。即便是高层主动要和你谈话，最好事先和你的顶头上司事先沟通，当然不用太正式，比如在闲聊的时间抱怨一下：&quot;今天下午又要被老大找去One on One，项目这么忙，不知道有啥事情可谈的&quot;，呵呵，一些术而已，姑妄言之姑听之吧。</p>
<p>对你最重要的永远是你的顶头上司，当高层听完你的建议，OK, I will take it into consideration之后，便和你没有啥关系了，绝不会存在当你的顶头上司决定给你涨薪7%的时候，高层会出来说一句，我觉得他表现还不错，涨10%吧。</p>
<p>当然，按照公司的规定，你的顶头上司也会过一段时间和你来一次One on One，问问当前的情况，问问有啥意见等等，这可不是推心置腹的时候，需要把握火候，对当前的情况说的太满意，感觉不真诚，太不满意自然领导不爱听，说没意见显得对Team不够关心，说太多意见会让人感觉你不安全。</p>
<p>所以总的原则是：</p>
<ul>
<li>要多提改善性意见(&quot;code review预留的时间应该更长一些&quot;)，少提颠覆性意见(&quot;现在的项目流程有很大问题&quot;)，</li>
<li>多提有证据的具体意见(&quot;我们有几十个Bug，可能一个星期确实做不完&quot;)，少提抽象型意见(&quot;Team之间的沟通有问题&quot;)，</li>
<li>多说与项目相关的意见，少说与自己相关的意见(尤其不要太真实的说自己的人生规划)，</li>
<li>多说在领导意料范围之内的意见(这样会给领导以对Team的控制感，比如说天天加班到10点，领导也看在眼中，可以提一下)，少说在领导意料之外的意见(即便有，请事先沟通，让领导在One on One之前就心里有数)。</li>
</ul>
<p>Open还体现来另外的方面，比如领导会和员工一起参加各种工作之外的活动，比如打球，比如年会表演，比如一起健身等等，而且在此过程中，往往是充满的欢声笑语的，但一定不要忘记领导就是领导，哪怕不在项目中，千万不要因为你曾经是学校的篮球高手，或是文艺主干，就能在此类的活动中充当领袖角色，在你的项目领导面前指手画脚，虽然在活动中他会夸你，没想到你还有这方面的才能，但是在领导面前充老大，这笔账是迟早要还的，比如在项目的后期不能够完成美国派来的任务的时候，你会被冠以虽然前一阵成功组织了活动，但是耽误了一些项目进度的罪名，从而影响你的绩效。</p>
<p>如果你在健身房遇到领导，和你一起健身，你们可以边健身边聊的很开心，但是领导的心中的第一个想法一定是，这小子项目干完了吗，还有空工作时间健身？，并且会在以后的工作中反映出来，比如时常关心你的工作进度，加大你的工作量等。</p>
<h2 id="-">浮动工作制</h2>
<p>所谓浮动工作制，很好听的名字，就是你早上可以推迟来，晚上可以早些走，只要能够完成任务，每天工作6个小时都可以。</p>
<p>初入外企的时候，看到很多前辈可以早上十点，甚至十一点才到公司，认为浮动工作制太好了，于是拼命的工作，企图在6小时干完10个小时的活，然后有时间或学习或休息。然而最后发现，活是永远干不完的，资本家花钱请了你，会让你轻松应对？</p>
<p>浮动工作制，其实就是加班不给加班费的另一种说法，也即合同中也许会写着&quot;所有的加班费已经被计入了薪水中&quot;。只要能够完成任务，每天工作12个小时也是应该的。晚上留下来很晚，或是早上很早被拉起来和老美开会，也是浮动的时间之中，你无话可说。为了改美国客户的一个Bug，深夜加班，你无话可说。在中国是休息日，但美国不是休息日的时候派去美国，并不补偿你的休息日，也不给三倍工资，你无话可说。</p>
<h2 id="-">年假</h2>
<p>外企的年假是相对较多的，也是外企在校园宣讲中经常引以为豪的一点。然而年假又有多少真正能够落到实处呢？其时大部分是休不到的，项目不允许，领导不允许，外国人也不允许。</p>
<p>不允许当然不是显式的，而是潜规则的。项目永远是紧的，即便不那么紧，也会被人们喊得使大家觉得很紧，如果一个Team有很多人休很多假，对领导来说，好像对上面不太好交代。</p>
<p>如果Team中你单独休假，你会被提醒，现在大家都在赶进度，不要因为你这个模块把项目block了。</p>
<p>如果Team中大家想一起休假，领导会说，大家都在这个时候休，连backup都没有，出了事情找不到人啊。</p>
<p>如果你平时想休息一天，领导会说，有什么事情吗？没什么事情可以等项目闲了些集中休息一下，明天早上可以晚来些，可能这一阵确实太累了。</p>
<p>如果你想连着长假一起休，领导会说，本来就有一个星期了，还另外请，不如平时累的时候休息一天，效果好。</p>
<p>如果美国人放假(如圣诞)，中国不放假，美国人会在放假前有很多任务布置过来，要在这个期间赶上美国的进度。</p>
<p>如果美国不放假，中国放假(如过年)，总不能让美国老板找不到人吧。</p>
<p>当然以上借口只是在你提出请假的时候，以商量的口气被提及，如果你真想请假，领导还是会毫不犹豫的批准的，因为我们是Open的嘛。然而以上借口却会使得多数员工不太敢于请假，因为大家都明白，有一种关系叫表面上支持，心中的隔阂。</p>
<p>当然即便假期被批准，还是有条件的，比如&quot;没问题，好好休息，走之前把文档(报告，邮件，代码)发出来(提交到svn)就行了&quot;。一般这个附加条件都会耗费一些时间的，一般是第二天休，前一天晚上至少九十点走，早上请，中午才能走，中午请，下午三点多才能走。</p>
<h2 id="-">完善的流程</h2>
<p>外企的流程是非常完善的，甚至是极度的完善，过分的完善。</p>
<p>所以外企一般都会有会议室预定系统，会议室永远是被占着的，一天一天的总是开会，讨论。</p>
<p>例会就有模块组的，开发组的(包含多个模块)，项目组的(开发和测试)，Group的(同一个大老板的多个项目)，all-hands的(整个公司)。</p>
<p>写一篇文档要模块组review，开发组review，测试组review，和美国开会review，重新改了第二轮review。以及code review，bug review。</p>
<p>每个项目组作了一个阶段后给整个项目组的demo，甚至给整个group及老外demo，说是增加visualbility。</p>
<p>一般要到下午晚些时候才能够清净些写代码，晚餐后才是代码的高峰期。</p>
<p>这也是为什么小公司半年作出来的东西，大公司要做几年。当然大公司这样做自然有它的道理，大公司稳定，不愁客户资源，不差钱，今年做出来或是明年做出来，客户别无选择，员工也养得起。这些小公司都做不到，必须尽快的满足客户的需要，必须在钱花完之前拉到下一个项目。</p>
<p>然而这对程序员的职业生涯来说好么，我不敢评价。只是在和很多朋友讨论的时候，他们发现，自己一直在忙啊忙，当跳槽试图总结自己做了啥的时候，却发现就不多的东西，不多的技术，当他们去面创业公司的时候，经常会被问，你们这么长时间，怎么就做了这么个东西？</p>
<p>大公司完善的流程还有一个特点，就是这个流程是完全为此公司定制的，当然公司大，自然可以有钱从头到尾弄自己的东西，既不用常用的，也不用开源的，无论是开发工具，测试工具，代码管理工具。这也导致了员工的粘性特别强，当走出这家公司，就像换了一片天地，原来会的别人用不到，别人常用的，却不怎么会，最后只好在公司养老，好在薪水也不错，福利也不错。</p>
<h2 id="-">设施</h2>
<p>最后提及的是各种美好的设施，这是很有吸引力的。然而为了您的前途，虽不能说敬而远之，也要注意享用的时间，如中午，晚上。</p>
<p>尽量不要在工作时间娱乐，甚至喧哗，人民的眼睛是雪亮的，领导的眼睛也是雪亮的，尤其是对于软件这种成果极难量化的产品，有时候表现和态度反而成了一种指标，不像销售一样，给公司带来的是真金白银，我无论怎么玩，能拿回单子就行，然而对于软件，你有绝对的证据证明成果超越别人吗？</p>
<p>所以外企有个很有意思的现象，一个团队的座位，离食品的距离越近越好，离娱乐设备的距离越远越好。离食品近，取用方便，领导看到你拿吃的也不会说什么，然而离娱乐设备近，领导办公室的门都开着，有谁胆敢长时间玩耍啊。所以娱乐设备上面玩耍的人一般都是座位离得比较远的。</p>
<p>此篇就写到这里的，在外企多年，其实发生了很多有趣的事情和现象，当走过几个外企的时候，发现有很多相似的潜规则。</p>
<p>进入中国的外企，其实是有中国特色的外企。中华文化的强大，使得所有的东西一到中国就会中国化，甚至改变了味道。很多民族如满族，回族的很多人都失去了原来民族的特色。也只有在中国，才可能存在儒释道三教合一的说法，不知道释迦摩尼有何感想。上学的时候，一个我很佩服的大物老师，年纪很大，他是坚定的马克思主义者，但是他曾经说，上个星期我病的厉害，差点就去见马克思了。我笑道，马克思是唯物的，是不相信死后有鬼的，死后去见阎王是迷信，去见马克思就不是了？</p>
<p>等有空的时候，再接着给大家讲外企的故事。</p>
<p>分类: <a href="http://www.cnblogs.com/forfuture1978/category/300669.html" target="_blank">IT外企那点儿事</a></p>
<p>绿色通道： <a href="">好文要顶</a> <a href="">关注我</a> <a href="">收藏该文</a><a href="http://space.cnblogs.com/msg/send/%e8%a7%89%e5%85%88" target="_blank">与我联系</a> <a href="&quot;分享至新浪微博&quot;"><img src="" alt=""></a>
<a href="http://home.cnblogs.com/u/forfuture1978/" target="_blank"><img src="" alt=""></a></p>
<p><a href="http://home.cnblogs.com/u/forfuture1978/" target="_blank">觉先</a>
<a href="http://home.cnblogs.com/u/forfuture1978/followees" target="_blank">关注 - 3</a>
<a href="http://home.cnblogs.com/u/forfuture1978/followers" target="_blank">粉丝 - 560</a></p>
<p>荣誉：<a href="http://www.cnblogs.com/expert/" target="_blank">推荐博客</a>
<a href="">+加关注</a></p>
<p>18</p>
<p>0
(请您对文章做出评价)</p>
<p><a href="http://www.cnblogs.com/forfuture1978/archive/2010/04/29/1723417.html" target="_blank">«</a> 上一篇：<a href="http://www.cnblogs.com/forfuture1978/archive/2010/04/29/1723417.html" title="发布于2010-04-29 00:24" target="_blank">高级Linux程序设计第五章：进程间通信</a>
<a href="http://www.cnblogs.com/forfuture1978/archive/2010/05/01/1725761.html" target="_blank">»</a> 下一篇：<a href="http://www.cnblogs.com/forfuture1978/archive/2010/05/01/1725761.html" title="发布于2010-05-01 20:57" target="_blank">信息检索导论(译)：第一章 布尔检索(1)</a>
posted on 2010-04-30 21:30 <a href="http://www.cnblogs.com/forfuture1978/" target="_blank">觉先</a> 阅读(8943) 评论(26) <a href="http://www.cnblogs.com/forfuture1978/admin/EditPosts.aspx?postid=1725341" target="_blank">编辑</a> <a href="">收藏</a></p>
<p><a href=""></a></p>
<h3 id="-">评论</h3>
<p><a href="">/#1楼</a><a href=""></a>  2010-04-30 21:54  <a href="http://www.cnblogs.com/lguyss/">土星的狗狗</a> <a href="http://space.cnblogs.com/msg/send/%e5%9c%9f%e6%98%9f%e7%9a%84%e7%8b%97%e7%8b%97" title="发送站内短消息" target="_blank"> </a></p>
<p>写的太好了，又学习了~哈哈，直接映射了我之前的日本公司。</p>
<p><a href="">支持(0)</a><a href="">反对(0)</a>
<a href="http://pic.cnitblog.com/face/u23929.jpg" target="_blank">http://pic.cnitblog.com/face/u23929.jpg</a></p>
<p><a href="">/#2楼</a><a href=""></a>  2010-04-30 23:57  <a href="http://www.cnblogs.com/wenjl520/">温景良(Jason)</a> <a href="http://space.cnblogs.com/msg/send/%e6%b8%a9%e6%99%af%e8%89%af(Jason" target="_blank"> </a> &quot;发送站内短消息&quot;)</p>
<p>没去过,期待中</p>
<p><a href="">支持(0)</a><a href="">反对(0)</a>
<a href="http://pic.cnitblog.com/face/u33118.jpg" target="_blank">http://pic.cnitblog.com/face/u33118.jpg</a></p>
<p><a href="">/#3楼</a><a href=""></a>  2010-05-01 10:41  <a href="http://www.cnblogs.com/ilovedotnet/">ilovedotnet</a> <a href="http://space.cnblogs.com/msg/send/ilovedotnet" title="发送站内短消息" target="_blank"> </a></p>
<p><a href="&quot;查看所回复的评论&quot;">@</a>土星的狗狗
我觉得日企和韩企不能算是真正的外企，大家通常说的外企只包括欧美企业。</p>
<p><a href="">支持(0)</a><a href="">反对(0)</a>
<a href="http://pic.cnitblog.com/face/u26921.jpg" target="_blank">http://pic.cnitblog.com/face/u26921.jpg</a></p>
<p><a href="">/#4楼</a><a href=""></a>  2010-05-01 21:59  <a href="http://www.cnblogs.com/skyyang/">DarroldYang</a> <a href="http://space.cnblogs.com/msg/send/DarroldYang" title="发送站内短消息" target="_blank"> </a></p>
<p>说的很像
加入这样的工作环境没多久
他们一个项目也是做了几年了
后面就一直在bug</p>
<p><a href="">支持(0)</a><a href="">反对(0)</a>
<a href="http://pic.cnitblog.com/face/u23975.png" target="_blank">http://pic.cnitblog.com/face/u23975.png</a></p>
<p><a href="">/#5楼</a><a href=""></a>  2010-05-04 09:26  <a href="http://www.cnblogs.com/peon/">加菲猫</a> <a href="http://space.cnblogs.com/msg/send/%e5%8a%a0%e8%8f%b2%e7%8c%ab" title="发送站内短消息" target="_blank"> </a></p>
<p>作者深得外企三味，不过假如你有心，在薪水上超过大部分民企的同学问题不大，华为腾讯的除外</p>
<p><a href="">支持(0)</a><a href="">反对(0)</a>
<a href="http://pic.cnitblog.com/face/u28333.jpg" target="_blank">http://pic.cnitblog.com/face/u28333.jpg</a></p>
<p><a href="">/#6楼</a><a href=""></a>  2010-05-05 09:28  <a href="http://www.cnblogs.com/jciwolf/">Jerry Qian</a> <a href="http://space.cnblogs.com/msg/send/Jerry+Qian" title="发送站内短消息" target="_blank"> </a></p>
<p>楼主打击人啦，我现在在学英语啊。</p>
<p><a href="">支持(0)</a><a href="">反对(0)</a></p>
<p><a href="">/#7楼</a><a href=""></a>  2010-05-05 11:43  <a href="http://www.cnblogs.com/bobliu/">Bob Liu</a> <a href="http://space.cnblogs.com/msg/send/Bob+Liu" title="发送站内短消息" target="_blank"> </a></p>
<p>看看，了解一下外企～</p>
<p><a href="">支持(0)</a><a href="">反对(0)</a>
<a href="http://pic.cnitblog.com/face/u126058.jpg" target="_blank">http://pic.cnitblog.com/face/u126058.jpg</a></p>
<p><a href="">/#8楼</a><a href=""></a>  2010-05-05 12:01  <a href="http://www.cnblogs.com/Aaron_Anubis/">Aaron_Aanubis</a> <a href="http://space.cnblogs.com/msg/send/Aaron_Aanubis" title="发送站内短消息" target="_blank"> </a></p>
<p>楼主，太好了···把这些说出来，叫我们更加了解外企，我们就更能根据自身来进行职业规划了！！！3q</p>
<p><a href="">支持(0)</a><a href="">反对(0)</a></p>
<p><a href="">/#9楼</a><a href=""></a>  2010-05-05 12:25  <a href="http://www.cnblogs.com/Jong/">Caspar Jiong</a> <a href="http://space.cnblogs.com/msg/send/Caspar+Jiong" title="发送站内短消息" target="_blank"> </a></p>
<p>深有同感！</p>
<p><a href="">支持(0)</a><a href="">反对(0)</a></p>
<p><a href="">/#10楼</a><a href=""></a>  2010-05-05 12:51  <a href="http://www.cnblogs.com/jerry_cong/">鸟瞰</a> <a href="http://space.cnblogs.com/msg/send/%e9%b8%9f%e7%9e%b0" title="发送站内短消息" target="_blank"> </a></p>
<p>对外企的经营模式，如果想老板的朋友们，某些地方还是比较值得借鉴的</p>
<p><a href="">支持(0)</a><a href="">反对(0)</a>
<a href="http://pic.cnitblog.com/face/u122874.jpg?id=16080143" target="_blank">http://pic.cnitblog.com/face/u122874.jpg?id=16080143</a></p>
<p><a href="">/#11楼</a><a href=""></a>  2010-05-05 15:13  <a href="http://www.cnblogs.com/facingwaller/">撞破南墙</a> <a href="http://space.cnblogs.com/msg/send/%e6%92%9e%e7%a0%b4%e5%8d%97%e5%a2%99" title="发送站内短消息" target="_blank"> </a></p>
<p>看了觉得果然很中国化，不知道GG也是这样的吗？</p>
<p><a href="">支持(0)</a><a href="">反对(0)</a>
<a href="http://pic.cnitblog.com/face/u69696.jpg?id=16133304" target="_blank">http://pic.cnitblog.com/face/u69696.jpg?id=16133304</a></p>
<p><a href="">/#12楼</a><a href=""></a>  2010-05-05 15:46  <a href="http://www.cnblogs.com/ArthasCui/">Arthas-Cui</a> <a href="http://space.cnblogs.com/msg/send/Arthas-Cui" title="发送站内短消息" target="_blank"> </a></p>
<p>你知道的太多了。。。</p>
<p><a href="">支持(0)</a><a href="">反对(0)</a>
<a href="http://pic.cnitblog.com/face/u37616.jpg?id=29112918" target="_blank">http://pic.cnitblog.com/face/u37616.jpg?id=29112918</a></p>
<p><a href="">/#13楼</a><a href=""></a>  2010-05-05 16:11  <a href="http://www.cnblogs.com/daxianren/">卡通一下</a> <a href="http://space.cnblogs.com/msg/send/%e5%8d%a1%e9%80%9a%e4%b8%80%e4%b8%8b" title="发送站内短消息" target="_blank"> </a></p>
<p>大家可能不太清楚，在美国顶头上司才是你真正的老板。他要开你走，董事会是不会举行表决的，哈哈！</p>
<p><a href="">支持(0)</a><a href="">反对(0)</a></p>
<p><a href="">/#14楼</a><a href=""></a>  2010-05-05 16:15  <a href="http://www.cnblogs.com/daxianren/">卡通一下</a> <a href="http://space.cnblogs.com/msg/send/%e5%8d%a1%e9%80%9a%e4%b8%80%e4%b8%8b" title="发送站内短消息" target="_blank"> </a></p>
<p>楼主文章的题目，<strong>“外企那点儿事”；“也就那么回事”</strong>，可以看出楼主当前的心态！
就好象世上有人说的，一不小心发财了！一不小心成功了......
呵呵！</p>
<p><a href="">支持(0)</a><a href="">反对(0)</a></p>
<p><a href="">/#15楼</a><a href=""></a>[楼主]  2010-05-05 18:36  <a href="http://www.cnblogs.com/forfuture1978/">觉先</a> <a href="http://space.cnblogs.com/msg/send/%e8%a7%89%e5%85%88" title="发送站内短消息" target="_blank"> </a></p>
<p><a href="&quot;查看所回复的评论&quot;">@</a>卡通一下
外企那点儿事是学了当前的流行语，明朝那点儿事，Java那点儿事...
及历史是什么玩意儿等，吸引人注意的一个噱头而已。</p>
<p><a href="">支持(0)</a><a href="">反对(0)</a>
<a href="http://pic.cnitblog.com/face/u103165.jpg" target="_blank">http://pic.cnitblog.com/face/u103165.jpg</a></p>
<p><a href="">/#16楼</a><a href=""></a>  2010-05-05 19:20  <a href="http://www.cnblogs.com/daxianren/">卡通一下</a> <a href="http://space.cnblogs.com/msg/send/%e5%8d%a1%e9%80%9a%e4%b8%80%e4%b8%8b" title="发送站内短消息" target="_blank"> </a></p>
<p><a href="&quot;查看引用原文&quot;">引用</a>觉先：
@卡通一下
外企那点儿事是学了当前的流行语，明朝那点儿事，Java那点儿事...
及历史是什么玩意儿等，吸引人注意的一个噱头而已。
朋友间聊天我们也经常地说，只是在正式场合是不说的，呵呵！</p>
<p><a href="">支持(0)</a><a href="">反对(0)</a></p>
<p><a href="">/#17楼</a><a href=""></a>  2010-05-09 21:17  <a href="http://www.cnblogs.com/dytes/">dytes</a> <a href="http://space.cnblogs.com/msg/send/dytes" title="发送站内短消息" target="_blank"> </a></p>
<p>不要一概而论，就我的经历而言，还是相当宽松的。-</p>
<p><a href="">支持(0)</a><a href="">反对(0)</a></p>
<p><a href="">/#18楼</a><a href=""></a>  2010-05-10 13:02  <a href="http://www.cnblogs.com/Koy/">Koy</a> <a href="http://space.cnblogs.com/msg/send/Koy" title="发送站内短消息" target="_blank"> </a></p>
<p>講得好好，頂一下。</p>
<p><a href="">支持(0)</a><a href="">反对(0)</a></p>
<p><a href="">/#19楼</a><a href=""></a>  2010-05-10 22:53  <a href="http://home.cnblogs.com/u/127908/">小飞哥</a> <a href="http://space.cnblogs.com/msg/send/%e5%b0%8f%e9%a3%9e%e5%93%a5" title="发送站内短消息" target="_blank"> </a></p>
<p>哈哈楼主你知道得太多了</p>
<p><a href="">支持(0)</a><a href="">反对(0)</a></p>
<p><a href="">/#20楼</a><a href=""></a>  2010-05-13 10:56  <a href="http://www.cnblogs.com/KissKnife/">SnowToday</a> <a href="http://space.cnblogs.com/msg/send/SnowToday" title="发送站内短消息" target="_blank"> </a></p>
<p>基本上是这个样子，不过外企跟外企也不太一样，部门跟部门不太一样，项目跟项目不太一样，比如请假放假，我们这请假绝大多数不会有问题，请假就请了，不会有那么多顾虑，还有老外的一些重要节日我们也会跟着放比如圣诞、复活节。</p>
<p><a href="">支持(0)</a><a href="">反对(0)</a>
<a href="http://pic.cnitblog.com/face/u10907.jpg" target="_blank">http://pic.cnitblog.com/face/u10907.jpg</a></p>
<p><a href="">/#21楼</a><a href=""></a>  2010-05-13 11:11  <a href="http://home.cnblogs.com/u/132808/">足球王子</a> <a href="http://space.cnblogs.com/msg/send/%e8%b6%b3%e7%90%83%e7%8e%8b%e5%ad%90" title="发送站内短消息" target="_blank"> </a></p>
<p>在外企参与项目的机会会少很多，但是没有进过外企，就好像没怎么见过世面一样。</p>
<p><a href="">支持(0)</a><a href="">反对(0)</a></p>
<p><a href="">/#22楼</a><a href=""></a>  2010-05-13 15:46  <a href="http://www.cnblogs.com/Abbott/">Abbott zhao</a> <a href="http://space.cnblogs.com/msg/send/Abbott+zhao" title="发送站内短消息" target="_blank"> </a></p>
<p>这也是为什么小公司半年作出来的东西，大公司要做几年。当然大公司这样做自然有它的道理，大公司稳定，不愁客户资源，不差钱，今年做出来或是明年做出来，客户别无选择，员工也养得起。这些小公司都做不到，必须尽快的满足客户的需要，必须在钱花完之前拉到下一个项目。
这句话有点偏。小公司做的产品真的不能恭维。</p>
<p><a href="">支持(1)</a><a href="">反对(0)</a></p>
<p><a href="">/#23楼</a><a href=""></a>  2010-05-13 21:28  <a href="http://www.cnblogs.com/417533880/">Forrest Liu</a> <a href="http://space.cnblogs.com/msg/send/Forrest+Liu" title="发送站内短消息" target="_blank"> </a></p>
<p>我也在一家小外企工作，最近遇到点事很郁闷。前两天女朋友来公司附近办事，我就带她在公司里待会，等着和我一起吃午饭。。结果她待了没有十分钟，公司老板从我身边过就看到她了，过了一会我的team leader就用communicater跟我说，让她出去，我就带她出去了。。回来后我的leader跟我说老板看到我女朋友了，很生气。因为我之前的公司很随便，以前也带朋友同学什么的进去过。当时也就觉得没什么。今天早上leader又跟我谈话，说客户那边反应对我的工作不满意。我当时很奇怪，因为我跟我的客户一直保持沟通，而且分配给我的task我也都完成的很不错。前一个月的时候我还特意问过客户对我的工作有什么意见，我在哪方面可以做的更好，结果客户给我回复说对我的工作很满意。我不知道是因为我得罪了谁或者我做错了什么。。现在很困惑，前辈给我指点一下吧~~</p>
<p><a href="">支持(0)</a><a href="">反对(0)</a>
<a href="http://pic.cnitblog.com/face/u39386.jpg" target="_blank">http://pic.cnitblog.com/face/u39386.jpg</a></p>
<p><a href="">/#24楼</a><a href=""></a>  2010-05-14 18:32  <a href="http://www.cnblogs.com/cjc1021/">开心每一天ㄨ</a> <a href="http://space.cnblogs.com/msg/send/%e5%bc%80%e5%bf%83%e6%af%8f%e4%b8%80%e5%a4%a9%e3%84%a8" title="发送站内短消息" target="_blank"> </a></p>
<p><a href="&quot;查看所回复的评论&quot;">@</a>Forrest Liu
感觉比较没有人情味就是。做事都是规规矩矩的。
如果你在民企或者是小企，哪怕是国企可能都不一定。外企业也是不一样，但你目前所处的就是挺没人情味的感觉~</p>
<p><a href="">支持(0)</a><a href="">反对(0)</a>
<a href="http://pic.cnitblog.com/face/u25669.jpg" target="_blank">http://pic.cnitblog.com/face/u25669.jpg</a></p>
<p><a href="">/#25楼</a><a href=""></a>  2010-05-14 23:16  <a href="http://www.cnblogs.com/qingteng1983/">无待</a> <a href="http://space.cnblogs.com/msg/send/%e6%97%a0%e5%be%85" title="发送站内短消息" target="_blank"> </a></p>
<p>有意思，受教了。</p>
<p><a href="">支持(0)</a><a href="">反对(0)</a>
<a href="http://pic.cnitblog.com/face/u131056.jpg?id=02141056" target="_blank">http://pic.cnitblog.com/face/u131056.jpg?id=02141056</a></p>
<p><a href="">/#26楼</a><a href=""></a>18306582010/5/22 21:08:21  2010-05-22 21:08  <a href="http://home.cnblogs.com/u/134973/">fyljf</a> <a href="http://space.cnblogs.com/msg/send/fyljf" title="发送站内短消息" target="_blank"> </a></p>
<p>有些点真是深有体会</p>
<p><a href="">支持(0)</a><a href="">反对(0)</a></p>
<p><a href="">刷新评论</a><a href="">刷新页面</a><a href="">返回顶部</a></p>
<p>注册用户登录后才能发表评论，请 <a href="">登录</a> 或 <a href="">注册</a>，<a href="http://www.cnblogs.com/" target="_blank">访问</a>网站首页。
<a href="http://www.cnblogs.com/" title="程序员的网上家园" target="_blank">博客园首页</a><a href="http://q.cnblogs.com/" title="程序员问答社区" target="_blank">博问</a><a href="http://news.cnblogs.com/" title="IT新闻" target="_blank">新闻</a><a href="http://home.cnblogs.com/ing/" target="_blank">闪存</a><a href="http://job.cnblogs.com/" target="_blank">程序员招聘</a><a href="http://kb.cnblogs.com/" target="_blank">知识库</a></p>
<p><strong>最新IT新闻</strong>:
· <a href="http://news.cnblogs.com/n/182486/" target="_blank">新硬硬整合时代</a>
· <a href="http://news.cnblogs.com/n/182485/" target="_blank">狗血的百度91并购案啊 阿里和周鸿祎都曾掺和</a>
· <a href="http://news.cnblogs.com/n/182483/" target="_blank">如何让搜索引擎抓取AJAX内容？</a>
· <a href="http://news.cnblogs.com/n/182482/" target="_blank">避免代码注释的五大理由</a>
· <a href="http://news.cnblogs.com/n/182481/" target="_blank">OpenWrt——适用于路由器的Linux系统</a>
» <a href="http://news.cnblogs.com/" title="IT新闻" target="_blank">更多新闻...</a></p>
<p><strong>最新知识库文章</strong>:
· <a href="http://kb.cnblogs.com/page/141892/" target="_blank">阿里巴巴集团去IOE运动的思考与总结</a>
· <a href="http://kb.cnblogs.com/page/182265/" target="_blank">硅谷归来7点分享：创业者，做你自己</a>
· <a href="http://kb.cnblogs.com/page/182200/" target="_blank">我为什么不能坚持？</a>
· <a href="http://kb.cnblogs.com/page/168725/" target="_blank">成为高效程序员的7个重要习惯</a>
· <a href="http://kb.cnblogs.com/page/182047/" target="_blank">谈谈对BPM的理解</a>
» <a href="http://kb.cnblogs.com/" target="_blank">更多知识库文章...</a>
Powered by:
<a href="http://www.cnblogs.com/" target="_blank">博客园</a>
Copyright © 觉先</p>

      
    </div>
    
    
      
    
    
        <footer id="post-meta">
        <span class="categories">Posted in<span class="breadcrumb fa fa-folder"><li><a href="/categories/zhiya/">zhiya</a></li></span><span class="breadcrumb"><li><a href="/categories/职涯/">职涯</a></li><li><a href="/categories/职涯/IT外企/">IT外企</a></li></span></span> | <span class="tags">Tagged <a href="/tags/IT外企/" class="label label-primary">IT外企</a><a href="/tags/zhiya/" class="label label-success">zhiya</a></span> | <span class="time">recent updated:<time title="2014-03-07 09:54:36"datetime="2014-03-07 09:54:36"> mar. 7 2014</time></span> | <span class="comment-link">
<a href="http://itsolife.com/2014/02/02/2014-02-02-zhiya-IT外企--IT外企那点儿事1：外企也就那么回事/#comments" class="ds-thread-count comment-link" data-thread-key="2014-02-02-zhiya-IT外企--IT外企那点儿事1：外企也就那么回事" data-count-type="comments">暂无评论</a></span>
        </footer>
    
    <div class="clearfix"></div>
  </div>
</div>
</article>





<ul id="pagination" class="pagination pagination-lg">
  <li><a class="extend prev" href="/page/106/">&laquo;</a></li><li><a class="page-number" href="/">1</a></li><li><a class="page-number" href="/page/2/">2</a></li><li><span class="space">&hellip;</span></li><li><a class="page-number" href="/page/104/">104</a></li><li><a class="page-number" href="/page/105/">105</a></li><li><a class="page-number" href="/page/106/">106</a></li><li class="active"><li><span class="page-number current">107</span></li><li><a class="page-number" href="/page/108/">108</a></li><li><a class="page-number" href="/page/109/">109</a></li><li><a class="page-number" href="/page/110/">110</a></li><li><span class="space">&hellip;</span></li></li><li><a class="page-number" href="/page/161/">161</a></li><li><a class="page-number" href="/page/162/">162</a></li><li><a class="extend next" href="/page/108/">&raquo;</a></li>
  <div class="clearfix"></div>
</ul></div><!--wapper-->
       </div><!-- ID main-col END -->
       <aside id="sidebar" class="alignright col-sx-6 col-sm-4 col-md-3 col-lg-3">
<div id="widget_search" class="widget panel panel-primary">
    <form action="//google.com/search" method="get" accept-charset="utf-8">
  <div class="input-group">
    <input class="form-control" id="searchbox" type="search" name="q" results="0" placeholder="search">
    <span class="input-group-btn">
      <button class="btn btn-default" type="submit">Go!</button>
    </span>
    <input type="hidden" name="q" value="site:itsolife.com">
  </div>
</form>
</div>

<div id="widget_category" class="widget panel panel-primary">
  <div class="panel-heading">category</div>  <div data-src='category' class='ajax_widgets'>正在加载...</div>
</div>

<div id="widget_recent_posts" class="widget panel panel-primary">
  <div class="panel-heading">recent posts</div>  <div data-src='recent_posts' class='ajax_widgets'>正在加载...</div></div>

<div id="widget_tagcloud" class="widget panel panel-primary">
  <div class="panel-heading">tagcloud</div>  <div data-src='tagcloud' class='ajax_widgets'>正在加载...</div></div>

<div id="widget_latest_update_posts" class="widget panel panel-primary">
  <div class="panel-heading">最近更新</div>  <div data-src='latest_update_posts' class='ajax_widgets'>正在加载...</div></div>

<div id="widget_recent_comments" class="widget panel panel-primary">
  <div class="panel-heading">recent comments</div>  

<div class="list-group-item ds-recent-comments" data-num-items="6" data-show-avatars="1" data-show-time="1" data-show-title="1" data-show-admin="1" data-excerpt-length="50"></div>



</div>

</aside>
       <div class="clearfix"></div>
     </div><!-- row END -->
  </div>
  <footer id="footer" class="container">
  <div class="panel panel-info">
  <section id='footer_widget'></section>  <div class="panel-footer">
  <div id="site-info">
    <span class='author'>
  
  &copy; 2014 RobinChia
  
    &nbsp;&nbsp;</span>
  
  <span id='analytics-51la'></span><span id='analytics-google'>
<script type="text/javascript">
  var _gaq = _gaq || [];
  _gaq.push(['_setAccount', 'UA-48559895-1']);
  _gaq.push(['_trackPageview']);
  _js2load.push({src:('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js'});
</script></span><span id='analytics-cnzz'>
<script type="text/javascript">var cnzz_protocol = (("https:" == document.location.protocol) ? " https://" : " http://");document.write(unescape("%3Cspan id='cnzz_stat_icon_5774006'%3E%3C/span%3E%3Cscript src='" + cnzz_protocol + "s17.cnzz.com/stat.php%3Fid%3D5774006%26show%3Dpic' type='text/javascript'%3E%3C/script%3E"));</script>
</span><span id='analytics-baidu'>
<script>
var _hmt = _hmt || [];
_js2load.push({src:"//hm.baidu.com/hm.js?1442f50724afc42380b51f097c43082c"});
</script>
</span>  </div>
  <div id="copyright">Site powered by <a href='http://zespia.tw/hexo/'><strong>hexo</strong></a>  update time: <em>2014-04-07 17:30:36</em></span></div>
</div>
<div class="clearfix"></div>


  </div>
  </footer>
  
        <script src="http://cdn.bootcss.com/jquery/1.10.2/jquery.min.js"></script>        
        <script src="http://cdn.staticfile.org/twitter-bootstrap/3.1.0/js/bootstrap.min.js"></script>        
                
        <script src="http://cdn.bootcss.com/prettify/r298/prettify.min.js"></script>    <script type="text/javascript">
   var lang=["bsh", "c", "cc", "cpp", "cs", "csh", "cyc", "cv", "htm", "html",
    "java", "js", "m", "mxml", "perl", "pl", "pm", "py", "rb", "sh",
    "xhtml", "xml", "xsl"];
   var pretty_base='';
   $('script').each(function(){
	var c = $(this).attr('src');
	if (!c)
	    return;
	if (c.match(/(\/)?prettify(\.min)?\.js/i))
	{
	    var index = c.lastIndexOf('/');
	    if (index != -1)
		pretty_base = c.substr(0,index + 1);
	    return false;
	}
   })
   $('pre code').each(function(){
	var c = $(this).attr('class')
	if (!c)
	    return;
	c = c.match(/\s?(lang\-\w+)/i);
	if (c && lang.indexOf(c[1]) == -1)
	{
	    lang.push(c[1]);
	    $.getScript(pretty_base + c[1] + '.min.js');
	}
   })

    $(window).load(function(){
       $("pre").addClass("prettyprint");
       prettyPrint();
    })
</script>    
            <script type="text/javascript">
var duoshuoQuery = {short_name:"robinchia"};
_js2load.push({src:'http://static.duoshuo.com/embed.js',charset:'UTF-8'});
</script>
    
            <!--wumii_relatedItems-->
    
        <script src="http://cdn.bootcss.com/fancybox/2.1.5/jquery.fancybox.min.js"></script>    <script type="text/javascript">
(function($){
  $('.entry').each(function(i){
    $(this).find('img').each(function(){
      var alt = this.alt;

      if (alt){
        $(this).before('<span class="caption">' + alt + '</span>');
      }

      $(this).wrap('<a href="' + this.src + '" title="' + alt + '" class="fancybox" rel="fancybox' + i + '" />');
    });
  });

  $('.fancybox').fancybox();
})(jQuery);
</script>    
        <script src="http://cdn.bootcss.com/mathjax/2.3/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>    <script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      processEscapes: true
    }
  });
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>
    
<script type="text/javascript">
$('.ajax_widgets').each(function(){var src=$(this).attr('data-src');if(src)$(this).load('/widgets/'+src+'.html');});
$.each(_js2load,function(index,obj){loadjs(obj.src,obj.charset)});
</script>

<div id="scroll2top">
<img src="/scroll2top/arrow.png"/>
</div>
<script src="/scroll2top/scroll2top.min.js"></script>
<div id="winterland">
  <canvas></canvas>
</div>
<script src="/js/winterland.min.js"></script>

  </body>
</html>
