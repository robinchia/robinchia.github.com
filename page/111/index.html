
<!DOCTYPE HTML>
<html>
<head>
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<title>The 111 Page | It so life</title>
<meta name="author" content="RobinChia">

<meta name="description" content="It so life">


<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">


<meta property="og:site_name" content="It so life"/>

<!--[if IE]><style>.testIE.IE{display:inline;}</style><![endif]-->
<!--[if lte IE 7]><link rel="stylesheet" href="/css/ie7.css" type="text/css"><![endif]-->
<!--[if (lt IE 9)&(gt IE 7)]><style>.testIE.IE8{display:inline;}</style><![endif]-->
<!--[if gt IE 8]><style>.testIE.IE9{display:inline;}</style><![endif]-->

<link href="/favicon.png" rel="icon">
<link rel="alternate" href="/atom.xml" title="It so life Feed" type="application/atom+xml">

<link rel="stylesheet" href="http://cdn.staticfile.org/twitter-bootstrap/3.1.0/css/bootstrap.min.css" type="text/css">
<link rel="stylesheet" href="http://cdn.bootcss.com/font-awesome/4.0.3/css/font-awesome.min.css" type="text/css">
<link rel="stylesheet" href="http://cdn.bootcss.com/prettify/r298/prettify.min.css" type="text/css">
<link rel="stylesheet" href="http://cdn.bootcss.com/fancybox/2.1.5/jquery.fancybox.min.css" type="text/css">
<link rel="stylesheet" href="/css/style.css" type="text/css">
<!--[if lt IE 9]>
   <style>article,aside,dialog,footer,header,section,footer,nav,figure,menu{display:block}</style>
   <script src="http://cdn.staticfile.org/html5shiv/r29/html5.min.js"></script>
   <script src="http://cdn.staticfile.org/respond.js/1.4.2/respond.min.js"></script>
<link href="http://cdn.staticfile.org/respond.js/1.4.2/respond-proxy.html" id="respond-proxy" rel="respond-proxy" />
<link href="/js/respond.proxy.gif" id="respond-redirect" rel="respond-redirect" />
<script src="/js/respond.proxy.js"></script>
   <script src="http://cdn.bootcss.com/selectivizr/1.0.2/selectivizr-min.js"></script>
<![endif]-->
<script type="text/javascript">
function loadjs(c,d){var a=document.createElement("script");a.async=!0;a.type="text/javascript";a.src=c;a.charset=d||"gbk";(document.getElementsByTagName("HEAD")[0]||document.getElementsByTagName("BODY")[0]).appendChild(a)};
var _protocol = (("https:" == document.location.protocol) ? " https://" : " http://");
var _js2load = [];
</script>

</head>
<body>
      <header id="header" class="container"><nav id="main-nav" class="navbar navbar-default navbar-fixed-top " role="navigation">
  <div class="container">
    <div class="navbar-header">
    <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".navbar-collapse">
      <span class="sr-only">Toggle navigation</span>
      <span class="icon-bar"></span>
      <span class="icon-bar"></span>
      <span class="icon-bar"></span>
    </button>
    <a class="navbar-brand" href="/">It so life</a>
    </div>
    <div  class="collapse navbar-collapse">
      <ul  class="nav navbar-nav">
  
        <li><a href="/" title="Home">Home</a></li>      
        <li><a href="/about/" title="About">About</a></li>      
        <li><a href="/archives/" title="Archives">Archives</a></li>      
      <li class='dropdown'>
        <a class='dropdown-toggle' data-toggle='dropdown' href='#'>website<b class='caret'></b></a>
        <ul class='dropdown-menu pure-menu-selected'>
    
          <li><a href="//groups.google.com/forum/#!forum/pongba" title="TopLanguage">TopLanguage</a></li>    
          <li><a href="//itpub.net/" title="ITPub">ITPub</a></li>    
          <li><a href="//blog.jobbole.com/" title="Bole">Bole</a></li>    
          <li><a href="//nosql-database.org/" title="nosql">nosql</a></li>    
          <li><a href="//gitimmersion.googol.im/" title="Git">Git</a></li>    
        </ul>
      </li>
    
      </ul>
      <ul class="nav navbar-nav navbar-right">
      
        <li><a href="/atom.xml">RSS</a></li>
      
      
        <li><a href="https://twitter.com/robinchia">twitter</a></li>
      
      
      
      
        <li><a href="https://github.com/robinchia">github</a></li>
      
      </ul>
    </div>
  </div>
</nav>
<div class="clearfix"></div>
</header>
  <div id='content' class="container">
     <div class="page-header-wrapper">
      <!--[if lt IE 9]><div class="alert alert-warning alert-dismissable"><button type="button" class="close" data-dismiss="alert" aria-hidden="true">&times;</button><strong>提示:</strong>您的浏览器版本太低了,建议升级到 <strong><a href="http://windows.microsoft.com/zh-cn/internet-explorer/download-ie" title="IE9">IE9</a></strong> 以上,本站使用<a href="https://www.google.com/intl/zh-CN/chrome/">Chrome浏览器</a>可以获得最好的显示效果.</div><![endif]-->
      <div class="page-header"><h1 align="center"><big>It so life</big> </h1>
        <h5 align="center"><big>love as life</big></h5>
      </div>
     </div>
     <div class="row">
       <div id="main-col" class="alignleft col-sx-12 col-sm-8 col-md-9 col-lg-9">
      <section id='header_widget'></section>
          <div id="wrapper"><article>
<div id="post" class="post well">
  <div class="post-content">
    <header class="well-sm">
      <i class="fa icon fa-5x pull-left"></i>
      <h1 class="title"><a href="/2014/02/02/2014-02-02-hadoop--CentOS的Hadoop集群配置/">CentOS的Hadoop集群配置</a></h1>
      
        <span>Posted on<time datetime="2014-02-02T01:54:36.000Z"> <a href="/2014/02/02/2014-02-02-hadoop--CentOS的Hadoop集群配置/">feb. 2 2014</a></time></span>      
    </header>
    
    
    <div class="entry">
      
        <h1 id="centos-hadoop-">CentOS的Hadoop集群配置</h1>
<h3 id="-centos-hadoop-http-blog-csdn-net-inte_sleeper-article-details-6569985-"><a href="http://blog.csdn.net/inte_sleeper/article/details/6569985" target="_blank">CentOS的Hadoop集群配置（一）</a></h3>
<h3 id="-"> </h3>
<p>参考资料：</p>
<p><a href="http://www.michael-noll.com/tutorials/running-hadoop-on-ubuntu-linux-single-node-cluster/" target="_blank"><a href="http://www.michael-noll.com/tutorials/running-hadoop-on-ubuntu-linux-single-node-cluster/">http://www.michael-noll.com/tutorials/running-hadoop-on-ubuntu-linux-single-node-cluster/</a></a></p>
<p><a href="http://www.michael-noll.com/tutorials/running-hadoop-on-ubuntu-linux-multi-node-cluster/" target="_blank"><a href="http://www.michael-noll.com/tutorials/running-hadoop-on-ubuntu-linux-multi-node-cluster/">http://www.michael-noll.com/tutorials/running-hadoop-on-ubuntu-linux-multi-node-cluster/</a></a></p>
<p><a href="http://www.michael-noll.com/tutorials/writing-an-hadoop-mapreduce-program-in-python/" target="_blank"><a href="http://www.michael-noll.com/tutorials/writing-an-hadoop-mapreduce-program-in-python/">http://www.michael-noll.com/tutorials/writing-an-hadoop-mapreduce-program-in-python/</a></a></p>
<p><a href="http://hadoop.apache.org/common/docs/current/cluster_setup.html" target="_blank"><a href="http://hadoop.apache.org/common/docs/current/cluster_setup.html">http://hadoop.apache.org/common/docs/current/cluster_setup.html</a></a></p>
<p>以下集群配置内容，以两台机器为例。其中一台是 master ，另一台是 slave1 。</p>
<p>master 上运行 name node, data node, task tracker, job tracker ， secondary name node ；</p>
<p>slave1 上运行 data node, task tracker 。</p>
<p>前面加 /* 表示对两台机器采取相同的操作</p>
<ol>
<li>安装 JDK /*</li>
</ol>
<p>yum install java-1.6.0-openjdk-devel</p>
<ol>
<li>设置环境变量 /*</li>
</ol>
<p>编辑 /etc/profile 文件，设置 JAVA_HOME 环境变量以及类路径：</p>
<p>export JAVA_HOME=&quot;/usr/lib/jvm/java-1.6.0-openjdk-1.6.0.0.x86_64&quot;</p>
<p>export PATH=$PATH:$JAVA_HOME/bin</p>
<p>export CLASSPATH=.:$JAVA_HOME/lib/tools.jar:$JAVA_HOME/lib/dt.jar</p>
<ol>
<li>添加 hosts 的映射 /*</li>
</ol>
<p>编辑 /etc/hosts 文件，<strong>注意</strong> <strong>host name **</strong>不要有下划线，见下步骤 9**</p>
<p>192.168.225.16 master</p>
<p>192.168.225.66 slave1</p>
<ol>
<li>配置 SSH /*</li>
</ol>
<p>cd /root &amp; mkdir .ssh</p>
<p>chmod 700 .ssh &amp; cd .ssh</p>
<p>创建密码为空的 RSA 密钥对：</p>
<p>ssh-keygen -t rsa -P &quot;&quot;</p>
<p>在提示的对称密钥名称中输入 id_rsa</p>
<p>将公钥添加至 authorized_keys 中：</p>
<p>cat id_rsa.pub &gt;&gt; authorized_keys</p>
<p>chmod 644 authorized_keys <strong>/#</strong> <strong>重要</strong></p>
<p>编辑 sshd 配置文件 /etc/ssh/sshd_config ，把 /#AuthorizedKeysFile  .ssh/authorized_keys 前面的注释取消掉。</p>
<p>重启 sshd 服务：</p>
<p>service sshd restart</p>
<p>测试 SSH 连接。连接时会提示是否连接，按回车后会将此公钥加入至 knows_hosts 中：</p>
<p>ssh localhost</p>
<ol>
<li>配置 master 和 slave1 的 ssh 互通</li>
</ol>
<p>在 slave1 中重复步骤 4 ，然后把 slave1 中的 .ssh/authorized_keys 复制至 master 的 .ssh/authorized_keys中。注意复制过去之后，要看最后的类似 root@localhost 的字符串，修改成 root@slave1 。同样将 master的 key 也复制至 slave1 ，并将最后的串修改成 root@master 。</p>
<p>或者使用如下命令：</p>
<p>ssh-copy-id -i ~/.ssh/id_rsa.pub root@slave1</p>
<p>测试 SSH 连接：</p>
<p>在 master 上运行：</p>
<p>ssh slave1</p>
<p>在 slave1 上运行：</p>
<p>ssh master</p>
<ol>
<li>安装 Hadoop</li>
</ol>
<p>下载 hadoop 安装包：</p>
<p>wget <a href="http://mirror.bjtu.edu.cn/apache/hadoop/common/hadoop-0.20.203.0/hadoop-0.20.203.0rc1.tar.gz" target="_blank"><a href="http://mirror.bjtu.edu.cn/apache/hadoop/common/hadoop-0.20.203.0/hadoop-0.20.203.0rc1.tar.gz">http://mirror.bjtu.edu.cn/apache/hadoop/common/hadoop-0.20.203.0/hadoop-0.20.203.0rc1.tar.gz</a></a></p>
<p>复制安装包至 slave1 ：</p>
<p>scp hadoop-0.20.203.0rc1.tar.gz root@slave1:/root/</p>
<p>解压：</p>
<p>tar xzvf hadoop-0.20.203.0rc1.tar.gz</p>
<p>mkdir /usr/local/hadoop</p>
<p>mv hadoop-0.20.203.0//* /usr/local/hadoop</p>
<pre><code>     修改 .bashrc 文件（位于用户目录下，即 ~/.bashrc ，对于 root ，即为 /root/.bashrc ）

     添加环境变量：

     export HADOOP_HOME=/usr/local/hadoop

export PATH=$PATH:$HADOOP_HOME/bin
</code></pre><ol>
<li>配置 Hadoop 环境变量 /*</li>
</ol>
<p><strong>以下所有 hadoop **</strong>目录下的文件，均以相对路径 hadoop <strong>**开始</strong></p>
<p>修改 hadoop/conf/hadoop-env.sh 文件，将里面的 JAVA_HOME 改成步骤 2 中设置的值。</p>
<ol>
<li>创建 Hadoop 本地临时文件夹 /*</li>
</ol>
<p>mkdir /root/hadoop_tmp （<strong>注意这一步，千万不要放在</strong> <strong>/tmp **</strong>目录下面！！因为 <strong><strong>/tmp </strong></strong>默认分配的空间是很小的，往 <strong><strong>hdfs </strong></strong>里放几个大文件就会导致空间满了，就会报错）**</p>
<p>修改权限：</p>
<p>chown -R hadoop:hadoop /root/hadoop_tmp</p>
<p>更松地，也可以这样：</p>
<p>chmod –R 777 /root/hadoop_tmp</p>
<ol>
<li>配置 Hadoop</li>
</ol>
<p>修改 master 的 hadoop/conf/core-site.xml ，在 <configuration> 节中添加如下内容：</p>
<p>注意： <strong>fs.default.name **</strong>的值不能带下划线**</p>
<property>

    <name>hadoop.tmp.dir</name>

    <value>/root/hadoop<em>tmp/hadoop</em>${user.name}</value>

</property>

<property>

    <name>fs.default.name</name>

    <value>hdfs://localhost:54310</value> 

</property>

<property>

    <name>io.sort.mb</name>

    <value>1024</value> 

</property>

<pre><code>     其中 io.sort.mb 值，指定了排序使用的内存，大的内存可以加快 job 的处理速度。



     修改 hadoop/conf/mapred-site.xml ，在 &lt;configuration&gt; 节中添加如下内容：
</code></pre><property>

    <name>mapred.job.tracker</name>

    <value>localhost:54311</value>

</property>

<property>

    <name>mapred.map.child.java.opts</name>

    <value>-Xmx4096m</value>

</property>

<property>

    <name>mapred.reduce.child.java.opts</name>

    <value>-Xmx4096m</value>

</property>

<pre><code>     其中 mapred.map.child.java.opts, mapred.reduce.child.java.opts 分别指定 map/reduce 任务使用的最大堆内存。较小的内存可能导致程序抛出 OutOfMemoryException 。
</code></pre><p>修改 conf/hdfs -site.xml ，在 <configuration> 节中添加如下内容：</p>
<property>

    <name>dfs.replication</name>

    <value>2</value>

</property>



<p>同样，修改 slave1 的 /usr/local/hadoop/conf/core-site.xml ，在 <configuration> 节中添加如下内容：</p>
<property>

    <name>hadoop.tmp.dir</name>

    <value>/root/hadoop<em>tmp/hadoop</em>${user.name}</value>

</property>

<property>

    <name>fs.default.name</name>

    <value>hdfs://localhost:54310</value> 

</property>

<property>

    <name>io.sort.mb</name>

    <value>1024</value> 

</property>



<pre><code>     修改 conf/mapred-site.xml ，在 &lt;configuration&gt; 节中添加如下内容：
</code></pre><property>

    <name>mapred.job.tracker</name>

    <value>localhost:54311</value>

</property>

<property>

    <name>mapred.map.child.java.opts</name>

    <value>-Xmx4096m</value>

</property>

<property>

    <name>mapred.reduce.child.java.opts</name>

    <value>-Xmx4096m</value>

</property>



<pre><code>     修改 conf/hdfs -site.xml ，在 &lt;configuration&gt; 节中添加如下内容：
</code></pre><property>

    <name>dfs.replication</name>

    <value>2</value>

    </property>



<ol>
<li>修改 hadoop/bin/hadoop 文件</li>
</ol>
<p>把 221 行修改成如下。因为对于 root 用户， -jvm 参数是有问题的，所以需要加一个判断 ( 或者以非 root 用户运行这个脚本也没问题 )</p>
<p>HADOOP_OPTS=&quot;$HADOOP_OPTS -jvm server $HADOOP_DATANODE_OPTS&quot;  à</p>
<pre><code>/#for root, -jvm option is invalid.

CUR_USER=`whoami`

if [ &quot;$CUR_USER&quot; = &quot;root&quot; ]; then

    HADOOP_OPTS=&quot;$HADOOP_OPTS -server $HADOOP_DATANODE_OPTS&quot;

else

    HADOOP_OPTS=&quot;$HADOOP_OPTS -jvm server $HADOOP_DATANODE_OPTS&quot;

fi 
</code></pre><p>unset $CUR_USER</p>
<p>至此， master 和 slave1 都已经完成了 single_node 的搭建，可以分别在两台机器上测试单节点。</p>
<p>启动节点：</p>
<p>hadoop/bin/start-all.sh</p>
<p>运行 jps 命令，应能看到类似如下的输出：</p>
<p>937 DataNode</p>
<p>9232 Jps</p>
<p>8811 NameNode</p>
<p>12033 JobTracker</p>
<p>12041 TaskTracker
来源： <a href="[http://blog.csdn.net/inte_sleeper/article/details/6569985](http://blog.csdn.net/inte_sleeper/article/details/6569985)">[http://blog.csdn.net/inte_sleeper/article/details/6569985](http://blog.csdn.net/inte_sleeper/article/details/6569985)</a> </p>
<p><a href="http://blog.csdn.net/inte_sleeper/article/details/6569990" target="_blank">CentOS的Hadoop集群配置（二）</a>
下面的教程把它们合并至 multi-node cluster 。</p>
<ol>
<li>合并 single-node 至 multi-node cluster</li>
</ol>
<p>修改 master 的 hadoop/conf/core-site.xml ：</p>
<property>

    <name>hadoop.tmp.dir</name>

    <value>/root/hadoop<em>tmp/hadoop</em>${user.name}</value>

</property>

<property>

    <name>fs.default.name</name>

    <value>hdfs://<strong>master</strong> :54310</value>

</property>

<property>

    <name>io.sort.mb</name>

    <value>1024</value> 

</property>



<pre><code>     修改 conf/mapred-site.xml ，在 &lt;configuration&gt; 节中添加如下内容：
</code></pre><property>

    <name>mapred.job.tracker</name>

    <value><strong>master</strong> :54311</value>

</property>

<property>

    <name>mapred.map.child.java.opts</name>

    <value>-Xmx4096m</value>

</property>

<property>

    <name>mapred.reduce.child.java.opts</name>

    <value>-Xmx4096m</value>

</property>

<pre><code>     修改 conf/hdfs -site.xml ，在 &lt;configuration&gt; 节中添加如下内容：
</code></pre><property>

    <name>dfs.replication</name>

    <value>2</value>

</property>

<p>把这三个文件复制至 slave1 相应的目录 hadoop/conf 中 <strong>( **</strong>即 master <strong><strong>和 slave1 </strong></strong>的内容完全一致 )**</p>
<pre><code>     修改所有节点的 hadoop/conf/masters ，把文件内容改成： master

     修改所有节点的 hadoop/conf/slaves ，把文件内容改成：
</code></pre><p>master</p>
<pre><code>slave1



     分别删除 master 和 slave1 的 dfs/data 文件：

     rm –rf /root/hadoop_tmp/hadoop_root/dfs/data
</code></pre><p>重新格式化 namenode ：</p>
<pre><code>     hadoop/bin/hadoop namenode -format



     测试，在 master 上运行：

     hadoop/bin/start-all.sh

     在 master 上运行 jps 命令
</code></pre><p>此时输出应类似于：</p>
<pre><code>     11648 TaskTracker
</code></pre><p>11166 NameNode</p>
<p>11433 SecondaryNameNode</p>
<p>12552 Jps</p>
<p>11282 DataNode</p>
<p>11525 JobTracker</p>
<p>在 slave1 上运行 jps</p>
<p>此时输出应包含 ( 即至少有 DataNode, 否则即为出错 ) ：</p>
<p>3950 Jps</p>
<p>3121 TaskTracker</p>
<p>3044 DataNode</p>
<ol>
<li>测试一个 JOB</li>
</ol>
<p>首先升级 python( 可选，如果 JOB 是 python 写的 ) ：</p>
<p>cd /etc/yum.repos.d/</p>
<p>wget <a href="http://mirrors.geekymedia.com/centos/geekymedia.repo" target="_blank">http://mirrors.geekymedia.com/centos/geekymedia.repo</a></p>
<p>yum makecache</p>
<p>yum -y install python26</p>
<p><strong>升级 python **</strong>的教程，见另外一篇文档。如果已经通过以上方法安装了 python2.6 <strong>**，那需要先卸载：</strong></p>
<p>yum remove python26 python26-devel</p>
<pre><code>     CentOS 的 yum 依赖于 python2.4 ，而 /usr/bin 中 python 程序即为 python2.4 。我们需要把它修改成python2.6 。



     cd /usr/bin/

     编辑 yum 文件，把第一行的
</code></pre><p>/#!/usr/bin/python   à   /#!/usr/bin/python2.4  </p>
<p>保存文件。</p>
<pre><code>     删除旧版本的 python 可执行文件（这个文件跟该目录下 python2.4 其实是一样的，所以可以直接删除）

     rm -f python

     让 python 指向 python2.6 的可执行程序。

     ln -s python26 python  
</code></pre><ol>
<li>Word count python 版本</li>
</ol>
<p><strong>Map.py</strong></p>
<p>/#! /usr/bin/python</p>
<p>import sys;</p>
<p>for line in sys.stdin:</p>
<p>  line =  line.strip();</p>
<p>  words = line.split();</p>
<p>  for word in words:</p>
<pre><code>  print &#39;%s/t%s&#39; % (word,1);
</code></pre><p><strong>Reduce.py</strong></p>
<p>/#!/usr/bin/python</p>
<p>import sys;</p>
<p>wc = {};</p>
<p>for line in sys.stdin:</p>
<p>  line = line.strip();</p>
<p>  word,count = line.split(&#39;/t&#39;,1);</p>
<p>  try:</p>
<pre><code>  count = int(count);
</code></pre><p>  except Error:</p>
<pre><code>  pass;
</code></pre><p>  if wc.has_key(word):</p>
<pre><code>  wc[word] += count;
</code></pre><p>  else: wc[word] = count;</p>
<p>for key in wc.keys():</p>
<p>  print &#39;%s/t%s&#39; % (key, wc[key]);</p>
<p>本机测试：</p>
<p>echo &quot;foo foo bar bar foo abc&quot; | map.py</p>
<p>echo &quot;foo foo bar bar foo abc&quot; | map.py | sort | reduce.py</p>
<p>在 hadoop 中测试：</p>
<p>hadoop jar /usr/local/hadoop/contrib/streaming/hadoop-streaming-0.20.203.0.jar -file mapper.py -mapper mapper.py -file reducer.py -reducer reducer.py -input wc//* -output wc-out</p>
<p>Job 成功后，会在 HDFS 中生成 wc-out 目录。</p>
<p>查看结果：</p>
<p>hadoop fs –ls wc-out</p>
<p>hadoop fs –cat wc-out/part-00000</p>
<ol>
<li>集群增加新节点</li>
</ol>
<p>a.       执行步骤 1 ， 2.</p>
<p>b.       修改 hosts 文件，将集群中的 hosts 加入本身 /etc/hosts 中。并修改集群中其他节点的 hosts ，将新节点加入。</p>
<p>c.       master 的 conf/slaves 文件中，添加新节点。</p>
<p>d.       启动 datanode 和 task tracker 。</p>
<p>hadoop-daemon.sh start datanode</p>
<p>hadoop-daemon.sh start tasktracker</p>
<ol>
<li>Trouble-shooting</li>
</ol>
<p>hadoop 的日志在 hadoop/logs 中。</p>
<p>其中， logs 根目录包含的是 namenode, datanode, jobtracker, tasktracker 等的日志。分别以 hadoop-{username}-namenode/datanode/jobtracker/tasktracker-hostname.log 命名。</p>
<p>userlogs 目录里包含了具体的 job 日志，每个 job 有一个单独的目录，以 job<em>YYYYmmddHHmm_xxxx 命名。里面包含数个 attempt</em>{jobname}<em>m_xxxxx 或 attempt</em>{jobname}_r_xxxx 等数个目录。其中目录名中的m 表示 map 任务的日志， r 表示 reduce 任务的日志。因此，出错时，可以有针对性地查看特定的日志。</p>
<p>常见错误：</p>
<ol>
<li>出现类似：</li>
</ol>
<p><em>ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: java.io.IOException: Incompatible namespaceIDs …</em></p>
<p>的异常，是因为先格式化了 namenode ，后来又修改了配置导致。将 dfs/data 文件夹内容删除，再重新格式化 namenode 即可。</p>
<ol>
<li>出现类似：</li>
</ol>
<p><em>INFO org.apache.hadoop.ipc.Client: Retrying connect to server:…</em></p>
<p>的异常，首先确认 name node 是否启动。如果已经启动，有可能是 master 或 slave1 中的配置出错，集群配置参考步骤 11 。也有可能是防火墙问题，需添加以下例外：</p>
<p>50010 端口用于数据传输， 50020 用于 RPC 调用， 50030 是 WEB 版的 JOB 状态监控， 54311 是job tracker ， 54310 是与 master 通信的端口。</p>
<p>完整的端口列表见：</p>
<p><a href="http://www.cloudera.com/blog/2009/08/hadoop-default-ports-quick-reference/" target="_blank"><a href="http://www.cloudera.com/blog/2009/08/hadoop-default-ports-quick-reference/">http://www.cloudera.com/blog/2009/08/hadoop-default-ports-quick-reference/</a></a></p>
<p>iptables -A RH-Firewall-1-INPUT -p tcp -m tcp --dport 50010 -j ACCEPT</p>
<p>iptables -A RH-Firewall-1-INPUT -p tcp -m tcp --dport 50020 -j ACCEPT</p>
<p>iptables -A RH-Firewall-1-INPUT -p tcp -m tcp --dport 50030 -j ACCEPT</p>
<p>iptables -A RH-Firewall-1-INPUT -p tcp -m tcp --dport 50060 -j ACCEPT</p>
<p>iptables -A RH-Firewall-1-INPUT -p tcp -m tcp --dport 54310 -j ACCEPT</p>
<p>iptables -A RH-Firewall-1-INPUT -p tcp -m tcp --dport 54311 -j ACCEPT</p>
<p>iptables -A OUTPUT -p tcp -m tcp --dport 50010 -j ACCEPT</p>
<p>iptables -A OUTPUT -p tcp -m tcp --dport 50020 -j ACCEPT</p>
<p>iptables -A OUTPUT -p tcp -m tcp --sport 50010 -j ACCEPT</p>
<p>iptables -A OUTPUT -p tcp -m tcp --sport 50020 -j ACCEPT</p>
<p>iptables -A OUTPUT -p tcp -m tcp --dport 50030 -j ACCEPT</p>
<p>iptables -A OUTPUT -p tcp -m tcp --sport 50030 -j ACCEPT</p>
<p>iptables -A OUTPUT -p tcp -m tcp --dport 50060 -j ACCEPT</p>
<p>iptables -A OUTPUT -p tcp -m tcp --sport 50060 -j ACCEPT</p>
<p>iptables -A OUTPUT -p tcp -m tcp --dport 54310 -j ACCEPT</p>
<p>iptables -A OUTPUT -p tcp -m tcp --sport 54310 -j ACCEPT</p>
<p>iptables -A OUTPUT -p tcp -m tcp --dport 54311 -j ACCEPT</p>
<p>iptables -A OUTPUT -p tcp -m tcp --sport 54311 -j ACCEPT</p>
<p>保存规则：</p>
<p>/etc/init.d/iptables save</p>
<p>重启 iptables 服务：</p>
<p>service iptables restart</p>
<p>如果还是出现问题 2 的错误，那可能需要手工修改 /etc/sysconfig/iptables 的规则。手动添加这些规则。若有 ”reject-with icmp-host-prohibited” 的规则，需将规则加到它的前面。注意修改配置文件的时候，不需要带 iptables 命令。直接为类似于：</p>
<p>-A OUTPUT -p tcp -m tcp --sport 54311 -j ACCEPT</p>
<p>或关闭防火墙 <strong>( **</strong>建议，因为端口太多，要加的例外很多 )**</p>
<p>service iptables stop</p>
<ol>
<li><p>在 /etc/hosts 文件中，确保一个 host 只对应一个 IP ，否则会出错（如同时将 slave1 指向 127.0.0.1 和192.168.225.66 ），<strong>可能导致数据无法从一个节点复制至另一节点。</strong></p>
</li>
<li><p>出现类似：</p>
</li>
</ol>
<p><em>FATAL org.apache.hadoop.mapred.TaskTracker: Error running child : java.lang.OutOfMemoryError: Java heap space…</em></p>
<p>的异常，是因为堆内存不够。有以下几个地方可以考虑配置：</p>
<p>a.       conf/hadoop-env.sh 中， export HADOOP_HEAPSIZE=1000 这一行，默认为注释掉，堆大小为1000M ，可以取消注释，将这个值调大一些（对于 16G 的内存，可以调至 8G ）。</p>
<p>b.       conf/mapred-site.xml 中，添加 mapred.map.child.java.opts 属性，手动指定 JAVA 堆的参数值为 -Xmx2048m 或更大。这个值调整 map 任务的堆大小。即：</p>
<property>

    <name>mapred.map.child.java.opts </name>

    <value>-Xmx2048m</value>

</property>

<p>c.       conf/mapred-site.xml 中，添加 mapred.reduce.child.java.opts 属性，手动指定 JAVA 堆的参数值为 -Xmx2048m 或更大。这个值调整 reduce 任务的堆大小。即：</p>
<property>

    <name>mapred.reduce.child.java.opts </name>

    <value>-Xmx2048m</value>

</property>

<pre><code>               注意调整这些值之后，要重启 name node 。
</code></pre><p><em>5.       </em>出现类似： <em>java.io.IOException: File /user/root/pv_product_110124 could only be replicated to 0 nodes, instead of 1…</em></p>
<p>的异常，首先确保 hadoop 临时文件夹中有足够的空间，空间不够会导致这个错误。</p>
<p>如果空间没问题，那就尝试把临时文件夹中 dfs/data 目录删除，然后重新格式化 name node ：</p>
<p>hadoop namenode -format</p>
<p>注意：此命令会删除 hdfs 上的文件</p>
<p><em>6.       </em>出现类似： <em>java.io.IOException: Broken pipe…</em></p>
<p>的异常，检查你的程序吧，没准输出了不该输出的信息，如调试信息等。
来源： <a href="[http://blog.csdn.net/inte_sleeper/article/details/6569990](http://blog.csdn.net/inte_sleeper/article/details/6569990)">[http://blog.csdn.net/inte_sleeper/article/details/6569990](http://blog.csdn.net/inte_sleeper/article/details/6569990)</a> </p>

      
    </div>
    
    
      
    
    
        <footer id="post-meta">
        <span class="categories">Posted in<span class="breadcrumb fa fa-folder"><li><a href="/categories/hadoop/">hadoop</a></li></span></span> | <span class="tags">Tagged <a href="/tags/hadoop/" class="label label-primary">hadoop</a></span> | <span class="time">recent updated:<time title="2014-03-07 09:54:36"datetime="2014-03-07 09:54:36"> mar. 7 2014</time></span> | <span class="comment-link">
<a href="http://itsolife.com/2014/02/02/2014-02-02-hadoop--CentOS的Hadoop集群配置/#comments" class="ds-thread-count comment-link" data-thread-key="2014-02-02-hadoop--CentOS的Hadoop集群配置" data-count-type="comments">暂无评论</a></span>
        </footer>
    
    <div class="clearfix"></div>
  </div>
</div>
</article>



  <article>
<div id="post" class="post well">
  <div class="post-content">
    <header class="well-sm">
      <i class="fa icon fa-5x pull-left"></i>
      <h1 class="title"><a href="/2014/02/02/2014-02-02-hadoop--hdfs_design/">hdfs_design</a></h1>
      
        <span>Posted on<time datetime="2014-02-02T01:54:36.000Z"> <a href="/2014/02/02/2014-02-02-hadoop--hdfs_design/">feb. 2 2014</a></time></span>      
    </header>
    
    
    <div class="entry">
      
        <h1 id="hdfs_design">hdfs_design</h1>
<p>HDFS Architecture
by Dhruba Borthakur
Table of contents
1 2
Introduction .......................................................................................................................3 Assumptions and Goals .....................................................................................................3
2.1 2.2 2.3 2.4 2.5 2.6
Hardware Failure .......................................................................................................... 3 Streaming Data Access .................................................................................................3 Large Data Sets .............................................................................................................3 Simple Coherency Model ............................................................................................. 4 “Moving Computation is Cheaper than Moving Data” ................................................4 Portability Across Heterogeneous Hardware and Software Platforms .........................4
3 4 5
NameNode and DataNodes ...............................................................................................4 The File System Namespace ............................................................................................. 5 Data Replication ................................................................................................................6
5.1 5.2 5.3
Replica Placement: The First Baby Steps .................................................................... 7 Replica Selection .......................................................................................................... 8 Safemode ...................................................................................................................... 8
6 7 8
The Persistence of File System Metadata ......................................................................... 8 The Communication Protocols ......................................................................................... 9 Robustness ........................................................................................................................ 9
8.1 8.2 8.3 8.4 8.5
Data Disk Failure, Heartbeats and Re-Replication .....................................................10 Cluster Rebalancing ....................................................................................................10 Data Integrity ..............................................................................................................10 Metadata Disk Failure ................................................................................................ 10 Snapshots ....................................................................................................................11
Copyright © 2008 The Apache Software Foundation. All rights reserved.
HDFS Architecture
9
Data Organization ........................................................................................................... 11
9.1 9.2 9.3
Data Blocks ................................................................................................................ 11 Staging ........................................................................................................................11 Replication Pipelining ................................................................................................ 12 FS Shell .....................................................................................................................12 DFSAdmin ................................................................................................................ 13 Browser Interface ......................................................................................................13 File Deletes and Undeletes ....................................................................................... 13 Decrease Replication Factor ..................................................................................... 14
10
Accessibility .................................................................................................................. 12
10.1 10.2 10.3 11
Space Reclamation ........................................................................................................ 13
11.1 11.2 12
References ..................................................................................................................... 14
Page 2
Copyright © 2008 The Apache Software Foundation. All rights reserved.
HDFS Architecture</p>
<ol>
<li>Introduction
The Hadoop Distributed File System (HDFS) is a distributed file system designed to run on commodity hardware. It has many similarities with existing distributed file systems. However, the differences from other distributed file systems are significant. HDFS is highly fault-tolerant and is designed to be deployed on low-cost hardware. HDFS provides high throughput access to application data and is suitable for applications that have large data sets. HDFS relaxes a few POSIX requirements to enable streaming access to file system data. HDFS was originally built as infrastructure for the Apache Nutch web search engine project. HDFS is part of the Apache Hadoop Core project. The project URL is <a href="http://hadoop.apache.org/core/" target="_blank">http://hadoop.apache.org/core/</a>.</li>
<li>Assumptions and Goals
2.1. Hardware Failure
Hardware failure is the norm rather than the exception. An HDFS instance may consist of hundreds or thousands of server machines, each storing part of the file system’s data. The fact that there are a huge number of components and that each component has a non-trivial probability of failure means that some component of HDFS is always non-functional. Therefore, detection of faults and quick, automatic recovery from them is a core architectural goal of HDFS.
2.2. Streaming Data Access
Applications that run on HDFS need streaming access to their data sets. They are not general purpose applications that typically run on general purpose file systems. HDFS is designed more for batch processing rather than interactive use by users. The emphasis is on high throughput of data access rather than low latency of data access. POSIX imposes many hard requirements that are not needed for applications that are targeted for HDFS. POSIX semantics in a few key areas has been traded to increase data throughput rates.
2.3. Large Data Sets
Applications that run on HDFS have large data sets. A typical file in HDFS is gigabytes to terabytes in size. Thus, HDFS is tuned to support large files. It should provide high aggregate data bandwidth and scale to hundreds of nodes in a single cluster. It should support tens of millions of files in a single instance.
Page 3
Copyright © 2008 The Apache Software Foundation. All rights reserved.
HDFS Architecture
2.4. Simple Coherency Model
HDFS applications need a write-once-read-many access model for files. A file once created, written, and closed need not be changed. This assumption simplifies data coherency issues and enables high throughput data access. A Map/Reduce application or a web crawler application fits perfectly with this model. There is a plan to support appending-writes to files in the future.
2.5. “Moving Computation is Cheaper than Moving Data”
A computation requested by an application is much more efficient if it is executed near the data it operates on. This is especially true when the size of the data set is huge. This minimizes network congestion and increases the overall throughput of the system. The assumption is that it is often better to migrate the computation closer to where the data is located rather than moving the data to where the application is running. HDFS provides interfaces for applications to move themselves closer to where the data is located.
2.6. Portability Across Heterogeneous Hardware and Software Platforms
HDFS has been designed to be easily portable from one platform to another. This facilitates widespread adoption of HDFS as a platform of choice for a large set of applications.</li>
<li>NameNode and DataNodes
HDFS has a master/slave architecture. An HDFS cluster consists of a single NameNode, a master server that manages the file system namespace and regulates access to files by clients. In addition, there are a number of DataNodes, usually one per node in the cluster, which manage storage attached to the nodes that they run on. HDFS exposes a file system namespace and allows user data to be stored in files. Internally, a file is split into one or more blocks and these blocks are stored in a set of DataNodes. The NameNode executes file system namespace operations like opening, closing, and renaming files and directories. It also determines the mapping of blocks to DataNodes. The DataNodes are responsible for serving read and write requests from the file system’s clients. The DataNodes also perform block creation, deletion, and replication upon instruction from the NameNode.
Page 4
Copyright © 2008 The Apache Software Foundation. All rights reserved.
<img src="" alt=""> HDFS Architecture
The NameNode and DataNode are pieces of software designed to run on commodity machines. These machines typically run a GNU/Linux operating system (OS). HDFS is built using the Java language; any machine that supports Java can run the NameNode or the DataNode software. Usage of the highly portable Java language means that HDFS can be deployed on a wide range of machines. A typical deployment has a dedicated machine that runs only the NameNode software. Each of the other machines in the cluster runs one instance of the DataNode software. The architecture does not preclude running multiple DataNodes on the same machine but in a real deployment that is rarely the case. The existence of a single NameNode in a cluster greatly simplifies the architecture of the system. The NameNode is the arbitrator and repository for all HDFS metadata. The system is designed in such a way that user data never flows through the NameNode.</li>
<li>The File System Namespace
HDFS supports a traditional hierarchical file organization. A user or an application can create directories and store files inside these directories. The file system namespace hierarchy is
Page 5
Copyright © 2008 The Apache Software Foundation. All rights reserved.
HDFS Architecture
similar to most other existing file systems; one can create and remove files, move a file from one directory to another, or rename a file. HDFS does not yet implement user quotas or access permissions. HDFS does not support hard links or soft links. However, the HDFS architecture does not preclude implementing these features. The NameNode maintains the file system namespace. Any change to the file system namespace or its properties is recorded by the NameNode. An application can specify the number of replicas of a file that should be maintained by HDFS. The number of copies of a file is called the replication factor of that file. This information is stored by the NameNode.</li>
<li>Data Replication
HDFS is designed to reliably store very large files across machines in a large cluster. It stores each file as a sequence of blocks; all blocks in a file except the last block are the same size. The blocks of a file are replicated for fault tolerance. The block size and replication factor are configurable per file. An application can specify the number of replicas of a file. The replication factor can be specified at file creation time and can be changed later. Files in HDFS are write-once and have strictly one writer at any time. The NameNode makes all decisions regarding replication of blocks. It periodically receives a Heartbeat and a Blockreport from each of the DataNodes in the cluster. Receipt of a Heartbeat implies that the DataNode is functioning properly. A Blockreport contains a list of all blocks on a DataNode.
Page 6
Copyright © 2008 The Apache Software Foundation. All rights reserved.
<img src="" alt=""> HDFS Architecture
5.1. Replica Placement: The First Baby Steps
The placement of replicas is critical to HDFS reliability and performance. Optimizing replica placement distinguishes HDFS from most other distributed file systems. This is a feature that needs lots of tuning and experience. The purpose of a rack-aware replica placement policy is to improve data reliability, availability, and network bandwidth utilization. The current implementation for the replica placement policy is a first effort in this direction. The short-term goals of implementing this policy are to validate it on production systems, learn more about its behavior, and build a foundation to test and research more sophisticated policies. Large HDFS instances run on a cluster of computers that commonly spread across many racks. Communication between two nodes in different racks has to go through switches. In most cases, network bandwidth between machines in the same rack is greater than network bandwidth between machines in different racks. The NameNode determines the rack id each DataNode belongs to via the process outlined in Rack Awareness. A simple but non-optimal policy is to place replicas on unique racks. This prevents losing data when an entire rack fails and allows use of bandwidth from multiple
Page 7
Copyright © 2008 The Apache Software Foundation. All rights reserved.
HDFS Architecture
racks when reading data. This policy evenly distributes replicas in the cluster which makes it easy to balance load on component failure. However, this policy increases the cost of writes because a write needs to transfer blocks to multiple racks. For the common case, when the replication factor is three, HDFS’s placement policy is to put one replica on one node in the local rack, another on a different node in the local rack, and the last on a different node in a different rack. This policy cuts the inter-rack write traffic which generally improves write performance. The chance of rack failure is far less than that of node failure; this policy does not impact data reliability and availability guarantees. However, it does reduce the aggregate network bandwidth used when reading data since a block is placed in only two unique racks rather than three. With this policy, the replicas of a file do not evenly distribute across the racks. One third of replicas are on one node, two thirds of replicas are on one rack, and the other third are evenly distributed across the remaining racks. This policy improves write performance without compromising data reliability or read performance. The current, default replica placement policy described here is a work in progress.
5.2. Replica Selection
To minimize global bandwidth consumption and read latency, HDFS tries to satisfy a read request from a replica that is closest to the reader. If there exists a replica on the same rack as the reader node, then that replica is preferred to satisfy the read request. If angg/ HDFS cluster spans multiple data centers, then a replica that is resident in the local data center is preferred over any remote replica.
5.3. Safemode
On startup, the NameNode enters a special state called Safemode. Replication of data blocks does not occur when the NameNode is in the Safemode state. The NameNode receives Heartbeat and Blockreport messages from the DataNodes. A Blockreport contains the list of data blocks that a DataNode is hosting. Each block has a specified minimum number of replicas. A block is considered safely replicated when the minimum number of replicas of that data block has checked in with the NameNode. After a configurable percentage of safely replicated data blocks checks in with the NameNode (plus an additional 30 seconds), the NameNode exits the Safemode state. It then determines the list of data blocks (if any) that still have fewer than the specified number of replicas. The NameNode then replicates these blocks to other DataNodes.</li>
<li>The Persistence of File System Metadata
Page 8
Copyright © 2008 The Apache Software Foundation. All rights reserved.
HDFS Architecture
The HDFS namespace is stored by the NameNode. The NameNode uses a transaction log called the EditLog to persistently record every change that occurs to file system metadata. For example, creating a new file in HDFS causes the NameNode to insert a record into the EditLog indicating this. Similarly, changing the replication factor of a file causes a new record to be inserted into the EditLog. The NameNode uses a file in its local host OS file system to store the EditLog. The entire file system namespace, including the mapping of blocks to files and file system properties, is stored in a file called the FsImage. The FsImage is stored as a file in the NameNode’s local file system too. The NameNode keeps an image of the entire file system namespace and file Blockmap in memory. This key metadata item is designed to be compact, such that a NameNode with 4 GB of RAM is plenty to support a huge number of files and directories. When the NameNode starts up, it reads the FsImage and EditLog from disk, applies all the transactions from the EditLog to the in-memory representation of the FsImage, and flushes out this new version into a new FsImage on disk. It can then truncate the old EditLog because its transactions have been applied to the persistent FsImage. This process is called a checkpoint. In the current implementation, a checkpoint only occurs when the NameNode starts up. Work is in progress to support periodic checkpointing in the near future. The DataNode stores HDFS data in files in its local file system. The DataNode has no knowledge about HDFS files. It stores each block of HDFS data in a separate file in its local file system. The DataNode does not create all files in the same directory. Instead, it uses a heuristic to determine the optimal number of files per directory and creates subdirectories appropriately. It is not optimal to create all local files in the same directory because the local file system might not be able to efficiently support a huge number of files in a single directory. When a DataNode starts up, it scans through its local file system, generates a list of all HDFS data blocks that correspond to each of these local files and sends this report to the NameNode: this is the Blockreport.</li>
<li>The Communication Protocols
All HDFS communication protocols are layered on top of the TCP/IP protocol. A client establishes a connection to a configurable TCP port on the NameNode machine. It talks the ClientProtocol with the NameNode. The DataNodes talk to the NameNode using the DataNode Protocol. A Remote Procedure Call (RPC) abstraction wraps both the Client Protocol and the DataNode Protocol. By design, the NameNode never initiates any RPCs. Instead, it only responds to RPC requests issued by DataNodes or clients.</li>
<li>Robustness
The primary objective of HDFS is to store data reliably even in the presence of failures. The
Page 9
Copyright © 2008 The Apache Software Foundation. All rights reserved.
HDFS Architecture
three common types of failures are NameNode failures, DataNode failures and network partitions.
8.1. Data Disk Failure, Heartbeats and Re-Replication
Each DataNode sends a Heartbeat message to the NameNode periodically. A network partition can cause a subset of DataNodes to lose connectivity with the NameNode. The NameNode detects this condition by the absence of a Heartbeat message. The NameNode marks DataNodes without recent Heartbeats as dead and does not forward any new IO requests to them. Any data that was registered to a dead DataNode is not available to HDFS any more. DataNode death may cause the replication factor of some blocks to fall below their specified value. The NameNode constantly tracks which blocks need to be replicated and initiates replication whenever necessary. The necessity for re-replication may arise due to many reasons: a DataNode may become unavailable, a replica may become corrupted, a hard disk on a DataNode may fail, or the replication factor of a file may be increased.
8.2. Cluster Rebalancing
The HDFS architecture is compatible with data rebalancing schemes. A scheme might automatically move data from one DataNode to another if the free space on a DataNode falls below a certain threshold. In the event of a sudden high demand for a particular file, a scheme might dynamically create additional replicas and rebalance other data in the cluster. These types of data rebalancing schemes are not yet implemented.
8.3. Data Integrity
It is possible that a block of data fetched from a DataNode arrives corrupted. This corruption can occur because of faults in a storage device, network faults, or buggy software. The HDFS client software implements checksum checking on the contents of HDFS files. When a client creates an HDFS file, it computes a checksum of each block of the file and stores these checksums in a separate hidden file in the same HDFS namespace. When a client retrieves file contents it verifies that the data it received from each DataNode matches the checksum stored in the associated checksum file. If not, then the client can opt to retrieve that block from another DataNode that has a replica of that block.
8.4. Metadata Disk Failure
The FsImage and the EditLog are central data structures of HDFS. A corruption of these files can cause the HDFS instance to be non-functional. For this reason, the NameNode can be configured to support maintaining multiple copies of the FsImage and EditLog. Any update
Page 10
Copyright © 2008 The Apache Software Foundation. All rights reserved.
HDFS Architecture
to either the FsImage or EditLog causes each of the FsImages and EditLogs to get updated synchronously. This synchronous updating of multiple copies of the FsImage and EditLog may degrade the rate of namespace transactions per second that a NameNode can support. However, this degradation is acceptable because even though HDFS applications are very data intensive in nature, they are not metadata intensive. When a NameNode restarts, it selects the latest consistent FsImage and EditLog to use. The NameNode machine is a single point of failure for an HDFS cluster. If the NameNode machine fails, manual intervention is necessary. Currently, automatic restart and failover of the NameNode software to another machine is not supported.
8.5. Snapshots
Snapshots support storing a copy of data at a particular instant of time. One usage of the snapshot feature may be to roll back a corrupted HDFS instance to a previously known good point in time. HDFS does not currently support snapshots but will in a future release.</li>
<li>Data Organization
9.1. Data Blocks
HDFS is designed to support very large files. Applications that are compatible with HDFS are those that deal with large data sets. These applications write their data only once but they read it one or more times and require these reads to be satisfied at streaming speeds. HDFS supports write-once-read-many semantics on files. A typical block size used by HDFS is 64 MB. Thus, an HDFS file is chopped up into 64 MB chunks, and if possible, each chunk will reside on a different DataNode.
9.2. Staging
A client request to create a file does not reach the NameNode immediately. In fact, initially the HDFS client caches the file data into a temporary local file. Application writes are transparently redirected to this temporary local file. When the local file accumulates data worth over one HDFS block size, the client contacts the NameNode. The NameNode inserts the file name into the file system hierarchy and allocates a data block for it. The NameNode responds to the client request with the identity of the DataNode and the destination data block. Then the client flushes the block of data from the local temporary file to the specified DataNode. When a file is closed, the remaining un-flushed data in the temporary local file is transferred to the DataNode. The client then tells the NameNode that the file is closed. At this point, the NameNode commits the file creation operation into a persistent store. If the
Page 11
Copyright © 2008 The Apache Software Foundation. All rights reserved.
HDFS Architecture
NameNode dies before the file is closed, the file is lost. The above approach has been adopted after careful consideration of target applications that run on HDFS. These applications need streaming writes to files. If a client writes to a remote file directly without any client side buffering, the network speed and the congestion in the network impacts throughput considerably. This approach is not without precedent. Earlier distributed file systems, e.g. AFS, have used client side caching to improve performance. A POSIX requirement has been relaxed to achieve higher performance of data uploads.
9.3. Replication Pipelining
When a client is writing data to an HDFS file, its data is first written to a local file as explained in the previous section. Suppose the HDFS file has a replication factor of three. When the local file accumulates a full block of user data, the client retrieves a list of DataNodes from the NameNode. This list contains the DataNodes that will host a replica of that block. The client then flushes the data block to the first DataNode. The first DataNode starts receiving the data in small portions (4 KB), writes each portion to its local repository and transfers that portion to the second DataNode in the list. The second DataNode, in turn starts receiving each portion of the data block, writes that portion to its repository and then flushes that portion to the third DataNode. Finally, the third DataNode writes the data to its local repository. Thus, a DataNode can be receiving data from the previous one in the pipeline and at the same time forwarding data to the next one in the pipeline. Thus, the data is pipelined from one DataNode to the next.</li>
<li>Accessibility
HDFS can be accessed from applications in many different ways. Natively, HDFS provides a FileSystem Java API for applications to use. A C language wrapper for this Java API is also available. In addition, an HTTP browser can also be used to browse the files of an HDFS instance. Work is in progress to expose HDFS through the WebDAV protocol.
10.1. FS Shell
HDFS allows user data to be organized in the form of files and directories. It provides a commandline interface called FS shell that lets a user interact with the data in HDFS. The syntax of this command set is similar to other shells (e.g. bash, csh) that users are already familiar with. Here are some sample action/command pairs:
Action Create a directory named /foodir Command bin/hadoop dfs -mkdir /foodir
Page 12
Copyright © 2008 The Apache Software Foundation. All rights reserved.
HDFS Architecture
Remove a directory named /foodir View the contents of a file named /foodir/myfile.txt
bin/hadoop dfs -rmr /foodir bin/hadoop dfs -cat /foodir/myfile.txt
FS shell is targeted for applications that need a scripting language to interact with the stored data.
10.2. DFSAdmin
The DFSAdmin command set is used for administering an HDFS cluster. These are commands that are used only by an HDFS administrator. Here are some sample action/command pairs:
Action Put the cluster in Safemode Generate a list of DataNodes Recommission or decommission DataNode(s) Command bin/hadoop dfsadmin -safemode enter bin/hadoop dfsadmin -report bin/hadoop dfsadmin -refreshNodes
10.3. Browser Interface
A typical HDFS install configures a web server to expose the HDFS namespace through a configurable TCP port. This allows a user to navigate the HDFS namespace and view the contents of its files using a web browser.</li>
<li>Space Reclamation
11.1. File Deletes and Undeletes
When a file is deleted by a user or an application, it is not immediately removed from HDFS. Instead, HDFS first renames it to a file in the /trash directory. The file can be restored quickly as long as it remains in /trash. A file remains in /trash for a configurable amount of time. After the expiry of its life in /trash, the NameNode deletes the file from the HDFS namespace. The deletion of a file causes the blocks associated with the file to be freed. Note that there could be an appreciable time delay between the time a file is deleted by a user and the time of the corresponding increase in free space in HDFS. A user can Undelete a file after deleting it as long as it remains in the /trash directory. If a user wants to undelete a file that he/she has deleted, he/she can navigate the /trash directory and retrieve the file. The /trash directory contains only the latest copy of the file
Page 13
Copyright © 2008 The Apache Software Foundation. All rights reserved.
HDFS Architecture
that was deleted. The /trash directory is just like any other directory with one special feature: HDFS applies specified policies to automatically delete files from this directory. The current default policy is to delete files from /trash that are more than 6 hours old. In the future, this policy will be configurable through a well defined interface.
11.2. Decrease Replication Factor
When the replication factor of a file is reduced, the NameNode selects excess replicas that can be deleted. The next Heartbeat transfers this information to the DataNode. The DataNode then removes the corresponding blocks and the corresponding free space appears in the cluster. Once again, there might be a time delay between the completion of the setReplication API call and the appearance of free space in the cluster.</li>
<li>References
Hadoop JavaDoc API. HDFS source code: <a href="http://hadoop.apache.org/core/version_control.html" target="_blank">http://hadoop.apache.org/core/version_control.html</a>
Page 14
Copyright © 2008 The Apache Software Foundation. All rights reserved.</li>
</ol>

      
    </div>
    
    
      
    
    
        <footer id="post-meta">
        <span class="categories">Posted in<span class="breadcrumb fa fa-folder"><li><a href="/categories/hadoop/">hadoop</a></li></span></span> | <span class="tags">Tagged <a href="/tags/hadoop/" class="label label-primary">hadoop</a></span> | <span class="time">recent updated:<time title="2014-03-07 09:54:36"datetime="2014-03-07 09:54:36"> mar. 7 2014</time></span> | <span class="comment-link">
<a href="http://itsolife.com/2014/02/02/2014-02-02-hadoop--hdfs_design/#comments" class="ds-thread-count comment-link" data-thread-key="2014-02-02-hadoop--hdfs_design" data-count-type="comments">暂无评论</a></span>
        </footer>
    
    <div class="clearfix"></div>
  </div>
</div>
</article>



  <article>
<div id="post" class="post well">
  <div class="post-content">
    <header class="well-sm">
      <i class="fa icon fa-5x pull-left"></i>
      <h1 class="title"><a href="/2014/02/02/2014-02-02-hadoop--HDFS写入和读取流程/">HDFS写入和读取流程</a></h1>
      
        <span>Posted on<time datetime="2014-02-02T01:54:36.000Z"> <a href="/2014/02/02/2014-02-02-hadoop--HDFS写入和读取流程/">feb. 2 2014</a></time></span>      
    </header>
    
    
    <div class="entry">
      
        <h1 id="hdfs-">HDFS写入和读取流程</h1>
<p>您还未登录！|<a href="https://passport.csdn.net/account/login" target="_blank">登录</a>|<a href="https://passport.csdn.net/account/register" target="_blank">注册</a>|<a href="https://passport.csdn.net/help/faq" target="_blank">帮助</a></p>
<ul>
<li><a href="http://www.csdn.net/" target="_blank">首页</a></li>
<li><a href="http://news.csdn.net/" target="_blank">业界</a></li>
<li><a href="http://mobile.csdn.net/" target="_blank">移动</a></li>
<li><a href="http://cloud.csdn.net/" target="_blank">云计算</a></li>
<li><a href="http://sd.csdn.net/" target="_blank">研发</a></li>
<li><a href="http://bbs.csdn.net/" target="_blank">论坛</a></li>
<li><a href="http://blog.csdn.net/" target="_blank">博客</a></li>
<li><a href="http://download.csdn.net/" target="_blank">下载</a></li>
<li><h2 id="-"><a href="">更多</a></h2>
</li>
</ul>
<h1 id="-guisu-http-blog-csdn-net-hguisu-"><a href="http://blog.csdn.net/hguisu" target="_blank">guisu，程序人生。</a></h1>
<h2 id="-a-clever-person-solves-a-problem-a-wise-person-avoids-it-">能干的人解决问题。智慧的人绕开问题(A clever person solves a problem. A wise person avoids it)</h2>
<ul>
<li><a href="http://blog.csdn.net/hguisu?viewmode=contents" target="_blank"><img src="" alt="">目录视图</a></li>
<li><a href="http://blog.csdn.net/hguisu?viewmode=list" target="_blank"><img src="" alt="">摘要视图</a></li>
<li><a href="http://blog.csdn.net/hguisu/rss/list" target="_blank"><img src="" alt="">订阅</a>
<a href="http://blog.csdn.net/blogdevteam/article/details/11889881" target="_blank">2014年1月微软MVP申请开始啦！</a>      <a href="http://bbs.csdn.net/topics/390594487" target="_blank">CSDN社区中秋晒福利活动正式开始啦！</a>        <a href="http://www.csdn.net/article/2013-09-17/2816962" target="_blank">专访钟声：Java程序员，上班那点事儿</a>      <a href="http://blog.csdn.net/adali/article/details/9813651" target="_blank">独一无二的职位：开源社区经理</a>      <a href="http://blog.csdn.net/blogdevteam/article/details/11975399" target="_blank">“说说家乡的互联网”主题有奖征文</a></li>
</ul>
<h3 id="-hdfs-"><a href="">HDFS写入和读取流程</a></h3>
<p>分类： <a href="http://blog.csdn.net/hguisu/article/category/1072794" target="_blank">云计算hadoop</a>  2012-02-14 23:50 8282人阅读 <a href="">评论</a>(17) <a href="&quot;收藏&quot;">收藏</a> <a href="&quot;举报&quot;">举报</a>
<a href="http://blog.csdn.net/tag/details.html?tag=%e5%ad%98%e5%82%a8" target="_blank">存储</a><a href="http://blog.csdn.net/tag/details.html?tag=hadoop" target="_blank">hadoop</a><a href="http://blog.csdn.net/tag/details.html?tag=image" target="_blank">image</a><a href="http://blog.csdn.net/tag/details.html?tag=system" target="_blank">system</a><a href="http://blog.csdn.net/tag/details.html?tag=mysql" target="_blank">mysql</a></p>
<p>目录<a href="&quot;系统根据文章中H1到H6标签自动生成文章目录&quot;">(?)</a><a href="&quot;展开&quot;">[+]</a></p>
<ol>
<li><a href="">一HDFS</a></li>
<li><a href="">二HDFS的体系结构</a></li>
<li><p><a href="">三读写流程</a></p>
</li>
<li><p><a href="">GFS论文提到的文件读取简单流程</a></p>
</li>
<li><a href=""></a></li>
<li><a href=""></a></li>
<li><a href=""></a></li>
<li><a href="">详细流程</a></li>
<li><a href=""></a></li>
<li><p><a href="">GFS论文提到的写入文件简单流程</a></p>
</li>
<li><p><a href="">详细流程</a></p>
</li>
<li><a href=""></a><h2 id="-hdfs"><a href=""></a>一、HDFS</h2>
</li>
</ol>
<p>HDFS全称是Hadoop Distributed System。HDFS是为以流的方式存取大文件而设计的。适用于几百MB，GB以及TB，并写一次读多次的场合。而对于低延时数据访问、大量小文件、同时写和任意的文件修改，则并不是十分适合。</p>
<p>目前HDFS支持的使用接口除了Java的还有，Thrift、C、FUSE、WebDAV、HTTP等。HDFS是以block-sized chunk组织其文件内容的，默认的block大小为64MB，对于不足64MB的文件，其会占用一个block，但实际上不用占用实际硬盘上的64MB，这可以说是HDFS是在文件系统之上架设的一个中间层。之所以将默认的block大小设置为64MB这么大，是因为block-sized对于文件定位很有帮助，同时大文件更使传输的时间远大于文件寻找的时间，这样可以最大化地减少文件定位的时间在整个文件获取总时间中的比例 。</p>
<h2 id="-hdfs-"><a href=""></a>二、HDFS的体系结构</h2>
<p>构成HDFS主要是Namenode（master）和一系列的Datanode（workers）。Namenode是管理HDFS的目录树和相关的文件元数据，这些信息是以&quot;namespace image&quot;和&quot;edit log&quot;两个文件形式存放在本地磁盘，但是这些文件是在HDFS每次重启的时候重新构造出来的。Datanode则是存取文件实际内容的节点，Datanodes会定时地将block的列表汇报给Namenode。</p>
<p>由于Namenode是元数据存放的节点，如果Namenode挂了那么HDFS就没法正常运行，因此一般使用将元数据持久存储在本地或远程的机器上，或者使用secondary namenode来定期同步Namenode的元数据信息，secondary namenode有点类似于MySQL的Master/Salves中的Slave，&quot;edit log&quot;就类似&quot;bin log&quot;。如果Namenode出现了故障，一般会将原Namenode中持久化的元数据拷贝到secondary namenode中，使secondary namenode作为新的Namenode运行起来。</p>
<pre><code>                        ![]()
</code></pre><h2 id="-"><a href=""></a>三、读写流程</h2>
<h3 id="-gfs-"><a href=""></a>GFS论文提到的文件读取简单流程：</h3>
<h3 id="-"><a href=""></a></h3>
<h3 id="-"><a href=""></a>                <img src="" alt=""></h3>
<h3 id="-"><a href=""></a><em>**</em></h3>
<h3 id="-reading-data-from-hdfs-http-blog-endlesscode-com-wp-content-uploads-2010-06-reading-data-from-hdfs-png-reading-data-from-hdfs-"><a href=""></a><strong>详细流程：</strong><img src="http://blog.endlesscode.com/wp-content/uploads/2010/06/reading-data-from-hdfs.png" alt="reading data from hdfs" title="reading data from hdfs"></h3>
<p>文件读取的过程如下：</p>
<ol>
<li>使用HDFS提供的客户端开发库Client，向远程的Namenode发起RPC请求；</li>
<li>Namenode会视情况返回文件的部分或者全部block列表，对于每个block，Namenode都会返回有该block拷贝的DataNode地址；</li>
<li>客户端开发库Client会选取离客户端最接近的DataNode来读取block；如果客户端本身就是DataNode,那么将从本地直接获取数据.</li>
<li>读取完当前block的数据后，关闭与当前的DataNode连接，并为读取下一个block寻找最佳的DataNode；</li>
<li>当读完列表的block后，且文件读取还没有结束，客户端开发库会继续向Namenode获取下一批的block列表。</li>
<li>读取完一个block都会进行checksum验证，如果读取datanode时出现错误，客户端会通知Namenode，然后再从下一个拥有该block拷贝的datanode继续读。</li>
</ol>
<h3 id="-"><a href=""></a></h3>
<h3 id="-gfs-"><a href=""></a>GFS论文提到的写入文件简单流程：</h3>
<pre><code>                                 ![]()             
</code></pre><h2 id="-writing-data-to-hdfs-http-blog-endlesscode-com-wp-content-uploads-2010-06-writing-data-to-hdfs-png-writing-data-to-hdfs-"><a href=""></a>详细流程：<img src="http://blog.endlesscode.com/wp-content/uploads/2010/06/writing-data-to-hdfs.png" alt="writing data to hdfs" title="writing data to hdfs"></h2>
<p>写入文件的过程比读取较为复杂：</p>
<ol>
<li>使用HDFS提供的客户端开发库Client，向远程的Namenode发起RPC请求；</li>
<li>Namenode会检查要创建的文件是否已经存在，创建者是否有权限进行操作，成功则会为文件<strong>创建一个记录</strong>，否则会让客户端抛出异常；</li>
<li>当客户端开始写入文件的时候，开发库会将文件切分成多个packets，并在内部以数据队列&quot;data queue&quot;的形式管理这些packets，并向Namenode申请新的blocks，获取用来存储replicas的合适的datanodes列表，列表的大小根据在Namenode中对replication的设置而定。</li>
<li>开始以pipeline（管道）的形式将packet写入所有的replicas中。开发库把packet以流的方式写入第一个datanode，该datanode把该packet存储之后，再将其传递给在此pipeline中的下一个datanode，直到最后一个datanode，这种写数据的方式呈流水线的形式。</li>
<li>最后一个datanode成功存储之后会返回一个ack packet，在pipeline里传递至客户端，在客户端的开发库内部维护着&quot;ack queue&quot;，成功收到datanode返回的ack packet后会从&quot;ack queue&quot;移除相应的packet。</li>
<li>如果传输过程中，有某个datanode出现了故障，那么当前的pipeline会被关闭，出现故障的datanode会从当前的pipeline中移除，剩余的block会继续剩下的datanode中继续以pipeline的形式传输，同时Namenode会分配一个新的datanode，保持replicas设定的数量。</li>
</ol>
<h2 id="-"><a href=""></a></h2>
<p>分享到： <a href="&quot;分享到新浪微博&quot;"></a><a href="&quot;分享到腾讯微博&quot;"></a></p>
<ol>
<li>上一篇：<a href="http://blog.csdn.net/hguisu/article/details/7256833" target="_blank">Hadoop Hive sql语法详解</a></li>
<li>下一篇：<a href="http://blog.csdn.net/hguisu/article/details/7261145" target="_blank">Hadoop HDFS分布式文件系统设计要点与架构</a></li>
</ol>
<p>顶 3 踩 1
查看评论<a href=""></a></p>
<p>3楼 <a href="http://blog.csdn.net/huangbo1988911" target="_blank">huangbo1988911</a> 2012-04-09 11:25发表 <a href="&quot;回复&quot;">[回复]</a>  <a href="&quot;引用&quot;">[引用]</a> <a href="&quot;举报&quot;">[举报]</a><a href="http://blog.csdn.net/huangbo1988911" target="_blank"><img src="" alt=""></a>在写数据的过程中，一个文件被分割成很多blocks，这些block是按顺序一个个操作的，还是并发的进行传输的？Re: <a href="http://blog.csdn.net/hguisu" target="_blank">真实的归宿</a> 2012-04-09 12:45发表 <a href="&quot;回复&quot;">[回复]</a>  <a href="&quot;引用&quot;">[引用]</a> <a href="&quot;举报&quot;">[举报]</a><a href="http://blog.csdn.net/hguisu" target="_blank"><img src="" alt=""></a>回复huangbo1988911：写数据的是以流的方式传输，即管道的方式，一个一个block顺序传输。而不是像树形拓扑结构那样分散传输。Re: <a href="http://blog.csdn.net/huangbo1988911" target="_blank">huangbo1988911</a> 2012-04-17 18:03发表 <a href="&quot;回复&quot;">[回复]</a>  <a href="&quot;引用&quot;">[引用]</a> <a href="&quot;举报&quot;">[举报]</a><a href="http://blog.csdn.net/huangbo1988911" target="_blank"><img src="" alt=""></a>回复hguisu：您好，很感谢您回答我，但是我仍有点疑惑，hdfs中的文件大小区分为：chunk&lt;packet&lt;block,在每个packet的传输到多个DN（datanode）的过程中是以pipeline方式，但是当其中一个block在以这种方式传输时，其他的block是要等待还是并发的进行呢？谢谢！Re: <a href="http://blog.csdn.net/hguisu" target="_blank">真实的归宿</a> 2012-04-20 16:23发表 <a href="&quot;回复&quot;">[回复]</a>  <a href="&quot;引用&quot;">[引用]</a> <a href="&quot;举报&quot;">[举报]</a><a href="http://blog.csdn.net/hguisu" target="_blank"><img src="" alt=""></a>回复huangbo1988911：管道方式，即是队列方式传输。只能一个block传完了，接着传下个block。Re: <a href="http://blog.csdn.net/huangbo1988911" target="_blank">huangbo1988911</a> 2012-04-30 18:18发表 <a href="&quot;回复&quot;">[回复]</a>  <a href="&quot;引用&quot;">[引用]</a> <a href="&quot;举报&quot;">[举报]</a><a href="http://blog.csdn.net/huangbo1988911" target="_blank"><img src="" alt=""></a>回复hguisu：你好！如果这样，那hdfs的并发写实如何体现的呢？Re: <a href="http://blog.csdn.net/hguisu" target="_blank">真实的归宿</a> 2012-05-02 09:29发表 <a href="&quot;回复&quot;">[回复]</a>  <a href="&quot;引用&quot;">[引用]</a> <a href="&quot;举报&quot;">[举报]</a><a href="http://blog.csdn.net/hguisu" target="_blank"><img src="" alt=""></a>回复huangbo1988911：hdfs没有并发写入。Re: <a href="http://blog.csdn.net/huangbo1988911" target="_blank">huangbo1988911</a> 2012-05-03 23:39发表 <a href="&quot;回复&quot;">[回复]</a>  <a href="&quot;引用&quot;">[引用]</a> <a href="&quot;举报&quot;">[举报]</a><a href="http://blog.csdn.net/huangbo1988911" target="_blank"><img src="" alt=""></a>回复hguisu：这样的话，那hadoop是比较适合大数据的处理了，对于文件的写的速度并没有多大的提高了?Re: <a href="http://blog.csdn.net/hguisu" target="_blank">真实的归宿</a> 2012-05-04 09:40发表 <a href="&quot;回复&quot;">[回复]</a>  <a href="&quot;引用&quot;">[引用]</a> <a href="&quot;举报&quot;">[举报]</a><a href="http://blog.csdn.net/hguisu" target="_blank"><img src="" alt=""></a>回复huangbo1988911：hadoop本来就是通往云服务的捷径，为处理超大数据集而准备。Re: <a href="http://blog.csdn.net/huangbo1988911" target="_blank">huangbo1988911</a> 2012-05-07 10:57发表 <a href="&quot;回复&quot;">[回复]</a>  <a href="&quot;引用&quot;">[引用]</a> <a href="&quot;举报&quot;">[举报]</a><a href="http://blog.csdn.net/huangbo1988911" target="_blank"><img src="" alt=""></a>回复hguisu：这样hadoop的主要优势是在map/reduce那一块，而其文件系统有什么样的优势呢（在文件的读写方面，和其他的文件系统）？Re: <a href="http://blog.csdn.net/hguisu" target="_blank">真实的归宿</a> 2012-05-07 11:50发表 <a href="&quot;回复&quot;">[回复]</a>  <a href="&quot;引用&quot;">[引用]</a> <a href="&quot;举报&quot;">[举报]</a><a href="http://blog.csdn.net/hguisu" target="_blank"><img src="" alt=""></a>回复huangbo1988911：hdfs可以存储超大数据，而map/reduce要处理的数据存储在hdfs上，即MR分布式运算。Re: <a href="http://blog.csdn.net/huangbo1988911" target="_blank">huangbo1988911</a> 2012-05-09 22:35发表 <a href="&quot;回复&quot;">[回复]</a>  <a href="&quot;引用&quot;">[引用]</a> <a href="&quot;举报&quot;">[举报]</a><a href="http://blog.csdn.net/huangbo1988911" target="_blank"><img src="" alt=""></a>回复hguisu： 那多个文件同时向hdfs写入是如何进行的呢？Re: <a href="http://blog.csdn.net/huangbo1988911" target="_blank">huangbo1988911</a> 2012-05-09 00:39发表 <a href="&quot;回复&quot;">[回复]</a>  <a href="&quot;引用&quot;">[引用]</a> <a href="&quot;举报&quot;">[举报]</a><a href="http://blog.csdn.net/huangbo1988911" target="_blank"><img src="" alt=""></a>回复hguisu：恩，hdfs相对于其他的文件系统，除了更适合存储大数据以外，而且有很强的容错能力，但是对数据的读写等，没有并发性，只是采用了管道的方式，这可能是它的一个小缺点吧。2楼 <a href="http://blog.csdn.net/CD_xiaoxin" target="_blank">CD_xiaoxin</a> 2012-03-19 09:44发表 <a href="&quot;回复&quot;">[回复]</a>  <a href="&quot;引用&quot;">[引用]</a> <a href="&quot;举报&quot;">[举报]</a><a href="http://blog.csdn.net/CD_xiaoxin" target="_blank"><img src="" alt=""></a>很详细 很有帮助 谢谢1楼 <a href="http://blog.csdn.net/lin_FS" target="_blank">lin_FS</a> 2012-03-16 10:01发表 <a href="&quot;回复&quot;">[回复]</a>  <a href="&quot;引用&quot;">[引用]</a> <a href="&quot;举报&quot;">[举报]</a><a href="http://blog.csdn.net/lin_FS" target="_blank"><img src="" alt=""></a>client端的那个queue是在内存中，还是写在临时文件里了？Re: <a href="http://blog.csdn.net/hguisu" target="_blank">真实的归宿</a> 2012-03-19 09:43发表 <a href="&quot;回复&quot;">[回复]</a>  <a href="&quot;引用&quot;">[引用]</a> <a href="&quot;举报&quot;">[举报]</a><a href="http://blog.csdn.net/hguisu" target="_blank"><img src="" alt=""></a>回复lin_FS：client分割数据成一个个block（packet）这些数据都不在内存中，你可以想象，如果一个数据是100G，它你那个放进内存吗？Re: <a href="http://blog.csdn.net/lin_FS" target="_blank">lin_FS</a> 2012-03-21 17:26发表 <a href="&quot;回复&quot;">[回复]</a>  <a href="&quot;引用&quot;">[引用]</a> <a href="&quot;举报&quot;">[举报]</a><a href="http://blog.csdn.net/lin_FS" target="_blank"><img src="" alt=""></a>在client端， 多个block（packet）组成一个队列，然后可以想象把文件（100G）分成若干个packet，如果队列满了就根本写不进去数据了，根本不会出现你想象的那种情况。我想了解的是，这个队列在内存中还是以文件的形式，呵呵Re: <a href="http://blog.csdn.net/hguisu" target="_blank">真实的归宿</a> 2012-03-31 18:47发表 <a href="&quot;回复&quot;">[回复]</a>  <a href="&quot;引用&quot;">[引用]</a> <a href="&quot;举报&quot;">[举报]</a><a href="http://blog.csdn.net/hguisu" target="_blank"><img src="" alt=""></a>回复lin_FS：这个队列肯定是文件的形式存在的。
您还没有登录,请<a href="">[登录]</a>或<a href="http://passport.csdn.net/account/register?from=http%3A%2F%2Fblog.csdn.net%2Fhguisu%2Farticle%2Fdetails%2F7259716" target="_blank">[注册]</a></p>
<p>/* 以上用户言论只代表其个人观点，不代表CSDN网站的观点或立场<a href=""></a><a href=""></a></p>
<p><a href="">hguisu</a>
<a href="&quot;回到顶部&quot;"><img src="" alt="TOP"></a></p>
<p>个人资料</p>
<p><a href="http://my.csdn.net/hguisu" target="_blank"><img src="&quot;访问我的空间&quot;" alt=""></a>
<a href="http://my.csdn.net/hguisu" target="_blank">真实的归宿</a></p>
<p><a href="&quot;[加关注]&quot;"></a> <a href="&quot;[发私信]&quot;"></a>
<a href="http://medal.blog.csdn.net/allmedal.aspx" target="_blank"><img src="" alt=""></a></p>
<ul>
<li>访问：481035次</li>
<li>积分：6666分</li>
<li><p>排名：第614名</p>
</li>
<li><p>原创：190篇</p>
</li>
<li>转载：1篇</li>
<li>译文：0篇</li>
<li>评论：313条</li>
</ul>
<p>文章分类</p>
<ul>
<li><a href="http://blog.csdn.net/hguisu/article/category/1253451" target="_blank">操作系统</a>(5)</li>
<li><a href="http://blog.csdn.net/hguisu/article/category/796967" target="_blank">Linux</a>(17)</li>
<li><a href="http://blog.csdn.net/hguisu/article/category/796963" target="_blank">MySQL</a>(12)</li>
<li><a href="http://blog.csdn.net/hguisu/article/category/796962" target="_blank">PHP</a>(41)</li>
<li><a href="http://blog.csdn.net/hguisu/article/category/1104862" target="_blank">PHP内核</a>(11)</li>
<li><a href="http://blog.csdn.net/hguisu/article/category/796968" target="_blank">技术人生</a>(7)</li>
<li><a href="http://blog.csdn.net/hguisu/article/category/1054628" target="_blank">数据结构与算法</a>(27)</li>
<li><a href="http://blog.csdn.net/hguisu/article/category/1072794" target="_blank">云计算hadoop</a>(20)</li>
<li><a href="http://blog.csdn.net/hguisu/article/category/1075597" target="_blank">网络知识</a>(7)</li>
<li><a href="http://blog.csdn.net/hguisu/article/category/1080443" target="_blank">c/c++</a>(22)</li>
<li><a href="http://blog.csdn.net/hguisu/article/category/1099674" target="_blank">memcache</a>(5)</li>
<li><a href="http://blog.csdn.net/hguisu/article/category/1111071" target="_blank">HipHop</a>(2)</li>
<li><a href="http://blog.csdn.net/hguisu/article/category/1112019" target="_blank">计算机原理</a>(4)</li>
<li><a href="http://blog.csdn.net/hguisu/article/category/1114530" target="_blank">Java</a>(7)</li>
<li><a href="http://blog.csdn.net/hguisu/article/category/1122753" target="_blank">socket网络编程</a>(5)</li>
<li><a href="http://blog.csdn.net/hguisu/article/category/1133340" target="_blank">设计模式</a>(26)</li>
<li><a href="http://blog.csdn.net/hguisu/article/category/1151353" target="_blank">AOP</a>(2)</li>
<li><a href="http://blog.csdn.net/hguisu/article/category/1152364" target="_blank">重构</a>(11)</li>
<li><a href="http://blog.csdn.net/hguisu/article/category/1173389" target="_blank">重构与模式</a>(1)</li>
<li><a href="http://blog.csdn.net/hguisu/article/category/1209788" target="_blank">大数据处理</a>(11)</li>
<li><a href="http://blog.csdn.net/hguisu/article/category/1230933" target="_blank">搜索引擎Search Engine</a>(15)</li>
<li><a href="http://blog.csdn.net/hguisu/article/category/1302430" target="_blank">HTML5</a>(1)</li>
<li><a href="http://blog.csdn.net/hguisu/article/category/1309674" target="_blank">Android</a>(1)</li>
<li><a href="http://blog.csdn.net/hguisu/article/category/1422000" target="_blank">webserver</a>(3)</li>
<li><p><a href="http://blog.csdn.net/hguisu/article/category/1429288" target="_blank">NOSQL</a>(6)
文章存档</p>
</li>
<li><p><a href="http://blog.csdn.net/hguisu/article/month/2013/09" target="_blank">2013年09月</a>(2)</p>
</li>
<li><a href="http://blog.csdn.net/hguisu/article/month/2013/08" target="_blank">2013年08月</a>(1)</li>
<li><a href="http://blog.csdn.net/hguisu/article/month/2013/07" target="_blank">2013年07月</a>(2)</li>
<li><a href="http://blog.csdn.net/hguisu/article/month/2013/06" target="_blank">2013年06月</a>(3)</li>
<li><a href="http://blog.csdn.net/hguisu/article/month/2013/05" target="_blank">2013年05月</a>(3)</li>
<li><a href="http://blog.csdn.net/hguisu/article/month/2013/03" target="_blank">2013年03月</a>(3)</li>
<li><a href="http://blog.csdn.net/hguisu/article/month/2013/02" target="_blank">2013年02月</a>(2)</li>
<li><a href="http://blog.csdn.net/hguisu/article/month/2013/01" target="_blank">2013年01月</a>(2)</li>
<li><a href="http://blog.csdn.net/hguisu/article/month/2012/12" target="_blank">2012年12月</a>(4)</li>
<li><a href="http://blog.csdn.net/hguisu/article/month/2012/11" target="_blank">2012年11月</a>(3)</li>
<li><a href="http://blog.csdn.net/hguisu/article/month/2012/10" target="_blank">2012年10月</a>(2)</li>
<li><a href="http://blog.csdn.net/hguisu/article/month/2012/09" target="_blank">2012年09月</a>(15)</li>
<li><a href="http://blog.csdn.net/hguisu/article/month/2012/08" target="_blank">2012年08月</a>(6)</li>
<li><a href="http://blog.csdn.net/hguisu/article/month/2012/07" target="_blank">2012年07月</a>(8)</li>
<li><a href="http://blog.csdn.net/hguisu/article/month/2012/06" target="_blank">2012年06月</a>(14)</li>
<li><a href="http://blog.csdn.net/hguisu/article/month/2012/05" target="_blank">2012年05月</a>(29)</li>
<li><a href="http://blog.csdn.net/hguisu/article/month/2012/04" target="_blank">2012年04月</a>(26)</li>
<li><a href="http://blog.csdn.net/hguisu/article/month/2012/03" target="_blank">2012年03月</a>(27)</li>
<li><a href="http://blog.csdn.net/hguisu/article/month/2012/02" target="_blank">2012年02月</a>(18)</li>
<li><a href="http://blog.csdn.net/hguisu/article/month/2011/12" target="_blank">2011年12月</a>(7)</li>
<li><a href="http://blog.csdn.net/hguisu/article/month/2011/01" target="_blank">2011年01月</a>(8)</li>
<li><a href="http://blog.csdn.net/hguisu/article/month/2010/07" target="_blank">2010年07月</a>(6)</li>
<li><a href="http://blog.csdn.net/hguisu/article/month/2007/12" target="_blank">2007年12月</a>(2)</li>
</ul>
<p>展开</p>
<p>阅读排行</p>
<ul>
<li><a href="http://blog.csdn.net/hguisu/article/details/7237395" title="Hadoop集群配置（最全面总结）" target="_blank">Hadoop集群配置（最全面总结）</a>(23024)</li>
<li><a href="http://blog.csdn.net/hguisu/article/details/7527842" title="设计模式（五）适配器模式Adapter（结构型）" target="_blank">设计模式（五）适配器模式Adapter（结构型）</a>(21421)</li>
<li><a href="http://blog.csdn.net/hguisu/article/details/7244413" title="hbase安装配置（整合到hadoop）" target="_blank">hbase安装配置（整合到hadoop）</a>(20780)</li>
<li><a href="http://blog.csdn.net/hguisu/article/details/7453390" title="socket阻塞与非阻塞，同步与异步、I/O模型" target="_blank">socket阻塞与非阻塞，同步与异步、I/O模型</a>(12788)</li>
<li><a href="http://blog.csdn.net/hguisu/article/details/7282050" title="Hadoop Hive与Hbase整合" target="_blank">Hadoop Hive与Hbase整合</a>(11869)</li>
<li><a href="http://blog.csdn.net/hguisu/article/details/7558249" title="设计模式 ( 十八 ) 策略模式Strategy（对象行为型）" target="_blank">设计模式 ( 十八 ) 策略模式Strategy（对象行为型）</a>(11737)</li>
<li><a href="http://blog.csdn.net/hguisu/article/details/7786014" title="B-树和B+树的应用：数据搜索和数据库索引" target="_blank">B-树和B+树的应用：数据搜索和数据库索引</a>(11507)</li>
<li><a href="http://blog.csdn.net/hguisu/article/details/5731880" title="Mysql 多表联合查询效率分析及优化" target="_blank">Mysql 多表联合查询效率分析及优化</a>(10454)</li>
<li><a href="http://blog.csdn.net/hguisu/article/details/7244991" title="谷歌三大核心技术（三）Google BigTable中文版" target="_blank">谷歌三大核心技术（三）Google BigTable中文版</a>(9958)</li>
<li><p><a href="&quot;HDFS写入和读取流程&quot;">HDFS写入和读取流程</a>(8282)
评论排行</p>
</li>
<li><p><a href="http://blog.csdn.net/hguisu/article/details/7558249" title="设计模式 ( 十八 ) 策略模式Strategy（对象行为型）" target="_blank">设计模式 ( 十八 ) 策略模式Strategy（对象行为型）</a>(33)</p>
</li>
<li><a href="&quot;HDFS写入和读取流程&quot;">HDFS写入和读取流程</a>(17)</li>
<li><a href="http://blog.csdn.net/hguisu/article/details/7529194" title="设计模式（六）桥连模式Bridge（结构型）" target="_blank">设计模式（六）桥连模式Bridge（结构型）</a>(14)</li>
<li><a href="http://blog.csdn.net/hguisu/article/details/7505909" title="设计模式（一）工厂模式Factory（创建型）" target="_blank">设计模式（一）工厂模式Factory（创建型）</a>(13)</li>
<li><a href="http://blog.csdn.net/hguisu/article/details/7880288" title="海量数据处理算法—Bit-Map" target="_blank">海量数据处理算法—Bit-Map</a>(13)</li>
<li><a href="http://blog.csdn.net/hguisu/article/details/7237395" title="Hadoop集群配置（最全面总结）" target="_blank">Hadoop集群配置（最全面总结）</a>(13)</li>
<li><a href="http://blog.csdn.net/hguisu/article/details/7448528" title="PHP SOCKET编程" target="_blank">PHP SOCKET编程</a>(11)</li>
<li><a href="http://blog.csdn.net/hguisu/article/details/7453390" title="socket阻塞与非阻塞，同步与异步、I/O模型" target="_blank">socket阻塞与非阻塞，同步与异步、I/O模型</a>(11)</li>
<li><a href="http://blog.csdn.net/hguisu/article/details/7282050" title="Hadoop Hive与Hbase整合" target="_blank">Hadoop Hive与Hbase整合</a>(10)</li>
<li><a href="http://blog.csdn.net/hguisu/article/details/7244413" title="hbase安装配置（整合到hadoop）" target="_blank">hbase安装配置（整合到hadoop）</a>(10)</li>
</ul>
<p>推荐文章
最新评论</p>
<ul>
<li><a href="http://blog.csdn.net/hguisu/article/details/7453390#comments" target="_blank">socket阻塞与非阻塞，同步与异步、I/O模型</a></li>
</ul>
<p><a href="http://blog.csdn.net/ctqctq99" target="_blank">ctqctq99</a>: 他的意思可能是阻塞IO和非阻塞IO的区别就在于：应用程序的调用是否立即返回！这句话说反了。应该是非阻...</p>
<ul>
<li><a href="http://blog.csdn.net/hguisu/article/details/7408047#comments" target="_blank">硬盘的读写原理</a></li>
</ul>
<p><a href="http://blog.csdn.net/m1013923728" target="_blank">m1013923728</a>: 写的通俗易懂！</p>
<ul>
<li><a href="http://blog.csdn.net/hguisu/article/details/7470695#comments" target="_blank">C语言中的宏定义</a></li>
</ul>
<p><a href="http://blog.csdn.net/ouwen3536" target="_blank">ouwen3536</a>: 很半，转起！</p>
<ul>
<li><a href="http://blog.csdn.net/hguisu/article/details/7470695#comments" target="_blank">C语言中的宏定义</a></li>
</ul>
<p><a href="http://blog.csdn.net/zhangyongbluesky" target="_blank">zhangyongbluesky</a>: good</p>
<ul>
<li><a href="http://blog.csdn.net/hguisu/article/details/7244413#comments" target="_blank">hbase安装配置（整合到hadoop）</a></li>
</ul>
<p><a href="http://blog.csdn.net/hguisu" target="_blank">真实的归宿</a>: @u012171806:conf/hbase-site.xml你应该看官方文档的快速入门。安装的东西...</p>
<ul>
<li><a href="http://blog.csdn.net/hguisu/article/details/7244413#comments" target="_blank">hbase安装配置（整合到hadoop）</a></li>
</ul>
<p><a href="http://blog.csdn.net/u012171806" target="_blank">JAVA_小陈</a>: 我把hbase下载了，你上面说的需要配置一个xml文件，请问配置在什么地方呢？然后启动的时候是在什么...</p>
<ul>
<li><a href="http://blog.csdn.net/hguisu/article/details/7408047#comments" target="_blank">硬盘的读写原理</a></li>
</ul>
<p><a href="http://blog.csdn.net/mxhlee" target="_blank">mxhlee</a>: 好东西啊！！！（实际是斜切向运动）这句话让我纠结了很长时间、我自己就感觉是这运动 但是没有一个介绍...</p>
<ul>
<li><a href="http://blog.csdn.net/hguisu/article/details/7453390#comments" target="_blank">socket阻塞与非阻塞，同步与异步、I/O模型</a></li>
</ul>
<p><a href="http://blog.csdn.net/hguisu" target="_blank">真实的归宿</a>: @liaokailin:并没有反。实际就是这样的。</p>
<ul>
<li><a href="http://blog.csdn.net/hguisu/article/details/7453390#comments" target="_blank">socket阻塞与非阻塞，同步与异步、I/O模型</a></li>
</ul>
<p><a href="http://blog.csdn.net/hguisu" target="_blank">真实的归宿</a>: @liaokailin:实际就是这样的。</p>
<ul>
<li><a href="http://blog.csdn.net/hguisu/article/details/7453390#comments" target="_blank">socket阻塞与非阻塞，同步与异步、I/O模型</a></li>
</ul>
<p><a href="http://blog.csdn.net/liaokailin" target="_blank">廖凯林</a>: 同步IO和异步IO的区别就在于：数据拷贝的时候进程是否阻塞！阻塞IO和非阻塞IO的区别就在于：应用程...</p>
<p><a href="http://www.csdn.net/company/about.html" target="_blank">公司简介</a>|<a href="http://www.csdn.net/company/recruit.html" target="_blank">招贤纳士</a>|<a href="http://www.csdn.net/company/marketing.html" target="_blank">广告服务</a>|<a href="http://www.csdn.net/company/account.html" target="_blank">银行汇款帐号</a>|<a href="http://www.csdn.net/company/contact.html" target="_blank">联系方式</a>|<a href="http://www.csdn.net/company/statement.html" target="_blank">版权声明</a>|<a href="http://www.csdn.net/company/layer.html" target="_blank">法律顾问</a>|<a href="mailto:webmaster@csdn.net">问题报告</a><a href="http://wpa.qq.com/msgrd?v=3&amp;uin=2355263776&amp;site=qq&amp;menu=yes" target="_blank">QQ客服</a> <a href="http://e.weibo.com/csdnsupport/profile" target="_blank">微博客服</a> <a href="http://bbs.csdn.net/forums/Service" target="_blank">论坛反馈</a> <a href="mailto:webmaster@csdn.net">联系邮箱：webmaster@csdn.net</a> 服务热线：400-600-2320京 ICP 证 070598 号北京创新乐知信息技术有限公司 版权所有世纪乐知(北京)网络技术有限公司 提供技术支持江苏乐知网络技术有限公司 提供商务支持Copyright © 1999-2012, CSDN.NET, All Rights Reserved <a href="http://www.hd315.gov.cn/beian/view.asp?bianhao=010202001032100010" target="_blank"><img src="" alt="GongshangLogo"></a>
<img src="http://counter.csdn.net/pv.aspx?id=24" alt=""></p>

      
    </div>
    
    
      
    
    
        <footer id="post-meta">
        <span class="categories">Posted in<span class="breadcrumb fa fa-folder"><li><a href="/categories/hadoop/">hadoop</a></li></span></span> | <span class="tags">Tagged <a href="/tags/hadoop/" class="label label-primary">hadoop</a></span> | <span class="time">recent updated:<time title="2014-03-07 09:54:36"datetime="2014-03-07 09:54:36"> mar. 7 2014</time></span> | <span class="comment-link">
<a href="http://itsolife.com/2014/02/02/2014-02-02-hadoop--HDFS写入和读取流程/#comments" class="ds-thread-count comment-link" data-thread-key="2014-02-02-hadoop--HDFS写入和读取流程" data-count-type="comments">暂无评论</a></span>
        </footer>
    
    <div class="clearfix"></div>
  </div>
</div>
</article>



  <article>
<div id="post" class="post well">
  <div class="post-content">
    <header class="well-sm">
      <i class="fa icon fa-5x pull-left"></i>
      <h1 class="title"><a href="/2014/02/02/2014-02-02-hadoop--Hadoop121编译Eclipse插件/">Hadoop 1.2.1编译Eclipse插件</a></h1>
      
        <span>Posted on<time datetime="2014-02-02T01:54:36.000Z"> <a href="/2014/02/02/2014-02-02-hadoop--Hadoop121编译Eclipse插件/">feb. 2 2014</a></time></span>      
    </header>
    
    
    <div class="entry">
      
        <h1 id="hadoop-1-2-1-eclipse-">Hadoop 1.2.1编译Eclipse插件</h1>
<h2 id="-src-contrib-eclipse-plugin-build-xml">/src/contrib/eclipse-plugin/build.xml</h2>
<h3 id="1-ivy-download-">1）取消ivy-download：</h3>
<p><a href="http://static.oschina.net/uploads/space/2013/0520/194613_IBLw_256028.png" target="_blank"><img src="" alt=""></a></p>
<h3 id="2-plugin-jar-">2）添加将要打包到plugin中的第三方jar包列表：</h3>
<p>01</p>
<!-- Override jar target to specify manifest -->


<p>02</p>
<p>&lt;</p>
<p>target</p>
<p>name</p>
<p>=</p>
<p>&quot;jar&quot;</p>
<p>depends</p>
<p>=</p>
<p>&quot;compile&quot;</p>
<p>unless</p>
<p>=</p>
<p>&quot;skip.contrib&quot;</p>
<p>&gt;
03</p>
<p>&lt;</p>
<p>mkdir</p>
<p>dir</p>
<p>=</p>
<p>&quot;${build.dir}/lib&quot;</p>
<p>/&gt; </p>
<p>04</p>
<p>05</p>
<!-- 自定义的修改内容：begin -->


<p>06</p>
<!--
07

<copy file="${hadoop.root}/build/hadoop-core-${version}.jar"

08



tofile="${build.dir}/lib/hadoop-core.jar" verbose="true"/>
09

<copy file="${hadoop.root}/build/ivy/lib/Hadoop/common/commons-cli-${commons-cli.version}.jar" 

10



todir="${build.dir}/lib" verbose="true"/> 
11

-->


<p>12</p>
<p>&lt;</p>
<p>copy</p>
<p>file</p>
<p>=</p>
<p>&quot;${hadoop.root}/hadoop-core-${version}.jar&quot;</p>
<p>tofile</p>
<p>=</p>
<p>&quot;${build.dir}/lib/hadoop-core.jar&quot;</p>
<p>verbose</p>
<p>=</p>
<p>&quot;true&quot;</p>
<p>/&gt;
13</p>
<p>&lt;</p>
<p>copy</p>
<p>file</p>
<p>=</p>
<p>&quot;${hadoop.root}/lib/commons-cli-1.2.jar&quot;</p>
<p>todir</p>
<p>=</p>
<p>&quot;${build.dir}/lib&quot;</p>
<p>verbose</p>
<p>=</p>
<p>&quot;true&quot;</p>
<p>/&gt; </p>
<p>14</p>
<p>&lt;</p>
<p>copy</p>
<p>file</p>
<p>=</p>
<p>&quot;${hadoop.root}/lib/commons-configuration-1.6.jar&quot;</p>
<p>todir</p>
<p>=</p>
<p>&quot;${build.dir}/lib&quot;</p>
<p>verbose</p>
<p>=</p>
<p>&quot;true&quot;</p>
<p>/&gt; 
15</p>
<p>&lt;</p>
<p>copy</p>
<p>file</p>
<p>=</p>
<p>&quot;${hadoop.root}/lib/commons-httpclient-3.0.1.jar&quot;</p>
<p>todir</p>
<p>=</p>
<p>&quot;${build.dir}/lib&quot;</p>
<p>verbose</p>
<p>=</p>
<p>&quot;true&quot;</p>
<p>/&gt; </p>
<p>16</p>
<p>&lt;</p>
<p>copy</p>
<p>file</p>
<p>=</p>
<p>&quot;${hadoop.root}/lib/commons-lang-2.4.jar&quot;</p>
<p>todir</p>
<p>=</p>
<p>&quot;${build.dir}/lib&quot;</p>
<p>verbose</p>
<p>=</p>
<p>&quot;true&quot;</p>
<p>/&gt;<br>17</p>
<p>&lt;</p>
<p>copy</p>
<p>file</p>
<p>=</p>
<p>&quot;${hadoop.root}/lib/jackson-core-asl-1.8.8.jar&quot;</p>
<p>todir</p>
<p>=</p>
<p>&quot;${build.dir}/lib&quot;</p>
<p>verbose</p>
<p>=</p>
<p>&quot;true&quot;</p>
<p>/&gt;</p>
<p>18</p>
<p>&lt;</p>
<p>copy</p>
<p>file</p>
<p>=</p>
<p>&quot;${hadoop.root}/lib/jackson-mapper-asl-1.8.8.jar&quot;</p>
<p>todir</p>
<p>=</p>
<p>&quot;${build.dir}/lib&quot;</p>
<p>verbose</p>
<p>=</p>
<p>&quot;true&quot;</p>
<p>/&gt;<br>19</p>
<!-- 自定义的修改内容：end -->


<p>20</p>
<p>&lt;</p>
<p>jar
21</p>
<p>jarfile</p>
<p>=</p>
<p>&quot;${build.dir}/hadoop-${name}-${version}.jar&quot;</p>
<p>22</p>
<p>manifest</p>
<p>=</p>
<p>&quot;${root}/META-INF/MANIFEST.MF&quot;</p>
<blockquote>
<p>23</p>
</blockquote>
<p>&lt;</p>
<p>fileset</p>
<p>dir</p>
<p>=</p>
<p>&quot;${build.dir}&quot;</p>
<p>includes</p>
<p>=</p>
<p>&quot;classes/ lib/&quot;</p>
<p>/&gt; </p>
<p>24</p>
<p>&lt;</p>
<p>fileset</p>
<p>dir</p>
<p>=</p>
<p>&quot;${root}&quot;</p>
<p>includes</p>
<p>=</p>
<p>&quot;resources/ plugin.xml&quot;</p>
<p>/&gt; 
25</p>
<p>&lt;/</p>
<p>jar</p>
<blockquote>
</blockquote>
<p>26</p>
<p>&lt;/</p>
<p>target</p>
<blockquote>
<p>&lt;</p>
</blockquote>
<p>span</p>
<p>style</p>
<p>=</p>
<p>&quot;font-size:10pt;line-height:1.5;font-family:&#39;sans serif&#39;, tahoma, verdana, helvetica;&quot;</p>
<blockquote>
<p> &lt;/</p>
</blockquote>
<p>span</p>
<p>&gt;</p>
<p><a href="http://static.oschina.net/uploads/space/2013/0520/194752_bBaX_256028.png" target="_blank"><img src="" alt=""></a> </p>
<h2 id="3-hadoop-src-contrib-build-contrib-xml-">3.%hadoop%/src/contrib/build-contrib.xml ：</h2>
<h3 id="1-hadoop-version-eclipse-eclipse-home-">1）添加hadoop的version和eclipse的eclipse.home属性：</h3>
<p>1</p>
<p>&lt;?</p>
<p>xml</p>
<p>version</p>
<p>=</p>
<p>&quot;1.0&quot;</p>
<p>?&gt;</p>
<p>2</p>
<!-- Imported by contrib//*/build.xml files to share generic targets. -->
3

&lt;

project


name

=

&quot;hadoopbuildcontrib&quot;


xmlns:ivy

=

&quot;antlib:org.apache.ivy.ant&quot;

&gt;

4

&lt;

property


name

=

&quot;name&quot;


value

=

&quot;${ant.project.name}&quot;

/&gt;
5

&lt;

property


name

=

&quot;root&quot;


value

=

&quot;${basedir}&quot;

/&gt;

6

&lt;

property


name

=

&quot;hadoop.root&quot;


location

=

&quot;${root}/../../../&quot;

/&gt;
7

<!-- hadoop版本、eclipse安装路径 -->

<p>8</p>
<p>&lt;</p>
<p>property</p>
<p>name</p>
<p>=</p>
<p>&quot;version&quot;</p>
<p>value</p>
<p>=</p>
<p>&quot;1.1.2&quot;</p>
<p>/&gt;
9</p>
<p>&lt;</p>
<p>property</p>
<p>name</p>
<p>=</p>
<p>&quot;eclipse.home&quot;</p>
<p>location</p>
<p>=</p>
<p>&quot;%eclipse%&quot;</p>
<p>/&gt;</p>
<p><a href="http://static.oschina.net/uploads/space/2013/0520/194838_mpUD_256028.png" target="_blank"><img src="" alt=""></a>  </p>
<h3 id="2-ivy-download-">2）取消ivy-download：</h3>
<p><a href="http://static.oschina.net/uploads/space/2013/0520/194859_Cc5J_256028.png" target="_blank"><img src="" alt=""></a> 
来源： <a href="[http://my.oschina.net/vigiles/blog/132238](http://my.oschina.net/vigiles/blog/132238)">[http://my.oschina.net/vigiles/blog/132238](http://my.oschina.net/vigiles/blog/132238)</a></p>
<p>配置/${hadoop-home}/src/contrib/eclipse-plugins/build.xml</p>
<p>找到</p>
<path id=”classpath”>

<p>然后添加</p>
<fileset dir=”${hadoop.root}/”>
      <include name=”/*.jar”/>
</fileset>

<p><strong>这一步很重要，如果不添加的话会出现找不到相应的程序包，错误如下（给出部分）</strong> ：</p>
<p>[javac]      Counters.Group group = counters.getGroup(groupName);
    [javac]              ^
    [javac] /home/summerdg/hadoop_src/hadoop-1.2.1/src/contrib/eclipse-plugin/src/java/org/apache/hadoop/eclipse/server/HadoopJob.java:305: 错误: 程序包Counters不存在
    [javac]      for (Counters.Counter counter : group) {
    [javac]                    ^
    [javac] /home/summerdg/hadoop_src/hadoop-1.2.1/src/contrib/eclipse-plugin/src/java/org/apache/hadoop/eclipse/dfs/DFSFile.java:74: 错误: 找不到符号
    [javac]      FileStatus fs = getDFS().getFileStatus(path);
    [javac]      ^
    [javac]  符号:  类 FileStatus
    [javac]  位置: 类 DFSFile
    [javac] 注: 某些输入文件使用或覆盖了已过时的 API。
    [javac] 注: 有关详细信息, 请使用 -Xlint:deprecation 重新编译。
    [javac] 注: 某些输入文件使用了未经检查或不安全的操作。
    [javac] 注: 有关详细信息, 请使用 -Xlint:unchecked 重新编译。
    [javac] 100 个错误</p>
<p>当然，出现这个错误还有个原因，就是你已经设置了这句，但你依然出现这个错误，那就是你eclipse版本的问题了。</p>
<p>找到
来源： <a href="[http://www.kankanews.com/ICkengine/archives/63441.shtml](http://www.kankanews.com/ICkengine/archives/63441.shtml)">[http://www.kankanews.com/ICkengine/archives/63441.shtml](http://www.kankanews.com/ICkengine/archives/63441.shtml)</a> </p>
<h2 id="4-hadoop_home-build-xml-">4.编辑%HADOOP_HOME%/build.xml：</h2>
<h3 id="1-hadoop-">1）修改hadoop版本号：</h3>
<p><a href="http://static.oschina.net/uploads/space/2013/0520/194950_g5cw_256028.png" target="_blank"><img src="" alt=""></a></p>
<h3 id="2-ivy-download-">2）取消ivy-download：</h3>
<p><a href="http://static.oschina.net/uploads/space/2013/0520/195011_l8BK_256028.png" target="_blank"><img src="" alt=""></a> </p>
<h2 id="5-hadoop-src-contrib-eclipse-plugin-meta-inf-manifest-mf-">5.修改%hadoop%/src/contrib/eclipse-plugin/META-INF/MANIFEST.MF：</h2>
<p>修改${HADOOP_HOME}/src/contrib/eclipse-plugin/META-INF/MANIFEST.MF的Bundle-ClassPath：
1</p>
<p>Bundle-ClassPath: classes/,</p>
<p>2</p>
<p>lib/hadoop-core.jar,
3</p>
<p>lib/commons-cli-1.2.jar,</p>
<p>4</p>
<p>lib/commons-configuration-1.6.jar,
5</p>
<p>lib/commons-httpclient-3.0.1.jar,</p>
<p>6</p>
<p>lib/commons-lang-2.4.jar,
7</p>
<p>lib/jackson-core-asl-1.8.8.jar,</p>
<p>8</p>
<p>lib/jackson-mapper-asl-1.8.8.jar</p>
<p><a href="http://static.oschina.net/uploads/space/2013/0520/195106_mqwV_256028.png" target="_blank"><img src="" alt=""></a> </p>
<h2 id="-build-contrib-eclipse-plugin-ant-jar-">进入到 /build/contrib/eclipse-plugin/执行 ant jar ：</h2>
<p>01</p>
<p>hep@hep-ubuntu:~$ </p>
<p>cd</p>
<p>~/hadoop/src/contrib/eclipse-plugin</p>
<p>02</p>
<p>hep@hep-ubuntu:~/hadoop/src/contrib/eclipse-plugin$ ant jar
03</p>
<p>Buildfile: /home/hep/hadoop/src/contrib/eclipse-plugin/build.xml</p>
<p>04</p>
<p>05</p>
<p>check-contrib:</p>
<p>06</p>
<p>07</p>
<p>init:</p>
<p>08</p>
<p>[</p>
<p>echo</p>
<p>] contrib: eclipse-plugin
09</p>
<p>10</p>
<p>init-contrib:
11</p>
<p>12</p>
<p>ivy-probe-antlib:
13</p>
<p>14</p>
<p>ivy-init-antlib:
15</p>
<p>16</p>
<p>ivy-init:
17</p>
<p>[ivy:configure] :: Ivy 2.1.0 - 20090925235825 :: <a href="http://ant.apache.org/ivy/" target="_blank">http://ant.apache.org/ivy/</a> ::</p>
<p>18</p>
<p>[ivy:configure] :: loading settings :: </p>
<p>file</p>
<p>= /home/hep/hadoop/ivy/ivysettings.xml
19</p>
<p>20</p>
<p>ivy-resolve-common:
21</p>
<p>[ivy:resolve] :: resolving dependencies :: org.apache.hadoop</p>
<p>/#eclipse-plugin;working@hep-ubuntu</p>
<p>22</p>
<p>[ivy:resolve]   confs: [common]
23</p>
<p>[ivy:resolve]   found commons-logging</p>
<p>/#commons-logging;1.0.4 in maven2</p>
<p>24</p>
<p>[ivy:resolve]   found log4j</p>
<p>/#log4j;1.2.15 in maven2
25</p>
<p>[ivy:resolve] :: resolution report :: resolve 171ms :: artifacts dl 4ms</p>
<p>26</p>
<hr>
<p>27</p>
<p>|                  |            modules            ||   artifacts   |</p>
<p>28</p>
<p>|       conf       | number| search|dwnlded|evicted|| number|dwnlded|
29</p>
<hr>
<p>30</p>
<p>|      common      |   2   |   0   |   0   |   0   ||   2   |   0   |
31</p>
<hr>
<p>32</p>
<p>33</p>
<p>ivy-retrieve-common:</p>
<p>34</p>
<p>[ivy:retrieve] :: retrieving :: org.apache.hadoop</p>
<p>/#eclipse-plugin [sync]
35</p>
<p>[ivy:retrieve]  confs: [common]</p>
<p>36</p>
<p>[ivy:retrieve]  0 artifacts copied, 2 already retrieved (0kB/6ms)
37</p>
<p>[ivy:cachepath] DEPRECATED: </p>
<p>&#39;ivy.conf.file&#39;</p>
<p>is deprecated, use </p>
<p>&#39;ivy.settings.file&#39;</p>
<p>instead</p>
<p>38</p>
<p>[ivy:cachepath] :: loading settings :: </p>
<p>file</p>
<p>= /home/hep/hadoop/ivy/ivysettings.xml
39</p>
<p>40</p>
<p>compile:
41</p>
<p>[</p>
<p>echo</p>
<p>] contrib: eclipse-plugin</p>
<p>42</p>
<p>[javac] /home/hep/hadoop/src/contrib/eclipse-plugin/build.xml:61: warning: </p>
<p>&#39;includeantruntime&#39;</p>
<p>was not </p>
<p>set</p>
<p>, defaulting to build.sysclasspath=last; </p>
<p>set</p>
<p>to </p>
<p>false</p>
<p>for</p>
<p>repeatable builds
43</p>
<p>44</p>
<p>jar:
45</p>
<p>[</p>
<p>mkdir</p>
<p>] Created </p>
<p>dir</p>
<p>: /home/hep/hadoop/build/contrib/eclipse-plugin/lib</p>
<p>46</p>
<p>[copy] Copying 1 </p>
<p>file</p>
<p>to /home/hep/hadoop/build/contrib/eclipse-plugin/lib
47</p>
<p>[copy] Copying /home/hep/hadoop/hadoop-core-1.1.2.jar to /home/hep/hadoop/build/contrib/eclipse-plugin/lib/hadoop-core.jar</p>
<p>48</p>
<p>[copy] Copying 1 </p>
<p>file</p>
<p>to /home/hep/hadoop/build/contrib/eclipse-plugin/lib
49</p>
<p>[copy] Copying /home/hep/hadoop/lib/commons-cli-1.2.jar to /home/hep/hadoop/build/contrib/eclipse-plugin/lib/commons-cli-1.2.jar</p>
<p>50</p>
<p>[copy] Copying 1 </p>
<p>file</p>
<p>to /home/hep/hadoop/build/contrib/eclipse-plugin/lib
51</p>
<p>[copy] Copying /home/hep/hadoop/lib/commons-configuration-1.6.jar to /home/hep/hadoop/build/contrib/eclipse-plugin/lib/commons-configuration-1.6.jar</p>
<p>52</p>
<p>[copy] Copying 1 </p>
<p>file</p>
<p>to /home/hep/hadoop/build/contrib/eclipse-plugin/lib
53</p>
<p>[copy] Copying /home/hep/hadoop/lib/commons-httpclient-3.0.1.jar to /home/hep/hadoop/build/contrib/eclipse-plugin/lib/commons-httpclient-3.0.1.jar</p>
<p>54</p>
<p>[copy] Copying 1 </p>
<p>file</p>
<p>to /home/hep/hadoop/build/contrib/eclipse-plugin/lib
55</p>
<p>[copy] Copying /home/hep/hadoop/lib/commons-lang-2.4.jar to /home/hep/hadoop/build/contrib/eclipse-plugin/lib/commons-lang-2.4.jar</p>
<p>56</p>
<p>[copy] Copying 1 </p>
<p>file</p>
<p>to /home/hep/hadoop/build/contrib/eclipse-plugin/lib
57</p>
<p>[copy] Copying /home/hep/hadoop/lib/jackson-core-asl-1.8.8.jar to /home/hep/hadoop/build/contrib/eclipse-plugin/lib/jackson-core-asl-1.8.8.jar</p>
<p>58</p>
<p>[copy] Copying 1 </p>
<p>file</p>
<p>to /home/hep/hadoop/build/contrib/eclipse-plugin/lib
59</p>
<p>[copy] Copying /home/hep/hadoop/lib/jackson-mapper-asl-1.8.8.jar to /home/hep/hadoop/build/contrib/eclipse-plugin/lib/jackson-mapper-asl-1.8.8.jar</p>
<p>60</p>
<p>[jar] Building jar: /home/hep/hadoop/build/contrib/eclipse-plugin/hadoop-eclipse-plugin-1.1.2.jar
61</p>
<p>62</p>
<p>BUILD SUCCESSFUL
63</p>
<p>Total </p>
<p>time</p>
<p>: 3 seconds</p>
<p>生成的插件jar就在本目录中。</p>
<p>把编译好的插件放入 $eclipse_home/dropins/hadoop/plugins/ (mkdir -p 创建)</p>
<p>启动eclipse 新建mapreduce project， 配置好map/reduce location, 就可以看到hdfs中的文件了</p>

      
    </div>
    
    
      
    
    
        <footer id="post-meta">
        <span class="categories">Posted in<span class="breadcrumb fa fa-folder"><li><a href="/categories/hadoop/">hadoop</a></li></span></span> | <span class="tags">Tagged <a href="/tags/hadoop/" class="label label-primary">hadoop</a></span> | <span class="time">recent updated:<time title="2014-03-07 09:54:36"datetime="2014-03-07 09:54:36"> mar. 7 2014</time></span> | <span class="comment-link">
<a href="http://itsolife.com/2014/02/02/2014-02-02-hadoop--Hadoop121编译Eclipse插件/#comments" class="ds-thread-count comment-link" data-thread-key="2014-02-02-hadoop--Hadoop121编译Eclipse插件" data-count-type="comments">暂无评论</a></span>
        </footer>
    
    <div class="clearfix"></div>
  </div>
</div>
</article>



  <article>
<div id="post" class="post well">
  <div class="post-content">
    <header class="well-sm">
      <i class="fa icon fa-5x pull-left"></i>
      <h1 class="title"><a href="/2014/02/02/2014-02-02-linux-redhat--centoslibxml2/">centos libxml2</a></h1>
      
        <span>Posted on<time datetime="2014-02-02T01:54:36.000Z"> <a href="/2014/02/02/2014-02-02-linux-redhat--centoslibxml2/">feb. 2 2014</a></time></span>      
    </header>
    
    
    <div class="entry">
      
        <h1 id="centos-libxml2">centos libxml2</h1>
<h2 id="introduction-to-libx-ml2">Introduction to libx‍ml2</h2>
<p>The libxml2 package contains          libraries and utilities used for parsing XML files.</p>
<p>This package is known to build and work properly using an LFS-7.4          platform.</p>
<h3 id="package-information">Package Information</h3>
<ul>
<li>Download (HTTP): <a href="http://xmlsoft.org/sources/libxml2-2.9.1.tar.gz" target="_blank"><a href="http://xmlsoft.org/sources/libxml2-2.9.1.tar.gz">http://xmlsoft.org/sources/libxml2-2.9.1.tar.gz</a></a></li>
<li>Download (FTP): <a href="ftp://xmlsoft.org/libxml2/libxml2-2.9.1.tar.gz">ftp://xmlsoft.org/libxml2/libxml2-2.9.1.tar.gz</a></li>
<li>Download MD5 sum: 9c0cfef285d5c4a5c80d00904ddab380</li>
<li>Download size: 5.0 MB</li>
<li>Estimated disk space required: 100 MB</li>
<li>Estimated build time: 0.6 SBU</li>
</ul>
<h3 id="additional-downloads">Additional Downloads</h3>
<ul>
<li>Optional Testsuite: <a href="http://www.w3.org/XML/Test/xmlts20130923.tar.gz" target="_blank"><a href="http://www.w3.org/XML/Test/xmlts20130923.tar.gz">http://www.w3.org/XML/Test/xmlts20130923.tar.gz</a></a>                - This enables <strong>make                check</strong> to do complete testing.</li>
</ul>
<h3 id="libxml2-dependencies">libxml2 Dependencies</h3>
<h3 id="recommended">Recommended</h3>
<p><a href="http://www.linuxfromscratch.org/blfs/view/svn/general/python2.html" title="Python-2.7.6" target="_blank">Python-2.7.6</a> (to build and install a          Python library module,          additionally it is required to run the full suite of tests)</p>
<p><img src="http://www.linuxfromscratch.org/blfs/view/svn/images/note.png" alt="[Note]">          </p>
<h3 id="note">Note</h3>
<p>Some packages which utilize libxml2 (such as GNOME Doc Utils) need the Python module installed to function properly            and some packages (such as MesaLib) will not build properly if            the Python module is not            available.</p>
<p>User Notes: <a href="http://wiki.linuxfromscratch.org/blfs/wiki/libxml2" target="_blank"><a href="http://wiki.linuxfromscratch.org/blfs/wiki/libxml2">http://wiki.linuxfromscratch.org/blfs/wiki/libxml2</a></a>        </p>
<h2 id="installation-of-libxml2">Installation of libxml2</h2>
<p>If you downloaded the testsuite, issue the following command:
tar xf ../xmlts20130923.tar.gz</p>
<p>Install libxml2 by running the          following commands:
./configure --prefix=/usr --disable-static --with-history &amp;&amp; make</p>
<p>To test the results, issue: <strong>make          check</strong>.</p>
<p>Now, as the</p>
<p>root
user:
make install</p>
<h2 id="command-explanations">Command Explanations</h2>
<p><em>
--disable-static
</em>: This          switch prevents installation of static versions of the libraries.</p>
<p>--with-history
: This switch enables          Readline support when running          <strong>xmlcatalog</strong> or          <strong>xmllint</strong> in shell          mode.</p>
<h2 id="contents">Contents</h2>
<p><strong>Installed Programs:</strong>              xml2-config, xmlcatalog and              xmllint            </p>
<p><strong>Installed Libraries:</strong>              libxml2.so and optionally, the              libxml2mod.so Python              module            </p>
<p><strong>Installed Directories:</strong>              /usr/include/libxml2,              /usr/share/doc/libxml2-2.9.1,              /usr/share/doc/libxml2-python-2.9.1 and              /usr/share/gtk-doc/html/libxml2            </p>
<h3 id="short-descriptions">Short Descriptions</h3>
<p><a href=""></a><strong>xml2-config</strong>                  </p>
<p>determines the compile and linker flags that should be                    used to compile and link programs that use</p>
<p>libxml2
.<a href=""></a><strong>xmlcatalog</strong>                  </p>
<p>is used to monitor and manipulate XML and SGML catalogs.<a href=""></a><strong>xmllint</strong>                  </p>
<p>parses XML files and outputs reports (based upon options)                    to detect errors in XML coding.<a href=""></a></p>
<p>libxml2.so</p>
<p>provides functions for programs to parse files that use                    the XML format.                   </p>

      
    </div>
    
    
      
    
    
        <footer id="post-meta">
        <span class="categories">Posted in<span class="breadcrumb fa fa-folder"><li><a href="/categories/linux/">linux</a></li></span><span class="breadcrumb"><li><a href="/categories/linux/">linux</a></li><li><a href="/categories/linux/redhat/">redhat</a></li></span></span> | <span class="tags">Tagged <a href="/tags/linux/" class="label label-primary">linux</a><a href="/tags/redhat/" class="label label-success">redhat</a></span> | <span class="time">recent updated:<time title="2014-03-07 09:54:36"datetime="2014-03-07 09:54:36"> mar. 7 2014</time></span> | <span class="comment-link">
<a href="http://itsolife.com/2014/02/02/2014-02-02-linux-redhat--centoslibxml2/#comments" class="ds-thread-count comment-link" data-thread-key="2014-02-02-linux-redhat--centoslibxml2" data-count-type="comments">暂无评论</a></span>
        </footer>
    
    <div class="clearfix"></div>
  </div>
</div>
</article>





<ul id="pagination" class="pagination pagination-lg">
  <li><a class="extend prev" href="/page/110/">&laquo;</a></li><li><a class="page-number" href="/">1</a></li><li><a class="page-number" href="/page/2/">2</a></li><li><span class="space">&hellip;</span></li><li><a class="page-number" href="/page/108/">108</a></li><li><a class="page-number" href="/page/109/">109</a></li><li><a class="page-number" href="/page/110/">110</a></li><li class="active"><li><span class="page-number current">111</span></li><li><a class="page-number" href="/page/112/">112</a></li><li><a class="page-number" href="/page/113/">113</a></li><li><a class="page-number" href="/page/114/">114</a></li><li><span class="space">&hellip;</span></li></li><li><a class="page-number" href="/page/161/">161</a></li><li><a class="page-number" href="/page/162/">162</a></li><li><a class="extend next" href="/page/112/">&raquo;</a></li>
  <div class="clearfix"></div>
</ul></div><!--wapper-->
       </div><!-- ID main-col END -->
       <aside id="sidebar" class="alignright col-sx-6 col-sm-4 col-md-3 col-lg-3">
<div id="widget_search" class="widget panel panel-primary">
    <form action="//google.com/search" method="get" accept-charset="utf-8">
  <div class="input-group">
    <input class="form-control" id="searchbox" type="search" name="q" results="0" placeholder="search">
    <span class="input-group-btn">
      <button class="btn btn-default" type="submit">Go!</button>
    </span>
    <input type="hidden" name="q" value="site:itsolife.com">
  </div>
</form>
</div>

<div id="widget_category" class="widget panel panel-primary">
  <div class="panel-heading">category</div>  <div data-src='category' class='ajax_widgets'>正在加载...</div>
</div>

<div id="widget_recent_posts" class="widget panel panel-primary">
  <div class="panel-heading">recent posts</div>  <div data-src='recent_posts' class='ajax_widgets'>正在加载...</div></div>

<div id="widget_tagcloud" class="widget panel panel-primary">
  <div class="panel-heading">tagcloud</div>  <div data-src='tagcloud' class='ajax_widgets'>正在加载...</div></div>

<div id="widget_latest_update_posts" class="widget panel panel-primary">
  <div class="panel-heading">最近更新</div>  <div data-src='latest_update_posts' class='ajax_widgets'>正在加载...</div></div>

<div id="widget_recent_comments" class="widget panel panel-primary">
  <div class="panel-heading">recent comments</div>  

<div class="list-group-item ds-recent-comments" data-num-items="6" data-show-avatars="1" data-show-time="1" data-show-title="1" data-show-admin="1" data-excerpt-length="50"></div>



</div>

</aside>
       <div class="clearfix"></div>
     </div><!-- row END -->
  </div>
  <footer id="footer" class="container">
  <div class="panel panel-info">
  <section id='footer_widget'></section>  <div class="panel-footer">
  <div id="site-info">
    <span class='author'>
  
  &copy; 2014 RobinChia
  
    &nbsp;&nbsp;</span>
  
  <span id='analytics-51la'></span><span id='analytics-google'>
<script type="text/javascript">
  var _gaq = _gaq || [];
  _gaq.push(['_setAccount', 'UA-48559895-1']);
  _gaq.push(['_trackPageview']);
  _js2load.push({src:('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js'});
</script></span><span id='analytics-cnzz'>
<script type="text/javascript">var cnzz_protocol = (("https:" == document.location.protocol) ? " https://" : " http://");document.write(unescape("%3Cspan id='cnzz_stat_icon_5774006'%3E%3C/span%3E%3Cscript src='" + cnzz_protocol + "s17.cnzz.com/stat.php%3Fid%3D5774006%26show%3Dpic' type='text/javascript'%3E%3C/script%3E"));</script>
</span><span id='analytics-baidu'>
<script>
var _hmt = _hmt || [];
_js2load.push({src:"//hm.baidu.com/hm.js?1442f50724afc42380b51f097c43082c"});
</script>
</span>  </div>
  <div id="copyright">Site powered by <a href='http://zespia.tw/hexo/'><strong>hexo</strong></a>  update time: <em>2014-04-07 17:30:37</em></span></div>
</div>
<div class="clearfix"></div>


  </div>
  </footer>
  
        <script src="http://cdn.bootcss.com/jquery/1.10.2/jquery.min.js"></script>        
        <script src="http://cdn.staticfile.org/twitter-bootstrap/3.1.0/js/bootstrap.min.js"></script>        
                
        <script src="http://cdn.bootcss.com/prettify/r298/prettify.min.js"></script>    <script type="text/javascript">
   var lang=["bsh", "c", "cc", "cpp", "cs", "csh", "cyc", "cv", "htm", "html",
    "java", "js", "m", "mxml", "perl", "pl", "pm", "py", "rb", "sh",
    "xhtml", "xml", "xsl"];
   var pretty_base='';
   $('script').each(function(){
	var c = $(this).attr('src');
	if (!c)
	    return;
	if (c.match(/(\/)?prettify(\.min)?\.js/i))
	{
	    var index = c.lastIndexOf('/');
	    if (index != -1)
		pretty_base = c.substr(0,index + 1);
	    return false;
	}
   })
   $('pre code').each(function(){
	var c = $(this).attr('class')
	if (!c)
	    return;
	c = c.match(/\s?(lang\-\w+)/i);
	if (c && lang.indexOf(c[1]) == -1)
	{
	    lang.push(c[1]);
	    $.getScript(pretty_base + c[1] + '.min.js');
	}
   })

    $(window).load(function(){
       $("pre").addClass("prettyprint");
       prettyPrint();
    })
</script>    
            <script type="text/javascript">
var duoshuoQuery = {short_name:"robinchia"};
_js2load.push({src:'http://static.duoshuo.com/embed.js',charset:'UTF-8'});
</script>
    
            <!--wumii_relatedItems-->
    
        <script src="http://cdn.bootcss.com/fancybox/2.1.5/jquery.fancybox.min.js"></script>    <script type="text/javascript">
(function($){
  $('.entry').each(function(i){
    $(this).find('img').each(function(){
      var alt = this.alt;

      if (alt){
        $(this).before('<span class="caption">' + alt + '</span>');
      }

      $(this).wrap('<a href="' + this.src + '" title="' + alt + '" class="fancybox" rel="fancybox' + i + '" />');
    });
  });

  $('.fancybox').fancybox();
})(jQuery);
</script>    
        <script src="http://cdn.bootcss.com/mathjax/2.3/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>    <script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      processEscapes: true
    }
  });
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>
    
<script type="text/javascript">
$('.ajax_widgets').each(function(){var src=$(this).attr('data-src');if(src)$(this).load('/widgets/'+src+'.html');});
$.each(_js2load,function(index,obj){loadjs(obj.src,obj.charset)});
</script>

<div id="scroll2top">
<img src="/scroll2top/arrow.png"/>
</div>
<script src="/scroll2top/scroll2top.min.js"></script>
<div id="winterland">
  <canvas></canvas>
</div>
<script src="/js/winterland.min.js"></script>

  </body>
</html>
